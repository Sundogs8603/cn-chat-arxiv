<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12968</link><description>&lt;p&gt;
LLMLingua-2: &#39640;&#25928;&#19988;&#24544;&#23454;&#30340;&#26080;&#20219;&#21153;Prompt&#21387;&#32553;&#30340;&#25968;&#25454;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#26080;&#20219;&#21153;&#30340;Prompt&#21387;&#32553;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#20174;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMa-7B&#65289;&#33719;&#24471;&#30340;&#20449;&#24687;&#29109;&#26469;&#21024;&#38500;token&#25110;&#35789;&#27719;&#21333;&#20301;&#26469;&#21387;&#32553;prompt&#12290;&#25361;&#25112;&#22312;&#20110;&#20449;&#24687;&#29109;&#21487;&#33021;&#26159;&#19968;&#20010;&#27425;&#20248;&#30340;&#21387;&#32553;&#24230;&#37327;&#65306;(i)&#23427;&#20165;&#21033;&#29992;&#21333;&#21521;&#19978;&#19979;&#25991;&#65292;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#25152;&#26377;&#29992;&#20110;prompt&#21387;&#32553;&#30340;&#20851;&#38190;&#20449;&#24687;&#65307;(ii)&#23427;&#19982;prompt&#21387;&#32553;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#36807;&#31243;&#65292;&#20174;LLM&#20013;&#33719;&#24471;&#30693;&#35782;&#20197;&#21387;&#32553;prompt&#32780;&#19981;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25277;&#21462;&#24335;&#25991;&#26412;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;prompt&#21387;&#32553;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;token&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;prompt&#19982;&#21407;&#22987;prompt&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12968v1 Announce Type: new  Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and 
&lt;/p&gt;</description></item><item><title>TexTile&#26159;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32441;&#29702;&#22270;&#20687;&#30340;&#24179;&#38138;&#24615;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#26126;&#26234;&#22320;&#21512;&#25104;&#21644;&#20998;&#26512;&#24179;&#38138;&#32441;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.12961</link><description>&lt;p&gt;
TexTile&#65306;&#19968;&#31181;&#21487;&#24494;&#30340;&#32441;&#29702;&#24179;&#38138;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
TexTile: A Differentiable Metric for Texture Tileability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12961
&lt;/p&gt;
&lt;p&gt;
TexTile&#26159;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32441;&#29702;&#22270;&#20687;&#30340;&#24179;&#38138;&#24615;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#26126;&#26234;&#22320;&#21512;&#25104;&#21644;&#20998;&#26512;&#24179;&#38138;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;TexTile&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#24230;&#37327;&#26041;&#24335;&#65292;&#29992;&#20110;&#37327;&#21270;&#32441;&#29702;&#22270;&#20687;&#21487;&#20197;&#22914;&#20309;&#19982;&#33258;&#36523;&#36830;&#25509;&#32780;&#19981;&#24341;&#20837;&#37325;&#22797;&#20266;&#24433;&#65288;&#21363;&#24179;&#38138;&#24615;&#65289;&#12290;&#29616;&#26377;&#30340;&#21487;&#24179;&#38138;&#32441;&#29702;&#21512;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#19968;&#33324;&#32441;&#29702;&#36136;&#37327;&#65292;&#20294;&#32570;&#20047;&#23545;&#32441;&#29702;&#22266;&#26377;&#21487;&#37325;&#22797;&#24615;&#23646;&#24615;&#30340;&#26174;&#24335;&#20998;&#26512;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;TexTile&#24230;&#37327;&#26377;&#25928;&#35780;&#20272;&#32441;&#29702;&#30340;&#21487;&#24179;&#38138;&#24615;&#65292;&#20026;&#26356;&#26126;&#26234;&#30340;&#24179;&#38138;&#32441;&#29702;&#21512;&#25104;&#21644;&#20998;&#26512;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#22312;&#32972;&#21518;&#65292;TexTile&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20180;&#32454;&#26500;&#24314;&#33258;&#19981;&#21516;&#39118;&#26684;&#12289;&#35821;&#20041;&#12289;&#35268;&#24459;&#21644;&#20154;&#31867;&#27880;&#37322;&#30340;&#22823;&#22411;&#32441;&#29702;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31995;&#21015;&#26550;&#26500;&#20462;&#25913;&#65292;&#20351;&#22522;&#32447;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#33021;&#22815;&#20811;&#26381;&#20854;&#22312;&#34913;&#37327;&#24179;&#38138;&#24615;&#26041;&#38754;&#30340;&#32570;&#38519;&#65292;&#20197;&#21450;&#38024;&#23545;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#22686;&#24378;&#21644;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12961v1 Announce Type: cross  Abstract: We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing rob
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#19990;&#30028;&#22352;&#26631;&#31995;&#30340;&#20016;&#23500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.12959</link><description>&lt;p&gt;
WHAC: &#19990;&#30028;&#22522;&#20934;&#20154;&#31867;&#19982;&#25668;&#20687;&#26426;
&lt;/p&gt;
&lt;p&gt;
WHAC: World-grounded Humans and Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#19990;&#30028;&#22352;&#26631;&#31995;&#30340;&#20016;&#23500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#20934;&#30830;&#20272;&#35745;&#19990;&#30028;&#22352;&#26631;&#31995;&#20013;&#20855;&#26377;&#27604;&#20363;&#30340;&#20154;&#31867;&#21644;&#25668;&#20687;&#26426;&#36712;&#36857;&#26159;&#19968;&#39033;&#38750;&#24120;&#29702;&#24819;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#27169;&#31946;&#23450;&#20041;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#12289;&#20154;&#31867;&#21644;&#25668;&#20687;&#26426;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#32852;&#21512;&#24674;&#22797;&#34920;&#29616;&#21147;&#24378;&#30340;&#21442;&#25968;&#21270;&#20154;&#20307;&#27169;&#22411;&#65288;&#21363;SMPL-X&#65289;&#21644;&#30456;&#24212;&#30340;&#25668;&#20687;&#26426;&#23039;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#30456;&#26426;&#26694;&#26550;&#19979;&#30340;SMPL-X&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24674;&#22797;&#32477;&#23545;&#20154;&#31867;&#28145;&#24230;&#12290;&#20854;&#27425;&#65292;&#20154;&#20307;&#21160;&#20316;&#26412;&#36136;&#19978;&#25552;&#20379;&#32477;&#23545;&#31354;&#38388;&#32447;&#32034;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;WHAC&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#22522;&#20110;&#19990;&#30028;&#30340;&#34920;&#29616;&#21147;&#24378;&#30340;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#65288;EHPS&#65289;&#65292;&#21516;&#26102;&#36827;&#34892;&#25668;&#20687;&#26426;&#23039;&#21183;&#20272;&#35745;&#65292;&#26080;&#38656;&#20381;&#36182;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;WHAC-A-Mole&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#31934;&#30830;&#27880;&#37322;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12959v1 Announce Type: cross  Abstract: Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated h
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;</title><link>https://arxiv.org/abs/2403.12952</link><description>&lt;p&gt;
&#21482;&#38656;&#36716;&#31227;&#23427;&#65306;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12952
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#22240;&#20026;&#39046;&#22495;&#36716;&#31227;&#32780;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20351;&#29992;&#26631;&#35760;&#27979;&#35797;&#36755;&#20837;&#26469;&#20351;VLM&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#35843;&#33410;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#24182;&#32531;&#23384;&#21407;&#22411;&#65292;TPS&#19981;&#20165;&#20419;&#36827;&#20102;&#26080;&#38656;&#20248;&#21270;&#30340;&#21407;&#22411;&#37325;&#29992;&#36827;&#34892;&#21518;&#32493;&#39044;&#27979;&#65292;&#36824;&#35753;&#20854;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#24403;&#21069;&#36827;&#23637;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;TPS&#20165;&#22522;&#20110;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12952v1 Announce Type: cross  Abstract: Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing class
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#39318;&#20010;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20248;&#21270;&#19988;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#22312;Condorcet&#21644;Borda&#20043;&#38388;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.12950</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#38750;&#24179;&#31283;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#22312;&#24191;&#20041;&#27874;&#36798;&#20934;&#21017;&#19979;
&lt;/p&gt;
&lt;p&gt;
Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#39318;&#20010;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20248;&#21270;&#19988;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#22312;Condorcet&#21644;Borda&#20043;&#38388;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#65292;&#23398;&#20064;&#32773;&#25509;&#25910;&#33218;&#20043;&#38388;&#30340;&#20559;&#22909;&#21453;&#39304;&#65292;&#24182;&#23558;&#26576;&#20010;&#33218;&#30340;&#36951;&#25022;&#23450;&#20041;&#20026;&#20854;&#30456;&#23545;&#20110;&#20248;&#32988;&#33218;&#30340;&#27425;&#20248;&#24615;&#12290;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#36341;&#21160;&#26426;&#30340;&#38750;&#24179;&#31283;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#21464;&#20307;&#20013;&#65292;&#20559;&#22909;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#36817;&#26399;&#22810;&#39033;&#24037;&#20316;&#30340;&#28966;&#28857;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#31639;&#27861;&#65292;&#32780;&#26080;&#38656;&#25552;&#21069;&#20102;&#35299;&#21464;&#21270;&#37327;&#12290;&#24050;&#30693;&#32467;&#26524;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20102;&#23380;&#22810;&#22622;&#20248;&#32988;&#32773;&#35774;&#32622;&#65292;&#20854;&#20013;&#20248;&#20808;&#20110;&#20854;&#20182;&#20219;&#20309;&#33218;&#30340;&#33218;&#22312;&#20219;&#20309;&#26102;&#20505;&#37117;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20248;&#32988;&#32773;&#21487;&#33021;&#24182;&#19981;&#23384;&#22312;&#65292;&#20026;&#20102;&#23545;&#27604;&#65292;&#27492;&#38382;&#39064;&#30340;&#27874;&#36798;&#29256;&#26412;&#65288;&#22987;&#32456;&#26377;&#26126;&#30830;&#23450;&#20041;&#65289;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#26368;&#20248;&#21644;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#31361;&#26174;&#20102;&#22312;&#23380;&#22810;&#22622;&#21644;&#27874;&#36798;&#20043;&#38388;&#30340;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12950v1 Announce Type: new  Abstract: In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;Real-\b{eta}-SafeOpt&#31639;&#27861;&#65292;&#26377;&#25928;&#20445;&#30041;&#20102;&#25152;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.12948</link><description>&lt;p&gt;
&#20851;&#20110;&#23433;&#20840;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Safety in Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;Real-\b{eta}-SafeOpt&#31639;&#27861;&#65292;&#26377;&#25928;&#20445;&#30041;&#20102;&#25152;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#26159;&#26426;&#22120;&#20154;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#21644;&#35768;&#22810;&#20854;&#20182;&#23398;&#31185;&#20013;&#30340;&#20013;&#24515;&#20219;&#21153;&#65292;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;(Safe Bayesian Optimization, BO)&#22312;&#36825;&#26041;&#38754;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#12290;&#30001;&#20110;&#36825;&#20123;&#24212;&#29992;&#30340;&#23433;&#20840;&#20851;&#38190;&#24615;&#36136;&#65292;&#20445;&#35777;&#36825;&#20123;&#31639;&#27861;&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#33021;&#33021;&#22815;&#26144;&#23556;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#31867;&#21035;SafeOpt&#31867;&#22411;&#31639;&#27861;&#30340;&#19977;&#20010;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243; (Gaussian Process, GP) &#22238;&#24402;&#30340;&#39057;&#29575;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#65292;&#20294;&#20855;&#20307;&#23454;&#29616;&#36890;&#24120;&#20351;&#29992;&#20351;&#25152;&#26377;&#23433;&#20840;&#20445;&#35777;&#26080;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;Real-\b{eta}-SafeOpt&#65292;&#36825;&#26159;SafeOpt&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;GP&#19978;&#30028;&#65292;&#22240;&#27492;&#20445;&#30041;&#20102;&#25152;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#22797;&#21046;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12948v1 Announce Type: new  Abstract: Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#24182;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#33021;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12946</link><description>&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31163;&#32447;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#24182;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#33021;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#32570;&#20047;&#31215;&#26497;&#25506;&#32034;&#38656;&#35201;&#20851;&#27880;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#35299;&#20915;&#27169;&#25311;&#21644;&#37096;&#32626;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20854;&#20013;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20197;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#36171;&#20104;&#23398;&#20064;&#31574;&#30053;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#36890;&#36807;&#24635;&#21464;&#24046;&#36317;&#31163;&#34920;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20998;&#24067;&#40065;&#26834;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#65292;&#24182;&#22312;&#26368;&#23567;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#20854;&#24615;&#33021;&#33267;&#23569;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#20248;&#20110;$\tilde{O}(d)$&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#34920;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12938</link><description>&lt;p&gt;
&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Differential Algebraic Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#34920;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;DAEs&#65289;&#25551;&#36848;&#20102;&#31526;&#21512;&#24494;&#20998;&#21644;&#20195;&#25968;&#32422;&#26463;&#30340;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#26159;&#21253;&#21547;&#20854;&#32452;&#20214;&#20043;&#38388;&#38544;&#24615;&#20851;&#31995;&#65288;&#22914;&#23432;&#24658;&#20851;&#31995;&#65289;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#36866;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#30340;DAE&#24314;&#27169;&#30340;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#12290;&#36825;&#19968;&#26041;&#27861;&#24314;&#31435;&#22312;&#36890;&#29992;&#24494;&#20998;&#26041;&#31243;&#30340;&#27010;&#24565;&#20043;&#19978;&#65307;&#21363;&#65292;&#26500;&#24314;&#20026;&#21463;&#29305;&#23450;&#31185;&#23398;&#39046;&#22495;&#29702;&#35770;&#25903;&#25345;&#30340;&#19968;&#32452;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;NDAEs&#25277;&#35937;&#36866;&#29992;&#20110;&#30456;&#20851;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#25152;&#31034;&#31034;&#20363;&#21253;&#25324;&#65288;i&#65289;&#27833;&#31665;&#27969;&#24418;&#21160;&#24577;&#30340;&#36870;&#38382;&#39064;&#21644;&#65288;ii&#65289;&#27893;&#12289;&#27833;&#31665;&#21644;&#31649;&#36947;&#32593;&#32476;&#30340;&#24046;&#24322;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#23545;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12938v1 Announce Type: new  Abstract: Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12918</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#27867;&#21270;&#21644;&#31283;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;PLMs&#20250;&#38754;&#20020;&#35832;&#22914;&#19981;&#31283;&#23450;&#24615;&#21644;&#36807;&#25311;&#21512;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#31574;&#30053;&#36873;&#25321;&#30340;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#26435;&#37325;&#22266;&#23450;&#20026;&#39044;&#35757;&#32451;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#23376;&#32593;&#32476;&#36873;&#25321;&#26631;&#20934;&#65292;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26435;&#37325;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#24494;&#35843;PLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#32593;&#32476;&#26435;&#37325;&#34920;&#31034;&#20026;&#20219;&#21153;&#29305;&#23450;&#26435;&#37325;&#21644;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#28151;&#21512;&#65292;&#30001;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#21442;&#25968;&#25511;&#21046;&#65292;&#25552;&#20379;&#23545;&#23376;&#32593;&#32476;&#36873;&#25321;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#21333;&#29420;&#25286;&#20998;&#19978;&#20351;&#29992;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#30340;&#26694;&#26550;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12918v1 Announce Type: cross  Abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving ge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35821;&#35328;&#32416;&#27491;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#38271;&#20037;&#20219;&#21153;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12910</link><description>&lt;p&gt;
&#23545;&#24744;&#30340;&#26426;&#22120;&#20154;&#22823;&#21898;&#65306;&#20174;&#35821;&#35328;&#32416;&#27491;&#20013;&#23454;&#26102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Yell At Your Robot: Improving On-the-Fly from Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35821;&#35328;&#32416;&#27491;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#38271;&#20037;&#20219;&#21153;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#35821;&#35328;&#21644;&#20302;&#32423;&#25511;&#21046;&#30340;&#20998;&#23618;&#31574;&#30053;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38271;&#35270;&#37326;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LLMs/VLMs&#65289;&#25110;&#22312;&#27880;&#37322;&#30340;&#26426;&#22120;&#20154;&#28436;&#31034;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#22797;&#26434;&#21644;&#28789;&#24039;&#30340;&#25216;&#33021;&#65292;&#23454;&#29616;&#22312;&#38271;&#35270;&#37326;&#20219;&#21153;&#19978;&#39640;&#25104;&#21151;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#8212;&#8212;&#20219;&#21153;&#36234;&#38271;&#65292;&#26576;&#20010;&#38454;&#27573;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#30452;&#35266;&#33258;&#28982;&#30340;&#21453;&#39304;&#24110;&#21161;&#26426;&#22120;&#20154;&#25345;&#32493;&#25913;&#36827;&#20854;&#38271;&#35270;&#37326;&#20219;&#21153;&#24615;&#33021;&#21527;&#65311;&#26412;&#25991;&#20013;&#25105;&#20204;&#21457;&#29616;&#65306;&#21487;&#20197;&#36890;&#36807;&#23500;&#21547;&#34920;&#36798;&#21147;&#30340;&#20302;&#32423;&#35821;&#35328;&#26465;&#20214;&#25216;&#33021;&#32034;&#24341;&#21040;&#39640;&#27700;&#24179;&#31574;&#30053;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#35821;&#35328;&#32416;&#27491;&#30340;&#24418;&#24335;&#36827;&#34892;&#20154;&#31867;&#30417;&#30563;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#29978;&#33267;&#31934;&#32454;&#30340;&#32416;&#27491;&#65292;&#22914;&#23567;&#21160;&#20316;&#65288;&#8220;&#31227;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12910v1 Announce Type: cross  Abstract: Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements ("move a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.12900</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#25345;&#32493;&#30340;GenAI&#65306;&#20351;&#29992;&#29983;&#25104;&#25351;&#20196;&#23454;&#29616;&#30899;&#21451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25351;&#20196;&#30340;&#27010;&#24565;&#65292;&#24179;&#34913;&#20102;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#26381;&#21153;&#30899;&#25490;&#25918;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#24341;&#36215;&#20102;&#29615;&#22659;&#26041;&#38754;&#30340;&#37325;&#35201;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#30899;&#25490;&#25918;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sprout&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#26381;&#21153;&#30340;&#30899;&#36275;&#36857;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;Sprout&#21033;&#29992;&#21019;&#26032;&#27010;&#24565;&#8220;&#29983;&#25104;&#25351;&#20196;&#8221;&#26469;&#24341;&#23548;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#24378;&#30899;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31934;&#24515;&#24179;&#34913;&#20102;&#23545;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#21644;&#39640;&#36136;&#37327;&#29983;&#25104;&#25104;&#26524;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#25351;&#20196;&#20248;&#21270;&#22120;&#26469;&#23545;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#29983;&#25104;&#25351;&#20196;&#30340;&#25112;&#30053;&#20998;&#37197;&#21644;&#19968;&#20010;&#21407;&#21019;&#30340;&#31163;&#32447;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;Sprout&#22312;&#23454;&#38469;&#35780;&#20272;&#20013;&#26174;&#33879;&#20943;&#23569;&#20102;40%&#20197;&#19978;&#30340;&#30899;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12900v1 Announce Type: cross  Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of "generation directives" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations u
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31616;&#21333;&#20248;&#21270;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12887</link><description>&lt;p&gt;
&#29702;&#35299;&#20855;&#26377;&#26465;&#20214;&#26368;&#20248;&#36816;&#36755;&#30340;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;ResNets&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12887
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31616;&#21333;&#20248;&#21270;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#24615;&#12290; &#22914;&#26524;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#26159;&#38750;&#24120;&#28145;&#30340;&#26550;&#26500;&#30340;&#19968;&#20010;&#24120;&#35265;&#20363;&#23376;&#65292;&#37027;&#20040;&#30001;&#20110;&#30446;&#26631;&#30340;&#38750;&#20984;&#24615;&#21644;&#38750;&#24378;&#20984;&#24615;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#26500;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290; Yet, &#22312;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#35832;&#22914;&#26799;&#24230;&#19979;&#38477;&#31561;&#31616;&#21333;&#30340;&#20248;&#21270;&#31639;&#27861;&#25104;&#21151;&#35299;&#20915;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#19968;&#20010;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#30001;&#23618;&#21644;&#21442;&#25968;&#30340;&#20056;&#31215;&#38598;&#19978;&#30340;&#27010;&#29575;&#27979;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#22312;&#23618;&#38598;&#19978;&#20855;&#26377;&#24120;&#25968;&#36793;&#38469;&#12290; &#23454;&#38469;&#19978;&#65292;&#22312;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22343;&#22330;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#27010;&#29575;&#27979;&#24230;&#38598;&#19978;&#30340;Wasserstein&#24230;&#37327;&#26102;&#21463;&#30410;&#20110;&#31616;&#21270;&#30340;&#25439;&#22833;&#26223;&#35266;&#21644;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290; &#21463;&#36825;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12887v1 Announce Type: new  Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22825;&#31354;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#22122;&#22768;&#36755;&#20837;&#25913;&#21892;&#22826;&#38451;&#36752;&#29031;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#33410;&#32422;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12873</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#20256;&#36755;&#32422;&#26463;&#30340;&#30701;&#26399;&#22826;&#38451;&#36752;&#29031;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12873
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22825;&#31354;&#25668;&#20687;&#22836;&#22270;&#20687;&#21644;&#22122;&#22768;&#36755;&#20837;&#25913;&#21892;&#22826;&#38451;&#36752;&#29031;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#33410;&#32422;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#29992;&#20110;&#30701;&#26399;&#22826;&#38451;&#36752;&#29031;&#39044;&#27979;&#30340;&#25968;&#25454;&#33410;&#32422;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20837;&#21253;&#25324;&#22825;&#31354;&#25668;&#20687;&#22836;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#32463;&#36807;&#22788;&#29702;&#20197;&#28385;&#36275;&#25968;&#25454;&#20256;&#36755;&#32422;&#26463;&#12290;&#36755;&#20986;&#30340;&#36752;&#29031;&#20540;&#32463;&#36807;&#36716;&#25442;&#65292;&#20197;&#20851;&#27880;&#26410;&#30693;&#30340;&#30701;&#26399;&#21160;&#24577;&#12290;&#21463;&#25511;&#21046;&#29702;&#35770;&#21551;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;&#22122;&#22768;&#36755;&#20837;&#26469;&#21453;&#26144;&#26410;&#27979;&#37327;&#30340;&#21464;&#37327;&#65292;&#24182;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#26469;&#33258;NREL&#22826;&#38451;&#36752;&#23556;&#30740;&#31350;&#23454;&#39564;&#23460;&#20116;&#24180;&#30340;&#25968;&#25454;&#21019;&#24314;&#20102;&#19977;&#32452;&#28378;&#21160;&#35757;&#32451;&#39564;&#35777;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#26102;&#38388;&#30340;&#26368;&#20339;&#34920;&#31034;&#24418;&#24335;&#12289;&#36755;&#20837;&#27979;&#37327;&#30340;&#26368;&#20339;&#36328;&#24230;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#27169;&#22411;&#36755;&#20837;&#25968;&#25454;&#65288;&#29305;&#24449;&#65289;&#12290;&#23545;&#20110;&#25152;&#36873;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#19982;&#20351;&#29992;&#20113;&#37327;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#24615;&#27604;&#23545;&#65292;&#21462;&#24471;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;74.34 $W/m^2$ &#30340;&#25104;&#32489;&#65292;&#32780;&#22522;&#20934;&#20026;134.35 $W/m^2$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12873v1 Announce Type: new  Abstract: We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance. The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints. The output irradiance values are transformed to focus on unknown short-term dynamics. Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably. Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features). For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20248;&#21270;&#20102;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#65292;&#25972;&#21512;&#20102;CNNs&#21644;&#21152;&#25343;&#22823;&#28779;&#28798;&#22825;&#27668;&#25351;&#25968;&#65292;&#25104;&#21151;&#24314;&#31435;&#20102;&#19968;&#31181;&#35745;&#31639;&#37326;&#28779;&#39118;&#38505;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35782;&#21035;&#28903;&#27585;&#21306;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;95%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12871</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20248;&#21270;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Wildfire danger prediction optimization with transfer learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20248;&#21270;&#20102;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#65292;&#25972;&#21512;&#20102;CNNs&#21644;&#21152;&#25343;&#22823;&#28779;&#28798;&#22825;&#27668;&#25351;&#25968;&#65292;&#25104;&#21151;&#24314;&#31435;&#20102;&#19968;&#31181;&#35745;&#31639;&#37326;&#28779;&#39118;&#38505;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35782;&#21035;&#28903;&#27585;&#21306;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;95%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#25512;&#21160;&#20102;&#30446;&#26631;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;CNNs&#24212;&#29992;&#20110;&#20998;&#26512;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35782;&#21035;&#21463;&#37326;&#28779;&#24433;&#21709;&#30340;&#21306;&#22495;&#12290;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;CNN&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#38598;&#25104;&#20102;&#21152;&#25343;&#22823;&#28779;&#28798;&#22825;&#27668;&#25351;&#25968;&#65288;FWI&#65289;&#26469;&#35780;&#20272;&#27700;&#20998;&#26465;&#20214;&#12290;&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20174;0&#21040;5&#30340;&#37326;&#28779;&#39118;&#38505;&#32423;&#21035;&#65292;&#21160;&#24577;&#22320;&#19982;&#27668;&#35937;&#27169;&#24335;&#32852;&#31995;&#36215;&#26469;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#25972;&#21512;&#36801;&#31227;&#23398;&#20064;&#65292;CNN&#27169;&#22411;&#22312;&#35782;&#21035;&#28903;&#27585;&#21306;&#22495;&#26041;&#38754;&#36798;&#21040;&#20102;95%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;CNN&#30340;&#20869;&#22312;&#24037;&#20316;&#21407;&#29702;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#39044;&#27979;&#21644;&#20943;&#36731;&#37326;&#28779;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;CNN&#65292;&#26412;&#30740;&#31350;&#20026;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#36129;&#29486;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12871v1 Announce Type: new  Abstract: Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection. This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\% in identifying burnt areas. This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires. By combining transfer learning and CNNs, this study contributes a robust appr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12864</link><description>&lt;p&gt;
&#22826;&#31354;&#33337;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#33337;&#25805;&#20316;&#20855;&#26377;&#26497;&#39640;&#30340;&#20851;&#38190;&#24615;&#65292;&#35201;&#27714;&#20855;&#26377;&#26080;&#21487;&#25361;&#21076;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#30830;&#20445;&#22826;&#31354;&#33337;&#30340;&#26368;&#20339;&#24615;&#33021;&#38656;&#35201;&#21450;&#26089;&#26816;&#27979;&#21644;&#20943;&#36731;&#24322;&#24120;&#24773;&#20917;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#37096;&#20214;&#25110;&#20219;&#21153;&#22833;&#36133;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#31639;&#27861;&#22312;&#31354;&#38388;&#25805;&#20316;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#34920;&#29616;&#20986;&#20102;&#36739;&#22823;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27491;&#22312;&#30740;&#31350;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#12289;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#27599;&#19968;&#20010;&#37117;&#26159;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#22826;&#31354;&#33337;&#20219;&#21153;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#30340;&#65292;&#21253;&#25324;&#21508;&#31181;&#36816;&#34892;&#22330;&#26223;&#21644;&#24322;&#24120;&#31867;&#22411;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12864v1 Announce Type: new  Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-Cubed&#30340;&#26032;&#22411;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20174;&#29609;&#32781;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#28789;&#24039;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.12861</link><description>&lt;p&gt;
D-Cubed&#65306;&#29992;&#20110;&#28789;&#24039;&#21487;&#21464;&#24418;&#25805;&#20316;&#30340;&#28508;&#22312;&#25193;&#25955;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-Cubed&#30340;&#26032;&#22411;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20174;&#29609;&#32781;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#28789;&#24039;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#36890;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#23545;&#20110;&#20811;&#26381;&#24182;&#34892;&#22841;&#20855;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-Cubed&#30340;&#26032;&#22411;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29609;&#32781;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#26469;&#35299;&#20915;&#28789;&#24039;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;D-Cubed&#23398;&#20064;&#20102;&#19968;&#20010;&#25216;&#33021;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#29992;VAE&#22312;&#29609;&#32781;&#25968;&#25454;&#38598;&#20013;&#32534;&#30721;&#30701;&#35270;&#37326;&#21160;&#20316;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;LDM&#23558;&#25216;&#33021;&#28508;&#22312;&#32452;&#21512;&#25104;&#25216;&#33021;&#36712;&#36857;&#65292;&#20195;&#34920;&#25968;&#25454;&#38598;&#20013;&#30340;&#38271;&#35270;&#37326;&#21160;&#20316;&#36712;&#36857;&#12290;&#20026;&#20102;&#20248;&#21270;&#30446;&#26631;&#20219;&#21153;&#30340;&#36712;&#36857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26799;&#24230;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12861v1 Announce Type: cross  Abstract: Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12859</link><description>&lt;p&gt;
&#20855;&#26377;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#21407;&#22987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Primal Methods for Variational Inequality Problems with Functional Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#22240;&#20854;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#36816;&#31609;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#22791;&#21463;&#35748;&#21487;&#12290; &#39318;&#27425;&#26041;&#27861;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290; &#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#25110;&#32447;&#24615;&#26368;&#23567;&#21270;&#23637;&#24320;&#22120;&#26469;&#23548;&#33322;&#21487;&#34892;&#38598;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20250;&#22312;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290; &#35299;&#20915;&#36825;&#20123;&#21151;&#33021;&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;Lagrange&#20989;&#25968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#19978;&#12290; &#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#23384;&#22312;&#24182;&#19988;&#20107;&#20808;&#20102;&#35299;&#26368;&#20339;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#12290; &#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12859v1 Announce Type: cross  Abstract: Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12856</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#28145;&#24230;RL&#31574;&#30053;&#21644;&#20540;&#32593;&#32476;&#20998;&#21035;&#26159;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#20197;&#21033;&#29992;&#36825;&#20123;&#23545;&#31216;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#20851;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#26469;&#35774;&#35745;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#21482;&#33021;&#20351;&#29992;&#38750;&#24120;&#21463;&#38480;&#30340;&#32452;&#20214;&#24211;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31561;&#21464;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#26377;&#30410;&#20110;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.12847</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20998;&#21449;
&lt;/p&gt;
&lt;p&gt;
Policy Bifurcation in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12847
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20026;&#21463;&#38480;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38544;&#21547;&#22320;&#20551;&#35774;&#31574;&#30053;&#20989;&#25968;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#21363;&#31574;&#30053;&#20197;&#24179;&#31283;&#12289;&#36830;&#32493;&#30340;&#26041;&#24335;&#23558;&#29366;&#24577;&#26144;&#23556;&#21040;&#21160;&#20316;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#34892;&#31574;&#30053;&#24212;&#35813;&#26159;&#19981;&#36830;&#32493;&#25110;&#22810;&#20540;&#30340;&#65292;&#32780;&#22312;&#19981;&#36830;&#32493;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#20043;&#38388;&#25554;&#20540;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#32422;&#26463;&#36829;&#35268;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#35782;&#21035;&#20986;&#36825;&#31181;&#29616;&#35937;&#29983;&#25104;&#26426;&#21046;&#30340;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#25299;&#25169;&#20998;&#26512;&#20005;&#35880;&#22320;&#35777;&#26126;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#20998;&#21449;&#30340;&#23384;&#22312;&#65292;&#36825;&#23545;&#24212;&#20110;&#21487;&#36798;&#20803;&#32452;&#30340;&#21487;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#25581;&#31034;&#20102;&#22312;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#30340;&#24773;&#26223;&#20013;&#65292;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#65292;&#24847;&#21619;&#30528;&#20854;&#36755;&#20986;&#21160;&#20316;&#38656;&#35201;&#36805;&#36895;&#21709;&#24212;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.12844</link><description>&lt;p&gt;
MELTing point: &#31227;&#21160;&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELTing point: Mobile Evaluation of Language Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36880;&#28176;&#24212;&#29992;&#20110;&#26085;&#24120;&#20219;&#21153;&#65292;&#36171;&#20104;&#25105;&#20204;&#30340;&#35745;&#31639;&#26426;&#8220;&#26234;&#33021;&#30340;&#28779;&#33457;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#22312;&#20010;&#20154;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20197;&#21450;&#36805;&#36895;&#38544;&#31169;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#29616;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#65292;&#25903;&#25345;&#22312;&#35774;&#22791;&#19978;&#26080;&#30028;&#38754;&#25191;&#34892;&#21644;&#35780;&#20272;LLMs&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#27169;&#22411;&#12289;&#35774;&#22791;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;Android&#12289;iOS&#21644;Nvidia Jetson&#35774;&#22791;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27979;&#37327;&#23427;&#20204;&#30340;&#31471;&#21040;&#31471;&#21644;&#32454;&#31890;&#24230;&#24615;&#33021;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#20869;&#23384;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#21644;&#26377;&#25928;&#25351;&#26631;&#65292;&#36890;&#36807;&#25913;&#21464;&#23457;&#35745;&#25361;&#25112;&#30340;&#26041;&#24335;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.12830</link><description>&lt;p&gt;
&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#24050;&#32463;&#24471;&#21040;&#36866;&#24403;&#35780;&#20272;&#20102;&#21527;&#65311;&#20174;&#23457;&#35745;&#21040;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#21644;&#26377;&#25928;&#25351;&#26631;&#65292;&#36890;&#36807;&#25913;&#21464;&#23457;&#35745;&#25361;&#25112;&#30340;&#26041;&#24335;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#26085;&#30410;&#20851;&#27880;&#65292;&#24443;&#24213;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#34880;&#32479;&#30340;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;MLaaS&#25552;&#20379;&#32773;&#24076;&#26395;&#23558;&#20854;&#35270;&#20026;&#31526;&#21512;&#30417;&#31649;&#21512;&#35268;&#24615;&#30340;&#26368;&#32456;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#33267;&#20851;&#37325;&#35201;&#65292;&#38544;&#31169;&#31038;&#21306;&#39564;&#35777;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#25928;&#26524;&#30340;&#26041;&#27861;&#21457;&#23637;&#21644;&#23454;&#26045;&#30340;&#36895;&#24230;&#20196;&#20154;&#22833;&#26395;&#65292;&#36825;&#19968;&#20851;&#38190;&#39046;&#22495;&#32463;&#24120;&#26410;&#33021;&#24471;&#21040;&#36275;&#22815;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#19988;&#26377;&#25928;&#30340;&#25351;&#26631;&#65292;&#20026;&#40657;&#30418;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#25552;&#20379;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23457;&#35745;&#25361;&#25112;&#36716;&#21270;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20986;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#12290;&#36890;&#36807;&#20165;&#20381;&#36182;&#21407;&#22987;&#21644;&#24050;&#36951;&#24536;&#27169;&#22411;--&#28040;&#38500;&#20102;&#35757;&#32451;&#39069;&#22806;&#38452;&#24433;&#27169;&#22411;&#30340;&#38656;&#35201;--&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12830v1 Announce Type: new  Abstract: The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation
&lt;/p&gt;</description></item><item><title>FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.12821</link><description>&lt;p&gt;
FlowerFormer: &#20351;&#29992;&#22522;&#20110;&#27969;&#24863;&#30693;&#30340;&#22270;&#21464;&#25442;&#22120;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12821
&lt;/p&gt;
&lt;p&gt;
FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25104;&#21151;&#19982;&#20854;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#65307;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#31070;&#32463;&#32467;&#26500;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#35757;&#32451;&#25110;&#35780;&#20272;&#12290;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#22312;&#20272;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23558;&#26550;&#26500;&#35270;&#20026;&#22270;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FlowerFormer&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23427;&#34701;&#20837;&#20102;&#31070;&#32463;&#32467;&#26500;&#20869;&#30340;&#20449;&#24687;&#27969;&#12290; FlowerFormer&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;a&#65289;&#21463;&#27969;&#31243;&#21551;&#21457;&#30340;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#65307;&#65288;b&#65289;&#24314;&#31435;&#22312;&#22522;&#20110;&#27969;&#31243;&#30340;&#25513;&#30721;&#19978;&#30340;&#20840;&#23616;&#20851;&#27880;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FlowerFormer&#20248;&#20110;&#29616;&#26377;&#31070;&#32463;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12820</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#24067;&#26009;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A Physics-embedded Deep Learning Framework for Cloth Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#24067;&#26009;&#27169;&#25311;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25152;&#26399;&#26395;&#30340;&#12290;&#20026;&#25913;&#36827;&#21463;&#21147;&#20132;&#20114;&#12289;&#30896;&#25758;&#22788;&#29702;&#21644;&#25968;&#20540;&#31215;&#20998;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#20294;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25429;&#33719;&#24067;&#26009;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#29289;&#29702;&#29305;&#24449;&#30340;&#29289;&#29702;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34920;&#31034;&#36136;&#28857;-&#24377;&#31783;&#31995;&#32479;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20043;&#21518;&#35774;&#35745;&#20102;&#19977;&#20010;&#20998;&#25903;&#26469;&#23398;&#20064;&#24067;&#26009;&#29289;&#29702;&#30340;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#26102;&#38388;&#23548;&#25968;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#27169;&#25311;&#22120;&#25110;&#23376;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#22806;&#37096;&#21147;&#21644;&#30896;&#25758;&#22788;&#29702;&#36827;&#34892;&#38598;&#25104;&#12290;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#30340;&#24067;&#26009;&#21160;&#30011;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12820v1 Announce Type: cross  Abstract: Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#22312;&#26089;&#26399;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#23450;&#20301;&#21040;&#35686;&#25253;&#31574;&#30053;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#20107;&#20214;&#25351;&#26631;&#30340;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35686;&#25253;&#20248;&#20808;&#32423;&#26041;&#26696;&#26174;&#33879;&#25913;&#36827;&#20102;&#20107;&#20214;&#32423;&#25351;&#26631;&#65288;&#39640;&#36798;11%&#30340;AuPRC&#24046;&#24322;&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.12818</link><description>&lt;p&gt;
&#26089;&#26399;&#20107;&#20214;&#39044;&#27979;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Survival Analysis for Early Event Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#22312;&#26089;&#26399;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;&#39118;&#38505;&#23450;&#20301;&#21040;&#35686;&#25253;&#31574;&#30053;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#20107;&#20214;&#25351;&#26631;&#30340;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35686;&#25253;&#20248;&#20808;&#32423;&#26041;&#26696;&#26174;&#33879;&#25913;&#36827;&#20102;&#20107;&#20214;&#32423;&#25351;&#26631;&#65288;&#39640;&#36798;11%&#30340;AuPRC&#24046;&#24322;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#65288;DSA&#65289;&#25512;&#21160;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#26089;&#26399;&#20107;&#20214;&#39044;&#27979;&#65288;EEP&#65289;&#65292;&#36890;&#36807;&#23558;&#39118;&#38505;&#23450;&#20301;&#25972;&#21512;&#21040;&#35686;&#25253;&#31574;&#30053;&#20013;&#25552;&#39640;&#20102;&#20020;&#24202;&#20107;&#20214;&#25351;&#26631;&#12290;&#36890;&#36807;&#35843;&#25972;&#21644;&#35780;&#20272;DSA&#27169;&#22411;&#19982;&#20256;&#32479;EEP&#22522;&#20934;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#33021;&#22815;&#22312;&#26102;&#38388;&#27493;&#39588;&#27700;&#24179;&#19978;&#21305;&#37197;EEP&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35686;&#25253;&#20248;&#20808;&#32423;&#26041;&#26696;&#26174;&#33879;&#25913;&#21892;&#20107;&#20214;&#32423;&#25351;&#26631;&#65288;&#39640;&#36798;11%&#30340;AuPRC&#24046;&#24322;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20195;&#34920;&#20102;&#21307;&#30103;&#39044;&#27979;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20026;&#26089;&#26399;&#20107;&#20214;&#39044;&#27979;&#21644;&#31649;&#29702;&#25552;&#20379;&#20102;&#26356;&#21152;&#32454;&#33268;&#21644;&#21487;&#25805;&#20316;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12818v1 Announce Type: new  Abstract: This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#21442;&#25968;&#22238;&#24402;&#26694;&#26550;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#31526;&#26041;&#38754;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#21442;&#25968;&#25928;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12764</link><description>&lt;p&gt;
&#31070;&#32463;&#21442;&#25968;&#22238;&#24402;&#29992;&#20110;PDE&#35299;&#31639;&#31526;&#30340;&#26174;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Parameter Regression for Explicit Representations of PDE Solution Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12764
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21442;&#25968;&#22238;&#24402;&#26694;&#26550;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#31526;&#26041;&#38754;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#21442;&#25968;&#25928;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#21442;&#25968;&#22238;&#24402;&#65288;NPR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#20013;&#30340;&#35299;&#31639;&#31526;&#32780;&#24320;&#21457;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#36825;&#31181;&#38024;&#23545;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65292;Raissi&#31561;&#65292;2019&#65289;&#25216;&#26415;&#26469;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21442;&#25968;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;DeepONets&#65288;Lu&#31561;&#65292;2021&#65289;&#12290;&#36890;&#36807;&#26681;&#25454;&#29305;&#23450;&#21021;&#22987;&#26465;&#20214;&#23545;&#27599;&#20010;&#35299;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#23427;&#26377;&#25928;&#22320;&#36924;&#36817;&#20102;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21512;&#24182;&#20302;&#31209;&#30697;&#38453;&#25552;&#39640;&#20102;&#21442;&#25968;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#35813;&#26694;&#26550;&#34920;&#29616;&#20986;&#23545;&#26032;&#30340;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#30340;&#20986;&#33394;&#36866;&#24212;&#24615;&#65292;&#20801;&#35768;&#24555;&#36895;&#24494;&#35843;&#21644;&#25512;&#29702;&#65292;&#29978;&#33267;&#22312;&#20998;&#24067;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12764v1 Announce Type: new  Abstract: We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#65292;&#24314;&#31435;&#20102;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#65292;&#35777;&#26126;&#20854;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26356;&#32039;&#20945;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12732</link><description>&lt;p&gt;
&#23545;&#20110;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26356;&#32039;&#20945;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Confidence Bounds for Sequential Kernel Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12732
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#65292;&#24314;&#31435;&#20102;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#65292;&#35777;&#26126;&#20854;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26356;&#32039;&#20945;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#21306;&#38388;&#26159;&#20005;&#26684;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#25351;&#23548;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#26435;&#34913;&#65292;&#24182;&#26500;&#25104;&#35768;&#22810;&#24207;&#36143;&#23398;&#20064;&#21644;&#20915;&#31574;&#31639;&#27861;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#26356;&#32039;&#20945;&#30340;&#32622;&#20449;&#21306;&#38388;&#24102;&#26469;&#20102;&#20855;&#26377;&#26356;&#22909;&#32463;&#39564;&#24615;&#33021;&#21644;&#26356;&#22909;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#26469;&#24314;&#31435;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#26356;&#32039;&#20945;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32622;&#20449;&#21306;&#38388;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26469;&#30340;&#34892;&#21160;&#21462;&#20915;&#20110;&#20808;&#21069;&#30340;&#21382;&#21490;&#12290;&#24403;&#25105;&#20204;&#30340;&#32622;&#20449;&#21306;&#38388;&#21462;&#20195;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26102;&#65292;KernelUCB&#65288;GP-UCB&#65289;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#65292;&#21305;&#37197;&#30340;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;&#21644;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;MixupMP&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26500;&#24314;&#26356;&#29616;&#23454;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#26102;&#30340;&#22522;&#26412;&#27169;&#22411;&#31867;&#38169;&#35823;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12729</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Posterior Uncertainty Quantification in Neural Networks using Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12729
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;MixupMP&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26500;&#24314;&#26356;&#29616;&#23454;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#26102;&#30340;&#22522;&#26412;&#27169;&#22411;&#31867;&#38169;&#35823;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#26469;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25351;&#23450;&#26377;&#20851;&#26410;&#26469;&#26410;&#35265;&#25968;&#25454;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#20551;&#35774;&#26469;&#25429;&#25417;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#20010;&#35266;&#28857;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#38598;&#25104;&#65288;Lakshminarayanan&#31561;&#65292;2017&#65289;&#26159;&#19968;&#20010;&#22522;&#26412;&#19978;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#31867;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#26410;&#26469;&#25968;&#25454;&#20165;&#25903;&#25345;&#29616;&#26377;&#35266;&#23519;&#32467;&#26524; -- &#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#36935;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixupMP&#65292;&#19968;&#31181;&#20351;&#29992;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26500;&#24314;&#26356;&#29616;&#23454;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;MixupMP&#20316;&#20026;&#28145;&#24230;&#38598;&#25104;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#20013;&#27599;&#20010;&#38598;&#25104;&#25104;&#21592;&#37117;&#26159;&#22312;&#36825;&#20010;&#39044;&#27979;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#19978;&#35757;&#32451;&#30340;&#12290;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#39532;&#19969;&#26684;&#23572;&#21518;&#39564;&#26694;&#26550;&#65288;Fong&#31561;&#65292;2023&#65289;&#65292;MixupMP&#36820;&#22238;&#38544;&#24335;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12729v1 Announce Type: cross  Abstract: In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21452;&#23618;&#36229;&#22270;&#26426;&#21046;&#21644;&#26799;&#24230;&#39537;&#21160;&#27969;&#30340;&#26032;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35786;&#26029;&#26694;&#26550;&#65292;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12719</link><description>&lt;p&gt;
&#21452;&#23618;&#36229;&#22270;&#32593;&#32476;&#29992;&#20110;&#22810;&#27169;&#24335;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12719
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21452;&#23618;&#36229;&#22270;&#26426;&#21046;&#21644;&#26799;&#24230;&#39537;&#21160;&#27969;&#30340;&#26032;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35786;&#26029;&#26694;&#26550;&#65292;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21069;&#26399;&#38454;&#27573;&#23545;&#20110;&#26174;&#33879;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#29983;&#27963;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35786;&#26029;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21033;&#29992;&#26368;&#23569;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#21452;&#23618;&#36229;&#22270;&#20248;&#21270;&#26694;&#26550;&#65292;&#21516;&#26102;&#23398;&#20064;&#22270;&#25193;&#22686;&#31574;&#30053;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#21452;&#37325;&#23398;&#20064;&#31574;&#30053;&#34987;&#20551;&#35774;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20449;&#24687;&#20256;&#25773;&#30340;&#26032;&#36335;&#24452;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#39537;&#21160;&#27969;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12719v1 Announce Type: new  Abstract: Early detection of Alzheimer's disease's precursor stages is imperative for significantly enhancing patient outcomes and quality of life. This challenge is tackled through a semi-supervised multi-modal diagnosis framework. In particular, we introduce a new hypergraph framework that enables higher-order relations between multi-modal data, while utilising minimal labels. We first introduce a bilevel hypergraph optimisation framework that jointly learns a graph augmentation policy and a semi-supervised classifier. This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation. Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow. Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer's disease.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12712</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#21464;&#24418;&#35299;&#20915;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Source Scale Bias via Image Warping for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#30001;&#20110;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#21644;&#22270;&#20687;&#22823;&#23567;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#65292;&#23610;&#24230;&#20559;&#24046;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#27880;&#20837;&#23610;&#24230;&#19981;&#21464;&#24615;&#20808;&#39564;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#25110;&#32773;&#22312;&#25512;&#26029;&#26102;&#35843;&#25972;&#23610;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#23610;&#24230;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20250;&#22686;&#21152;&#35757;&#32451;&#36807;&#31243;&#30340;&#35745;&#31639;&#36127;&#36733;&#21644;&#25512;&#26029;&#36807;&#31243;&#30340;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#21147;&#22788;&#29702;&#8212;&#8212;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23601;&#22320;&#25197;&#26354;&#22270;&#20687;&#26469;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#28304;&#23610;&#24230;&#20998;&#24067;&#21487;&#20197;&#25913;&#21892;&#20027;&#24178;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#20197;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#22320;&#29702;&#12289;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#12289;&#21487;&#35299;&#37322;&#30340;&#21644;&#36816;&#21160;&#19968;&#33268;&#30340;&#38544;&#31169;&#23646;&#24615;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#36873;&#25321;&#30340;&#38544;&#31169;&#27169;&#26495;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.12710</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#21644;&#36816;&#21160;&#19968;&#33268;&#30340;&#38544;&#31169;&#23646;&#24615;&#28151;&#28102;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12710
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#12289;&#21487;&#35299;&#37322;&#30340;&#21644;&#36816;&#21160;&#19968;&#33268;&#30340;&#38544;&#31169;&#23646;&#24615;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#36873;&#25321;&#30340;&#38544;&#31169;&#27169;&#26495;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#20247;&#22270;&#20687;&#20013;&#34987;&#25429;&#25417;&#21040;&#30340;&#20010;&#20154;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#38544;&#31169;&#20445;&#25252;&#21160;&#20316;&#35782;&#21035;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20854;&#20013;&#28151;&#28102;&#24212;&#29992;&#20110;&#20840;&#23616;&#23548;&#33268;&#38544;&#31169;&#25935;&#24863;&#21306;&#22495;&#34987;&#38544;&#34255;&#65292;&#20294;&#20063;&#38544;&#34255;&#20102;&#21160;&#20316;&#35782;&#21035;&#20013;&#37325;&#35201;&#30340;&#29615;&#22659;&#21306;&#22495;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#24403;&#21069;&#33539;&#24335;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#20154;&#31867;&#36873;&#25321;&#30340;&#38544;&#31169;&#27169;&#26495;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#65292;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#38544;&#34255;&#23646;&#24615;&#19988;&#36824;&#23548;&#33268;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#28151;&#28102;&#26041;&#26696;&#65292;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26550;&#26500;&#26080;&#20851;&#65292;&#30452;&#25509;&#20462;&#25913;&#36755;&#20837;&#22270;&#20687;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26550;&#26500;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12710v1 Announce Type: cross  Abstract: Concerns for the privacy of individuals captured in public imagery have led to privacy-preserving action recognition. Existing approaches often suffer from issues arising through obfuscation being applied globally and a lack of interpretability. Global obfuscation hides privacy sensitive regions, but also contextual regions important for action recognition. Lack of interpretability erodes trust in these new technologies. We highlight the limitations of current paradigms and propose a solution: Human selected privacy templates that yield interpretability by design, an obfuscation scheme that selectively hides attributes and also induces temporal consistency, which is important in action recognition. Our approach is architecture agnostic and directly modifies input imagery, while existing approaches generally require architecture training. Our approach offers more flexibility, as no retraining is required, and outperforms alternatives on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#39062;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.12695</link><description>&lt;p&gt;
&#20855;&#26377;&#23458;&#25143;&#20869;&#21644;&#23458;&#25143;&#38388;&#19968;&#33268;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#39062;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20026;&#20998;&#21106;&#20219;&#21153;&#26631;&#35760;&#21307;&#23398;&#22270;&#20687;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#21644;&#25935;&#24863;&#24615;&#65292;&#20174;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#24314;&#31435;&#19968;&#20010;&#38598;&#20013;&#24335;&#20998;&#21106;&#25968;&#25454;&#38598;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#23396;&#31435;&#23458;&#25143;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#20132;&#25442;&#65292;&#36825;&#19982;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#38544;&#31169;&#24615;&#29305;&#24449;&#30456;&#21563;&#21512;&#12290;&#20026;&#35299;&#20915;&#26631;&#31614;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22312;&#38598;&#20013;&#24335;&#25968;&#25454;&#35774;&#32622;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;&#33267;&#20110;&#32852;&#21512;&#23398;&#20064;&#65292;&#22914;&#20309;&#22312;&#36825;&#31181;&#20998;&#24067;&#24335;&#22330;&#26223;&#19979;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#20540;&#24471;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12695v1 Announce Type: cross  Abstract: Medical image segmentation plays a vital role in clinic disease diagnosis and medical image analysis. However, labeling medical images for segmentation task is tough due to the indispensable domain expertise of radiologists. Furthermore, considering the privacy and sensitivity of medical images, it is impractical to build a centralized segmentation dataset from different medical institutions. Federated learning aims to train a shared model of isolated clients without local data exchange which aligns well with the scarcity and privacy characteristics of medical data. To solve the problem of labeling hard, many advanced semi-supervised methods have been proposed in a centralized data setting. As for federated learning, how to conduct semi-supervised learning under this distributed scenario is worth investigating. In this work, we propose a novel federated semi-supervised learning framework for medical image segmentation. The intra-client
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12690</link><description>&lt;p&gt;
LNPT&#65306;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#19982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
LNPT: Label-free Network Pruning and Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20043;&#21069;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#37096;&#32626;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#12290;&#36890;&#36807;&#20445;&#30041;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26435;&#37325;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26234;&#33021;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24182;&#24378;&#35843;&#23427;&#19982;&#27867;&#21270;&#30340;&#20934;&#30830;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#24046;&#36317;&#36890;&#36807;&#32593;&#32476;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#29305;&#24449;&#22270;&#24418;&#24335;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#21464;&#21270;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550; LNPT&#65292;&#20351;&#24471;&#20113;&#31471;&#25104;&#29087;&#32593;&#32476;&#33021;&#22815;&#25552;&#20379;&#22312;&#32447;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui
&lt;/p&gt;</description></item><item><title>SEVEN&#36890;&#36807;&#20445;&#30041;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#22312;&#21098;&#26525;Transformer&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12688</link><description>&lt;p&gt;
SEVEN: &#36890;&#36807;&#20445;&#30041;&#21736;&#20853;&#26469;&#21098;&#26525;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SEVEN: Pruning Transformer Model by Reserving Sentinels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12688
&lt;/p&gt;
&lt;p&gt;
SEVEN&#36890;&#36807;&#20445;&#30041;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#22312;&#21098;&#26525;Transformer&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21487;&#35266;&#30340;&#21442;&#25968;&#35268;&#27169;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#37492;&#20110;Transformer&#27169;&#22411;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26159;&#21160;&#24577;&#19988;&#38169;&#32508;&#22797;&#26434;&#30340;&#65292;&#24120;&#29992;&#30340;&#21098;&#26525;&#26041;&#27861;&#24448;&#24448;&#20250;&#20445;&#30041;&#20855;&#26377;&#36739;&#22823;&#26799;&#24230;&#22122;&#22768;&#30340;&#26435;&#37325;&#12290;&#36825;&#23548;&#33268;&#34987;&#21098;&#26525;&#30340;&#27169;&#22411;&#23545;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#38598;&#25935;&#24863;&#65292;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#31526;&#21495;&#19979;&#38477;&#65288;SD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#24494;&#35843;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;SD&#30340;&#32047;&#31215;&#36807;&#31243;&#25551;&#36848;Transformer&#27169;&#22411;&#19978;&#30340;&#22122;&#22768;&#25209;&#26799;&#24230;&#24207;&#21015;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35774;&#35745;&#21160;&#24577;&#35780;&#20272;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SEVEN&#65292;&#29305;&#21035;&#20559;&#21521;&#20110;&#20855;&#26377;&#25345;&#32493;&#39640;&#25935;&#24863;&#24230;&#30340;&#26435;&#37325;&#65292;&#21363;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12688v1 Announce Type: new  Abstract: Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved b
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24773;&#32490;&#27010;&#29575;&#65292;&#24182;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#36827;&#34892;&#20915;&#31574;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#28508;&#21147;&#20026;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#27880;&#37322;&#25552;&#20379;&#26234;&#33021;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.12687</link><description>&lt;p&gt;
&#22522;&#20110;&#36831;&#26469;&#30340;&#27169;&#24577;&#34701;&#21512;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20915;&#31574;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24773;&#32490;&#27010;&#29575;&#65292;&#24182;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#36827;&#34892;&#20915;&#31574;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#28508;&#21147;&#20026;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#27880;&#37322;&#25552;&#20379;&#26234;&#33021;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#20845;&#23626;ABAW&#27604;&#36187;&#30340;SUN&#22242;&#38431;&#38024;&#23545;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#35270;&#39057;&#22797;&#21512;&#34920;&#36798;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#34701;&#21512;&#24773;&#32490;&#27010;&#29575;&#27700;&#24179;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#32780;&#20851;&#20110;&#22797;&#21512;&#34920;&#36798;&#39044;&#27979;&#30340;&#20915;&#31574;&#22522;&#20110;&#39044;&#23450;&#20041;&#35268;&#21017;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#26009;&#35757;&#32451;&#21644;&#36328;&#35821;&#26009;&#39564;&#35777;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#20174;&#25361;&#25112;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#20026;&#22312;&#20154;&#31867;&#22522;&#26412;&#21644;&#22797;&#21512;&#24773;&#24863;&#24773;&#22659;&#19979;&#27880;&#37322;&#38899;&#35270;&#39057;&#25968;&#25454;&#30340;&#26234;&#33021;&#24037;&#20855;&#24320;&#21457;&#22880;&#23450;&#22522;&#30784;&#12290;&#28304;&#20195;&#30721;&#24050;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12687v1 Announce Type: cross  Abstract: This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12672</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#20271;&#21162;&#21033;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#20271;&#21162;&#21033;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;GBRBM&#65289;&#24120;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#22522;&#20110;GBRBM&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#26681;&#25454;&#36793;&#32536;GBRBM&#30340;&#33021;&#37327;&#20989;&#25968;&#30456;&#21516;&#30340;&#35780;&#20998;&#26469;&#23545;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#27861;&#35299;&#37322;&#35813;&#35780;&#20998;&#65292;&#24456;&#38590;&#35774;&#32622;&#36866;&#24403;&#30340;&#20998;&#31867;&#38408;&#20540;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#25913;&#36827;&#35780;&#20998;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#24230;&#37327;&#35774;&#32622;&#38408;&#20540;&#30340;&#20934;&#21017;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#28857;&#35774;&#32622;&#38408;&#20540;&#26102;&#65292;&#35813;&#20934;&#21017;&#26159;&#21512;&#29702;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35782;&#21035;&#24230;&#37327;&#28041;&#21450;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#26368;&#23567;&#35780;&#20998;&#20540;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#35780;&#20998;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12672v1 Announce Type: cross  Abstract: Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;cattleia&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;AutoML&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35780;&#20272;&#25351;&#26631;&#21644;&#26032;&#24230;&#37327;&#25351;&#26631;&#65292;&#20998;&#26512;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2403.12664</link><description>&lt;p&gt;
&#35299;&#23494;AutoML&#38598;&#25104;&#65306;cattleia&#22312;&#20915;&#31574;&#20013;&#30340;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12664
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;cattleia&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;AutoML&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35780;&#20272;&#25351;&#26631;&#21644;&#26032;&#24230;&#37327;&#25351;&#26631;&#65292;&#20998;&#26512;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#38598;&#25104;&#34987;&#35777;&#26126;&#27604;&#21333;&#20010;&#39044;&#27979;&#27169;&#22411;&#26356;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#20013;&#65292;&#23427;&#26159;&#26368;&#24120;&#35265;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;cattleia - &#19968;&#31181;&#33021;&#35299;&#23494;&#29992;&#20110;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#21644;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#25104;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#24037;&#20855;&#19982;&#19977;&#20010;AutoML&#21253;&#26500;&#24314;&#30340;&#27169;&#22411;&#19968;&#36215;&#24037;&#20316;&#65306;auto-sklearn&#12289;AutoGluon&#21644;FLAML&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#32473;&#23450;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#38598;&#25104;&#21450;&#20854;&#32452;&#20214;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#39044;&#27979;&#24615;&#33021;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#21644;&#20114;&#34917;&#24615;&#25193;&#23637;&#39564;&#35777;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#26469;&#26816;&#26597;v&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12664v1 Announce Type: cross  Abstract: In many applications, model ensembling proves to be better than a single predictive model. Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML). The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models. In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks. This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The given ensemble is analyzed from different perspectives. We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models. We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions. Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of v
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20256;&#32479;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.12659</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Zeolite Adsorption Property Prediction using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20256;&#32479;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#23545;&#20110;&#21152;&#36895;&#26032;&#26448;&#26009;&#35774;&#35745;&#36807;&#31243;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21560;&#38468;&#24615;&#33021;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#24471;&#21040;&#30340;&#25968;&#20540;&#19968;&#33268;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#24615;&#33021;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12659v1 Announce Type: cross  Abstract: The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations. To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#27169;&#20223;&#33258;&#36866;&#24212;&#26377;&#38480;&#20803;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#36924;&#36817;&#35823;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#21518;&#39564;&#35823;&#24046;&#20272;&#35745;&#22120;&#23558;&#36924;&#36817;&#21442;&#25968;&#20943;&#23569;&#21040;&#24456;&#23569;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#33258;&#36866;&#24212;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.12650</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24102;&#35823;&#24046;&#20272;&#35745;&#30340;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#27169;&#20223;&#33258;&#36866;&#24212;&#26377;&#38480;&#20803;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#36924;&#36817;&#35823;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#21518;&#39564;&#35823;&#24046;&#20272;&#35745;&#22120;&#23558;&#36924;&#36817;&#21442;&#25968;&#20943;&#23569;&#21040;&#24456;&#23569;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#33258;&#36866;&#24212;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#39640;&#32500;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;pPDEs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25968;&#25454;&#30340;&#21442;&#25968;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#26377;&#38480;&#20803;&#35299;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#24182;&#23454;&#29616;&#23545;&#36924;&#36817;&#35823;&#24046;&#30340;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#27169;&#20223;&#20102;&#33258;&#36866;&#24212;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;AFEM&#65289;&#12290;&#23427;&#36755;&#20986;&#19968;&#20010;&#31895;&#32593;&#26684;&#35299;&#21644;&#19968;&#31995;&#21015;&#22312;AFEM&#20013;&#20135;&#29983;&#30340;&#20462;&#27491;&#65292;&#20801;&#35768;&#36319;&#36394;&#32593;&#32476;&#21508;&#23618;&#20013;&#30340;&#35823;&#24046;&#34928;&#20943;&#12290;&#35266;&#23519;&#21040;&#30340;&#35823;&#24046;&#36890;&#36807;&#21487;&#38752;&#30340;&#22522;&#20110;&#27531;&#24046;&#30340;&#21518;&#39564;&#35823;&#24046;&#20272;&#35745;&#22120;&#26469;&#27979;&#37327;&#65292;&#20174;&#32780;&#23558;&#36924;&#36817;&#32593;&#32476;&#36755;&#20986;&#30340;&#21442;&#25968;&#20943;&#23569;&#21040;&#24456;&#23569;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#23616;&#37096;&#32454;&#21270;&#32593;&#26684;&#19978;&#30340;&#38382;&#39064;&#33258;&#36866;&#24212;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;AFEM&#30340;&#27599;&#20010;&#35299;&#37117;&#26159;&#22312;&#20998;&#23618;&#22522;&#30784;&#19978;&#31163;&#25955;&#21270;&#30340;&#12290;&#23545;&#20110;&#26550;&#26500;&#32780;&#35328;&#65292;&#37319;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12650v1 Announce Type: cross  Abstract: To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, convolutional neural networks (CNNs)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pro-QE&#30340;&#26597;&#35810;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#25972;&#21512;&#29616;&#26377;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#20449;&#24687;&#32858;&#21512;&#26469;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#20013;&#26032;&#23454;&#20307;&#30340;&#23884;&#20837;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12646</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#34701;&#21512;&#26694;&#26550;&#30340;&#24402;&#32435;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Prompt-fused framework for Inductive Logical Query Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pro-QE&#30340;&#26597;&#35810;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#25972;&#21512;&#29616;&#26377;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#20449;&#24687;&#32858;&#21512;&#26469;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#20013;&#26032;&#23454;&#20307;&#30340;&#23884;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#22238;&#31572;&#36923;&#36753;&#26597;&#35810;&#23545;&#26426;&#22120;&#25512;&#29702;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#38556;&#30861;&#26469;&#33258;KG&#30340;&#22266;&#26377;&#19981;&#23436;&#25972;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;KG&#20013;&#32570;&#22833;&#36793;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24573;&#35270;&#20102;&#21478;&#19968;&#20010;&#19981;&#23436;&#25972;&#24615;&#26041;&#38754;&#65306;&#26032;&#23454;&#20307;&#30340;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#21333;&#29420;&#23545;&#27599;&#20010;&#36923;&#36753;&#36816;&#31639;&#31526;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#19981;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20840;&#38754;&#20998;&#26512;&#26597;&#35810;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pro-QE&#30340;&#26597;&#35810;&#24863;&#30693;&#25552;&#31034;&#34701;&#21512;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#25972;&#21512;&#29616;&#26377;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#20449;&#24687;&#32858;&#21512;&#26469;&#35299;&#20915;&#26032;&#23454;&#20307;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#32534;&#30721;&#31526;&#21495;&#26597;&#35810;&#29983;&#25104;&#30340;&#26597;&#35810;&#25552;&#31034;&#65292;&#29992;&#20110;&#20174;&#25972;&#20307;&#35270;&#35282;&#25910;&#38598;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12646v1 Announce Type: new  Abstract: Answering logical queries on knowledge graphs (KG) poses a significant challenge for machine reasoning. The primary obstacle in this task stems from the inherent incompleteness of KGs. Existing research has predominantly focused on addressing the issue of missing edges in KGs, thereby neglecting another aspect of incompleteness: the emergence of new entities. Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the reasoning process. In this paper, we propose a query-aware prompt-fused framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation. Additionally, a query prompt, which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective. To evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.12641</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Contrastive Learning Strategy Search for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#20027;&#35201;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#25163;&#21160;&#26500;&#24314;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;CLS&#65289;&#20197;&#24212;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#24320;&#21457;CLS&#36890;&#24120;&#38656;&#35201;&#23545;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#26377;&#36807;&#22810;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23545;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#30340;&#19987;&#19994;&#35748;&#30693;&#65292;&#20197;&#21450;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#22823;&#37327;&#23454;&#39564;&#26469;&#30830;&#23450;&#35814;&#32454;&#30340;&#23398;&#20064;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#23454;&#36341;&#65292;&#35813;&#23454;&#36341;&#33258;&#21160;&#23398;&#20064;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#21363;&#33258;&#21160;&#21270;&#23545;&#27604;&#23398;&#20064;&#65288;AutoCL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12641v1 Announce Type: new  Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#28145;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.12636</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#32479;&#35745;&#36317;&#31163;&#30340;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Practical Guide to Statistical Distances for Evaluating Generative Models in Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#28145;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#26159;&#38750;&#24120;&#23453;&#36149;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#39640;&#32500;&#21644;&#22797;&#26434;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#36830;&#25509;&#32452;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#29702;&#35299;&#27969;&#34892;&#30340;&#32479;&#35745;&#36317;&#31163;&#27010;&#24565;&#25552;&#20379;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#20837;&#21475;&#28857;&#65292;&#21482;&#38656;&#35201;&#25968;&#23398;&#21644;&#32479;&#35745;&#23398;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20195;&#34920;&#19981;&#21516;&#26041;&#27861;&#35770;&#30340;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#27010;&#24565;&#65306;&#20351;&#29992;&#20302;&#32500;&#25237;&#24433;&#65288;Sliced-Wasserstein; SW)&#12289;&#20351;&#29992;&#20998;&#31867;&#22120;&#33719;&#21462;&#36317;&#31163;&#65288;Classifier Two-Sample Tests; C2ST)&#12289;&#36890;&#36807;&#26680;&#36827;&#34892;&#23884;&#20837;&#65288;Maximum Mean Discrepancy; MMD) &#25110;&#31070;&#32463;&#32593;&#32476;&#65288;Fr\'echet Inception Distance; FID)&#12290;&#25105;&#20204;&#24378;&#35843;&#27599;&#20010;&#36317;&#31163;&#32972;&#21518;&#30340;&#30452;&#35273;&#65292;&#24182;&#35299;&#37322;&#23427;&#20204;&#30340;&#20248;&#28857;&#12289;&#21487;&#20280;&#32553;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12636v1 Announce Type: new  Abstract: Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distanc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#38899;&#35270;&#39057;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#37326;&#22806;&#24773;&#32490;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#38024;&#23545;&#35270;&#39057;&#21644;&#38899;&#39057;&#27169;&#24577;&#20998;&#21035;&#22522;&#20110;&#24494;&#35843;&#30340;CNN&#21644;PDEM&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#24314;&#27169;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;ABAW'24&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12609</link><description>&lt;p&gt;
SUN&#22242;&#38431;&#23545;ABAW 2024&#27604;&#36187;&#30340;&#36129;&#29486;&#65306;&#38899;&#35270;&#39057;Valence-Arousal&#20272;&#35745;&#21644;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#38899;&#35270;&#39057;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#37326;&#22806;&#24773;&#32490;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#38024;&#23545;&#35270;&#39057;&#21644;&#38899;&#39057;&#27169;&#24577;&#20998;&#21035;&#22522;&#20110;&#24494;&#35843;&#30340;CNN&#21644;PDEM&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#24314;&#27169;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;ABAW'24&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24773;&#32490;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#33258;&#21160;&#24773;&#32490;&#35782;&#21035;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#31995;&#32479;&#22312;&#23454;&#39564;&#23460;&#25511;&#21046;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#36828;&#26410;&#33021;&#22312;&#38750;&#23454;&#39564;&#23460;&#25511;&#21046;&#30340;&#8220;&#37326;&#22806;&#8221;&#25968;&#25454;&#19978;&#25552;&#20379;&#29983;&#24577;&#25928;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#37326;&#22806;&#24773;&#32490;&#35782;&#21035;&#30340;&#38899;&#35270;&#39057;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#29305;&#21035;&#25506;&#35752;&#22522;&#20110;&#24494;&#35843;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Public Dimensional Emotion Model (PDEM) &#30340;&#26550;&#26500;&#23545;&#35270;&#39057;&#21644;&#38899;&#39057;&#27169;&#24577;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#36825;&#20123;&#22810;&#38454;&#27573;&#35757;&#32451;&#30340;&#27169;&#24577;&#29305;&#23450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23884;&#20837;&#36827;&#34892;&#26367;&#20195;&#26102;&#38388;&#24314;&#27169;&#21644;&#34701;&#21512;&#31574;&#30053;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) &#25361;&#25112;&#21327;&#35758;&#19979;&#20351;&#29992;AffWild2&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12609v1 Announce Type: new  Abstract: As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#35782;&#21035;&#24037;&#19994;&#23454;&#20307;&#65292;&#21462;&#24471;&#20102;&#22312;&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.12606</link><description>&lt;p&gt;
&#20851;&#20110;&#24322;&#36136;&#38598;&#25104;&#26041;&#27861;&#22312;&#37325;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#35782;&#21035;&#24037;&#19994;&#23454;&#20307;&#65292;&#21462;&#24471;&#20102;&#22312;&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#35782;&#21035;&#24037;&#19994;&#23454;&#20307;&#65292;&#20351;&#29992;&#33455;&#29255;&#26408;&#25176;&#30424;&#21644;&#38208;&#38156;&#37329;&#23646;&#26495;&#30340;&#22270;&#20687;&#20316;&#20026;&#25968;&#25454;&#38598;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#24120;&#29992;&#22797;&#26434;&#30340;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19968;&#32452;&#31616;&#21270;&#30340;&#22522;&#26412;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#22330;&#26223;&#19979;&#12290;&#27599;&#20010;&#38598;&#25104;&#23376;&#27169;&#22411;&#20351;&#29992;&#32473;&#23450;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#21462;&#29305;&#24449;&#20316;&#20026;&#20854;&#36755;&#20837;&#65292;&#20801;&#35768;&#22312;&#27604;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#38656;&#35201;&#30340;&#35757;&#32451;&#26102;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#20869;&#21019;&#24314;&#26377;&#25928;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;Rank-1&#20934;&#30830;&#29575;&#36229;&#36807;77%&#65292;Rank-10&#20934;&#30830;&#29575;&#36229;&#36807;99%&#65292;&#24182;&#20171;&#32461;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#38598;&#25104;&#26041;&#27861;&#30740;&#31350;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12606v1 Announce Type: new  Abstract: In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples. Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios. Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models. We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26681;&#25454;&#26410;&#26469;&#26080;&#23478;&#21487;&#24402;&#30340;&#39118;&#38505;&#65292;&#20934;&#30830;&#35782;&#21035;&#38656;&#35201;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#25552;&#39640;&#20102;&#20998;&#37197;&#25928;&#29575;&#65292;&#20844;&#24179;&#21644;&#20844;&#27491;&#65292;&#36991;&#20813;&#20102;&#19968;&#37096;&#20998;&#20154;&#22240;&#24403;&#21069;&#27969;&#31243;&#30340;&#30095;&#24573;&#32780;&#26080;&#23478;&#21487;&#24402;&#12290;</title><link>https://arxiv.org/abs/2403.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#30340;&#31199;&#37329;&#34917;&#21161;&#20998;&#21457;&#38450;&#27490;&#22240;&#39537;&#36880;&#32780;&#36896;&#25104;&#30340;&#26080;&#23478;&#21487;&#24402;
&lt;/p&gt;
&lt;p&gt;
Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26681;&#25454;&#26410;&#26469;&#26080;&#23478;&#21487;&#24402;&#30340;&#39118;&#38505;&#65292;&#20934;&#30830;&#35782;&#21035;&#38656;&#35201;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#25552;&#39640;&#20102;&#20998;&#37197;&#25928;&#29575;&#65292;&#20844;&#24179;&#21644;&#20844;&#27491;&#65292;&#36991;&#20813;&#20102;&#19968;&#37096;&#20998;&#20154;&#22240;&#24403;&#21069;&#27969;&#31243;&#30340;&#30095;&#24573;&#32780;&#26080;&#23478;&#21487;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31199;&#37329;&#34917;&#21161;&#35745;&#21010;&#20026;&#20010;&#20154;&#25552;&#20379;&#36130;&#25919;&#25588;&#21161;&#65292;&#20197;&#38450;&#27490;&#30001;&#39537;&#36880;&#24341;&#36215;&#30340;&#20303;&#25151;&#19981;&#31283;&#23450;&#24182;&#36991;&#20813;&#26080;&#23478;&#21487;&#24402;&#12290;&#30001;&#20110;&#36825;&#20123;&#35745;&#21010;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36816;&#20316;&#65292;&#23427;&#20204;&#24517;&#39035;&#20915;&#23450;&#35841;&#26159;&#20248;&#20808;&#32771;&#34385;&#30340;&#23545;&#35937;&#12290;&#36890;&#24120;&#65292;&#36164;&#37329;&#26159;&#36890;&#36807;&#19968;&#31181;&#21453;&#24212;&#24615;&#25110;&#20808;&#21040;&#20808;&#24471;&#30340;&#20998;&#37197;&#36807;&#31243;&#20998;&#21457;&#30340;&#65292;&#36825;&#31181;&#36807;&#31243;&#19981;&#31995;&#32479;&#22320;&#32771;&#34385;&#26410;&#26469;&#26080;&#23478;&#21487;&#24402;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#19982;&#23486;&#22805;&#27861;&#23612;&#20122;&#24030;&#38463;&#21202;&#26684;&#23612;&#21439;&#21512;&#20316;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#31215;&#26497;&#30340;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#20182;&#20204;&#26410;&#26469;&#26080;&#23478;&#21487;&#24402;&#30340;&#39118;&#38505;&#20248;&#20808;&#32771;&#34385;&#38754;&#20020;&#39537;&#36880;&#30340;&#20010;&#20154;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21033;&#29992;&#24030;&#21644;&#21439;&#30340;&#34892;&#25919;&#25968;&#25454;&#20934;&#30830;&#35782;&#21035;&#38656;&#35201;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#33267;&#23569;20%&#30340;&#31616;&#21333;&#20248;&#20808;&#32423;&#22788;&#29702;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#19978;&#20844;&#24179;&#20844;&#27491;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35782;&#21035;&#20986;28%&#30340;&#20010;&#20154;&#65292;&#36825;&#20123;&#20154;&#34987;&#24403;&#21069;&#30340;&#22788;&#29702;&#27969;&#31243;&#25152;&#24573;&#35270;&#24182;&#26368;&#32456;&#26080;&#23478;&#21487;&#24402;&#12290;&#38500;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12599v1 Announce Type: cross  Abstract: Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness. Since these programs operate under resource constraints, they must decide who to prioritize. Typically, funding is distributed by a reactive or first-come-first serve allocation process that does not systematically consider risk of future homelessness. We partnered with Allegheny County, PA to explore a proactive allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness. Our ML system that uses state and county administrative data to accurately identify individuals in need of support outperforms simpler prioritization approaches by at least 20% while being fair and equitable across race and gender. Furthermore, our approach would identify 28% of individuals who are overlooked by the current process and end up homeless. Beyond improvements 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.12588</link><description>&lt;p&gt;
&#20027;&#35201;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning of the Prime Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12588
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#21253;&#25324;&#21704;&#20195;-&#25289;&#39532;&#21162;&#37329;&#23450;&#29702;&#30340;&#19968;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#35770;&#35777;&#65292;&#35299;&#37322;&#20102;Y.-H. He&#20851;&#20110;&#32032;&#25968;&#21487;&#23398;&#24615;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#24182;&#20551;&#35774;Erd\H{o}s-Kac&#23450;&#24459;&#26497;&#19981;&#21487;&#33021;&#34987;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12562</link><description>&lt;p&gt;
&#36890;&#36807;&#33719;&#21462;&#36171;&#26435;&#65306;&#25903;&#25345;&#23567;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Equity through Access: A Case for Small-scale Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24471;&#30410;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#35745;&#31639;&#21147;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#36164;&#28304;&#34987;&#29992;&#20110;&#35757;&#32451;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#12289;&#25968;&#25454;&#12289;&#33021;&#28304;&#21644;&#30899;&#25490;&#25918;&#26041;&#38754;&#28040;&#32791;&#24040;&#22823;&#12290;&#36825;&#20123;&#25104;&#26412;&#27491;&#22312;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#26032;&#22411;&#20934;&#20837;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22312;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#36164;&#28304;&#26377;&#38480;&#30340;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#35270;&#20102;&#29616;&#26377;&#35270;&#35273;&#20219;&#21153;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;DL&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#24615;&#33021;&#19982;&#36164;&#28304;&#21333;&#20803;&#30340;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;PePR&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#65288;&#36328;&#24230;&#20174;1M&#21040;130M&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65289;&#21644;&#19977;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#26377;&#20851;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20851;&#31995;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.12559</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence Self-Calibration for Multi-Label Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MLCIL&#65289;&#20013;&#30340;&#37096;&#20998;&#26631;&#31614;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#21482;&#26377;&#26032;&#31867;&#21035;&#34987;&#26631;&#35760;&#65292;&#32780;&#36807;&#21435;&#21644;&#26410;&#26469;&#26631;&#31614;&#20173;&#28982;&#19981;&#21487;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#30001;&#20110;&#38169;&#35823;&#22320;&#39640;&#32622;&#20449;&#24230;&#22810;&#26631;&#31614;&#39044;&#27979;&#32780;&#20986;&#29616;&#22823;&#37327;&#35823;&#25253;&#38169;&#35823;&#65292;&#21152;&#21095;&#20102;&#22312;&#19981;&#21516;&#26631;&#31614;&#31354;&#38388;&#20869;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;MLCIL&#20013;&#25913;&#36827;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24515;&#33258;&#26657;&#20934;&#65288;CSC&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#26631;&#31614;&#20851;&#31995;&#26657;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31867;&#22686;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23398;&#20064;&#30340;&#12289;&#21160;&#24577;&#25193;&#23637;&#30340;&#26631;&#31614;&#20851;&#31995;&#22270;&#26469;&#36830;&#25509;&#23396;&#31435;&#30340;&#26631;&#31614;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20449;&#24515;&#26657;&#20934;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#22810;&#26631;&#31614;&#22686;&#37327;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#36807;&#20110;&#33258;&#20449;&#30340;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#24809;&#32602;&#65292;&#20419;&#36827;&#20102;&#20449;&#24515;&#30340;&#33258;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12559v1 Announce Type: cross  Abstract: The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph. Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoDA-NO&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#35937;&#22495;&#25110;&#36890;&#36947;&#31354;&#38388;&#23545;&#20989;&#25968;&#36827;&#34892;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;PDE&#31995;&#32479;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25110;&#39044;&#35757;&#32451;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22810;&#29289;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.12553</link><description>&lt;p&gt;
&#20026;&#27714;&#35299;&#22810;&#29289;&#29702;&#23398;&#20559;&#24494;&#20998;&#26041;&#31243;&#39044;&#35757;&#32451;&#35937;&#22495;&#20851;&#27880;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoDA-NO&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#36890;&#36807;&#22312;&#35937;&#22495;&#25110;&#36890;&#36947;&#31354;&#38388;&#23545;&#20989;&#25968;&#36827;&#34892;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;PDE&#31995;&#32479;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25110;&#39044;&#35757;&#32451;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22810;&#29289;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#22312;&#35299;&#20915;&#28041;&#21450;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22810;&#29289;&#29702;&#38382;&#39064;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#20960;&#20309;&#24418;&#29366;&#12289;&#29289;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#32570;&#20047;&#22823;&#37327;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35937;&#22495;&#20851;&#27880;&#31070;&#32463;&#31639;&#23376;&#65288;CoDA-NO&#65289;&#65292;&#35813;&#31639;&#23376;&#23558;&#20989;&#25968;&#22312;&#35937;&#22495;&#25110;&#36890;&#36947;&#31354;&#38388;&#19978;&#36827;&#34892;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;PDE&#31995;&#32479;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25110;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#20301;&#32622;&#32534;&#30721;&#12289;&#33258;&#27880;&#24847;&#21147;&#21644;&#24402;&#19968;&#21270;&#23618;&#25193;&#23637;&#21040;&#20989;&#25968;&#31354;&#38388;&#12290;CoDA-NO&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;PDE&#31995;&#32479;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#32771;&#34385;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;CoDA-NO&#20316;&#20026;&#23398;&#20064;&#22810;&#29289;&#29702;PDE&#30340;&#39592;&#24178;&#30340;&#28508;&#21147;&#12290;&#22312;&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#30340;&#22797;&#26434;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#27969;&#20307;&#27969;&#21160;&#27169;&#25311;&#21644;&#27969;&#22266;&#30456;&#20114;&#20316;&#29992;&#65289;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CoDA-N
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12553v1 Announce Type: new  Abstract: Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-N
&lt;/p&gt;</description></item><item><title>AffineQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#20248;&#21270;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#25928;&#30340;&#20223;&#23556;&#21464;&#25442;&#26469;&#25193;&#23637;&#20248;&#21270;&#33539;&#22260;&#24182;&#26174;&#33879;&#20943;&#23567;&#37327;&#21270;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12544</link><description>&lt;p&gt;
AffineQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20223;&#23556;&#21464;&#25442;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
AffineQuant: Affine Transformation Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12544
&lt;/p&gt;
&lt;p&gt;
AffineQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#20248;&#21270;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#25928;&#30340;&#20223;&#23556;&#21464;&#25442;&#26469;&#25193;&#23637;&#20248;&#21270;&#33539;&#22260;&#24182;&#26174;&#33879;&#20943;&#23567;&#37327;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25152;&#38656;&#30340;&#26174;&#33879;&#36164;&#28304;&#38656;&#27714;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#24320;&#21457;&#26088;&#22312;&#21387;&#32553;&#21644;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#22312;&#36825;&#20123;&#25216;&#26415;&#20013;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#30001;&#20110;&#22312;&#35757;&#32451;&#32972;&#26223;&#19979;&#30340;&#26174;&#33879;&#21387;&#32553;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#32780;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;LLMs&#21518;&#37327;&#21270;&#26041;&#27861;&#38480;&#21046;&#20248;&#21270;&#33539;&#22260;&#22312;&#21069;&#21518;&#37327;&#21270;&#26435;&#37325;&#20043;&#38388;&#30340;&#32553;&#25918;&#21464;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#31561;&#25928;&#20223;&#23556;&#21464;&#25442;&#36827;&#34892;&#30452;&#25509;&#20248;&#21270;&#65288;AffineQuant&#65289;&#30340;PTQ&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#20248;&#21270;&#33539;&#22260;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#24212;&#30340;&#36870;&#30697;&#38453;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;PTQ&#30340;&#21069;&#21518;&#37327;&#21270;&#36755;&#20986;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#65292;&#20174;&#32780;&#20445;&#25345;&#20854;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12544v1 Announce Type: new  Abstract: The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and genera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.12529</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#20449;&#24687;&#25552;&#21319;&#20102;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextualized Messages Boost Graph Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22788;&#29702;&#20197;&#22270;&#34920;&#31034;&#30340;&#20219;&#24847;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GNN&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#26412;&#22320;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#20351;&#29992;&#22270;&#35835;&#20986;&#20989;&#25968;&#21019;&#24314;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32858;&#21512;&#21644;&#32452;&#21512;&#31574;&#30053;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;GNN&#65292;&#24120;&#24120;&#21463;&#21551;&#21457;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20174;&#22522;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#29702;&#35770;&#35282;&#24230;&#25506;&#32034;GNN&#65292;&#35813;&#38382;&#39064;&#22266;&#26377;&#22320;&#20551;&#35774;&#21487;&#25968;&#30340;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#24037;&#20316;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21487;&#25968;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;GNN&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;GNN&#22312;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12529v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12511</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#26799;&#24230;&#30340;Frank-Wolfe&#20248;&#21270;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22312;&#27599;&#20010;&#32423;&#21035;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25110;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#20869;&#23384;&#65292;&#20351;&#21453;&#21521;&#20256;&#25773;&#25104;&#20026;&#35745;&#31639;&#26799;&#24230;&#30340;&#19968;&#31181;&#20302;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#20102;&#33879;&#21517;&#30340;Frank-Wolfe&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21363;&#26377;&#26465;&#20214;&#30340;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#35775;&#38382;&#22312;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20013;&#33719;&#24471;&#30340;&#30495;&#26799;&#24230;&#30340;&#26377;&#22122;&#22768;&#20272;&#35745;&#65292; &#21363;&#31216;&#20026;Projected Forward Gradient&#65292;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#30340;Frank-Wolfe&#31639;&#27861;&#65292;&#22312;&#25552;&#20379;Projected Fors
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;p&gt;
&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23041;&#32961;&#12289;&#28431;&#27934;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26684;&#23616;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#22788;&#29702;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23427;&#20204;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#29992;&#24615;&#22806;&#65292;LLMs&#36824;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#32771;&#34385;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65292;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;LLMs&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20174;&#20116;&#20010;&#20027;&#39064;&#35282;&#24230;&#36827;&#34892;&#65306;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12289;LLMs&#35823;&#29992;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#21361;&#23475;&#12289;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#35782;&#21035;&#24403;&#21069;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#21487;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#25195;&#35270;&#36335;&#24452;&#36716;&#25442;&#20026;&#21487;&#30452;&#25509;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12493</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25195;&#35270;&#36335;&#24452;&#20998;&#31867;&#30340;&#21487;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12493
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#21487;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#25195;&#35270;&#36335;&#24452;&#36716;&#25442;&#20026;&#21487;&#30452;&#25509;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25195;&#35270;&#36335;&#24452;&#20998;&#31867;&#26159;&#30524;&#21160;&#36319;&#36394;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#39046;&#22495;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#21046;&#36896;&#19994;&#20197;&#21450;&#21508;&#20010;&#39046;&#22495;&#23398;&#29983;&#30340;&#22521;&#35757;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#12290;&#35813;&#27169;&#22359;&#30340;&#30446;&#30340;&#26159;&#23558;&#25195;&#35270;&#36335;&#24452;&#36716;&#25442;&#20026;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#65292;&#30452;&#25509;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#35823;&#24046;&#65292;&#35813;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#35843;&#25972;&#20854;&#21442;&#25968;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32852;&#21512;&#21487;&#35757;&#32451;&#12290;&#36825;&#20010;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#30340;&#21160;&#26426;&#28304;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#35745;&#31639;&#25195;&#35270;&#36335;&#24452;&#19978;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22359;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12493v1 Announce Type: cross  Abstract: Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains. In this paper we propose a trainable feature extraction module for deep neural networks. The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture. Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance. Therefore, our feature extraction module is jointly trainable with the deep neural network. The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath. We evaluated our module on three public datasets and compared it to the state of the art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12486</link><description>&lt;p&gt;
&#22522;&#20110;NTK&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NTK-Guided Few-Shot Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21453;&#36951;&#24536;FSCIL&#23398;&#20064;&#32773;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20182;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#20943;&#23569;&#30693;&#35782;&#27969;&#22833;&#65292;&#32780;&#24573;&#35270;&#20102;&#27169;&#22411;&#28508;&#22312;&#33719;&#21462;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#35270;&#35282;&#28145;&#20837;&#25506;&#35752;&#20102;FSCIL&#27169;&#22411;&#27867;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#35774;&#35745;&#37325;&#28857;&#22312;&#20110;&#30830;&#20445;&#26368;&#20248;NTK&#25910;&#25947;&#21644;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#20316;&#20026;&#21331;&#36234;&#27867;&#21270;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;NTK&#25910;&#25947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26893;&#26681;&#20110;&#25968;&#23398;&#21407;&#29702;&#30340;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#25351;&#23548;&#25193;&#23637;&#32593;&#32476;&#20869;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#20174;&#22522;&#30784;&#23618;&#38754;&#24320;&#22987;&#65292;&#20248;&#21270;&#26500;&#25104;&#20854;&#27867;&#21270;&#25439;&#22833;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#30784;&#20250;&#35805;&#19978;&#21551;&#21160;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#22609;&#36896;&#21021;&#22987;ne
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne
&lt;/p&gt;</description></item><item><title>TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12481</link><description>&lt;p&gt;
TT-BLIP&#65306;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#22686;&#24378;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12481
&lt;/p&gt;
&lt;p&gt;
TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;   &#25688;&#35201;&#65306;&#26816;&#27979;&#20551;&#26032;&#38395;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#29420;&#31435;&#32534;&#30721;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20018;&#32852;&#65292;&#24573;&#30053;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#32570;&#20047;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TT-BLIP&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;BLIP&#65289;&#65306;BERT &#21644; BLIP\textsubscript{Txt} &#29992;&#20110;&#25991;&#26412;&#65292;ResNet &#21644; BLIP\textsubscript{Img} &#29992;&#20110;&#22270;&#20687;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#21452;&#21521; BLIP &#32534;&#30721;&#22120;&#12290;&#22810;&#27169;&#24577;&#19977;&#35282;&#21464;&#25442;&#22120;&#20351;&#29992;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#19977;&#27169;&#24577;&#29305;&#24449;&#65292;&#30830;&#20445;&#20102;&#22686;&#24378;&#34920;&#31034;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#20010;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24494;&#21338;&#21644;Gossipcop&#12290; &#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 Announce Type: new  Abstract: Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.12474</link><description>&lt;p&gt;
FairSIN&#65306;&#36890;&#36807;&#25935;&#24863;&#20449;&#24687;&#20013;&#21644;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;GNNs&#20063;&#23481;&#26131;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20570;&#20986;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20844;&#24179;&#32771;&#34385;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#20986;&#20174;&#36755;&#20837;&#25110;&#34920;&#31034;&#20013;&#36807;&#28388;&#25481;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#21024;&#38500;&#36793;&#25110;&#23631;&#34109;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#27492;&#31867;&#36807;&#28388;&#31574;&#30053;&#21487;&#33021;&#20063;&#20250;&#36807;&#28388;&#25481;&#19968;&#20123;&#38750;&#25935;&#24863;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#20135;&#29983;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;&#20013;&#21644;&#22522;&#30784;&#33539;&#24335;&#65292;&#21363;&#22312;&#20449;&#24687;&#20256;&#36882;&#20043;&#21069;&#23558;&#39069;&#22806;&#30340;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#25110;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;F3&#39044;&#26399;&#22312;&#32479;&#35745;&#19978;&#20013;&#21644;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#38750;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12474v1 Announce Type: new  Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#33021;&#22815;&#24102;&#26469;&#30340;&#23545;&#20110;&#35773;&#21050;&#35782;&#21035;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12469</link><description>&lt;p&gt;
&#26356;&#22810;&#32972;&#26223;&#20449;&#24687;&#20309;&#26102;&#26377;&#21161;&#20110;&#35782;&#21035;&#35773;&#21050;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do "More Contexts" Help with Sarcasm Recognition?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#33021;&#22815;&#24102;&#26469;&#30340;&#23545;&#20110;&#35773;&#21050;&#35782;&#21035;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#35782;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#30340;&#30495;&#23454;&#24847;&#22270;&#65292;&#36825;&#19982;&#23383;&#38754;&#24847;&#20041;&#30456;&#21453;&#25110;&#19981;&#21516;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31995;&#21015;&#25552;&#20379;&#26356;&#20016;&#23500;$contexts$&#65288;&#20363;&#22914;&#24773;&#24863;&#25110;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65289;&#32473;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#21333;&#29420;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#38598;&#20307;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#39069;&#22806;&#32972;&#26223;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#23545;&#35773;&#21050;&#30340;&#35782;&#21035;&#31243;&#24230;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38598;&#25104;&#29616;&#26377;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#23558;&#26356;&#22810;&#32972;&#26223;&#20449;&#24687;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#22810;&#20010;&#19978;&#19979;&#25991;&#32447;&#32034;&#24182;&#27979;&#35797;&#19981;&#21516;&#26041;&#27861;&#12290;&#22312;&#23545;&#19977;&#20010;&#35773;&#21050;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#30340;&#22235;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#36880;&#27493;&#28155;&#21152;&#26356;&#22810;co&#26377;&#30410;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12469v1 Announce Type: new  Abstract: Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more co
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26377;&#26102;&#29983;&#25104;&#30340;&#25968;&#25454;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.12448</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#25968;&#25454;&#24635;&#26159;&#26377;&#21161;&#20110;&#23545;&#27604;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Generated Data Always Help Contrastive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12448
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26377;&#26102;&#29983;&#25104;&#30340;&#25968;&#25454;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#25104;&#20026;&#26080;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#26368;&#25104;&#21151;&#30340;&#33539;&#24335;&#20043;&#19968;&#65292;&#28982;&#32780;&#23427;&#24448;&#24448;&#20381;&#36182;&#22823;&#37327;&#25163;&#24037;&#25968;&#25454;&#22686;&#24378;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#36924;&#30495;&#22270;&#20687;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35748;&#21487;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#19968;&#31181;&#31216;&#20026;&#8220;&#25968;&#25454;&#33192;&#32960;&#8221;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#25968;&#25454;&#65288;&#29978;&#33267;&#26469;&#33258;&#20687;DDPM&#36825;&#26679;&#30340;&#22909;&#25193;&#25955;&#27169;&#22411;&#65289;&#26377;&#26102;&#29978;&#33267;&#20250;&#23545;&#23545;&#27604;&#23398;&#20064;&#36896;&#25104;&#20260;&#23475;&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#33192;&#32960;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#25968;&#25454;&#33192;&#32960;&#24212;&#35813;&#20276;&#38543;&#30528;&#26356;&#24369;&#30340;&#22686;&#24378;&#65292;&#21453;&#20043;&#20134;&#28982;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12448v1 Announce Type: cross  Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TransformMix&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#22909;&#30340;&#21464;&#25442;&#21644;&#28151;&#21512;&#22686;&#24378;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.12429</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21464;&#25442;&#21644;&#28151;&#21512;&#31574;&#30053;&#30340;TransformMix
&lt;/p&gt;
&lt;p&gt;
TransformMix: Learning Transformation and Mixing Strategies from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TransformMix&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#22909;&#30340;&#21464;&#25442;&#21644;&#28151;&#21512;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25193;&#20805;&#36890;&#36807;&#21512;&#25104;&#26356;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26679;&#26412;&#28151;&#21512;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#26679;&#26412;&#26469;&#21019;&#24314;&#38468;&#21152;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65292;&#22914;Mixup&#21644;Cutmix&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#28151;&#21512;&#25805;&#20316;&#26469;&#28151;&#21512;&#22810;&#20010;&#36755;&#20837;&#12290;&#23613;&#31649;&#36825;&#31181;&#32463;&#39564;&#27861;&#21017;&#26041;&#27861;&#22312;&#19968;&#20123;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23427;&#30450;&#30446;&#22320;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#19988;&#19981;&#20250;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#26377;&#25928;&#30340;&#28151;&#21512;&#31574;&#30053;&#36890;&#24120;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#26041;&#27861;&#27809;&#26377;&#27491;&#30830;&#37197;&#32622;&#65292;&#21487;&#33021;&#20250;&#21019;&#24314;&#35823;&#23548;&#24615;&#30340;&#28151;&#21512;&#22270;&#20687;&#65292;&#20174;&#32780;&#21361;&#21450;&#26679;&#26412;&#28151;&#21512;&#25193;&#20805;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;TransformMix&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#22909;&#30340;&#21464;&#25442;&#21644;&#28151;&#21512;&#25193;&#20805;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;TransformMix&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12429v1 Announce Type: cross  Abstract: Data augmentation improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs. Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data. In particular, TransformMix applies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26679;&#26412;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#22312;&#39034;&#24207;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#26174;&#33879;&#25913;&#36827;&#20102;&#32047;&#31215;&#36951;&#25022;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12428</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#26679;&#26412;&#30340;&#39034;&#24207;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Transfer in Sequential Multi-armed Bandits via Reward Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12428
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26679;&#26412;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#22312;&#39034;&#24207;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#26174;&#33879;&#25913;&#36827;&#20102;&#32047;&#31215;&#36951;&#25022;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20195;&#29702;&#19982;&#32769;&#34382;&#26426;&#22312;&#22810;&#20010;&#36718;&#27425;&#20013;&#36827;&#34892;&#20132;&#20114;&#12290;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#22312;&#19968;&#20010;&#36718;&#27425;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#20294;&#22312;&#19981;&#21516;&#36718;&#27425;&#20043;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20808;&#21069;&#36718;&#27425;&#20013;&#36716;&#31227;&#22870;&#21169;&#26679;&#26412;&#65292;&#24182;&#25913;&#21892;&#25152;&#26377;&#36718;&#27425;&#20013;&#30340;&#32047;&#31215;&#36951;&#25022;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#36951;&#25022;&#24615;&#33021;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#30456;&#27604;&#27809;&#26377;&#36716;&#31227;&#30340;&#26631;&#20934;UCB&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12428v1 Announce Type: new  Abstract: We consider a sequential stochastic multi-armed bandit problem where the agent interacts with bandit over multiple episodes. The reward distribution of the arms remain constant throughout an episode but can change over different episodes. We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes. We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer.
&lt;/p&gt;</description></item><item><title>Jetfire&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;INT8&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#20026;&#26631;&#20934;transformer&#22359;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;</title><link>https://arxiv.org/abs/2403.12422</link><description>&lt;p&gt;
Jetfire: &#20351;&#29992;INT8&#25968;&#25454;&#27969;&#21644;&#25353;&#22359;&#37327;&#21270;&#30340;&#39640;&#25928;&#20934;&#30830;Transformer&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12422
&lt;/p&gt;
&lt;p&gt;
Jetfire&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;INT8&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#20026;&#26631;&#20934;transformer&#22359;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;transformer&#36890;&#24120;&#32791;&#26102;&#36739;&#38271;&#12290;&#23436;&#20840;&#37327;&#21270;&#35757;&#32451;&#65288;FQT&#65289;&#26159;&#19968;&#31181;&#21152;&#36895;&#39044;&#35757;&#32451;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;FQT&#26041;&#27861;&#37319;&#29992;&#37327;&#21270;-&#35745;&#31639;-&#21453;&#37327;&#21270;&#30340;&#36807;&#31243;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#22312;transformers&#20013;&#20351;&#29992;&#26102;&#20986;&#29616;&#27425;&#20248;&#30340;&#21152;&#36895;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#38477;&#32423;&#65292;&#21407;&#22240;&#26159;&#39640;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#21644;&#20302;&#31934;&#24230;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jetfire&#65292;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;INT8&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;INT8&#25968;&#25454;&#27969;&#26469;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#65292;&#24182;&#37319;&#29992;&#25353;&#22359;&#37327;&#21270;&#26041;&#27861;&#26469;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#30340;&#20934;&#30830;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;INT8 FQT&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;transformers&#30340;INT8&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26631;&#20934;&#30340;transformer&#22359;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12422v1 Announce Type: new  Abstract: Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#25512;&#26029;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.12417</link><description>&lt;p&gt;
&#20851;&#20110;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#39044;&#27979;&#35268;&#21010;&#21644;&#21453;&#20107;&#23454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On Predictive planning and counterfactual learning in active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12417
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29702;&#35299;&#26234;&#33021;&#34892;&#20026;&#30340;&#22522;&#30784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#31181;&#34892;&#20026;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#20027;&#21160;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22797;&#26434;&#24615;&#22522;&#30784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#22522;&#20110;&#8220;&#35268;&#21010;&#8221;&#21644;&#8220;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#8221;&#20004;&#31181;&#20915;&#31574;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#20197;&#22312;&#36825;&#20123;&#31574;&#30053;&#20043;&#38388;&#24179;&#34913;&#25968;&#25454;&#22797;&#26434;&#24615;&#19982;&#21327;&#20316;&#65292;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#21183;&#20419;&#36827;&#24179;&#34913;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38656;&#35201;&#20195;&#29702;&#36866;&#24212;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32593;&#26684;&#19990;&#30028;&#24773;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20998;&#26512;&#21508;&#31181;&#21442;&#25968;&#28436;&#21464;&#30340;&#26426;&#20250;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12417v1 Announce Type: new  Abstract: Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#39564;&#32972;&#26223;&#21644;&#24067;&#26391;&#36816;&#21160;&#36827;&#34892;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#24555;&#33410;&#22863;&#30340;&#22522;&#20110;&#36718;&#27425;&#30340;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#32701;&#27611;&#29699;&#20316;&#20026;&#19968;&#31181;&#38656;&#35201;&#36873;&#25163;&#20381;&#36182;&#21464;&#21270;&#30340;&#20915;&#31574;&#30340;&#22266;&#26377;&#33539;&#20363;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#34429;&#28982;&#22312;&#39034;&#24207;&#20915;&#31574;&#30340;&#31163;&#32447;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#25152;&#28041;&#21450;&#65292;&#20294;&#22914;&#20309;&#20174;&#31163;&#32447;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#27169;&#20223;&#20154;&#31867;&#36873;&#25163;&#30340;&#27604;&#36187;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22797;&#21046;&#23545;&#25163;&#30340;&#34892;&#20026;&#26377;&#30410;&#20110;&#36873;&#25163;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27604;&#36187;&#21069;&#26377;&#26041;&#21521;&#22320;&#36827;&#34892;&#25112;&#30053;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#20250;&#21463;&#21040;&#27604;&#36187;&#30340;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#21644;&#30001;&#20110;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#30340;&#36873;&#25163;&#36718;&#27425;&#24615;&#36136;&#32780;&#20135;&#29983;&#30340;&#22797;&#21512;&#25928;&#24212;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RallyNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65306;&#65288;i&#65289;RallyNet&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v1 Announce Type: new  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#30340;&#25805;&#20316;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#26356;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#24433;&#21709;&#21644;&#36739;&#24930;&#25910;&#25947;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.12404</link><description>&lt;p&gt;
&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#65306;&#26426;&#21046;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Training-free Diffusion Guidance: Mechanisms and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#29702;&#35299;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#30340;&#25805;&#20316;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#35813;&#26041;&#27861;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#26356;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#24433;&#21709;&#21644;&#36739;&#24930;&#25910;&#25947;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#39069;&#22806;&#25511;&#21046;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22312;&#24178;&#20928;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;&#32593;&#32476;&#36827;&#34892;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#29983;&#25104;&#65292;&#36866;&#29992;&#20110;&#36890;&#29992;&#25511;&#21046;&#26684;&#24335;&#65292;&#30475;&#36215;&#26469;&#25552;&#20379;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#25193;&#25955;&#24341;&#23548;&#20013;&#30340;&#20813;&#36153;&#21320;&#39184;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#26080;&#38656;&#35757;&#32451;&#30340;&#24341;&#23548;&#30340;&#36816;&#34892;&#26426;&#21046;&#21644;&#22522;&#26412;&#38480;&#21046;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#25903;&#25345;&#26080;&#38656;&#35757;&#32451;&#24341;&#23548;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#21306;&#20998;&#20102;&#23427;&#19982;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#65288;&#25110;&#32773;&#26080;&#20998;&#31867;&#22120;&#30340;&#65289;&#24341;&#23548;&#12290;&#20026;&#20102;&#38416;&#26126;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26799;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12404v1 Announce Type: new  Abstract: Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free diffusion guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower conv
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;CSI-BERT&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12400</link><description>&lt;p&gt;
&#23547;&#25214;&#20002;&#22833;&#30340;&#25968;&#25454;&#65306;&#19968;&#31181;&#21463;BERT&#21551;&#21457;&#30340;&#26041;&#27861;&#24212;&#23545;&#26080;&#32447;&#20256;&#24863;&#20013;&#30340;&#25968;&#25454;&#21253;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;CSI-BERT&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#29992;&#20110;Wi-Fi&#20256;&#24863;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25968;&#25454;&#21253;&#20002;&#22833;&#24120;&#24120;&#23548;&#33268;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#38750;&#36830;&#32493;&#20272;&#35745;&#65292;&#20174;&#32780;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#30340;CSI&#24674;&#22797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;CSI-BERT&#12290;CSI-BERT&#21487;&#20197;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#25554;&#20540;&#26041;&#27861;&#19981;&#21516;&#65292;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#19968;&#20010;&#23376;&#36733;&#27874;&#65292;CSI-BERT&#25429;&#25417;&#20102;&#19981;&#21516;&#23376;&#36733;&#27874;&#20043;&#38388;&#30340;&#39034;&#24207;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#25554;&#20540;&#26041;&#27861;&#30456;&#27604;&#65292;&#21363;&#20351;&#38754;&#23545;&#39640;&#20002;&#21253;&#29575;&#65292;CSI-BERT&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;CSI-BERT&#33719;&#24471;&#30340;&#24674;&#22797;&#30340;CSI&#65292;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26080;&#32447;&#39057;&#35889;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12400v1 Announce Type: cross  Abstract: Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models. To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from Transformers (BERT) for CSI recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner on the target dataset without the need for additional data. Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers. Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates. Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#21306;&#25289;&#31080;&#30340;&#21160;&#24577;&#22810;&#27493;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20351;&#24471;&#23545;&#25163;&#33021;&#22815;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26469;&#31574;&#21010;&#30446;&#26631;&#36873;&#27665;&#30340;&#25805;&#32437;&#12290;</title><link>https://arxiv.org/abs/2403.12399</link><description>&lt;p&gt;
&#23558;&#32593;&#32476;&#36873;&#20030;&#21270;&#65306;&#29992;&#20110;&#31038;&#21306;&#25289;&#31080;&#30340;&#21160;&#24577;&#22810;&#27493;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#21306;&#25289;&#31080;&#30340;&#21160;&#24577;&#22810;&#27493;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20351;&#24471;&#23545;&#25163;&#33021;&#22815;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26469;&#31574;&#21010;&#30446;&#26631;&#36873;&#27665;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20170;&#22825;&#30340;&#19990;&#30028;&#20013;&#65292;&#23545;&#20110;&#31038;&#21306;&#25289;&#31080;&#30340;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#25805;&#32437;&#38382;&#39064;&#26159;&#19968;&#20010;&#30495;&#27491;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#21463;&#36873;&#27665;&#27169;&#22411;&#12289;&#32593;&#32476;&#19978;&#30340;&#35266;&#28857;&#21644;&#26497;&#21270;&#21160;&#24577;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#31038;&#21306;&#25289;&#31080;&#24314;&#27169;&#20026;&#19968;&#20010;&#36890;&#36807;&#23545;GNN&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#32780;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;GNN&#25915;&#20987;&#37117;&#26159;&#21333;&#27493;&#30340;&#65292;&#27809;&#26377;&#32771;&#34385;&#32593;&#32476;&#20013;&#20449;&#24687;&#20256;&#25773;&#30340;&#21160;&#24577;&#32423;&#32852;&#29305;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#23545;&#25163;&#20351;&#29992;GNN&#20316;&#20026;&#20195;&#29702;&#26469;&#39044;&#27979;&#21644;&#25805;&#32437;&#36873;&#27665;&#20559;&#22909;&#65292;&#29305;&#21035;&#26159;&#19981;&#30830;&#23450;&#30340;&#36873;&#27665;&#12290;&#23545;GNN&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#36890;&#30693;&#23545;&#25163;&#21487;&#20197;&#36827;&#34892;&#25112;&#30053;&#25805;&#32437;&#65292;&#20197;&#20351;&#24471;&#30446;&#26631;&#36873;&#27665;&#20837;&#25945;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\textit{&#31038;&#21306;&#25289;&#31080;&#30340;&#26368;&#23567;&#39044;&#31639;&#25915;&#20987;}$&#65288;MBACC&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;MBACC&#38382;&#39064;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21160;&#24577;&#22810;&#27493;&#23545;&#25239;&#24615;&#31038;&#21306;&#25289;&#31080;&#65288;MAC&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;MAC m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12399v1 Announce Type: new  Abstract: The problem of online social network manipulation for community canvassing is of real concern in today's world. Motivated by the study of voter models, opinion and polarization dynamics on networks, we model community canvassing as a dynamic process over a network enabled via gradient-based attacks on GNNs. Existing attacks on GNNs are all single-step and do not account for the dynamic cascading nature of information diffusion in networks. We consider the realistic scenario where an adversary uses a GNN as a proxy to predict and manipulate voter preferences, especially uncertain voters. Gradient-based attacks on the GNN inform the adversary of strategic manipulations that can be made to proselytize targeted voters. In particular, we explore $\textit{minimum budget attacks for community canvassing}$ (MBACC). We show that the MBACC problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community Canvassing (MAC) to address it. MAC m
&lt;/p&gt;</description></item><item><title>FairSTG&#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#19982;&#24050;&#23398;&#20064;&#26679;&#26412;&#30340; mix-up &#23454;&#29616;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.12391</link><description>&lt;p&gt;
FairSTG: &#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairSTG: Countering performance heterogeneity via collaborative sample-level optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12391
&lt;/p&gt;
&lt;p&gt;
FairSTG&#36890;&#36807;&#21327;&#20316;&#26679;&#26412;&#32423;&#20248;&#21270;&#23545;&#25239;&#24615;&#33021;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#19982;&#24050;&#23398;&#20064;&#26679;&#26412;&#30340; mix-up &#23454;&#29616;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12391v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234;  &#25688;&#35201;&#65306;&#26102;&#31354;&#23398;&#20064;&#22312;&#31227;&#21160;&#35745;&#31639;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20197;&#36171;&#20104;&#26234;&#33021;&#22478;&#24066;&#26356;&#24378;&#22823;&#30340;&#21151;&#33021;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#24573;&#35270;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24615;&#33021;&#24322;&#36136;&#24615;&#23450;&#20026;&#19981;&#20844;&#24179;&#26102;&#31354;&#23398;&#20064;&#30340;&#21407;&#22240;&#65292;&#36825;&#19981;&#20165;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#23454;&#38469;&#21151;&#33021;&#65292;&#36824;&#20250;&#32473;&#29616;&#23454;&#19990;&#30028;&#30340;&#22478;&#24066;&#24212;&#29992;&#24102;&#26469;&#20005;&#37325;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#29420;&#31435;&#30340;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26694;&#26550;FairSTG&#65292;&#20511;&#37492;&#20102;&#21033;&#29992;&#24050;&#20805;&#20998;&#23398;&#20064;&#26679;&#26412;&#30340;&#20248;&#21183;&#26469;&#21327;&#20316;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#24605;&#24819;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FairSTG&#21253;&#25324;&#19968;&#20010;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#22120;&#29992;&#20110;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#19968;&#20010;&#29992;&#20110;&#30693;&#35782;&#36716;&#31227;&#30340;&#21327;&#20316;&#34920;&#31034;&#22686;&#24378;&#27169;&#22359;&#65292;&#20174;&#24050;&#20805;&#20998;&#23398;&#20064;&#26679;&#26412;&#21040;&#22256;&#38590;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12391v1 Announce Type: cross  Abstract: Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36857;&#36866;&#24212;&#30340;Noise2Noise&#65288;LoTA-N2N&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36857;&#39033;&#30340;&#20248;&#21270;&#30446;&#26631;&#20943;&#23569;&#20102;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12382</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#30417;&#30563;&#30450;&#22270;&#20687;&#21435;&#22122;&#30340;&#20302;&#36857;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36857;&#36866;&#24212;&#30340;Noise2Noise&#65288;LoTA-N2N&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36857;&#39033;&#30340;&#20248;&#21270;&#30446;&#26631;&#20943;&#23569;&#20102;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#22122;&#22120;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#21435;&#22122;&#39046;&#22495;&#26368;&#36817;&#21457;&#23637;&#30340;&#28966;&#28857;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#24320;&#21457;&#33258;&#30417;&#30563;&#21435;&#22122;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#21482;&#38656;&#22024;&#26434;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#24178;&#20928;&#30340;&#22522;&#20934;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#19982;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#22122;&#22768;&#29305;&#24615;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#21463;Frobenius&#33539;&#25968;&#25193;&#23637;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#21152;&#20837;&#19968;&#20010;&#36857;&#39033;&#21487;&#20197;&#20943;&#23569;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#30340;&#20248;&#21270;&#30446;&#26631;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36857;&#32422;&#26463;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35774;&#35745;&#20102;&#20302;&#36857;&#36866;&#24212;Noise2Noise&#65288;LoTA-N2N&#65289;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12382v1 Announce Type: cross  Abstract: Deep learning-based denoiser has been the focus of recent development on image denoising. In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training. However, a performance gap remains between current self-supervised methods and their supervised counterparts. Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios. Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning. To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.12372</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSL&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36328;&#39046;&#22495;SSL&#39044;&#35757;&#32451;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38169;&#36807;&#20102;&#38598;&#25104;&#19981;&#21516;&#39046;&#22495;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;SSL&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#21508;&#31181;&#39046;&#22495;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12372v1 Announce Type: new  Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a seque
&lt;/p&gt;</description></item><item><title>&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;InstructTime&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#37325;&#26032;&#24418;&#25104;&#20026;&#19968;&#20010;&#23398;&#20064;&#29983;&#25104;&#30340;&#33539;&#20363;&#65292;&#20197;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#32534;&#30721;&#21644;&#36328;&#39046;&#22495;&#27169;&#22411;&#23398;&#20064;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12371</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Advancing Time Series Classification with Multimodal Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12371
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;InstructTime&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#37325;&#26032;&#24418;&#25104;&#20026;&#19968;&#20010;&#23398;&#20064;&#29983;&#25104;&#30340;&#33539;&#20363;&#65292;&#20197;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#32534;&#30721;&#21644;&#36328;&#39046;&#22495;&#27169;&#22411;&#23398;&#20064;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#23457;&#26597;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#23398;&#20064;&#20998;&#31867;&#33539;&#24335; - &#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#35797;&#22270;&#23398;&#20064;&#24207;&#21015;&#36755;&#20837;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#30446;&#26631;&#26631;&#31614;&#30001;&#19968;&#28909;&#20998;&#24067;&#32534;&#30721;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#33539;&#24335;&#38544;&#34255;&#20102;&#20004;&#20010;&#22266;&#26377;&#38480;&#21046;&#65306;(1) &#20351;&#29992;&#19968;&#28909;&#20998;&#24067;&#23545;&#30446;&#26631;&#31867;&#21035;&#36827;&#34892;&#32534;&#30721;&#19981;&#33021;&#21453;&#26144;&#26631;&#31614;&#20043;&#38388;&#30340;&#21487;&#27604;&#24615;&#21644;&#30456;&#20284;&#24615;&#65292;(2) &#38750;&#24120;&#38590;&#20197;&#22312;&#39046;&#22495;&#38388;&#23398;&#20064;&#21487;&#36716;&#31227;&#27169;&#22411;&#65292;&#36825;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#36890;&#29992;&#26381;&#21153;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructTime&#65292;&#19968;&#20010;&#37325;&#22609;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20026;&#23398;&#20064;&#29983;&#25104;&#33539;&#24335;&#30340;&#26032;&#23581;&#35797;&#12290;&#20381;&#38752;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#26102;&#38388;&#24207;&#21015;&#30340;&#20998;&#31867;&#20316;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#20219;&#21153;&#26469;&#21046;&#23450;&#65292;&#20854;&#20013;&#20219;&#21153;&#29305;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12371v1 Announce Type: new  Abstract: For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.12367</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#35745;&#20998;&#21305;&#37197;&#31639;&#27861;&#35780;&#20272;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Semisupervised score based matching algorithm to evaluate the effect of public health interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#21305;&#37197;&#31639;&#27861;&#22312;&#35266;&#23519;&#24615;&#30740;&#31350;&#20013;&#8220;&#37197;&#23545;&#8221;&#30456;&#20284;&#30340;&#30740;&#31350;&#21333;&#20803;&#65292;&#20197;&#28040;&#38500;&#30001;&#20110;&#32570;&#20047;&#38543;&#26426;&#24615;&#32780;&#24341;&#36215;&#30340;&#28508;&#22312;&#20559;&#20506;&#21644;&#28151;&#26434;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#26032;&#22411;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#26435;&#37325;$\beta$&#34987;&#35774;&#35745;&#20026;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12367v1 Announce Type: cross  Abstract: Multivariate matching algorithms "pair" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of "pairs" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a "training" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;U-Net&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;EnKF&#31639;&#27861;&#20013;&#30340;&#23616;&#37096;&#38598;&#21512;&#21327;&#26041;&#24046;&#65292;&#25552;&#20986;&#20102;UNetKF&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.12366</link><description>&lt;p&gt;
U-Net Kalman Filter&#65288;UNetKF&#65289;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;U-Net&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;EnKF&#31639;&#27861;&#20013;&#30340;&#23616;&#37096;&#38598;&#21512;&#21327;&#26041;&#24046;&#65292;&#25552;&#20986;&#20102;UNetKF&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#23558;&#35266;&#27979;&#21644;&#25968;&#20540;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26377;&#24456;&#22823;&#28508;&#21147;&#29992;&#20110;&#34701;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;ML/AI&#65289;&#25216;&#26415;&#12290;&#26412;&#25991;&#20351;&#29992;U-Net&#65292;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#31867;&#22411;&#65292;&#26469;&#39044;&#27979;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EnKF&#65289;&#31639;&#27861;&#30340;&#23616;&#37096;&#38598;&#21512;&#21327;&#26041;&#24046;&#12290;&#22312;&#19968;&#20010;2&#23618;&#20934;&#22320;&#36716;&#27969;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;EnKF DA&#23454;&#39564;&#30340;&#25968;&#25454;&#35757;&#32451;U-Nets&#12290;&#35757;&#32451;&#21518;&#30340;U-Nets&#34987;&#29992;&#20110;&#39044;&#27979;&#19982;&#27969;&#21160;&#30456;&#20851;&#30340;&#23616;&#37096;&#35823;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#22312;U-Net Kalman Filter&#65288;UNetKF&#65289;&#23454;&#39564;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#20123;&#32467;&#26524;&#19982;&#20256;&#32479;&#30340;&#19977;&#32500;&#21464;&#20998;&#65288;3DVar&#65289;&#12289;&#38598;&#21512;&#19977;&#32500;&#21464;&#20998;&#65288;En3DVar&#65289;&#21644;EnKF&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;UNetKF&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;3DVar&#12289;En3DVar&#25110;EnKF&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;U-Nets&#21487;&#20197;&#36716;&#31227;&#21040;&#26356;&#39640;&#30340;&#22320;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12366v1 Announce Type: new  Abstract: Machine learning techniques have seen a tremendous rise in popularity in weather and climate sciences. Data assimilation (DA), which combines observations and numerical models, has great potential to incorporate machine learning and artificial intelligence (ML/AI) techniques. In this paper, we use U-Net, a type of convolutional neutral network (CNN), to predict the localized ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a 2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA experiments. The trained U-Nets are then used to predict the flow-dependent localized error covariance matrices in U-Net Kalman Filter (UNetKF) experiments, which are compared to traditional 3-dimensional variational (3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that trained U-Nets can be transferred to a higher-r
&lt;/p&gt;</description></item><item><title>DMAD&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23384;&#20648;&#22120;&#38134;&#34892;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32479;&#19968;&#30340;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#65292;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#36317;&#31163;&#21644;&#29305;&#24449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12362</link><description>&lt;p&gt;
DMAD: &#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#23384;&#20648;&#22120;&#38134;&#34892;
&lt;/p&gt;
&lt;p&gt;
DMAD: Dual Memory Bank for Real-World Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12362
&lt;/p&gt;
&lt;p&gt;
DMAD&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23384;&#20648;&#22120;&#38134;&#34892;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32479;&#19968;&#30340;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#65292;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#36317;&#31163;&#21644;&#29305;&#24449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#34987;&#35748;&#20026;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29420;&#23478;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#30340;&#22810;&#31867;&#35774;&#32622;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#23569;&#37327;&#20294;&#37325;&#35201;&#30340;&#21487;&#35775;&#38382;&#24322;&#24120;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DMAD&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#19968;&#20010;&#32479;&#19968;&#65288;&#22810;&#31867;&#65289;&#35774;&#32622;&#20013;&#22788;&#29702;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#12290;DMAD&#37319;&#29992;&#21452;&#23384;&#20648;&#22120;&#38134;&#34892;&#26469;&#35745;&#31639;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#29305;&#24449;&#36317;&#31163;&#21644;&#29305;&#24449;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#23553;&#35013;&#20851;&#20110;&#27491;&#24120;&#21644;&#24322;&#24120;&#23454;&#20363;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#26500;&#24314;&#29992;&#20110;&#24322;&#24120;&#20998;&#25968;&#23398;&#20064;&#30340;&#22686;&#24378;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;MVTec-AD&#21644;VisA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DMAD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12362v1 Announce Type: cross  Abstract: Training a unified model is considered to be more suitable for practical industrial anomaly detection scenarios due to its generalization ability and storage efficiency. However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world. To address the challenge of real-world anomaly detection, we propose a new framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD). This framework handles both unsupervised and semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances. This knowledge is then used to construct an enhanced representation for anomaly score learning. We evaluated DMAD on the MVTec-AD and VisA datasets. The resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12354</link><description>&lt;p&gt;
&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;Sim2Real&#65306;&#21033;&#29992;&#22686;&#24378;&#35774;&#22791;&#20449;&#24687;&#25968;&#25454;&#27169;&#25311;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12354
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#65292;&#21363;Sim2Real&#65292;&#29992;&#20110;&#37325;&#24314;&#20809;&#35889;&#20449;&#21495;&#65292;&#22312;&#26377;&#25928;&#25968;&#25454;&#37319;&#26679;&#21644;&#24555;&#36895;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#28857;&#30740;&#31350;&#12290;&#35813;&#24037;&#20316;&#32858;&#28966;&#20110;&#22312;&#20165;&#26377;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#37325;&#24314;&#30495;&#23454;&#19990;&#30028;&#20809;&#35889;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#27169;&#25311;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#25105;&#20204;&#30340;&#20998;&#20809;&#20202;&#35774;&#22791;&#27979;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sim2Real&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12354v1 Announce Type: new  Abstract: This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training. Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts. To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#24341;&#20837;&#8220;&#21451;&#22909;SAM&#8221;&#65288;F-SAM&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#25209;&#29305;&#23450;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#20013;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#65292;&#20174;&#32780;&#25552;&#21319;SAM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12350</link><description>&lt;p&gt;
&#21451;&#22909;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Friendly Sharpness-Aware Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#24341;&#20837;&#8220;&#21451;&#22909;SAM&#8221;&#65288;F-SAM&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#25209;&#29305;&#23450;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#20013;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#65292;&#20174;&#32780;&#25552;&#21319;SAM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#25439;&#22833;&#30340;&#38160;&#24230;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;SAM&#32972;&#21518;&#30340;&#22686;&#24378;&#27867;&#21270;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#30340;&#26680;&#24515;&#32452;&#20214;&#20197;&#25552;&#21319;&#27867;&#21270;&#65292;&#24341;&#20837;&#20102;&#8220;&#21451;&#22909;SAM&#8221;&#65288;F-SAM&#65289;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;SAM&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25209;&#29305;&#23450;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#22312;&#23545;&#25239;&#25200;&#21160;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#21363;&#24403;&#21069;&#23567;&#25209;&#37327;&#26799;&#24230;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;SAM&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SAM&#20013;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20998;&#35299;&#20026;&#23436;&#25972;&#26799;&#24230;&#21644;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#32452;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20381;&#36182;&#23436;&#25972;&#26799;&#24230;&#32452;&#20214;&#20250;&#38477;&#20302;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#25490;&#38500;&#23436;&#25972;&#26799;&#24230;&#32452;&#20214;&#20250;&#23548;&#33268;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12350v1 Announce Type: new  Abstract: Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness. Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization. In this work, we investigate SAM's core components for generalization improvement and introduce "Friendly-SAM" (F-SAM) to further enhance SAM's generalization. Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance. By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance. The possibl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12338</link><description>&lt;p&gt;
&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Halpern iteration in normed spaces and applications to reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;Halpern&#36845;&#20195;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#36817;&#20284;&#26377;&#30028;&#21644;&#25910;&#32553;&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#22312;&#19968;&#20010;&#26377;&#38480;&#32500;&#36171;&#33539;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24213;&#23618;&#30340;&#38543;&#26426;Oracle&#20855;&#26377;&#19968;&#33268;&#26377;&#30028;&#30340;&#26041;&#24046;&#65292;&#21017;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#24635;&#30340;Oracle&#22797;&#26434;&#24230;&#20026;$ \tilde{O} (\varepsilon^{-5})$&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#20026;&#38543;&#26426;Krasnoselskii-Mann&#36845;&#20195;&#24314;&#31435;&#30340;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; $\Omega (\varepsilon^{-3})$&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#25152;&#26377;&#24102;&#26377;&#23567;&#25209;&#22788;&#29702;&#30340;&#24179;&#22343;&#36845;&#20195;&#12290;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#31639;&#23376;&#20026; $\gamma$-&#25910;&#32553;&#30340;&#24773;&#20917;&#19979;&#19968;&#20010; $O(\varepsilon^{-2}(1-\gamma)^{-3})$&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29992;&#20110;&#24179;&#22343;&#22870;&#21169;&#21644;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#26102;&#38388;&#19968;&#33268;Koopman&#33258;&#32534;&#30721;&#22120;&#65288;tcKAE&#65289;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#22312;&#26377;&#38480;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#36890;&#36807;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12335</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;Koopman&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12335
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#26102;&#38388;&#19968;&#33268;Koopman&#33258;&#32534;&#30721;&#22120;&#65288;tcKAE&#65289;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#22312;&#26377;&#38480;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#36890;&#36807;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#32463;&#24120;&#26159;&#39640;&#32500;&#26102;&#31354;&#21160;&#24577;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20013;&#20851;&#38190;&#25361;&#25112;&#12290;Koopman&#33258;&#32534;&#30721;&#22120;&#65288;KAEs&#65289;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#32500;&#33021;&#21147;&#20197;&#21450;Koopman&#31639;&#23376;&#30340;&#35889;&#29305;&#24615;&#65292;&#23398;&#20064;&#20855;&#26377;&#26356;&#31616;&#21333;&#32447;&#24615;&#21160;&#24577;&#30340;&#38477;&#38454;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;KAEs&#30340;&#26377;&#25928;&#24615;&#21463;&#38480;&#20110;&#26377;&#38480;&#32780;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#38388;&#19968;&#33268;Koopman&#33258;&#32534;&#30721;&#22120;&#65288;tcKAE&#65289;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21363;&#20351;&#22312;&#21463;&#38480;&#19988;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#12290;&#36825;&#26159;&#36890;&#36807;&#24378;&#21046;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#19978;&#20445;&#25345;&#39044;&#27979;&#19968;&#33268;&#24615;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;tcKAE&#30456;&#23545;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12335v1 Announce Type: new  Abstract: Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justifica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedFisher&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#36827;&#34892;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#35757;&#32451;&#20986;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#21644;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#37327;&#30340;&#22686;&#21152;&#65292;FedFisher&#20840;&#23616;&#27169;&#22411;&#30340;&#35823;&#24046;&#20250;&#21464;&#24471;&#38750;&#24120;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.12329</link><description>&lt;p&gt;
FedFisher&#65306;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedFisher: Leveraging Fisher Information for One-Shot Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedFisher&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#36827;&#34892;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#35757;&#32451;&#20986;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#21644;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#37327;&#30340;&#22686;&#21152;&#65292;FedFisher&#20840;&#23616;&#27169;&#22411;&#30340;&#35823;&#24046;&#20250;&#21464;&#24471;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;(FL)&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#22810;&#36718;&#36890;&#20449;&#65292;&#36825;&#20855;&#26377;&#20960;&#20010;&#32570;&#28857;&#65292;&#21253;&#25324;&#38656;&#35201;&#24658;&#23450;&#30340;&#32593;&#32476;&#36830;&#36890;&#24615;&#65292;&#37325;&#22797;&#25237;&#20837;&#35745;&#31639;&#36164;&#28304;&#65292;&#20197;&#21450;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#19968;&#27425;&#24615;FL&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#26381;&#21153;&#22120;&#22312;&#19968;&#36718;&#36890;&#20449;&#20013;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFisher&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19968;&#27425;&#24615;FL&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Fisher&#20449;&#24687;&#30697;&#38453;&#65292;&#21463;&#21040;FL&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#21551;&#21457;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#20004;&#23618;&#36807;&#21442;&#25968;&#21270;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#35282;&#24230;&#23545;FedFisher&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19968;&#27425;&#24615;FedFisher&#20840;&#23616;&#27169;&#22411;&#30340;&#35823;&#24046;&#20250;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#37327;&#22686;&#21152;&#26102;&#21464;&#24471;&#26080;&#38480;&#23567;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;F&#30340;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12329v1 Announce Type: new  Abstract: Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of F
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2403.12328</link><description>&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#27969;&#20013;&#28418;&#31227;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Methods for Generating Drift in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12328
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#20013;&#27010;&#24565;&#28418;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#26469;&#24110;&#21161;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.12328v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#31995;&#32479;&#21644;&#20010;&#20307;&#19981;&#26029;&#20135;&#29983;&#25968;&#25454;&#12290; &#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#20154;&#20204;&#20998;&#20139;&#20182;&#20204;&#30340;&#30693;&#35782;&#65292;&#24773;&#24863;&#21644;&#24847;&#35265;&#65292;&#25552;&#20379;&#20851;&#20110;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#35780;&#35770;&#31561;&#12290; &#33258;&#21160;&#20174;&#36825;&#20123;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#20197;&#20026;&#32452;&#32455;&#21644;&#26426;&#26500;&#25552;&#20379;&#35265;&#35299;&#65292;&#20174;&#32780;&#38450;&#27490;&#36130;&#21153;&#24433;&#21709;&#65292;&#20363;&#22914;&#12290; &#20026;&#20102;&#38543;&#26102;&#38388;&#23398;&#20064;&#25991;&#26412;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#32771;&#34385;&#27010;&#24565;&#28418;&#31227;&#12290; &#27010;&#24565;&#28418;&#31227;&#26159;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#39057;&#32321;&#29616;&#35937;&#65292;&#23545;&#24212;&#20110;&#26102;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#26356;&#25913;&#12290; &#20363;&#22914;&#65292;&#24403;&#24773;&#24863;&#21464;&#21270;&#25110;&#21333;&#35789;&#21547;&#20041;&#38543;&#26102;&#38388;&#35843;&#25972;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#27010;&#24565;&#28418;&#31227;&#12290; &#23613;&#31649;&#27010;&#24565;&#28418;&#31227;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#35265;&#12290; &#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#31181;&#25991;&#26412;&#28418;&#31227;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20415;&#31616;&#21270;&#20135;&#29983;&#20855;&#26377;&#26631;&#35760;&#28418;&#31227;&#30340;&#25968;&#25454;&#38598;&#12290; &#36825;&#20123;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;Ye
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#22238;&#39038;&#20102;GT-Rain&#21333;&#22270;&#20687;&#21435;&#38632;&#25361;&#25112;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#30740;&#31350;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#38632;&#22825;&#27668;&#35937;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#26032;&#39062;&#30340;&#30495;&#23454;&#19990;&#30028;&#38632;&#22825;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28608;&#21457;&#21019;&#26032;&#24605;&#36335;&#20197;&#20419;&#36827;&#21333;&#22270;&#20687;&#21435;&#38632;&#26041;&#27861;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12327</link><description>&lt;p&gt;
GT-Rain&#21333;&#22270;&#20687;&#21435;&#38632;&#25361;&#25112;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GT-Rain Single Image Deraining Challenge Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#22238;&#39038;&#20102;GT-Rain&#21333;&#22270;&#20687;&#21435;&#38632;&#25361;&#25112;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#30740;&#31350;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#38632;&#22825;&#27668;&#35937;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#26032;&#39062;&#30340;&#30495;&#23454;&#19990;&#30028;&#38632;&#22825;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28608;&#21457;&#21019;&#26032;&#24605;&#36335;&#20197;&#20419;&#36827;&#21333;&#22270;&#20687;&#21435;&#38632;&#26041;&#27861;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#22238;&#39038;&#20102;2023&#24180;CVPR UG2+&#30740;&#35752;&#20250;&#19978;GT-Rain&#21333;&#22270;&#20687;&#21435;&#38632;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#35813;&#31454;&#36187;&#26088;&#22312;&#30740;&#31350;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#38632;&#22825;&#27668;&#29616;&#35937;&#65292;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#30495;&#23454;&#19990;&#30028;&#38632;&#22825;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#28608;&#21457;&#36827;&#19968;&#27493;&#21457;&#23637;&#30495;&#23454;&#22270;&#20687;&#21333;&#22270;&#20687;&#21435;&#38632;&#26041;&#27861;&#30340;&#21019;&#26032;&#24605;&#36335;&#12290;&#21442;&#36187;&#32773;&#22312;GT-Rain&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21253;&#21547;15&#20010;&#39069;&#22806;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#25193;&#23637;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;GT-Rain&#20013;&#30340;&#22330;&#26223;&#30001;&#30495;&#23454;&#38632;&#22825;&#22270;&#20687;&#21644;&#38632;&#20572;&#21518;&#25293;&#25668;&#30340;&#30495;&#23454;&#22270;&#20687;&#32452;&#25104;&#12290;&#25361;&#25112;&#20013;&#20849;&#26377;275&#21517;&#21442;&#19982;&#32773;&#27880;&#20876;&#65292;&#20854;&#20013;55&#21517;&#21442;&#21152;&#20102;&#26368;&#32456;&#30340;&#27979;&#35797;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12327v1 Announce Type: cross  Abstract: This report reviews the results of the GT-Rain challenge on single image deraining at the UG2+ workshop at CVPR 2023. The aim of this competition is to study the rainy weather phenomenon in real world scenarios, provide a novel real world rainy image dataset, and to spark innovative ideas that will further the development of single image deraining methods on real images. Submissions were trained on the GT-Rain dataset and evaluated on an extension of the dataset consisting of 15 additional scenes. Scenes in GT-Rain are comprised of real rainy image and ground truth image captured moments after the rain had stopped. 275 participants were registered in the challenge and 55 competed in the final testing phase.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12326</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#25552;&#31034;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#32463;&#31579;&#36873;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23398;&#20064;&#21644;&#38543;&#21518;&#20256;&#25773;&#19981;&#33391;&#27010;&#24565;&#65288;&#22914;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65289;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#32467;&#21512;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#12290;&#36825;&#21487;&#23398;&#20064;&#25552;&#31034;&#20805;&#24403;&#38468;&#21152;&#20869;&#23384;&#65292;&#23558;&#19981;&#33391;&#27010;&#24565;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20013;&#65292;&#24182;&#20943;&#23569;&#36825;&#20123;&#27010;&#24565;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#30456;&#24212;&#25991;&#26412;&#36755;&#20837;&#30340;&#20381;&#36182;&#12290;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#25552;&#31034;&#20013;&#65292;&#28040;&#38500;&#36825;&#20123;&#19981;&#33391;&#27010;&#24565;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#20182;&#27010;&#24565;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Hyperdimensional Computing&#65288;HDC&#65289;&#35774;&#35745;&#36866;&#29992;&#20110;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#30340;&#21450;&#26102;&#24178;&#39044;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#30382;&#19979;&#37202;&#31934;&#27700;&#24179;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12323</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#36229;&#32500;&#35745;&#31639;&#22686;&#24378;&#30382;&#19979;&#37202;&#31934;&#27700;&#24179;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12323
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Hyperdimensional Computing&#65288;HDC&#65289;&#35774;&#35745;&#36866;&#29992;&#20110;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#30340;&#21450;&#26102;&#24178;&#39044;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#30382;&#19979;&#37202;&#31934;&#27700;&#24179;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37202;&#31934;&#28040;&#36153;&#23545;&#20010;&#20154;&#20581;&#24247;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#36807;&#37327;&#39278;&#37202;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#21518;&#26524;&#12290;&#20419;&#36827;&#20581;&#24247;&#39278;&#37202;&#20064;&#24815;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23454;&#26045;&#21450;&#26102;&#24178;&#39044;&#65292;&#21363;&#22312;&#37207;&#37202;&#26102;&#27573;&#21457;&#36865;&#25351;&#31034;&#37257;&#37202;&#30340;&#21450;&#26102;&#36890;&#30693;&#12290;&#28982;&#32780;&#65292;&#24178;&#39044;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#25110;&#20405;&#20837;&#24615;&#21487;&#33021;&#20250;&#38459;&#27490;&#20010;&#20154;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#25910;&#38598;&#30340;&#36816;&#21160;&#25968;&#25454;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26469;&#23545;&#37207;&#37202;&#26102;&#27573;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#23545;&#20110;&#31227;&#21160;&#35774;&#22791;&#26469;&#35828;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#24182;&#19981;&#23454;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#35774;&#35745;&#19968;&#20010;&#36866;&#29992;&#20110;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#37096;&#32626;&#30340;&#21450;&#26102;&#24178;&#39044;&#26041;&#27861;&#12290;HDC&#26159;&#19968;&#20010;&#22312;&#22788;&#29702;&#23454;&#26102;&#20256;&#24863;&#22120;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#39564;&#35777;&#32467;&#26524;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12323v1 Announce Type: new  Abstract: Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive. One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes. However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice. Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices. Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment. HDC is a framework that has proven results in processing real-time sensor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#21033;&#29992;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.12320</link><description>&lt;p&gt;
&#36817;&#20284;&#20284;&#28982;&#27604;&#65306;Boosting&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21069;&#21521;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12320
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#21033;&#29992;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12320v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#27604;&#65292;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#39069;&#22806;&#20551;&#35774;&#31561;&#38382;&#39064;&#20351;&#24471;&#39640;&#25928;&#12289;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#20123;&#38480;&#21046;&#20102;&#23545;&#26356;&#28145;&#23618;&#27425;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20284;&#28982;&#27604;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#20294;&#22312;&#37096;&#32626;&#22810;&#20010;&#25968;&#25454;&#21103;&#26412;&#20197;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#26102;&#65292;&#21463;&#21040;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20284;&#28982;&#27604;&#65288;LR&#65289;&#26041;&#27861;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#36890;&#36807;&#21033;&#29992;LR&#22312;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#33258;&#28982;&#24182;&#34892;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21516;&#26102;&#31649;&#36947;&#21270;&#20102;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#19987;&#29992;&#30828;&#20214;&#30340;&#35745;&#31639;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12320v1 Announce Type: cross  Abstract: Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20013;&#25913;&#36827;LoRA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;LoRA&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#24046;&#20998;&#38544;&#31169;&#22122;&#22768;&#25918;&#22823;&#21644;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12313</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20013;&#25913;&#36827; LoRA
&lt;/p&gt;
&lt;p&gt;
Improving LoRA in Privacy-preserving Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20013;&#25913;&#36827;LoRA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;LoRA&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#24046;&#20998;&#38544;&#31169;&#22122;&#22768;&#25918;&#22823;&#21644;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#26368;&#27969;&#34892;&#30340;&#38754;&#21521;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;LoRA&#22312;&#27599;&#20010;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#27169;&#22359;&#39030;&#37096;&#27880;&#20837;&#20102;&#20004;&#20010;&#21487;&#35757;&#32451;&#31209;&#20998;&#35299;&#30697;&#38453;&#30340;&#20056;&#31215;&#12290;&#28982;&#32780;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#24212;&#29992;&#26102;&#65292;LoRA&#21487;&#33021;&#30001;&#20110;&#20197;&#19979;&#20107;&#23454;&#32780;&#21464;&#24471;&#19981;&#31283;&#23450;&#65306;1&#65289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#22810;&#27493;&#26412;&#22320;&#26356;&#26032;&#30340;&#24433;&#21709;&#19981;&#21487;&#24573;&#30053;&#65292;2&#65289;&#20026;&#20102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#32780;&#24378;&#21046;&#25191;&#34892;&#30340;&#28155;&#21152;&#22122;&#22768;&#21487;&#33021;&#20250;&#34987;&#25918;&#22823;&#65292;3&#65289;&#26368;&#32456;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#23548;&#33268;&#36825;&#20123;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#26412;&#22320;&#23458;&#25143;&#31471;&#32852;&#21512;&#20248;&#21270;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20998;&#21035;&#32858;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12313v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12309</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#20174;&#24310;&#36831;&#35266;&#23519;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Delayed Observations via World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#36890;&#24120;&#20551;&#23450;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#31435;&#21363;&#33719;&#24471;&#20851;&#20110;&#34892;&#21160;&#25928;&#26524;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#29289;&#29702;&#38480;&#21046;&#65292;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#65292;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#22312;&#25972;&#21512;&#36807;&#21435;&#35266;&#23519;&#21644;&#23398;&#20064;&#21160;&#24577;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;&#24310;&#36831;POMDP&#38477;&#20302;&#20026;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#24310;&#36831;MDP&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20854;&#20013;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#35266;&#23519;&#24615;&#38477;&#20302;&#26102;&#23454;&#29616;&#27425;&#20248;&#24615;&#33021;&#29978;&#33267;&#36805;&#36895;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#19979;&#32988;&#36807;&#26420;&#32032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#36798;&#21040;30%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12309v1 Announce Type: cross  Abstract: In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the f
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#35745;&#31639;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#19982;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12307</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#32500;&#22270;&#20998;&#31867;&#30340;&#20998;&#23376;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Molecular Classification Using Hyperdimensional Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12307
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#32500;&#35745;&#31639;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#19982;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22270;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#20013;&#30340;&#21033;&#29992;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#36825;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#23588;&#20026;&#26174;&#33879;&#65292;&#37027;&#37324;&#20174;&#22270;&#34920;&#31034;&#20013;&#23398;&#20064;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#39046;&#22495;&#20869;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#28041;&#21450;&#36328;&#19981;&#21516;&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;&#30284;&#32454;&#32990;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;HDC&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25110;Weisfieler-Lehman&#22270;&#26680;&#65288;WL&#65289;&#30456;&#23218;&#32654;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#36229;&#36234;&#20808;&#21069;&#25552;&#20986;&#30340;&#39640;&#32500;&#35745;&#31639;&#22270;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#22312;&#24615;&#33021;&#25552;&#21319;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#22521;&#20859;&#38454;&#27573;&#21152;&#36895;40&#20493;&#65292;&#25512;&#26029;&#26102;&#38388;&#25913;&#36827;&#20102;15&#20493;&#65292;&#30456;&#23545;&#20110;GNN
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12307v1 Announce Type: cross  Abstract: Our work introduces an innovative approach to graph learning by leveraging Hyperdimensional Computing. Graphs serve as a widely embraced method for conveying information, and their utilization in learning has gained significant attention. This is notable in the field of chemoinformatics, where learning from graph representations plays a pivotal role. An important application within this domain involves the identification of cancerous cells across diverse molecular structures.   We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like Graph Neural Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it outperforms previously proposed hyperdimensional computing graph learning methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to GNN a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#27169;&#22411;&#30340;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#24773;&#24863;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#21463;&#30410;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.12285</link><description>&lt;p&gt;
FinLlama&#65306;&#29992;&#20110;&#31639;&#27861;&#20132;&#26131;&#24212;&#29992;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12285
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#27169;&#22411;&#30340;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#24773;&#24863;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#21463;&#30410;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#26032;&#38395;&#22312;&#32447;&#26377;&#22810;&#20010;&#26469;&#28304;&#24433;&#21709;&#24066;&#22330;&#36208;&#21183;&#21644;&#20132;&#26131;&#21592;&#30340;&#20915;&#31574;&#12290;&#36825;&#31361;&#20986;&#20102;&#20934;&#30830;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#38500;&#20102;&#38656;&#35201;&#36866;&#24403;&#30340;&#31639;&#27861;&#20132;&#26131;&#25216;&#26415;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20132;&#26131;&#20915;&#31574;&#12290;&#26631;&#20934;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#24773;&#24863;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#23427;&#20204;&#22312;&#36741;&#21161;&#37329;&#34701;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#23384;&#22312;&#19982;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#21644;&#35789;&#24207;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#21487;&#20197;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#19981;&#26159;&#29305;&#23450;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#20419;&#36827;&#19968;&#31181;&#29305;&#23450;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;LLM&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Llama 2 7B&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20415;&#20174;&#20854;&#29983;&#25104;&#24615;&#36136;&#21644;&#20840;&#38754;&#30340;&#35821;&#35328;&#25805;&#20316;&#20013;&#21463;&#30410;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#23569;&#37096;&#20998;&#30417;&#30563;&#37329;&#34701;&#24773;&#24863;&#25968;&#25454;&#19978;&#24494;&#35843;Llama2 7B&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12285v1 Announce Type: new  Abstract: There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment 
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;</title><link>https://arxiv.org/abs/2403.12278</link><description>&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12278
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#38543;&#26426;&#33293;&#20837;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27969;&#34892;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#30697;&#38453;$\mathbf{A}$&#30340;&#38543;&#26426;&#36817;&#20284;&#33293;&#20837;&#65292;&#20854;&#20013;&#34892;&#25968;&#36828;&#36828;&#22810;&#20110;&#21015;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35777;&#25454;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#25903;&#25345;&#65292;&#39640;&#27010;&#29575;&#19979;&#65292;&#38543;&#26426;&#33293;&#20837;&#30697;&#38453;&#30340;&#26368;&#23567;&#22855;&#24322;&#20540;&#36828;&#31163;&#38646;--&#26080;&#35770;$\mathbf{A}$&#25509;&#36817;&#22855;&#24322;&#36824;&#26159;$\mathbf{A}$&#22855;&#24322;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#26426;&#33293;&#20837;\textit{&#38544;&#24335;&#27491;&#21017;&#21270;}&#39640;&#30246;&#30697;&#38453;$\mathbf{A}$&#65292;&#20351;&#24471;&#33293;&#20837;&#21518;&#30340;&#29256;&#26412;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#26377;&#21147;&#32467;&#26524;&#65292;&#20197;&#21450;&#38543;&#26426;&#33293;&#20837;&#35823;&#24046;&#19981;&#38598;&#20013;&#22312;&#20302;&#32500;&#21015;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#25552;&#39640;CLIP&#24615;&#33021;&#27604;&#22686;&#21152;&#25968;&#25454;&#37327;&#26356;&#20026;&#26377;&#25928;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12267</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65306;&#20248;&#20808;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12267
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#25552;&#39640;CLIP&#24615;&#33021;&#27604;&#22686;&#21152;&#25968;&#25454;&#37327;&#26356;&#20026;&#26377;&#25928;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#25913;&#36827;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#24050;&#34987;&#35777;&#26126;&#27604;&#22686;&#21152;&#25968;&#37327;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#33021;&#22815;&#35777;&#26126;&#36798;&#21040;&#26368;&#20339;&#27867;&#21270;&#25928;&#26524;&#30340;&#23567;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;CLIP&#30340;&#29702;&#35770;&#20005;&#35880;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#35777;&#26126;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#24615;&#33021;&#30340;&#23376;&#38598;&#25509;&#36817;&#20445;&#30041;&#23436;&#25972;&#25968;&#25454;&#30340;&#22270;&#20687;&#21644;&#23383;&#24149;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;ConceptualCaptions3M&#21644;ConceptualCaptions12M&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;\method\&#25214;&#21040;&#30340;&#23376;&#38598;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#19979;&#19968;&#20010;&#26368;&#20339;&#22522;&#32447;&#25552;&#39640;&#20102;2.7&#20493;&#21644;1.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12267v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet an
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;&#65292;&#20197;&#22312;&#19981;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#27979;&#36317;&#21644;&#24863;&#30693;</title><link>https://arxiv.org/abs/2403.12254</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Adaptive LPD Radar Waveform Design with Generative Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12254
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;&#65292;&#20197;&#22312;&#19981;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#27979;&#36317;&#21644;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#29983;&#25104;&#20302;&#27010;&#29575;&#26816;&#27979;&#65288;LPD&#65289;&#38647;&#36798;&#27874;&#24418;&#65292;&#20351;&#20854;&#19982;&#20854;&#25805;&#20316;&#29615;&#22659;&#34701;&#20026;&#19968;&#20307;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#27874;&#24418;&#26088;&#22312;&#36981;&#24490;&#19968;&#20010;&#19982;&#29615;&#22659;&#20013;&#30340;&#26080;&#32447;&#30005;&#39057;&#29575;&#65288;RF&#65289;&#32972;&#26223;&#26080;&#27861;&#21306;&#20998;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20173;&#28982;&#26377;&#25928;&#29992;&#20110;&#27979;&#36317;&#21644;&#24863;&#30693;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23545;&#25239;&#24615;&#23398;&#20064;&#26694;&#26550;&#65307;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26088;&#22312;&#22256;&#24785;&#35780;&#35770;&#23478;&#32593;&#32476;&#30340;&#27874;&#24418;&#65292;&#35780;&#35770;&#23478;&#32593;&#32476;&#34987;&#20248;&#21270;&#20026;&#21306;&#20998;&#29983;&#25104;&#30340;&#27874;&#24418;&#19982;&#32972;&#26223;&#12290;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#29983;&#25104;&#30340;&#27874;&#24418;&#23545;&#20110;&#24863;&#30693;&#20173;&#28982;&#26377;&#25928;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#26368;&#23567;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#31946;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#21040;&#29983;&#25104;&#30340;&#27874;&#24418;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#29983;&#25104;&#30340;&#27874;&#24418;&#30340;&#21333;&#33033;&#20914;&#21487;&#26816;&#27979;&#24615;&#19982;&#20351;&#29992;&#21333;&#29420;&#35757;&#32451;&#30340;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#26469;&#27604;&#36739;&#20256;&#32479;LPD&#27874;&#24418;&#30340;&#24615;&#33021;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;ge
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12254v1 Announce Type: cross  Abstract: We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can ge
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;</title><link>https://arxiv.org/abs/2403.12237</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#30340;&#39640;&#25928;&#22522;&#20110;Transformer&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;IoT&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#36807;&#31243;&#23545;&#20110;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;HPO&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#20197;&#20854;&#21487;&#35266;&#30340;&#35745;&#31639;&#21344;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#32780;&#38395;&#21517;&#65307;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;Transformer&#26550;&#26500;&#21644;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;TRL-HPO&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;TRL-HPO&#37197;&#22791;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#21270;&#21644;&#28176;&#36827;&#29983;&#25104;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;TRL-HPO&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;CNN&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30456;&#21516;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;TRL-HPO&#30340;&#20998;&#31867;&#32467;&#26524;&#20248;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;6.8%&#65292;&#35777;&#26126;&#20102;TRL-HPO&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
&lt;/p&gt;</description></item><item><title>&#22312;&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#20316;&#20026;&#39564;&#35777;&#38598;&#65292;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12236</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22256;&#38590;&#26679;&#26412;&#19978;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization via Meta-Learning on Hard Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12236
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#20316;&#20026;&#39564;&#35777;&#38598;&#65292;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#37325;&#21152;&#26435;(LRW)&#26041;&#27861;&#29992;&#19968;&#20010;&#20248;&#21270;&#20934;&#21017;&#20026;&#35757;&#32451;&#23454;&#20363;&#20998;&#37197;&#26435;&#37325;&#65292;&#20197;&#20415;&#22312;&#19968;&#20010;&#20195;&#34920;&#24615;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#24418;&#24335;&#21270;&#20102;LRW&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#39564;&#35777;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39564;&#35777;&#38598;&#20013;&#20351;&#29992;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#26082;&#19982;&#29702;&#35770;&#30456;&#20851;&#65292;&#21448;&#26377;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#36825;&#20010;&#20803;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#27425;&#35757;&#32451;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#36827;&#34892;&#35880;&#24910;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#26131;&#39564;&#35777;&#25968;&#25454;&#19968;&#36215;&#30340;LRW&#34920;&#29616;&#22987;&#32456;&#27604;&#38590;&#39564;&#35777;&#25968;&#25454;&#19968;&#36215;&#30340;LRW&#34920;&#29616;&#24046;&#65292;&#20174;&#32780;&#30830;&#31435;&#20102;&#25105;&#20204;&#30340;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#25361;&#25112;&#19978;&#32988;&#36807;&#20102;&#21508;&#31181;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FloodCast&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#21355;&#26143;&#35266;&#27979;&#21644;&#27700;&#21160;&#21147;&#24314;&#27169;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#27946;&#27700;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#27714;&#35299;&#22120;&#65288;GeoPINS&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.12226</link><description>&lt;p&gt;
&#20351;&#29992;FloodCast&#36827;&#34892;&#22823;&#35268;&#27169;&#27946;&#27700;&#24314;&#27169;&#19982;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large-scale flood modeling and forecasting with FloodCast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FloodCast&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#21355;&#26143;&#35266;&#27979;&#21644;&#27700;&#21160;&#21147;&#24314;&#27169;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#27946;&#27700;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#27714;&#35299;&#22120;&#65288;GeoPINS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27700;&#21160;&#21147;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#31354;&#38388;&#32593;&#26684;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#20934;&#30830;&#39044;&#27979;&#27946;&#27700;&#23792;&#20540;&#24182;&#21457;&#24067;&#26102;&#38388;&#20851;&#38190;&#30340;&#21361;&#38505;&#35686;&#21578;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#31283;&#23450;&#12289;&#20934;&#30830;&#12289;&#20998;&#36776;&#29575;&#19981;&#21464;&#12289;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#27946;&#27700;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#19979;&#36816;&#34892;&#65292;&#21517;&#20026;FloodCast&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#22810;&#21355;&#26143;&#35266;&#27979;&#21644;&#27700;&#21160;&#21147;&#24314;&#27169;&#12290;&#22312;&#22810;&#21355;&#26143;&#35266;&#27979;&#27169;&#22359;&#20013;&#65292;&#25552;&#20986;&#20102;&#23454;&#26102;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#21644;&#38477;&#38632;&#22788;&#29702;&#19982;&#20998;&#26512;&#24037;&#20855;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22810;&#21355;&#26143;&#35266;&#27979;&#22312;&#22823;&#35268;&#27169;&#27946;&#27700;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#27700;&#21160;&#21147;&#24314;&#27169;&#27169;&#22359;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#27714;&#35299;&#22120;&#65288;GeoPINS&#65289;&#65292;&#21463;&#30410;&#20110;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12226v1 Announce Type: new  Abstract: Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for trainin
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21644;&#24179;&#26041;&#27861;&#30340;&#31169;&#26377;&#22270;&#20272;&#35745;&#31639;&#27861;&#39318;&#27425;&#23454;&#29616;&#20102;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#19982;&#20043;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#20855;&#26377;&#30456;&#21305;&#37197;&#30340;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.12213</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#27425;&#21644;&#26041;&#27861;&#36827;&#34892;&#31169;&#26377;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Private graphon estimation via sum-of-squares
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12213
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21644;&#24179;&#26041;&#27861;&#30340;&#31169;&#26377;&#22270;&#20272;&#35745;&#31639;&#27861;&#39318;&#27425;&#23454;&#29616;&#20102;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#19982;&#20043;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#20855;&#26377;&#30456;&#21305;&#37197;&#30340;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#23398;&#20064;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#22270;&#20272;&#35745;&#30340;&#31532;&#19968;&#20010;&#32431;&#33410;&#28857;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;&#20010;&#22359;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#12290;&#32479;&#35745;&#25928;&#29992;&#20445;&#35777;&#19982;&#20808;&#21069;&#26368;&#20339;&#30340;&#20449;&#24687;&#35770;&#65288;&#25351;&#25968;&#26102;&#38388;&#65289;&#33410;&#28857;&#31169;&#26377;&#26426;&#21046;&#30456;&#21305;&#37197;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#25351;&#25968;&#26426;&#21046;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23450;&#20041;&#20026;&#20381;&#36182;&#20110;&#22359;&#25968;&#37327;&#30340;&#20108;&#27425;&#21644;&#26494;&#24347;&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#65306;(1) &#22312;&#24418;&#24335;&#19978;&#23450;&#20041;&#20026;&#20108;&#27425;&#20248;&#21270;&#22312;&#21452;&#37325;&#38543;&#26426;&#30697;&#38453;&#30340;&#22810;&#32990;&#20307;&#19978;&#30340;&#36317;&#31163;&#30340;&#29305;&#24449;&#21270;&#22359;&#22270;&#23450;&#20041;&#65292;(2) &#19968;&#33324;&#30340;&#22810;&#39033;&#24335;&#20248;&#21270;&#30340;&#21644;&#24179;&#26041;&#27861;&#22312;&#20219;&#24847;&#22810;&#32990;&#20307;&#19978;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20197;&#21450;(3) &#25191;&#34892;&#21033;&#26222;&#24076;&#33576;&#25193;&#23637;&#30340;&#24471;&#20998;&#20989;&#25968;&#20316;&#20026;&#20108;&#27425;&#21644;&#31639;&#27861;&#33539;&#20363;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12213v1 Announce Type: cross  Abstract: We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26469;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12210</link><description>&lt;p&gt;
&#23558;&#25511;&#21046;Lyapunov&#20989;&#25968;&#20998;&#35299;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12210
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#26469;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#35757;&#32451;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#34920;&#29616;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;RL&#24182;&#26410;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#22330;&#26223;&#12290;&#26412;&#25991;&#26500;&#24314;&#20110;&#29616;&#26377;&#23558;RL&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#37325;&#22609;&#30340;&#24037;&#20316;&#22522;&#30784;&#20043;&#19978;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF&#65289;&#65292;&#35777;&#26126;&#33021;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#27492;&#20844;&#24335;&#38656;&#35201;&#30693;&#36947;&#31995;&#32479;&#30340;&#19968;&#20010;CLF&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#24120;&#24456;&#38590;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;CLF&#12290;&#29616;&#26377;&#24037;&#20316;&#21487;&#20197;&#36890;&#36807;&#21704;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;&#21487;&#36798;&#24615;&#31243;&#24207;&#35745;&#31639;&#20302;&#32500;CLFs&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#26041;&#27861;&#22312;&#39640;&#32500;&#31995;&#32479;&#19978;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#65292;&#36825;&#26159;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12210v1 Announce Type: cross  Abstract: Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;</title><link>https://arxiv.org/abs/2403.12206</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#25311;&#21512;&#30340;&#23454;&#29992;&#32039;&#20945;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Useful Compact Representations for Data-Fitting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;2&#38454;&#23548;&#25968;&#20449;&#24687;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#20013;&#65292;&#20272;&#35745;Hessian&#30697;&#38453;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25216;&#26415;&#29983;&#25104;&#30340;&#31264;&#23494;&#30697;&#38453;&#23545;&#20110;&#22823;&#22411;&#38382;&#39064;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#26377;&#38480;&#20869;&#23384;&#32039;&#20945;&#34920;&#31034;&#23558;&#31264;&#23494;&#25968;&#32452;&#34920;&#31034;&#20026;&#20302;&#31209;&#34920;&#31034;&#65292;&#24050;&#25104;&#20026;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#36719;&#20214;&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#21521;&#37327;&#36873;&#25321;&#21442;&#25968;&#21270;&#30340;&#26032;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#36873;&#25321;&#65292;&#23427;&#20204;&#21487;&#20197;&#31616;&#21270;&#20026;&#29616;&#26377;&#30340;&#33879;&#21517;&#20844;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32039;&#20945;&#34920;&#31034;&#22312;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12206v1 Announce Type: cross  Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;</title><link>https://arxiv.org/abs/2403.12203</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#30340;&#22686;&#24378;&#23398;&#20064;&#20026;&#22522;&#20110;&#35270;&#35273;&#30340;&#25935;&#25463;&#39134;&#34892;&#24341;&#23548;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12203
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26377;&#25928;&#24615;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#25928;&#29575;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35797;&#38169;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#22797;&#26434;&#25511;&#21046;&#22120;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#38754;&#20020;&#30528;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#32500;&#24230;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;IL&#22312;&#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#25928;&#29575;&#65292;&#20294;&#21463;&#21040;&#28436;&#31034;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#38754;&#20020;&#35832;&#22914;&#21327;&#21464;&#37327;&#28418;&#31227;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;RL&#21644;IL&#20248;&#21183;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#29305;&#26435;&#29366;&#24577;&#20449;&#24687;&#30340;&#24072;&#20613;&#31574;&#30053;&#30340;&#21021;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;IL&#23558;&#27492;&#31574;&#30053;&#33976;&#39311;&#20026;&#23398;&#29983;&#31574;&#30053;&#65292;&#20197;&#21450;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12203v1 Announce Type: cross  Abstract: We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tunin
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#25506;&#32034;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#31616;&#21333;&#30340;&#39034;&#24207;&#20989;&#25968;&#20018;&#32852;&#65292;&#36824;&#33021;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12201</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#20013;&#30340;&#20989;&#25968;&#26500;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional learning of functions in humans and machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12201
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#25506;&#32034;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#31616;&#21333;&#30340;&#39034;&#24207;&#20989;&#25968;&#20018;&#32852;&#65292;&#36824;&#33021;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#32452;&#25104;&#20989;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#20154;&#31867;&#22312;&#26377;&#25928;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#20854;&#33021;&#22815;&#28789;&#27963;&#27867;&#21270;&#65292;&#20363;&#22914;&#26681;&#25454;&#24050;&#30693;&#28921;&#39274;&#36807;&#31243;&#21019;&#36896;&#26032;&#33756;&#32948;&#12290;&#38500;&#20102;&#20989;&#25968;&#30340;&#39034;&#24207;&#38142;&#24335;&#20018;&#32852;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#23398;&#25991;&#29486;&#34920;&#26126;&#65292;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#20132;&#20114;&#20989;&#25968;&#32452;&#21512;&#65292;&#20854;&#20013;&#36755;&#20986;&#20135;&#29983;&#21462;&#20915;&#20110;&#30001;&#19981;&#21516;&#20989;&#25968;&#25490;&#24207;&#24341;&#36215;&#30340;&#19978;&#19979;&#25991;&#21464;&#21270;&#12290;&#23558;&#35843;&#26597;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20989;&#25968;&#23398;&#20064;&#33539;&#20363;&#65292;&#20197;&#25506;&#32034;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#20114;&#26465;&#20214;&#19979;&#23398;&#20064;&#21644;&#25512;&#29702;&#20855;&#26377;&#32452;&#21512;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#22312;&#23545;&#20010;&#20307;&#20989;&#25968;&#36827;&#34892;&#31616;&#35201;&#35757;&#32451;&#21518;&#65292;&#23545;&#20154;&#31867;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#32452;&#21512;&#20004;&#20010;&#23398;&#20064;&#36807;&#30340;&#20989;&#25968;&#65292;&#28085;&#30422;&#22235;&#31181;&#20027;&#35201;&#30340;&#20132;&#20114;&#31867;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#24212;&#29992;&#31532;&#19968;&#20010;&#20989;&#25968;&#20250;&#21019;&#24314;&#25110;&#21024;&#38500;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12201v1 Announce Type: new  Abstract: The ability to learn and compose functions is foundational to efficient learning and reasoning in humans, enabling flexible generalizations such as creating new dishes from known cooking processes. Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings. Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and reasoning with compositional functions under varied interaction conditions. Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the c
&lt;/p&gt;</description></item><item><title>FLex&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#25668;&#20687;&#26426;&#23039;&#21183;&#65292;&#25913;&#21892;&#20102;&#20869;&#31397;&#38236;&#37325;&#24314;&#30340;&#25928;&#29575;&#21644;&#22788;&#29702;&#35268;&#27169;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12198</link><description>&lt;p&gt;
FLex: &#29992;&#20110;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#30340;&#23039;&#21183;&#21644;&#21160;&#24577;&#36752;&#23556;&#22330;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12198
&lt;/p&gt;
&lt;p&gt;
FLex&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#20869;&#31397;&#38236;&#35270;&#39057;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#25668;&#20687;&#26426;&#23039;&#21183;&#65292;&#25913;&#21892;&#20102;&#20869;&#31397;&#38236;&#37325;&#24314;&#30340;&#25928;&#29575;&#21644;&#22788;&#29702;&#35268;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#31397;&#38236;&#22330;&#26223;&#30340;&#37325;&#24314;&#23545;&#20110;&#21508;&#31181;&#21307;&#30103;&#24212;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36164;&#20135;&#65292;&#20174;&#26415;&#21518;&#20998;&#26512;&#21040;&#25945;&#32946;&#22521;&#35757;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#28210;&#26579;&#22312;&#20855;&#26377;&#21464;&#24418;&#32452;&#32455;&#30340;&#20869;&#31397;&#38236;&#37325;&#24314;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35813;&#35774;&#32622;&#19968;&#30452;&#23616;&#38480;&#20110;&#38745;&#24577;&#20869;&#31397;&#38236;&#12289;&#26377;&#38480;&#30340;&#21464;&#24418;&#25110;&#38656;&#35201;&#22806;&#37096;&#36319;&#36394;&#35774;&#22791;&#26469;&#33719;&#21462;&#20869;&#31397;&#38236;&#30456;&#26426;&#30340;&#25668;&#20687;&#26426;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;FLex&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#21464;&#24418;&#32452;&#32455;&#29615;&#22659;&#20013;&#31227;&#21160;&#20869;&#31397;&#38236;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#38544;&#24335;&#22330;&#26223;&#20998;&#20026;&#22810;&#20010;&#37325;&#21472;&#30340;4D&#31070;&#32463;&#36752;&#23556;&#22330;(NeRFs)&#20197;&#21450;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#65292;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#20248;&#21270;&#37325;&#24314;&#21644;&#25668;&#20687;&#26426;&#23039;&#21183;&#12290;&#36825;&#25552;&#39640;&#20102;&#26131;&#29992;&#24615;&#65292;&#24182;&#20801;&#35768;&#25193;&#23637;&#37325;&#24314;&#33021;&#21147;&#20197;&#22788;&#29702;5,000&#24103;&#20197;&#19978;&#30340;&#25163;&#26415;&#35270;&#39057;&#65307;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12198v1 Announce Type: cross  Abstract: Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared 
&lt;/p&gt;</description></item><item><title>PETScML&#26694;&#26550;&#25645;&#24314;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#36719;&#20214;&#24037;&#20855;&#29992;&#20110;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#20256;&#32479;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12188</link><description>&lt;p&gt;
PETScML&#65306;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;&#38382;&#39064;&#30340;&#20108;&#38454;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
PETScML: Second-order solvers for training regression problems in Scientific Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12188
&lt;/p&gt;
&lt;p&gt;
PETScML&#26694;&#26550;&#25645;&#24314;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#36719;&#20214;&#24037;&#20855;&#29992;&#20110;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#20256;&#32479;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#30417;&#30563;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#27714;&#35299;&#30340;&#39640;&#24230;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20415;&#25658;&#24335;&#21644;&#21487;&#25193;&#23637;&#31185;&#23398;&#35745;&#31639;&#24037;&#20855;&#21253;&#30340;&#36731;&#37327;&#32423;&#36719;&#20214;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#21644;&#29992;&#20110;&#26080;&#32422;&#26463;&#26368;&#23567;&#21270;&#30340;&#20256;&#32479;&#27714;&#35299;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12188v1 Announce Type: new  Abstract: In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#20989;&#25968;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#36924;&#36817;&#30340;&#26222;&#36866;&#24615;&#65292;&#25512;&#23548;&#20102;&#36870;&#22810;&#37325;&#20108;&#27425;&#12289;&#39640;&#26031;&#21644;Sobolev&#26680;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#36924;&#36817;&#24191;&#20041;&#20989;&#25968;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.12187</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;RKHS&#20989;&#25968;&#22411;
&lt;/p&gt;
&lt;p&gt;
Approximation of RKHS Functionals by Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#20989;&#25968;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#36924;&#36817;&#30340;&#26222;&#36866;&#24615;&#65292;&#25512;&#23548;&#20102;&#36870;&#22810;&#37325;&#20108;&#27425;&#12289;&#39640;&#26031;&#21644;Sobolev&#26680;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#36924;&#36817;&#24191;&#20041;&#20989;&#25968;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#26102;&#38388;&#24207;&#21015;&#21644;&#22270;&#20687;&#31561;&#20016;&#23500;&#21151;&#33021;&#24615;&#25968;&#25454;&#30340;&#21551;&#21457;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#23558;&#36825;&#20123;&#25968;&#25454;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#20174;&#20989;&#25968;&#31354;&#38388;&#21040;R&#65288;&#21363;&#20989;&#25968;&#22411;&#65289;&#23398;&#20064;&#26144;&#23556;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#20989;&#25968;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;RKHS&#19978;&#20989;&#25968;&#22411;&#36924;&#36817;&#30340;&#26222;&#36866;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36890;&#36807;&#36870;&#22810;&#37325;&#20108;&#27425;&#12289;&#39640;&#26031;&#21644;Sobolev&#26680;&#24341;&#36215;&#30340;&#26126;&#30830;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#24212;&#29992;&#20110;&#20989;&#25968;&#22238;&#24402;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#36924;&#36817;&#24191;&#20041;&#20989;&#25968;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#26144;&#23556;&#12290;&#29616;&#26377;&#30340;&#21151;&#33021;&#24615;&#23398;&#20064;&#20316;&#21697;&#38656;&#35201;&#31215;&#20998;&#22411;&#22522;&#20989;&#25968;&#23637;&#24320;&#19982;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#22522;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;RKHS&#20013;&#21033;&#29992;&#25554;&#20540;&#27491;&#20132;&#25237;&#24433;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12187v1 Announce Type: cross  Abstract: Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12166</link><description>&lt;p&gt;
&#23569;&#25968;&#20010;&#20307;&#30340;&#21147;&#37327;&#65306;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#21152;&#36895;&#21644;&#20248;&#21270;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12166
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#26029;&#21457;&#23637;&#65292;&#36235;&#21183;&#26159;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#23558;&#35745;&#31639;&#25104;&#26412;&#25552;&#39640;&#21040;&#19981;&#21487;&#25345;&#32493;&#30340;&#27700;&#24179;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24494;&#22937;&#30340;&#24179;&#34913;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110; strategically selected coreset&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#20102;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#37325;&#26032;&#26657;&#20934;&#30340;&#26435;&#37325;&#34987;&#26144;&#23556;&#22238;&#24182;&#20256;&#25773;&#21040;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#22312;Dirichlet&#28151;&#21512;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#29992;&#20110;&#24555;&#36895;&#27169;&#22411;&#27604;&#36739;&#21644;&#31283;&#20581;&#20272;&#35745;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12158</link><description>&lt;p&gt;
&#21464;&#20998;&#26041;&#27861;&#29992;&#20110;Dirichlet&#28151;&#21512;&#27169;&#22411;&#20013;KL&#25955;&#24230;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12158
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#22312;Dirichlet&#28151;&#21512;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#29992;&#20110;&#24555;&#36895;&#27169;&#22411;&#27604;&#36739;&#21644;&#31283;&#20581;&#20272;&#35745;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;Dirichlet&#28151;&#21512;&#27169;&#22411;&#65288;DMM&#65289;&#20013;&#39640;&#25928;&#20272;&#35745;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#65292;&#36825;&#23545;&#20110;&#23545;&#25104;&#20998;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;DMM&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#33719;&#24471;KL&#25955;&#24230;&#30340;&#35299;&#26512;&#35299;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#36807;&#21435;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#31283;&#20581;&#30340;&#20272;&#35745;&#35780;&#20272;&#12290;&#23454;&#38469;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#39564;&#35777;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#20934;&#30830;&#65292;&#20026;&#24555;&#36895;&#25506;&#32034;&#19981;&#21516;DMM&#27169;&#22411;&#21644;&#25512;&#36827;&#25104;&#20998;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12158v1 Announce Type: cross  Abstract: This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for clustering compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#20998;&#31867;&#22120;DistClassiPy&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65292;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.12120</link><description>&lt;p&gt;
&#20351;&#29992;DistClassiPy&#36827;&#34892;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Light Curve Classification with DistClassiPy: a new distance-based classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#20998;&#31867;&#22120;DistClassiPy&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65292;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#25991;&#23398;&#39046;&#22495;&#30340;&#24033;&#22825;&#35843;&#26597;&#30340;&#20852;&#36215;&#24341;&#39046;&#20102;&#26102;&#22495;&#22825;&#25991;&#23398;&#20013;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20351;&#24471;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#30740;&#31350;&#22825;&#20307;&#23545;&#35937;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#36317;&#31163;&#24230;&#37327;&#26469;&#36741;&#21161;&#23545;&#35937;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#26032;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;DistClassiPy&#12290;&#30452;&#25509;&#20351;&#29992;&#36317;&#31163;&#24230;&#37327;&#26159;&#19968;&#31181;&#22312;&#26102;&#22495;&#22825;&#25991;&#23398;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20294;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23545;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;10&#20010;&#31867;&#21035;&#20013;&#30340;6000&#39063;&#21487;&#21464;&#26143;&#30340;&#30446;&#24405;&#24212;&#29992;18&#31181;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;&#21644;&#32500;&#24230;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12120v1 Announce Type: cross  Abstract: The rise of synoptic sky surveys has ushered in an era of big data in time-domain astronomy, making data science and machine learning essential tools for studying celestial objects. Tree-based (e.g. Random Forests) and deep learning models represent the current standard in the field. We explore the use of different distance metrics to aid in the classification of objects. For this, we developed a new distance metric based classifier called DistClassiPy. The direct use of distance metrics is an approach that has not been explored in time-domain astronomy, but distance-based methods can aid in increasing the interpretability of the classification result and decrease the computational costs. In particular, we classify light curves of variable stars by comparing the distances between objects of different classes. Using 18 distance metrics applied to a catalog of 6,000 variable stars in 10 classes, we demonstrate classification and dimensio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#30740;&#31350;&#22810;&#22495;&#32467;&#26500;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#39046;&#22495;&#24863;&#30693;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.12117</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for T-Cell Response Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12117
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#30740;&#31350;&#22810;&#22495;&#32467;&#26500;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#39046;&#22495;&#24863;&#30693;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29305;&#23450;&#32473;&#23450;&#32957;&#27573;&#30340;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#36825;&#21487;&#20197;&#26159;&#21521;&#20010;&#24615;&#21270;&#30284;&#30151;&#30123;&#33495;&#21457;&#23637;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12117v1 Announce Type: cross  Abstract: We study the prediction of T-cell response for specific given peptides, which could, among other applications, be a crucial step towards the development of personalized cancer vaccines. It is a challenging task due to limited, heterogeneous training data featuring a multi-domain structure; such data entail the danger of shortcut learning, where models learn general characteristics of peptide sources, such as the source organism, rather than specific peptide characteristics associated with T-cell response.   Using a transformer model for T-cell response prediction, we show that the danger of inflated predictive performance is not merely theoretical but occurs in practice. Consequently, we propose a domain-aware evaluation scheme. We then study different transfer learning techniques to deal with the multi-domain structure and shortcut learning. We demonstrate a per-source fine tuning approach to be effective across a wide range of peptid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12116</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#23450;&#20041;&#29983;&#29289;&#21551;&#21457;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25110;&#32773;&#37319;&#29992;&#36890;&#36807;&#31867;&#20284;Hebbian&#23398;&#20064;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#36880;&#23618;&#35757;&#32451;&#65292;&#20351;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#20860;&#23481;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#26368;&#32456;&#23618;&#30340;&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#65292;&#33258;&#21160;&#21270;&#27979;&#37327;&#33034;&#26609;&#20391;&#20984;&#30340; Cobb &#35282;&#24230;&#65292;&#25552;&#20379;&#28165;&#26224;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#27979;&#37327;&#32791;&#26102;&#19988;&#23384;&#22312;&#24046;&#24322;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12115</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#19987;&#23478;&#35266;&#23519;&#21592;&#30456;&#27604;&#33258;&#21160;&#21270;&#20102;Cobb&#35282;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep learning automates Cobb angle measurement compared with multi-expert observers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#65292;&#33258;&#21160;&#21270;&#27979;&#37327;&#33034;&#26609;&#20391;&#20984;&#30340; Cobb &#35282;&#24230;&#65292;&#25552;&#20379;&#28165;&#26224;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#27979;&#37327;&#32791;&#26102;&#19988;&#23384;&#22312;&#24046;&#24322;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#26609;&#20391;&#20984;&#26159;&#19968;&#31181;&#24120;&#35265;&#30142;&#30149;&#65292;&#20854;&#29305;&#28857;&#26159;&#33034;&#26609;&#24322;&#24120;&#26354;&#24230;&#23548;&#33268;&#30072;&#24418;&#65292;&#38656;&#35201;&#31934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#35786;&#26029;&#21644;&#31649;&#29702;&#12290; Cobb &#35282;&#24230;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33034;&#26609;&#20391;&#20984;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#20542;&#26012;&#26894;&#39592;&#20043;&#38388;&#30340;&#26354;&#24230;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27979;&#37327; Cobb &#35282;&#24230;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#30340;&#24046;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#26576;&#20123;&#29616;&#26377;&#33258;&#21160;&#21270;&#26041;&#27861;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#36719;&#20214;&#65292;&#19981;&#20165;&#21487;&#20197;&#31934;&#30830;&#27979;&#37327; Cobb &#35282;&#24230;&#65292;&#36824;&#21487;&#20197;&#28165;&#26224;&#21487;&#35270;&#21270;&#36825;&#20123;&#27979;&#37327;&#32467;&#26524;&#12290;&#35813;&#36719;&#20214;&#38598;&#25104;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33034;&#26609;&#21306;&#22495;&#26816;&#27979;&#21644;&#20998;&#21106;&#12289;&#33034;&#26609;&#20013;&#24515;&#32447;&#35782;&#21035;&#12289;&#26631;&#35760;&#26368;&#26126;&#26174;&#20542;&#26012;&#30340;&#26894;&#39592;&#20197;&#21450;&#22312;&#21407;&#22987;&#22270;&#20687;&#19978;&#30452;&#25509;&#21487;&#35270;&#21270; Cobb &#35282;&#24230;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12115v1 Announce Type: cross  Abstract: Scoliosis, a prevalent condition characterized by abnormal spinal curvature leading to deformity, requires precise assessment methods for effective diagnosis and management. The Cobb angle is a widely used scoliosis quantification method that measures the degree of curvature between the tilted vertebrae. Yet, manual measuring of Cobb angles is time-consuming and labor-intensive, fraught with significant interobserver and intraobserver variability. To address these challenges and the lack of interpretability found in certain existing automated methods, we have created fully automated software that not only precisely measures the Cobb angle but also provides clear visualizations of these measurements. This software integrates deep neural network-based spine region detection and segmentation, spine centerline identification, pinpointing the most significantly tilted vertebrae, and direct visualization of Cobb angles on the original images
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#33719;&#21462;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#21644;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#26469;&#22686;&#24378;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#23545;&#25239;&#25968;&#25454;&#28418;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.12109</link><description>&lt;p&gt;
GCAM: &#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#39640;&#26031;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCAM: Gaussian and causal-attention model of food fine-grained recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12109
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#39135;&#29289;&#32454;&#31890;&#24230;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#33719;&#21462;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#21644;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#26469;&#22686;&#24378;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#23545;&#25239;&#25968;&#25454;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#39135;&#29289;&#35782;&#21035;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26377;&#25928;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#39135;&#29289;&#26679;&#26412;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#35299;&#20915;&#39135;&#29289;&#35782;&#21035;&#20013;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#30340;&#36843;&#20999;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37319;&#29992;&#39640;&#26031;&#21644;&#22240;&#26524;&#20851;&#27880;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#29289;&#20307;&#35782;&#21035;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35757;&#32451;&#20197;&#33719;&#24471;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#39640;&#26031;&#29305;&#24449;&#65292;&#28982;&#21518;&#20174;&#23545;&#35937;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#20174;&#32780;&#22686;&#24378;&#30446;&#26631;&#21306;&#22495;&#30340;&#29305;&#24449;&#26144;&#23556;&#33021;&#21147;&#12290;&#20026;&#20102;&#23545;&#25239;&#30001;&#19981;&#22343;&#21248;&#25968;&#25454;&#20998;&#24067;&#23548;&#33268;&#30340;&#25968;&#25454;&#28418;&#31227;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#20064;&#30340;&#22270;&#20687;&#27880;&#24847;&#26426;&#21046;&#23545;&#32593;&#32476;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#33719;&#21462;&#26356;&#26377;&#29992;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#35782;&#21035;&#27880;&#24847;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12109v1 Announce Type: cross  Abstract: Currently, most food recognition relies on deep learning for category classification. However, these approaches struggle to effectively distinguish between visually similar food samples, highlighting the pressing need to address fine-grained issues in food recognition. To mitigate these challenges, we propose the adoption of a Gaussian and causal-attention model for fine-grained object recognition.In particular, we train to obtain Gaussian features over target regions, followed by the extraction of fine-grained features from the objects, thereby enhancing the feature mapping capabilities of the target regions. To counteract data drift resulting from uneven data distributions, we employ a counterfactual reasoning approach. By using counterfactual interventions, we analyze the impact of the learned image attention mechanism on network predictions, enabling the network to acquire more useful attention weights for fine-grained image recogn
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#26159;&#23545;Belief Propagation&#65288;BP&#65289;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#26469;&#38480;&#21046;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#22312;&#20108;&#20803;&#27010;&#29575;&#22270;&#19978;&#34920;&#29616;&#20248;&#20110;BP&#21644;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12106</link><description>&lt;p&gt;
&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#29992;&#20110;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Circular Belief Propagation for Approximate Probabilistic Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12106
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#26159;&#23545;Belief Propagation&#65288;BP&#65289;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#26469;&#38480;&#21046;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#22312;&#20108;&#20803;&#27010;&#29575;&#22270;&#19978;&#34920;&#29616;&#20248;&#20110;BP&#21644;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Belief Propagation&#65288;BP&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#30340;&#22270;&#20013;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#28040;&#24687;&#26469;&#23454;&#29616;&#12290;&#20854;&#31867;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#34920;&#26126;&#65292;&#23427;&#21487;&#33021;&#23545;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#24212;&#29992;&#20110;&#26080;&#29615;&#22270;&#26102;&#65292;BP&#20165;&#20165;&#26159;&#31934;&#30830;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#35813;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;CBP&#65289;&#65292;&#36825;&#26159;BP&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#36890;&#36807;&#23398;&#20064;&#26816;&#27979;&#21644;&#21462;&#28040;&#24490;&#29615;&#24341;&#36215;&#30340;&#28040;&#24687;&#21453;&#21709;&#32780;&#38480;&#21046;&#20102;&#38169;&#35823;&#30456;&#20851;&#21644;&#20449;&#24565;&#25918;&#22823;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#20108;&#20803;&#27010;&#29575;&#22270;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;CBP&#36828;&#36828;&#20248;&#20110;BP&#65292;&#24182;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12106v1 Announce Type: new  Abstract: Belief Propagation (BP) is a simple probabilistic inference algorithm, consisting of passing messages between nodes of a graph representing a probability distribution. Its analogy with a neural network suggests that it could have far-ranging applications for neuroscience and artificial intelligence. Unfortunately, it is only exact when applied to cycle-free graphs, which restricts the potential of the algorithm. In this paper, we propose Circular Belief Propagation (CBP), an extension of BP which limits the detrimental effects of message reverberation caused by cycles by learning to detect and cancel spurious correlations and belief amplifications. We show in numerical experiments involving binary probabilistic graphs that CBP far outperforms BP and reaches good performance compared to that of previously proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12100</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#26641;&#23398;&#20064;&#26102;&#38388;&#27573;&#20559;&#22909;&#36827;&#34892;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#31227;&#21160;&#26641;&#8221;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#65292;&#20197;&#25552;&#21319;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#24403;&#21069;&#30340;&#31614;&#21040;&#36712;&#36857;&#25552;&#20379;POI&#30340;&#21160;&#24577;&#25490;&#21517;&#12290;&#26412;&#20219;&#21153;&#30340;&#25512;&#33616;&#24615;&#33021;&#21462;&#20915;&#20110;&#36890;&#36807;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#65288;LBSNs&#65289;&#25968;&#25454;&#20840;&#38754;&#20102;&#35299;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31227;&#21160;&#26641;&#8221;&#30340;&#21019;&#26032;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#20998;&#23618;&#25551;&#36848;&#29992;&#25143;&#30340;&#31614;&#21040;&#35760;&#24405;&#12290;&#31227;&#21160;&#26641;&#21253;&#21547;&#22810;&#31890;&#24230;&#26102;&#38388;&#27573;&#33410;&#28857;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#36328;&#19981;&#21516;&#26102;&#38388;&#27573;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31227;&#21160;&#26641;&#32593;&#32476;&#65288;MTNet&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12100v1 Announce Type: cross  Abstract: Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#29983;&#25104;&#35774;&#35745;&#20013;&#65292;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#23558;&#22797;&#26434;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#28040;&#38500;&#19981;&#21487;&#21046;&#36896;&#29305;&#24615;&#65292;&#23558;&#37325;&#28857;&#25918;&#22312;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#21046;&#36896;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.12098</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Design for Mass Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#29983;&#25104;&#35774;&#35745;&#20013;&#65292;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#23558;&#22797;&#26434;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#28040;&#38500;&#19981;&#21487;&#21046;&#36896;&#29305;&#24615;&#65292;&#23558;&#37325;&#28857;&#25918;&#22312;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#21046;&#36896;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35774;&#35745;&#65288;GD&#65289;&#20316;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35774;&#35745;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#31639;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#26469;&#21019;&#36896;&#36229;&#36234;&#20256;&#32479;&#38480;&#21046;&#30340;&#22810;&#26679;&#21270;&#21644;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#35774;&#35745;&#22312;&#22797;&#26434;&#35774;&#35745;&#30340;&#21487;&#21046;&#36896;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#20462;&#25913;&#65292;&#22240;&#20026;&#26631;&#20934;&#21046;&#36896;&#36807;&#31243;&#23384;&#22312;&#38480;&#21046;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24182;&#19981;&#36866;&#21512;&#22823;&#35268;&#27169;&#29983;&#20135;&#30340;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#19982;&#21387;&#38136;&#21644;&#27880;&#22609;&#30456;&#20851;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;GD&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#28145;&#24230;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#21487;&#21046;&#36896;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#22797;&#26434;&#30340;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#31616;&#21270;&#20026;&#21487;&#21046;&#36896;&#30340;&#36718;&#24275;&#65292;&#21435;&#38500;&#19981;&#21487;&#21046;&#36896;&#30340;&#24748;&#25361;&#31561;&#19981;&#21487;&#34892;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#30452;&#25509;&#32771;&#34385;&#21402;&#24230;&#21644;&#32907;&#35774;&#35745;&#31561;&#37325;&#35201;&#21046;&#36896;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12098v1 Announce Type: cross  Abstract: Generative Design (GD) has evolved as a transformative design approach, employing advanced algorithms and AI to create diverse and innovative solutions beyond traditional constraints. Despite its success, GD faces significant challenges regarding the manufacturability of complex designs, often necessitating extensive manual modifications due to limitations in standard manufacturing processes and the reliance on additive manufacturing, which is not ideal for mass production. Our research introduces an innovative framework addressing these manufacturability concerns by integrating constraints pertinent to die casting and injection molding into GD, through the utilization of 2D depth images. This method simplifies intricate 3D geometries into manufacturable profiles, removing unfeasible features such as non-manufacturable overhangs and allowing for the direct consideration of essential manufacturing aspects like thickness and rib design. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12094</link><description>&lt;p&gt;
LLMs&#26159;&#19968;&#20010;&#22909;&#30340;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#27714;&#35299;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Good Cryptic Crossword Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#26159;&#19968;&#31181;&#35868;&#39064;&#65292;&#19981;&#20165;&#20381;&#36182;&#20110;&#19968;&#33324;&#30693;&#35782;&#65292;&#36824;&#20381;&#36182;&#20110;&#27714;&#35299;&#32773;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#25805;&#32437;&#35821;&#35328;&#24182;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#23383;&#28216;&#25103;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;NLP&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#31867;&#35868;&#39064;&#20063;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#27969;&#34892;&#30340;LLMs -- LLaMA2&#12289;Mistral&#21644;ChatGPT&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#26174;&#31034;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12090</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Foundation Models and Information Retrieval in Digital Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12090
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#12289;LLM&#12289;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#26816;&#32034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12090v1 Announce Type: cross  Abstract: The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.12082</link><description>&lt;p&gt;
&#29983;&#36824;&#30340;&#30007;&#23401;&#65306;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#27604;&#25253;&#36947;&#30340;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#21457;&#29616;&#65292;&#20174;LLM&#20013;&#21024;&#38500;&#21704;&#21033;&#27874;&#29305;&#20869;&#23481;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;"&#25105;&#20204;&#26377;&#25928;&#22320;&#25273;&#38500;&#20102;&#27169;&#22411;&#29983;&#25104;&#25110;&#22238;&#24518;&#21704;&#21033;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;"&#28982;&#32780;&#65292;&#19968;&#39033;&#23567;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#35828;&#27861;&#36807;&#20110;&#23485;&#27867;&#12290;&#23569;&#20110;&#21313;&#27425;&#35797;&#39564;&#23548;&#33268;&#37325;&#22797;&#21644;&#20855;&#20307;&#25552;&#21450;&#21704;&#21033;&#27874;&#29305;&#65292;&#21253;&#25324;"&#21834;&#65292;&#25105;&#26126;&#30333;&#20102;&#65281;"&#40635;&#29916;"&#26159;&#29305;&#37324;&#183;&#26222;&#25289;&#20999;&#29305;&#30340;&#21704;&#21033;&#27874;&#29305;&#31995;&#21015;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;...''&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12082v1 Announce Type: cross  Abstract: Recent work arXiv.2310.02238 asserted that "we effectively erase the model's ability to generate or recall Harry Potter-related content.'' This claim is shown to be overbroad. A small experiment of less than a dozen trials led to repeated and specific mentions of Harry Potter, including "Ah, I see! A "muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22320;&#36136;&#32972;&#26223;&#23545;&#33258;&#21160;&#38684;&#20923;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#35266;&#23519;&#21040;&#30340;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12080</link><description>&lt;p&gt;
&#22312;&#28779;&#26143;&#21487;&#35265;&#20809;&#21355;&#26143;&#35266;&#27979;&#20013;&#35780;&#20272;&#22320;&#24418;&#20381;&#36182;&#24615;&#23545;&#38684;&#20923;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating Terrain-Dependent Performance for Martian Frost Detection in Visible Satellite Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22320;&#36136;&#32972;&#26223;&#23545;&#33258;&#21160;&#38684;&#20923;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#35266;&#23519;&#21040;&#30340;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#26143;&#34920;&#38754;&#30340;&#23395;&#33410;&#24615;&#32467;&#38684;&#21644;&#34701;&#20912;&#34987;&#20551;&#35774;&#25512;&#21160;&#30528;&#27668;&#20505;&#36807;&#31243;&#20197;&#21450;&#24418;&#25104;&#21644;&#28436;&#21270;&#22320;&#35980;&#29305;&#24449;&#65292;&#22914;&#20914;&#27807;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#26469;&#33258;&#36712;&#36947;&#39640;&#20998;&#36776;&#29575;&#21487;&#35265;&#20809;&#35266;&#27979;&#25163;&#21160;&#20998;&#26512;&#28779;&#26143;&#21271;&#21322;&#29699;&#20013;&#32428;&#24230;&#22320;&#21306;&#38684;&#20923;&#21608;&#26399;&#30340;&#34892;&#20026;&#12290;&#20840;&#29699;&#25193;&#23637;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#21033;&#29992;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33258;&#21160;&#26816;&#27979;&#38684;&#20923;&#12290;&#28982;&#32780;&#65292;&#38684;&#20923;&#23384;&#22312;&#30340;&#21487;&#35265;&#25351;&#26631;&#21487;&#33021;&#20250;&#26681;&#25454;&#38684;&#20923;&#25152;&#21472;&#21152;&#30340;&#22320;&#36136;&#32972;&#26223;&#32780;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#24615;&#33021;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#65292;(2)&#35828;&#26126;&#20102;&#22320;&#36136;&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#33258;&#21160;&#38684;&#20923;&#26816;&#27979;&#65292;(3)&#25552;&#20986;&#20102;&#32531;&#35299;&#33258;&#21160;&#38684;&#20923;&#26816;&#27979;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12080v1 Announce Type: cross  Abstract: Seasonal frosting and defrosting on the surface of Mars is hypothesized to drive both climate processes and the formation and evolution of geomorphological features such as gullies. Past studies have focused on manually analyzing the behavior of the frost cycle in the northern mid-latitude region of Mars using high-resolution visible observations from orbit. Extending these studies globally requires automating the detection of frost using data science techniques such as convolutional neural networks. However, visible indications of frost presence can vary significantly depending on the geologic context on which the frost is superimposed. In this study, we (1) present a novel approach for spatially partitioning data to reduce biases in model performance estimation, (2) illustrate how geologic context affects automated frost detection, and (3) propose mitigations to observed biases in automated frost detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#27969;&#27966;&#26159;&#24433;&#21709;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26102;&#38388;&#36235;&#21183;&#21644;&#29305;&#24449;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.12079</link><description>&lt;p&gt;
&#36229;&#36234;&#33410;&#22863;&#65306;&#27468;&#26354;&#27969;&#34892;&#30340;&#31192;&#35776;&#65311;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Beats: A Recipe to Song Popularity? A machine learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25506;&#35752;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#27969;&#27966;&#26159;&#24433;&#21709;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26102;&#38388;&#36235;&#21183;&#21644;&#29305;&#24449;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#27969;&#34892;&#24230;&#39044;&#27979;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#36825;&#24471;&#30410;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21644;Spotify&#31561;&#27969;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#28085;&#30422;1957&#24180;&#33267;2020&#24180;&#21508;&#31181;&#27969;&#27966;&#30340;30,000&#39318;&#27468;&#26354;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#27468;&#26354;&#27969;&#34892;&#24230;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#12289;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#65288;MARS&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31639;&#27861;&#26469;&#20998;&#26512;&#27468;&#26354;&#29305;&#24449;&#21450;&#20854;&#23545;&#27969;&#34892;&#24230;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#22238;&#24402;&#20998;&#26512;&#34920;&#26126;&#27969;&#27966;&#26159;&#27969;&#34892;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#32780;&#19988;&#38543;&#26102;&#38388;&#21487;&#35265;&#26174;&#33879;&#30340;&#36235;&#21183;&#12290;MARS&#24314;&#27169;&#31361;&#20986;&#20102;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#19982;&#29305;&#24449;&#22914;&#22120;&#20048;&#24230;&#21644;&#26102;&#38271;&#30456;&#20851;&#12290;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#27169;&#22411;&#24378;&#35843;&#20102;&#27969;&#27966;&#65292;&#23588;&#20854;&#26159;&#30005;&#23376;&#33310;&#26354;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12079v1 Announce Type: cross  Abstract: Music popularity prediction has garnered significant attention in both industry and academia, fuelled by the rise of data-driven algorithms and streaming platforms like Spotify. This study aims to explore the predictive power of various machine learning models in forecasting song popularity using a dataset comprising 30,000 songs spanning different genres from 1957 to 2020. Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse song characteristics and their impact on popularity. Results: Ordinary Least Squares (OLS) regression analysis reveals genre as the primary influencer of popularity, with notable trends over time. MARS modelling highlights the complex relationship between variables, particularly with features like instrumentalness and duration. Random Forest and XGBoost models underscore the importance of genre, especially EDM, in predict
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;</title><link>https://arxiv.org/abs/2403.12076</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuron-centric Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#23398;&#20064;&#26426;&#21046;&#32972;&#21518;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#32467;&#26500;&#21644;&#21151;&#33021;&#21487;&#22609;&#24615;&#35843;&#25972;&#20854;&#31361;&#35302;&#12290;&#23613;&#31649;&#31361;&#35302;&#22312;&#20256;&#36882;&#20449;&#24687;&#21040;&#25972;&#20010;&#22823;&#33041;&#20013;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#65292;&#20294;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#26159;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20135;&#29983;&#20102;&#23545;&#31361;&#35302;&#30340;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#35774;&#35745;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22914;ABCD&#35268;&#21017;&#65292;&#20391;&#37325;&#20110;&#31361;&#35302;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#20248;&#21270;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#20248;&#21270;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31361;&#35302;&#37117;&#19982;&#22810;&#20010;&#36203;&#24067;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#65288;NcHL&#65289;&#65292;&#20854;&#20248;&#21270;&#20391;&#37325;&#20110;&#31070;&#32463;&#20803;&#32780;&#19981;&#26159;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#19982;ABCD&#35268;&#21017;&#30456;&#27604;&#65292;NcHL&#23558;&#21442;&#25968;&#20943;&#23569;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20110;&#25968;&#37327;&#30340;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.12074</link><description>&lt;p&gt;
&#36229;&#36234;&#25968;&#37327;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#36136;&#37327;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12074
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20110;&#25968;&#37327;&#30340;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23545;&#22478;&#24066;&#22320;&#21306;&#30340;&#22522;&#30784;&#35774;&#26045;&#36136;&#37327;&#19981;&#24179;&#31561;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#24050;&#32463;&#24847;&#35782;&#21040;&#29305;&#24449;&#21270;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#20197;&#25351;&#23548;&#22478;&#24066;&#21457;&#23637;&#35268;&#21010;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#37327;&#22522;&#30784;&#35774;&#26045;&#30340;&#25968;&#37327;&#19978;&#65292;&#20551;&#23450;&#26356;&#22810;&#30340;&#22522;&#30784;&#35774;&#26045;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25351;&#25968;&#30340;&#26041;&#27861;&#19978;&#65292;&#20854;&#20013;&#22478;&#24066;&#22320;&#21306;&#22522;&#30784;&#35774;&#26045;&#20379;&#32473;&#29366;&#20917;&#26159;&#26681;&#25454;&#35774;&#23450;&#30340;&#20027;&#35266;&#26435;&#37325;&#30830;&#23450;&#30340;&#12290;&#23545;&#22522;&#30784;&#35774;&#26045;&#25968;&#37327;&#30340;&#20851;&#27880;&#21644;&#20351;&#29992;&#26469;&#33258;&#20027;&#35266;&#26435;&#37325;&#30340;&#25351;&#25968;&#24050;&#32463;&#22952;&#30861;&#20102;&#36866;&#24403;&#22320;&#30740;&#31350;&#22522;&#30784;&#35774;&#26045;&#19981;&#24179;&#31561;&#19982;&#22478;&#24066;&#19981;&#24179;&#31561;&#21644;&#29615;&#22659;&#27491;&#20041;&#32771;&#34385;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12074v1 Announce Type: cross  Abstract: The objective of this study is to characterize inequality in infrastructure quality across urban areas. While a growing of body of literature has recognized the importance of characterizing infrastructure inequality in cities and provided quantified metrics to inform urban development plans, the majority of the existing approaches focus primarily on measuring the quantity of infrastructure, assuming that more infrastructure is better. Also, the existing research focuses primarily on index-based approaches in which the status of infrastructure provision in urban areas is determined based on assumed subjective weights. The focus on infrastructure quantity and use of indices obtained from subjective weights has hindered the ability to properly examine infrastructure inequality as it pertains to urban inequality and environmental justice considerations. Recognizing this gap, we propose a machine learning-based approach in which infrastruct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.12072</link><description>&lt;p&gt;
Floralens&#65306;&#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Floralens: a Deep Learning Model for the Portuguese Native Flora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20013;&#23545;&#29983;&#29289;&#29289;&#31181;&#36827;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36275;&#22815;&#22823;&#23567;&#21644;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#32593;&#32476;&#20197;&#21450;&#32593;&#32476;&#26550;&#26500;&#30340;&#36873;&#25321;&#26412;&#36523;&#20173;&#28982;&#24456;&#23569;&#26377;&#25991;&#29486;&#35760;&#24405;&#65292;&#22240;&#27492;&#19981;&#23481;&#26131;&#34987;&#22797;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#35895;&#27468;&#30340;AutoML Vision&#20113;&#26381;&#21153;&#25552;&#20379;&#30340;&#29616;&#25104;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#26159;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#65292;&#22522;&#20110;&#30001;&#33889;&#33796;&#29273;&#26893;&#29289;&#23398;&#20250;&#25552;&#20379;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26469;&#33258;iNaturalist&#12289;Pl@ntNet&#21644;Observation.org&#30340;&#37319;&#38598;&#25968;&#25454;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35880;&#24910;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12072v1 Announce Type: cross  Abstract: Machine-learning techniques, namely deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. However, the construction of critically sized and sampled datasets to train the networks and the choice of the network architectures itself remains little documented and, therefore, does not lend itself to be easily replicated. In this paper, we develop a streamlined methodology for building datasets for biological taxa from publicly available research-grade datasets and for deriving models from these datasets using off-the-shelf deep convolutional neural networks such as those provided by Google's AutoML Vision cloud service. Our case study is the Portuguese native flora, anchored in a high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica, scaled up by adding sampled data from iNaturalist, Pl@ntNet, and Observation.org. We find that with a careful
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.12069</link><description>&lt;p&gt;
&#32570;&#20047;&#22320;&#38754;&#30495;&#30456;&#24773;&#20917;&#19979;&#25552;&#21319;&#24314;&#27169;&#30340;&#20844;&#24179;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#37319;&#29992;&#21152;&#36895;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#36827;&#34892;&#35780;&#20272;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#22320;&#38754;&#30495;&#30456;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20811;&#26381;&#32570;&#23569;&#22320;&#38754;&#30495;&#30456;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#26367;&#20195;&#21697;&#26469;&#20805;&#24403;&#25552;&#21319;&#24314;&#27169;&#27963;&#21160;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#20195;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#26367;&#20195;&#22320;&#38754;&#30495;&#30456;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12069v1 Announce Type: cross  Abstract: The acceleration in the adoption of AI-based automated decision-making systems poses a challenge for evaluating the fairness of algorithmic decisions, especially in the absence of ground truth. When designing interventions, uplift modeling is used extensively to identify candidates that are likely to benefit from treatment. However, these models remain particularly susceptible to fairness evaluation due to the lack of ground truth on the outcome measure since a candidate cannot be in both treatment and control simultaneously. In this article, we propose a framework that overcomes the missing ground truth problem by generating surrogates to serve as a proxy for counterfactual labels of uplift modeling campaigns. We then leverage the surrogate ground truth to conduct a more comprehensive binary fairness evaluation. We show how to apply the approach in a comprehensive study from a real-world marketing campaign for promotional offers and d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30005;&#23376;&#23398;&#20064;&#36807;&#31243;&#20013;&#24212;&#29992;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;&#23398;&#29983;&#30340;&#33258;&#20027;&#35843;&#33410;&#23398;&#20064;&#36807;&#31243;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#25945;&#23398;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.12068</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#23398;&#20064;&#20013;&#21033;&#29992;&#36807;&#31243;&#25366;&#25496;&#36827;&#34892;&#33258;&#20027;&#35843;&#33410;&#23398;&#20064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Process mining for self-regulated learning assessment in e-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12068
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30005;&#23376;&#23398;&#20064;&#36807;&#31243;&#20013;&#24212;&#29992;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;&#23398;&#29983;&#30340;&#33258;&#20027;&#35843;&#33410;&#23398;&#20064;&#36807;&#31243;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#25945;&#23398;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#37324;&#65292;&#30005;&#23376;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#20869;&#23481;&#35780;&#20272;&#24471;&#21040;&#20102;&#24191;&#27867;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#24046;&#36317;&#65292;&#36825;&#23545;&#20110;&#19981;&#20165;&#20869;&#23481;&#35780;&#20272;&#65292;&#36824;&#20851;&#20110;&#23398;&#29983;&#26680;&#24515;&#25216;&#33021;&#22914;&#33258;&#20027;&#23398;&#20064;&#30340;&#35780;&#20272;&#25552;&#20986;&#20102;&#26377;&#36259;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#21457;&#29616;&#30005;&#23376;&#23398;&#20064;&#35838;&#31243;&#20013;&#23398;&#29983;&#30340;&#33258;&#20027;&#35843;&#33410;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;Moodle 2.0&#24179;&#21488;&#19978;&#30340;&#19968;&#20010;&#23398;&#26399;&#35838;&#31243;&#20013;&#24212;&#29992;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#31216;&#20026;Inductive Miner&#30340;&#26032;&#31639;&#27861;&#23545;101&#21517;&#22823;&#23398;&#29983;&#30340;&#20114;&#21160;&#36857;&#32447;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#25552;&#21462;&#33258;&#24179;&#21488;&#20107;&#20214;&#26085;&#24535;&#20013;&#30340;21629&#26465;&#36857;&#32447;&#25968;&#25454;&#65292;&#21457;&#29616;&#20102;&#26377;&#21161;&#20110;&#25913;&#21892;&#25945;&#23398;&#36807;&#31243;&#30340;&#23398;&#29983;&#33258;&#25105;&#35843;&#33410;&#27169;&#22411;&#12290;Inductive Miner&#31639;&#27861;&#22312;&#27492;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20102;&#36866;&#21512;&#36890;&#36807;&#21644;&#26410;&#36890;&#36807;&#23398;&#29983;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#20004;&#32773;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12068v1 Announce Type: cross  Abstract: Content assessment has broadly improved in e-learning scenarios in recent decades. However, the eLearning process can give rise to a spatial and temporal gap that poses interesting challenges for assessment of not only content, but also students' acquisition of core skills such as self-regulated learning. Our objective was to discover students' self-regulated learning processes during an eLearning course by using Process Mining Techniques. We applied a new algorithm in the educational domain called Inductive Miner over the interaction traces from 101 university students in a course given over one semester on the Moodle 2.0 platform. Data was extracted from the platform's event logs with 21629 traces in order to discover students' self-regulation models that contribute to improving the instructional process. The Inductive Miner algorithm discovered optimal models in terms of fitness for both Pass and Fail students in this dataset, as we
&lt;/p&gt;</description></item><item><title>&#23558;Segment Anything Model (SAM)&#19982;&#22522;&#20110;&#20999;&#29255;&#30340;Flood Filling Networks (FFN)&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#20307;&#31215;X&#23556;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12066</link><description>&lt;p&gt;
&#36866;&#24212;&#20219;&#24847;&#23610;&#23544;&#20307;&#31215;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;SAM&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting SAM for Volumetric X-Ray Data-sets of Arbitrary Sizes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12066
&lt;/p&gt;
&lt;p&gt;
&#23558;Segment Anything Model (SAM)&#19982;&#22522;&#20110;&#20999;&#29255;&#30340;Flood Filling Networks (FFN)&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#20307;&#31215;X&#23556;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Segment Anything Model (SAM)&#19982;&#22522;&#20110;&#20999;&#29255;&#30340;Flood Filling Networks (FFN)&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25968;&#25454;&#20013;&#30340;&#20307;&#31215;&#23454;&#20363;&#20998;&#21106;&#65292;&#29992;&#20110;&#38750;&#30772;&#22351;&#24615;&#27979;&#35797;&#65288;NDT&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35780;&#20272;&#20102;SAM&#22312;&#20307;&#31215;NDT&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25104;&#20687;&#22330;&#26223;&#20013;&#23545;&#23454;&#20363;&#36827;&#34892;&#20998;&#21106;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;&#25193;&#23637;&#22522;&#20110;&#22270;&#20687;&#30340;SAM&#31639;&#27861;&#20197;&#29992;&#20110;&#20307;&#31215;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#21033;&#29992;FFN&#30340;&#31354;&#38388;&#36866;&#24212;&#24615;&#26469;&#23454;&#29616;&#19977;&#32500;&#23545;&#35937;&#30340;&#20998;&#21106;&#12290;&#22522;&#20110;&#20999;&#29255;&#30340;SAM&#26041;&#27861;&#21033;&#29992;&#20102;FFN&#30340;&#33021;&#21147;&#26469;&#20998;&#21106;&#20219;&#20309;&#22823;&#23567;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#23494;&#38598;&#25552;&#31034;&#25351;&#23548;SAM&#23558;&#20998;&#21106;&#30340;&#20999;&#29255;&#32452;&#21512;&#20197;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;SAM&#19982;FFN&#32467;&#21512;&#29992;&#20110;&#20307;&#31215;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12066v1 Announce Type: cross  Abstract: Objective: We propose a new approach for volumetric instance segmentation in X-ray Computed Tomography (CT) data for Non-Destructive Testing (NDT) by combining the Segment Anything Model (SAM) with tile-based Flood Filling Networks (FFN). Our work evaluates the performance of SAM on volumetric NDT data-sets and demonstrates its effectiveness to segment instances in challenging imaging scenarios. Methods: We implemented and evaluated techniques to extend the image-based SAM algorithm fo the use with volumetric data-sets, enabling the segmentation of three-dimensional objects using FFN's spatially adaptability. The tile-based approach for SAM leverages FFN's capabilities to segment objects of any size. We also explore the use of dense prompts to guide SAM in combining segmented tiles for improved segmentation accuracy. Results: Our research indicates the potential of combining SAM with FFN for volumetric instance segmentation tasks, part
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#65292;&#20197;&#25913;&#36827;&#23545;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#20013;&#21518;&#39564;&#26679;&#26412;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12063</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#25913;&#36827;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Consistency Models Improve Diffusion Inverse Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#65292;&#20197;&#25913;&#36827;&#23545;&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#20013;&#21518;&#39564;&#26679;&#26412;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#36870;&#27714;&#35299;&#22120;&#65288;DIS&#65289;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#22312;&#25193;&#25955;&#20808;&#39564;&#31354;&#38388;&#20013;&#30340;&#22270;&#20687;$x$&#65292;&#28385;&#36275;&#32422;&#26463;$f(x)=y$&#65292;&#32473;&#23450;&#31639;&#23376;$f(\cdot)$&#21644;&#27979;&#37327;$y$&#12290;&#22823;&#22810;&#25968;&#38750;&#32447;&#24615;DIS&#20351;&#29992;&#21518;&#39564;&#22343;&#20540;$\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$&#26469;&#35780;&#20272;$f(\cdot)$&#24182;&#26368;&#23567;&#21270;&#36317;&#31163;$||f(\hat{x}_{0|t})-y||^2$&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#21518;&#39564;&#22343;&#20540;&#30340;&#36317;&#31163;&#26159;&#26377;&#20559;&#30340;&#65307;&#32780;&#21518;&#39564;&#26679;&#26412;$x_{0|t}\sim p_{\theta}(x_0|x_t)$&#25215;&#35834;&#26159;&#26356;&#22909;&#30340;&#20505;&#36873;&#12290;&#26412;&#25991;&#39318;&#20808;&#28548;&#28165;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#21518;&#39564;&#26679;&#26412;&#26356;&#22909;&#65306;$1)$&#24403;$f(\cdot)$&#26159;&#32447;&#24615;&#30340;&#26102;&#65292;&#20351;&#29992;&#21518;&#39564;&#22343;&#20540;&#30340;&#36317;&#31163;&#23601;&#20687;&#21333;&#20010;&#21518;&#39564;&#26679;&#26412;&#19968;&#26679;&#22909;&#65292;&#22240;&#27492;&#26356;&#21487;&#21462;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#33945;&#29305;&#21345;&#27931;&#65307;$2)$&#24403;$f(\cdot)$&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#20351;&#29992;&#21518;&#39564;&#26679;&#26412;&#30340;&#36317;&#31163;&#26356;&#22909;&#12290;&#30001;&#20110;&#20808;&#21069;&#23545;&#21518;&#39564;&#26679;&#26412;&#30340;&#36924;&#36817;&#19981;&#20687;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#20316;&#20026;&#39640;&#36136;&#37327;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12063v1 Announce Type: cross  Abstract: Diffusion inverse solvers (DIS) aim to find an image $x$ that lives on the diffusion prior while satisfying the constraint $f(x) = y$, given an operator $f(.)$ and measurement $y$. Most non-linear DIS use posterior mean $\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$ to evaluate $f(.)$ and minimize the distance $||f(\hat{x}_{0|t})-y||^2$. Previous works show that posterior mean-based distance is biased; instead, posterior sample $x_{0|t}\sim p_{\theta}(x_0|x_t)$ promises a better candidate. In this paper, we first clarify when is posterior sample better: $1)$ When $f(.)$ is linear, the distance with posterior mean is as good as single posterior sample, thus preferable as it does not require Monte Carlo; $2)$ When $f(.)$ is non-linear, the distance using posterior sample is better. As previous approximations to posterior sample do not look like a real image, we propose to use consistency model (CM) as a high quality approximation. In addition, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OralH&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#21475;&#33108;&#30142;&#30149;&#26816;&#27979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21475;&#33108;&#25195;&#25551;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#33719;&#24471;&#24555;&#36895;&#30340;&#21475;&#33108;&#20581;&#24247;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.12044</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#21475;&#33108;&#30142;&#30149;&#26816;&#27979;&#30340;&#31227;&#21160;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mobile Application for Oral Disease Detection using Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OralH&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#21475;&#33108;&#30142;&#30149;&#26816;&#27979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21475;&#33108;&#25195;&#25551;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#33719;&#24471;&#24555;&#36895;&#30340;&#21475;&#33108;&#20581;&#24247;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22068;&#24052;&#32463;&#24120;&#34987;&#35270;&#20026;&#36523;&#20307;&#20869;&#37096;&#29366;&#24577;&#30340;&#19968;&#25159;&#31383;&#25143;&#65292;&#23545;&#21453;&#26144;&#19968;&#20010;&#20154;&#30340;&#25972;&#20307;&#20581;&#24247;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21475;&#33108;&#21355;&#29983;&#19981;&#20339;&#20250;&#23548;&#33268;&#20005;&#37325;&#30142;&#30149;&#65292;&#22914;&#24515;&#33039;&#30149;&#12289;&#30284;&#30151;&#21644;&#31958;&#23615;&#30149;&#65292;&#32780;&#19981;&#24688;&#24403;&#30340;&#25252;&#29702;&#20250;&#23548;&#33268;&#19981;&#36866;&#12289;&#30140;&#30171;&#21644;&#26114;&#36149;&#30340;&#27835;&#30103;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#21487;&#29992;&#20110;&#27492;&#29992;&#20363;&#65292;&#22240;&#24739;&#32773;&#21475;&#33108;&#22270;&#20687;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#12290;FL&#36890;&#36807;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#23384;&#20648;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#22270;&#20687;&#24182;&#22312;&#36793;&#32536;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#26356;&#26032;&#30340;&#26435;&#37325;&#34987;&#32852;&#21512;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20854;&#20013;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#26435;&#37325;&#36890;&#36807;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#36827;&#34892;&#26356;&#26032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OralH&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#21475;&#33108;&#25195;&#25551;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#24182;&#25552;&#20379;&#24555;&#36895;&#30340;&#21475;&#33108;&#20581;&#24247;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12044v1 Announce Type: cross  Abstract: The mouth, often regarded as a window to the internal state of the body, plays an important role in reflecting one's overall health. Poor oral hygiene has far-reaching consequences, contributing to severe conditions like heart disease, cancer, and diabetes, while inadequate care leads to discomfort, pain, and costly treatments. Federated Learning (FL) for object detection can be utilized for this use case due to the sensitivity of the oral image data of the patients. FL ensures data privacy by storing the images used for object detection on the local device and trains the model on the edge. The updated weights are federated to a central server where all the collected weights are updated via The Federated Averaging algorithm. Finally, we have developed a mobile app named OralH which provides user-friendly solutions, allowing people to conduct self-assessments through mouth scans and providing quick oral health insights. Upon detection o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2403.12007</link><description>&lt;p&gt;
&#30830;&#23450;&#36890;&#36807;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25552;&#39640;&#30284;&#30151;&#24739;&#32773;&#31119;&#31049;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#65288;DBCI&#65289;&#27491;&#22312;&#25903;&#25345;&#26032;&#20581;&#24247;&#34892;&#20026;&#30340;&#21457;&#23637;&#12290;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#23545;&#20110;&#25913;&#36827;&#23427;&#20204;&#21644;&#29702;&#35299;&#25104;&#21151;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#20262;&#29702;&#38480;&#21046;&#30340;&#23567;&#35268;&#27169;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#20840;&#38754;&#25351;&#23548;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;CAPABLE&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;DBCI&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34913;&#37327;&#21442;&#19982;&#24230;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#23545;DBCI&#30340;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#35780;&#20272;DBCI&#24433;&#21709;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#29983;&#30340;&#22788;&#26041;&#26174;&#30528;&#22686;&#21152;&#20102;&#24739;&#32773;&#23545;&#31227;&#21160;DBCI&#30340;&#25345;&#32493;&#21442;&#19982;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#27599;&#21608;&#19968;&#27425;&#21442;&#19982;DBCI&#23601;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20174;&#22806;&#22312;&#21160;&#26426;&#21521;&#20869;&#22312;&#21160;&#26426;&#30340;&#36716;&#21464;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11395</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automated data processing and feature engineering for deep learning and big data applications: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11395
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#24182;&#22312;AI&#30340;&#21457;&#23637;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#29305;&#21035;&#26159;&#22312;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#23427;&#20063;&#31616;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#22240;&#20026;&#23398;&#20064;&#36807;&#31243;&#26159;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#37117;&#24050;&#33258;&#21160;&#21270;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24517;&#39035;&#22312;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20043;&#21069;&#32463;&#36807;&#25163;&#21160;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#33258;&#21160;&#21270;&#36825;&#20123;&#20219;&#21153;&#30340;&#29305;&#27530;&#25216;&#26415;&#12290;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#39537;&#21160;&#21147;&#26159;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#12289;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;A
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11366</link><description>&lt;p&gt;
JORA: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;
&lt;/p&gt;
&lt;p&gt;
JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#30340;JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#65292;&#36890;&#36807;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#21644;&#24352;&#37327;&#20998;&#29255;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#31649;&#29702;&#65292;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;RAG&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;JORA: JAX&#24352;&#37327;&#24182;&#34892;LoRA&#24211;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#12299;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20219;&#21153;&#30340;PEFT&#20860;&#23481;&#24494;&#35843;Llama-2&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;JAX&#30340;&#21363;&#26102;&#32534;&#35793;&#65288;JIT&#65289;&#21644;&#24352;&#37327;&#20998;&#29255;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#39640;&#25928;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#24494;&#35843;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#22797;&#26434;RAG&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#29978;&#33267;&#22312;GPU&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#21033;&#29992;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.11087</link><description>&lt;p&gt;
&#32467;&#21512;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Incorporating Higher-order Structural Information for Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#21033;&#29992;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#20855;&#26377;&#28145;&#36828;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#32858;&#31867;&#24037;&#20855;&#23853;&#38706;&#22836;&#35282;&#65292;&#38598;&#25104;&#20102;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#33410;&#28857;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#12290;&#26174;&#28982;&#65292;&#20301;&#20110;&#21516;&#19968;&#32858;&#31867;&#20013;&#30340;&#33410;&#28857;&#21487;&#20197;&#24314;&#31435;&#36828;&#36317;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#24212;&#29992;&#33258;&#30417;&#30563;&#27169;&#22359;&#26469;&#30417;&#25511;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20165;&#20851;&#27880;&#33410;&#28857;&#23646;&#24615;&#32780;&#24573;&#30053;&#22270;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#20114;&#20449;&#24687;&#26497;&#22823;&#21270;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#20102;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20803;&#33258;&#30417;&#30563;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11087v1 Announce Type: new  Abstract: Clustering holds profound significance in data mining. In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes. However, most existing methods ignore the higher-order structural information of the graph. Evidently, nodes within the same cluster can establish distant connections. Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure. In this paper, we propose a novel graph clustering network to make full use of graph structural information. To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includ
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#23545;GlassNet&#27169;&#22411;&#22312;&#39044;&#27979;&#29627;&#29827;&#31283;&#23450;&#24615;&#21442;&#25968;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#36825;&#20123;&#21442;&#25968;&#26469;&#20272;&#35745;&#29627;&#29827;&#30340;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10682</link><description>&lt;p&gt;
&#23545;GlassNet&#22312;&#29627;&#29827;&#31283;&#23450;&#24615;&#21644;&#24418;&#25104;&#33021;&#21147;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10682
&lt;/p&gt;
&lt;p&gt;
&#23545;GlassNet&#27169;&#22411;&#22312;&#39044;&#27979;&#29627;&#29827;&#31283;&#23450;&#24615;&#21442;&#25968;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#36825;&#20123;&#21442;&#25968;&#26469;&#20272;&#35745;&#29627;&#29827;&#30340;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29627;&#29827;&#26500;&#25104;&#20102;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20063;&#22312;&#26410;&#26469;&#21307;&#30103;&#21644;&#29615;&#22659;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#32452;&#25104;&#31354;&#38388;&#20351;&#24471;&#23545;&#26576;&#20123;&#24212;&#29992;&#36827;&#34892;&#35774;&#35745;&#21644;&#20248;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29627;&#29827;&#21152;&#24037;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#20272;&#35745;&#32473;&#23450;&#32452;&#25104;&#30340;&#29627;&#29827;&#25104;&#24418;&#33021;&#21147;&#65288;GFA&#65289;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#29627;&#29827;&#24418;&#25104;&#30340;&#29289;&#29702;&#26426;&#21046;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#20043;&#35868;&#65292;&#29305;&#21035;&#26159;&#22312;&#27687;&#21270;&#29627;&#29827;&#20013;&#12290;&#26174;&#32780;&#26131;&#35265;&#65292;&#29992;&#20110;&#20272;&#35745;GFA&#30340;&#20195;&#29702;&#23646;&#24615;&#23558;&#22312;&#29627;&#29827;&#21152;&#24037;&#21644;&#35774;&#35745;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23547;&#25214;&#36825;&#26679;&#19968;&#20010;&#26367;&#20195;&#24615;&#23646;&#24615;&#24050;&#34987;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;NN&#27169;&#22411;GlassNet&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#35745;&#31639;&#29627;&#29827;&#31283;&#23450;&#24615;&#65288;GS&#65289;&#25152;&#38656;&#30340;&#29305;&#24449;&#28201;&#24230;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#36825;&#20123;&#29289;&#29702;&#21551;&#21457;&#24335;ML&#65288;PIML&#65289;&#39044;&#27979;&#30340;GS&#21442;&#25968;&#26469;&#20272;&#35745;&#24418;&#25104;&#33021;&#21147;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10682v1 Announce Type: cross  Abstract: Glasses form the basis of many modern applications and also hold great potential for future medical and environmental applications. However, their structural complexity and large composition space make design and optimization challenging for certain applications. Of particular importance for glass processing is an estimate of a given composition's glass-forming ability (GFA). However, there remain many open questions regarding the physical mechanisms of glass formation, especially in oxide glasses. It is apparent that a proxy for GFA would be highly useful in glass processing and design, but identifying such a surrogate property has proven itself to be difficult. Here, we explore the application of an open-source pre-trained NN model, GlassNet, that can predict the characteristic temperatures necessary to compute glass stability (GS) and assess the feasibility of using these physics-informed ML (PIML)-predicted GS parameters to estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.10089</link><description>&lt;p&gt;
&#29992;&#20110;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximation and bounding techniques for the Fisher-Rao distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#30340;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#34987;&#23450;&#20041;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#35825;&#23548;&#30340;Riemannian&#27979;&#22320;&#36317;&#31163;&#12290;&#20026;&#20102;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Fisher-Rao&#36317;&#31163;&#65292;&#25105;&#20204;&#38656;&#35201;&#65288;1&#65289;&#25512;&#23548;&#20986;Fisher-Rao&#27979;&#22320;&#32447;&#30340;&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27839;&#30528;&#36825;&#20123;&#27979;&#22320;&#32447;&#31215;&#20998;Fisher&#38271;&#24230;&#20803;&#32032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#38381;&#21512;&#24418;&#24335;1D Fisher-Rao&#36317;&#31163;&#25253;&#21578;&#20102;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#36817;&#20284;&#26041;&#26696;&#65292;&#21462;&#20915;&#20110;Fisher-Rao&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#33021;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#25552;&#20379;Fisher-Rao&#39044;&#27979;&#27979;&#22320;&#32447;&#21644;&#20005;&#26684;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#26102;&#36817;&#20284;&#20135;&#29983;&#20219;&#24847;&#23567;&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09673</link><description>&lt;p&gt;
FoldToken&#65306;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21450;&#26356;&#22810;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
FoldToken: Learning Protein Language via Vector Quantization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09673
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#21516;&#26102;&#25551;&#36848;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#22806;&#35821;&#65311;&#30001;&#20110;&#36830;&#32493;3D&#28857;&#34920;&#31034;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#31163;&#25955;&#24207;&#21015;&#30340;&#23545;&#27604;&#24314;&#27169;&#26041;&#24335;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{FoldTokenizer}&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#28041;&#21450;&#23558;&#27531;&#22522;&#31867;&#22411;&#21644;&#32467;&#26500;&#25237;&#23556;&#21040;&#19968;&#20010;&#31163;&#25955;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#20449;&#24687;&#20445;&#23384;&#30340;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#31526;&#21495;&#31216;&#20026;\textbf{FoldToken}&#65292;&#32780;FoldTokens&#30340;&#24207;&#21015;&#21017;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#24418;&#24577;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#24212;&#29992;&#20110;&#26222;&#36890;&#20027;&#24178;&#20462;&#34917;&#21644;&#25239;&#20307;&#35774;&#35745;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#39318;&#20010;GPT&#39118;&#26684;&#27169;&#22411;(\textbf{FoldGPT})&#29992;&#20110;&#20855;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09673v1 Announce Type: cross  Abstract: Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancem
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09499</link><description>&lt;p&gt;
&#20351;&#29992;Q&#23398;&#20064;&#30340;&#22902;&#29275;&#20859;&#27542;&#22330;&#30005;&#27744;&#31649;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22902;&#29275;&#20859;&#27542;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#65292;&#26159;&#20892;&#19994;&#20013;&#19968;&#20010;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#37096;&#38376;&#12290;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#38598;&#25104;&#21040;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#30005;&#27744;&#31649;&#29702;&#23545;&#20110;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#33267;&#20851;&#37325;&#35201;&#12290;&#31649;&#29702;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#30001;&#20110;&#30005;&#33021;&#28040;&#32791;&#30340;&#27874;&#21160;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#38388;&#27463;&#24615;&#20197;&#21450;&#33021;&#28304;&#20215;&#26684;&#30340;&#27874;&#21160;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#65292;&#28982;&#32780;&#22312;&#36825;&#19968;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;&#29233;&#23572;&#20848;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#20197;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#20026;&#26680;&#24515;&#30340;2030&#24180;&#33021;&#28304;&#25112;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23433;&#25490;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08438</link><description>&lt;p&gt;
&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#65306;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#21644;&#21487;&#20877;&#29616;&#24615;&#30340;&#22256;&#38590;&#36817;&#24180;&#26469;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#38656;&#35201;&#21487;&#20877;&#29616;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#21516;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20419;&#36827;&#20102;&#24320;&#25918;&#21644;&#21487;&#35775;&#38382;&#30340;&#30740;&#31350;&#12289;&#31283;&#20581;&#30340;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#26032;&#21457;&#29616;&#30340;&#24555;&#36895;&#25972;&#21512;&#12290;&#35780;&#20272;&#30740;&#31350;&#20986;&#29256;&#29289;&#25903;&#25345;&#20877;&#29616;&#24615;&#30340;&#31243;&#24230;&#26159;&#26412;&#25991;&#30340;&#19968;&#20010;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#21162;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32500;&#24230;&#35781;&#21650;&#65292;&#23427;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#26356;&#38590;&#25214;&#21040;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#37327;&#21270;&#20102;&#32676;&#20307;&#19981;&#24179;&#34913;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#21644;&#24179;&#22343;&#20197;&#21450;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.07310</link><description>&lt;p&gt;
&#25512;&#21160;&#23569;&#25968;&#32676;&#20307;&#20221;&#39069;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#65311;&#20851;&#20110;&#19968;&#23618;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#32676;&#20307;&#19981;&#24179;&#34913;&#19978;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#37327;&#21270;&#20102;&#32676;&#20307;&#19981;&#24179;&#34913;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#21644;&#24179;&#22343;&#20197;&#21450;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#19981;&#24179;&#34913;&#19968;&#30452;&#26159;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#20013;&#24050;&#30693;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21462;&#24471;&#30340;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#20276;&#38543;&#30528;&#23569;&#25968;&#32676;&#20307;&#30340;&#20302;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#26377;&#31639;&#27861;&#21162;&#21147;&#25913;&#21892;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20851;&#20110;ERM&#22312;&#21508;&#20010;&#32676;&#20307;&#19978;&#30340;&#29702;&#35770;&#27867;&#21270;&#20998;&#26512;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#36890;&#36807;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#34920;&#36798;&#32676;&#20307;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#21508;&#20010;&#32676;&#20307;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#20197;&#21450;&#24179;&#22343;&#21644;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#38598;&#20013;&#22312;&#20351;&#29992;&#19968;&#23618;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#38500;&#20102;&#36890;&#24120;&#30740;&#31350;&#30340;&#24179;&#22343;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#19968;&#20123;&#35265;&#35299;&#21253;&#25324;&#24403;&#25152;&#26377;&#32676;&#20307;&#32423;&#21327;&#26041;&#24046;&#37117;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07310v1 Announce Type: cross  Abstract: Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in th
&lt;/p&gt;</description></item><item><title>RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06420</link><description>&lt;p&gt;
RLingua&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06420
&lt;/p&gt;
&lt;p&gt;
RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#20197;&#20854;&#20302;&#26679;&#26412;&#25928;&#29575;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLingua&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;RL&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25552;&#21462;LLMs&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#21021;&#27493;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#19981;&#23436;&#32654;&#65292;LLM&#29983;&#25104;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#34987;&#29992;&#20110;&#22312;rollout&#26102;&#20197;&#34928;&#20943;&#27010;&#29575;&#29983;&#25104;&#21160;&#20316;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#24182;&#20462;&#25913;&#20102;&#28436;&#21592;&#25439;&#22833;&#65292;&#20197;&#20351;&#31574;&#30053;&#23398;&#20064;&#26397;&#30528;LLM&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#35268;&#33539;&#21270;&#12290;RLingua&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#21892;&#19981;&#23436;&#32654;&#30340;LLM&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RLing
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.04720</link><description>&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#24322;&#36136;&#24615;&#34920;&#26684;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#20803;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20803;&#29305;&#24449;&#65292;&#20363;&#22914;&#65292;&#32479;&#35745;&#37327;&#25110;&#26631;&#24535;&#28857;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22914;Dataset2Vec&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;liltab&#21253;&#20013;&#65292;&#35813;&#21253;&#21487;&#22312;GitHub&#19978;&#25214;&#21040;https://github.com/azoz01/liltab&#12290;&#25105;&#20204;&#30340;&#21253;&#22522;&#20110;[Iwata and Kumagai, 2020]&#25552;&#20986;&#30340;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#34920;&#26684;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#22914;Dataset2Vec &#30340;&#32534;&#30721;&#29305;&#24449;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#35780;&#20215;&#20102;Dataset2Vec&#21644;liltab
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20219;&#21153;&#25512;&#23548;&#21644;&#20869;&#23384;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.02628</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;: &#24555;&#36895;&#19982;&#32531;&#24930;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Interactive Continual Learning: Fast and Slow Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20219;&#21153;&#25512;&#23548;&#21644;&#20869;&#23384;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#29983;&#21629;&#24418;&#24335;&#36890;&#36807;&#31070;&#32463;&#35748;&#30693;&#26426;&#21046;&#30340;&#21327;&#21516;&#20114;&#21160;&#65292;&#32456;&#36523;&#19981;&#26029;&#22320;&#33719;&#21462;&#21644;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#22312;&#27169;&#25311;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#36890;&#36807;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#26412;&#25991;&#22522;&#20110;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;ICL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21508;&#31181;&#35268;&#27169;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#20132;&#20114;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ViT&#27169;&#22411;&#25351;&#23450;&#20026;&#31532;&#19968;&#31995;&#32479;&#65292;&#23558;&#22810;&#27169;&#24577;LLM&#25351;&#23450;&#20026;&#31532;&#20108;&#31995;&#32479;&#12290;&#20026;&#20102;&#20351;&#20869;&#23384;&#27169;&#22359;&#33021;&#22815;&#20174;&#31867;&#20449;&#24687;&#20013;&#25512;&#23548;&#20219;&#21153;&#24182;&#22686;&#24378;Set2Set&#26816;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Class-Knowledge-Task Multi-Head Attention (CKT-MHA)&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36890;&#36807;&#22686;&#24378;&#30340;&#20960;&#20309;&#26816;&#32034;&#25913;&#36827;&#31532;&#19968;&#31995;&#32479;&#30340;&#20869;&#23384;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02628v1 Announce Type: cross  Abstract: Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric r
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#31572;&#20102;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#21644;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#38754;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20197;&#21450;&#21028;&#23450;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#24615;&#30340;NP&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16814</link><description>&lt;p&gt;
&#20999;&#21106;&#38754;&#21644;&#31435;&#26041;&#38754;&#30340;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;
&lt;/p&gt;
&lt;p&gt;
Cut Facets and Cube Facets of Lifted Multicut Polytopes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#31572;&#20102;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#21644;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#38754;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20197;&#21450;&#21028;&#23450;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#24615;&#30340;NP&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#38382;&#39064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26377;&#30528;&#22810;&#26679;&#30340;&#24212;&#29992;&#12290;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#31934;&#30830;&#31639;&#27861;&#38656;&#35201;&#23545;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#26377;&#25152;&#20102;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#22810;&#38754;&#20307;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#19968;&#30452;&#26410;&#33021;&#35299;&#20915;&#65306;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#23450;&#20041;&#20102;&#38754;&#65292;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#20102;&#38754;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24517;&#35201;&#12289;&#20805;&#20998;&#19988;&#39640;&#25928;&#21487;&#21028;&#23450;&#30340;&#26465;&#20214;&#26469;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;&#33267;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#34920;&#26126;&#21028;&#26029;&#20999;&#21106;&#19981;&#31561;&#24335;&#30340;&#38754;&#23450;&#20041;&#24615;&#26159;NP&#38590;&#30340;&#12290;&#36825;&#23436;&#25104;&#20102;&#23545;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#35268;&#33539;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16814v2 Announce Type: replace-cross  Abstract: The lifted multicut problem has diverse applications in the field of computer vision. Exact algorithms based on linear programming require an understanding of lifted multicut polytopes. Despite recent progress, two fundamental questions about these polytopes have remained open: Which lower cube inequalities define facets, and which cut inequalities define facets? In this article, we answer the first question by establishing conditions that are necessary, sufficient and efficiently decidable. Toward the second question, we show that deciding facet-definingness of cut inequalities is NP-hard. This completes the analysis of canonical facets of lifted multicut polytopes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14817</link><description>&lt;p&gt;
&#25668;&#20687;&#22836;&#20316;&#20026;&#23556;&#32447;: &#36890;&#36807;&#23556;&#32447;&#25193;&#25955;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Cameras as Rays: Pose Estimation via Ray Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#30456;&#26426;&#23039;&#21183;&#26159;3D&#37325;&#24314;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#37492;&#20110;&#35270;&#22270;&#31232;&#30095;&#65288;&lt;10&#65289;&#65292;&#35813;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#36861;&#27714;&#30456;&#26426;&#22806;&#21442;&#30340;&#20840;&#23616;&#21442;&#25968;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20801;&#35768;&#19982;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#32039;&#23494;&#32806;&#21512;&#65292;&#25552;&#39640;&#20102;&#23039;&#21183;&#31934;&#24230;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#34920;&#31034;&#33258;&#28982;&#36866;&#29992;&#20110;&#38598;&#21512;&#32423;&#21035;&#30340;Transformer&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#22359;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23556;&#32447;&#19978;&#12290;&#20026;&#20102;&#25429;&#25417;&#31232;&#30095;&#35270;&#35282;&#23039;&#21183;&#25512;&#26029;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37319;&#26679;&#21512;&#29702;&#30340;&#27169;&#24335;&#65292;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26082;&#26159;&#22522;&#20110;&#22238;&#24402;&#65292;&#20063;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#65292;&#22312;CO3D&#30456;&#26426;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (&lt;10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12518</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Neural Additive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#26377;&#26102;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#21457;&#23637;&#20986;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAMs&#65289;&#26159;&#22312;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#21521;&#19978;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NAM&#23376;&#31867;&#65292;&#23427;&#20351;&#29992;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#12290;GP-NAM&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20248;&#21183;&#12290;&#19982;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#25439;&#22833;&#65292;&#22240;&#20026;GPs&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#21442;&#25968;&#21333;&#21464;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GP-NAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.11137</link><description>&lt;p&gt;
TuneTables&#65306;&#21487;&#25193;&#23637;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#20998;&#31867;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#65288;PFN&#65289;&#30340;&#31361;&#30772;&#24615;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;PFN&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#22312;&#26032;&#20219;&#21153;&#19978;&#21462;&#24471;&#24378;&#22823;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PFN&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;TabPFN&#22312;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#38750;&#24120;&#24378;&#21170;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#22823;&#20110;1000&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;PFN&#24320;&#21457;&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;PFN&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TuneTables&#65292;&#19968;&#31181;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#26032;&#22411;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;TuneTables&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#24182;&#19988;&#22312;&#26377;&#30028;&#30340;&#20248;&#21270;&#39046;&#22495;&#20013;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.07793</link><description>&lt;p&gt;
&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tuning-Free Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#24182;&#19988;&#22312;&#26377;&#30028;&#30340;&#20248;&#21270;&#39046;&#22495;&#20013;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20351;&#24471;&#35843;&#21442;&#30340;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#26114;&#12290;&#36825;&#23548;&#33268;&#20102;&#38656;&#35201;&#33021;&#22815;&#21363;&#26102;&#33258;&#25105;&#35843;&#25972;&#30340;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#8220;&#26080;&#35843;&#21442;&#8221;&#31639;&#27861;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#65292;&#21363;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#21363;&#21487;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#35823;&#24046;&#20026;&#23545;&#25968;&#22810;&#39033;&#24335;&#22240;&#23376;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#33021;&#22815;&#19982;&#26368;&#20248;&#35843;&#21442;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#12290;&#24403;&#20248;&#21270;&#30340;&#22495;&#26159;&#26377;&#30028;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35843;&#21442;&#33258;&#30001;&#19982;SGD&#30340;&#21305;&#37197;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#20248;&#21270;&#30340;&#22495;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#65292;&#23545;&#20110;&#26368;&#23567;&#21270;&#20984;&#24179;&#28369;&#25110;&#32773;Lipschitz&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#26080;&#35843;&#21442;&#20248;&#21270;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#65292;&#20309;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26080;&#35843;&#21442;&#20248;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340; DoG &#21644; DoWG &#31639;&#27861;&#22312;&#22122;&#22768;&#20998;&#24067;&#36275;&#22815;&#26102;&#26159;&#26080;&#35843;&#21442;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05946</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#65306;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24322;&#24120;&#20107;&#20214;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#29702;&#35299;&#24322;&#24120;&#20107;&#20214;&#32972;&#21518;&#30340;&#22240;&#26524;&#21407;&#22240;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#31361;&#28982;&#21464;&#21270;&#12290;&#25581;&#31034;&#22240;&#26524;&#21407;&#22240;&#26377;&#21161;&#20110;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#25581;&#31034;&#35299;&#37322;&#35266;&#23519;&#20107;&#20214;&#30340;&#8220;&#22914;&#26524;-&#37027;&#20040;&#8221;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#28857;&#36807;&#31243;&#26469;&#24314;&#27169;&#25152;&#20851;&#27880;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19968;&#32452;&#28508;&#22312;&#35268;&#21017;&#26469;&#35299;&#37322;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#22312;E&#27493;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#34987;&#27599;&#20010;&#21457;&#29616;&#30340;&#35268;&#21017;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#26356;&#26032;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#21487;&#33021;&#24615;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#24494;&#20998;&#30340;&#26041;&#24335;&#20248;&#21270;&#35268;&#21017;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#35268;&#21017;&#21644;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04906</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#24178;&#39044;&#25928;&#26524;&#65292;&#21363;&#27835;&#30103;&#25928;&#26524;&#65292;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524; (CATE) &#20272;&#35745;&#31561;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#32780;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931; (CMC) &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644; CATE &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20135;&#29983;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#22122;&#22768;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;CMC&#26694;&#26550;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#21306;&#38388;&#23485;&#24230;&#65292;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02464</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#21315;&#35328;&#65306;&#20351;&#29992;&#32431;Transformer&#23558;&#22270;&#24418;&#27431;&#25289;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GraphsGPT&#65292;&#23427;&#20351;&#29992;&#32431;Transformer&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#30340;&#22270;&#24418;&#21333;&#35789;&#65292;&#24182;&#36890;&#36807;&#35299;&#30721;&#22120;&#23558;&#22270;&#24418;&#21333;&#35789;&#37325;&#26032;&#26500;&#24314;&#20026;&#21407;&#22987;&#22270;&#24418;&#65292;&#20445;&#35777;&#20102;&#20449;&#24687;&#30340;&#31561;&#20215;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;GraphsGPT&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#21644;&#22270;&#24418;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#20986;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#24314;&#27169;&#20026;&#32431;&#35821;&#35328;&#29978;&#33267;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#20449;&#24687;&#65311;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#19968;&#30452;&#26159;&#22270;&#24418;&#24314;&#27169;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;GNN&#21644;Graphformer&#21162;&#21147;&#23558;&#22270;&#24418;&#32534;&#30721;&#20026;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#65292;&#20294;&#20174;&#21521;&#37327;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#22270;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GraphsGPT&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#22270;&#24418;&#36716;&#25442;&#20026;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#21487;&#23398;&#20064;&#22270;&#24418;&#21333;&#35789;&#30340;Graph2Seq&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#22270;&#24418;&#21333;&#35789;&#37325;&#26500;&#21407;&#22987;&#22270;&#24418;&#20197;&#30830;&#20445;&#20449;&#24687;&#31561;&#20215;&#24615;&#30340;GraphGPT&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;100M&#20010;&#20998;&#23376;&#19978;&#39044;&#35757;&#32451;&#20102;GraphsGPT&#65292;&#24182;&#24471;&#21040;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;(1) &#39044;&#35757;&#32451;&#30340;Graph2Seq&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;8/9&#20010;&#22270;&#24418;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;(2) &#39044;&#35757;&#32451;&#30340;GraphGPT&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#29983;&#25104;&#22120;&#65292;&#20854;&#33021;&#22815;&#36827;&#34892;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;(3) Graph2Seq+Gr
&lt;/p&gt;
&lt;p&gt;
Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02334</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#38656;&#35201;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#20013;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;AMFormer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#21040;&#26368;&#36817;&#65292;&#20851;&#20110;&#28145;&#24230;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24402;&#32435;&#20559;&#35265;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#28145;&#24230;&#34920;&#26684;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#36731;&#24494;&#29305;&#24449;&#20132;&#20114;&#20551;&#35774;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#31639;&#26415;&#29305;&#24449;&#20132;&#20114;&#65292;&#31216;&#20026;AMFormer&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AMFormer&#22312;&#32454;&#31890;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#24378;&#23545;&#25163;&#12290;&#36825;&#24402;&#22240;&#20110;&#20854;&#24182;&#34892;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#20855;&#26377;&#31639;&#26415;&#24037;&#31243;&#29305;&#24449;&#30340;&#25193;&#23637;&#31354;&#38388;&#20013;&#20998;&#31163;&#34920;&#26684;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20063;&#39564;&#35777;&#20102;AMFormer&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#21512;&#29702;&#24615;&#65292;&#34920;&#26126;&#23427;&#24050;&#32463;&#24314;&#31435;&#20102;&#24378;&#26377;&#21147;&#30340;&#24402;&#32435;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2401.11576</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantum Architecture Search with Unsupervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#20195;&#34920;&#20102;&#19968;&#31181;&#21069;&#27839;&#26041;&#27861;&#65292;&#26377;&#26395;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#23454;&#29616;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#22823;&#22810;&#25968;QAS&#31639;&#27861;&#23558;&#23427;&#20204;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#35780;&#20272;&#22823;&#37327;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22522;&#20110;&#39044;&#27979;&#30340;QAS&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#26681;&#25454;&#30005;&#36335;&#32467;&#26500;&#20272;&#35745;&#30005;&#36335;&#30340;&#24615;&#33021;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#30340;&#39044;&#27979;&#22120;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#33719;&#24471;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#32463;&#20856;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;Arch2vec&#21551;&#21457;&#25105;&#20204;&#65292;&#34920;&#26126;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#20174;&#23558;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19982;&#25628;&#32034;&#36807;&#31243;&#20998;&#31163;&#20013;&#33719;&#30410;&#12290;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#33021;&#24110;&#21161;QAS
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11576v2 Announce Type: replace-cross  Abstract: Utilizing unsupervised representation learning for quantum architecture search (QAS) represents a cutting-edge approach poised to realize potential quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) devices. Most QAS algorithms combine their search space and search algorithms together and thus generally require evaluating a large number of quantum circuits during the search process. Predictor-based QAS algorithms can alleviate this problem by directly estimating the performance of circuits according to their structures. However, a high-performance predictor generally requires very time-consuming labeling to obtain a large number of labeled quantum circuits. Recently, a classical neural architecture search algorithm Arch2vec inspires us by showing that architecture search can benefit from decoupling unsupervised representation learning from the search process. Whether unsupervised representation learning can help QAS w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#25955;&#20110;&#19981;&#21516;&#25968;&#25454;&#23396;&#23707;&#30340;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#22320;&#21306;&#25968;&#25454;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30495;&#30456;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07931</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07931
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#25955;&#20110;&#19981;&#21516;&#25968;&#25454;&#23396;&#23707;&#30340;&#22330;&#26223;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#22320;&#21306;&#25968;&#25454;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30495;&#30456;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#22270;&#20687;&#30456;&#20851;&#38382;&#39064;&#30340;&#27969;&#34892;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#21644;&#33719;&#21462;&#26041;&#24335;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20449;&#24687;&#23384;&#20648;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23396;&#23707;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#38590;&#20197;&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#26041;&#24335;&#20013;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37096;&#20998;&#26412;&#22320;&#21270;&#25968;&#25454;&#21306;&#22495;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#26631;&#35760;&#30340;&#30495;&#30456;&#12290;&#36825;&#34920;&#26126;&#23427;&#20204;&#26377;&#33021;&#21147;&#36890;&#36807;&#25968;&#23383;&#26041;&#24335;&#24471;&#20986;&#32467;&#35770;&#65292;&#20294;&#22312;&#32570;&#20047;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#23588;&#20854;&#22312;&#23581;&#35797;&#24320;&#21457;&#36890;&#24120;&#38656;&#35201;&#35813;&#21151;&#33021;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#36825;&#26679;&#30340;&#30830;&#23450;&#36890;&#24120;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22312;&#36825;&#19968;&#24120;&#35265;&#26465;&#20214;&#19979;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07931v2 Announce Type: replace-cross  Abstract: With the popularization of AI solutions for image based problems, there has been a growing concern for both data privacy and acquisition. In a large number of cases, information is located on separate data silos and it can be difficult for a developer to consolidate all of it in a fashion that is appropriate for machine learning model development. Alongside this, a portion of these localized data regions may not have access to a labelled ground truth. This indicates that they have the capacity to reach conclusions numerically, but are not able to assign classifications amid a lack of pertinent information. Such a determination is often negligible, especially when attempting to develop image based solutions that often necessitate this capability. With this being the case, we propose an innovative vertical federated learning (VFL) model architecture that can operate under this common set of conditions. This is the first (and curr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09778</link><description>&lt;p&gt;
&#36229;&#22270;-MLP&#65306;&#22312;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#30340;&#36229;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-MLP: Learning on Hypergraphs without Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#24314;&#27169;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#12290;&#35768;&#22810;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22312;&#36229;&#22270;&#32467;&#26500;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#22686;&#24378;&#33410;&#28857;&#34920;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#22312;&#25512;&#29702;&#26102;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#39640;&#24310;&#36831;&#21644;&#25935;&#24863;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;&#65292;&#21363;&#23558;&#20851;&#20110;&#36229;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#35757;&#32451;&#30417;&#30563;&#20013;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20063;&#28040;&#38500;&#20102;&#23545;&#20854;&#30340;&#20381;&#36182;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hypergraph-MLP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;</title><link>https://arxiv.org/abs/2312.08255</link><description>&lt;p&gt;
OCTDL&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#22312;&#30524;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;OCT&#21487;&#20197;&#21487;&#35270;&#21270;&#35270;&#32593;&#33180;&#23618;&#65292;&#23545;&#26089;&#26399;&#26816;&#27979;&#21644;&#30417;&#27979;&#35270;&#32593;&#33180;&#30142;&#30149;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;OCT&#25968;&#25454;&#38598;&#65288;OCTDL&#65289;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26681;&#25454;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#26631;&#35760;&#30340;OCT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#24739;&#26377;&#32769;&#24180;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#12289;&#29627;&#29827;&#20307;&#35270;&#32593;&#33180;&#33180;&#65288;ERM&#65289;&#12289;&#35270;&#32593;&#33180;&#21160;&#33033;&#38381;&#22622;&#65288;RAO&#65289;&#12289;&#35270;&#32593;&#33180;&#38745;&#33033;&#38381;&#22622;&#65288;RVO&#65289;&#21644;&#29627;&#29827;&#20307;&#40644;&#26001;&#30028;&#38754;&#30142;&#30149;&#65288;VID&#65289;&#30340;&#24739;&#32773;&#30340;OCT&#35760;&#24405;&#12290;&#36825;&#20123;&#22270;&#20687;&#26159;&#20351;&#29992;Optovue Avanti RTVue XR&#37319;&#38598;&#30340;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25195;&#25551;&#38271;&#24230;&#30340;&#20809;&#26629;&#25195;&#25551;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
&lt;/p&gt;</description></item><item><title>GeoShapley&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#31354;&#38388;&#25928;&#24212;&#30340;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;&#23558;&#20301;&#32622;&#35270;&#20026;&#27169;&#22411;&#39044;&#27979;&#21338;&#24328;&#20013;&#30340;&#19968;&#21517;&#29609;&#23478;&#65292;&#33021;&#22815;&#37327;&#21270;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#24182;&#19982;&#20854;&#20182;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#36827;&#34892;&#37327;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.03675</link><description>&lt;p&gt;
GeoShapley&#65306;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#31354;&#38388;&#25928;&#24212;&#30340;&#21338;&#24328;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03675
&lt;/p&gt;
&lt;p&gt;
GeoShapley&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#31354;&#38388;&#25928;&#24212;&#30340;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;&#23558;&#20301;&#32622;&#35270;&#20026;&#27169;&#22411;&#39044;&#27979;&#21338;&#24328;&#20013;&#30340;&#19968;&#21517;&#29609;&#23478;&#65292;&#33021;&#22815;&#37327;&#21270;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#24182;&#19982;&#20854;&#20182;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeoShapley&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#31354;&#38388;&#25928;&#24212;&#30340;&#21338;&#24328;&#35770;&#26041;&#27861;&#12290;GeoShapley&#36890;&#36807;&#23558;&#20301;&#32622;&#27010;&#24565;&#21270;&#20026;&#27169;&#22411;&#39044;&#27979;&#21338;&#24328;&#20013;&#30340;&#19968;&#21517;&#29609;&#23478;&#65292;&#25193;&#23637;&#20102;&#21338;&#24328;&#35770;&#20013;&#30340;Nobel Prize-winning Shapley&#20540;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#37327;&#21270;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20301;&#32622;&#19982;&#27169;&#22411;&#20013;&#20854;&#20182;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;GeoShapley&#26159;&#19968;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#32467;&#26500;&#30340;&#32479;&#35745;&#23398;&#25110;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;GeoShapley&#30340;&#35299;&#37322;&#30452;&#25509;&#19982;&#29992;&#20110;&#35299;&#37322;&#31354;&#38388;&#25928;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#31995;&#25968;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#38750;&#31354;&#38388;&#25928;&#24212;&#30340;&#21152;&#27861;&#27169;&#22411;&#30456;&#36830;&#12290;&#21033;&#29992;&#27169;&#25311;&#25968;&#25454;&#65292;&#39564;&#35777;&#20102;GeoShapley&#20540;&#19982;&#24050;&#30693;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#29992;&#20110;&#23545;&#27604;&#19971;&#31181;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#25151;&#20215;&#24314;&#27169;&#30340;&#23454;&#35777;&#31034;&#20363;&#26469;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03675v2 Announce Type: replace  Abstract: This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models. GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model. GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures. The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects. Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models. An empirical example of house price modeling is used to illus
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;RDED&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03526</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;RDED&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#26426;&#22120;&#23398;&#20064;&#35201;&#27714;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#27492;&#38754;&#20020;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#38598;&#31934;&#28860;&#20316;&#20026;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#21387;&#32553;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30446;&#21069;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#20854;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#19977;&#20010;&#25152;&#38656;&#23646;&#24615;&#65292;&#21363;&#29616;&#23454;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RDED&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26377;&#25928;&#30340;&#25968;&#25454;&#31934;&#28860;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#29616;&#23454;&#24615;&#12290;&#23545;&#21508;&#31181;&#31070;&#32463;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;RDED&#30340;&#36827;&#23637;&#65306;&#25105;&#20204;&#21487;&#20197;&#23558;&#23436;&#25972;&#30340;ImageNet-1K&#31934;&#28860;&#20026;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03526v2 Announce Type: replace-cross  Abstract: Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprisin
&lt;/p&gt;</description></item><item><title>MACE&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2311.16191</link><description>&lt;p&gt;
&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#27491;&#24120;&#24615;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-Pattern Normalities in the Frequency Domain for Efficient Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16191
&lt;/p&gt;
&lt;p&gt;
MACE&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26174;&#33879;&#25552;&#21319;&#20102;&#20113;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26368;&#36817;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#22312;&#20113;&#29615;&#22659;&#20013;&#36935;&#21040;&#20102;&#23454;&#38469;&#25361;&#25112;&#65306;&#22312;&#32500;&#25252;&#27599;&#20010;&#26381;&#21153;&#30340;&#21807;&#19968;&#27169;&#22411;&#30340;&#19981;&#20999;&#23454;&#38469;&#24615;&#19982;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#22788;&#29702;&#21508;&#31181;&#27491;&#24120;&#27169;&#24335;&#30340;&#26377;&#38480;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#30683;&#30462;&#65292;&#20197;&#21450;&#22788;&#29702;&#23454;&#26102;&#39640;&#36127;&#36733;&#21644;&#30701;&#26399;&#24322;&#24120;&#26816;&#27979;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MACE&#65292;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#36866;&#24212;&#22810;&#27491;&#24120;&#27169;&#24335;&#19988;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#26377;&#19977;&#20010;&#26032;&#39062;&#30340;&#29305;&#28857;&#65306;&#65288;i&#65289;&#19968;&#31181;&#20248;&#31168;&#30340;&#27169;&#24335;&#25552;&#21462;&#26426;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#27491;&#24120;&#27169;&#24335;&#24182;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#26381;&#21153;&#27491;&#24120;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16191v2 Announce Type: replace-cross  Abstract: Anomaly detection significantly enhances the robustness of cloud systems. While neural network-based methods have recently demonstrated strong advantages, they encounter practical challenges in cloud environments: the contradiction between the impracticality of maintaining a unique model for each service and the limited ability to deal with diverse normal patterns by a unified model, as well as issues with handling heavy traffic in real time and short-term anomaly detection sensitivity.   Thus, we propose MACE, a multi-normal-pattern accommodated and efficient anomaly detection method in the frequency domain for time series anomaly detection. There are three novel characteristics of it: (i) a pattern extraction mechanism excelling at handling diverse normal patterns with a unified model, which enables the model to identify anomalies by examining the correlation between the data sample and its service normal pattern, instead of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#26679;&#26412;&#31232;&#32570;&#24615;&#25361;&#25112;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.15056</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15056
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#26679;&#26412;&#31232;&#32570;&#24615;&#25361;&#25112;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#21457;&#29616;&#28508;&#22312;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;(DDI)&#19968;&#30452;&#26159;&#20020;&#24202;&#27835;&#30103;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#29992;&#20110;DDI&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#65292;&#32780;&#24050;&#30693;DDI&#36739;&#20026;&#32597;&#35265;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;KnowDDI&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;KnowDDI&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22823;&#22411;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#20013;&#20016;&#23500;&#30340;&#37051;&#23621;&#20449;&#24687;&#22686;&#24378;&#33647;&#29289;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#23545;&#33647;&#29289;&#23398;&#20064;&#19968;&#20010;&#30693;&#35782;&#23376;&#22270;&#20197;&#35299;&#37322;&#39044;&#27979;&#30340;DDI&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#19982;&#36830;&#25509;&#24378;&#24230;&#30456;&#20851;&#32852;&#65292;&#25351;&#31034;&#24050;&#30693;DDI&#30340;&#37325;&#35201;&#24615;&#25110;&#31867;&#20284;&#24378;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#22312;&#33647;&#29289;&#23545;&#20043;&#38388;&#26159;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;DDI&#30340;&#24773;&#20917;&#36890;&#36807;&#20016;&#23500;&#30340;&#33647;&#29289;&#34920;&#31034;&#21644;&#20256;&#25773;&#30340;&#33647;&#29289;&#30456;&#20284;&#24615;&#24471;&#21040;&#38544;&#21547;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15056v2 Announce Type: replace-cross  Abstract: Background: Discovering potential drug-drug interactions (DDIs) is a long-standing challenge in clinical treatments and drug developments. Recently, deep learning techniques have been developed for DDI prediction. However, they generally require a huge number of samples, while known DDIs are rare.   Methods: In this work, we present KnowDDI, a graph neural network-based method that addresses the above challenge. KnowDDI enhances drug representations by adaptively leveraging rich neighborhood information from large biomedical knowledge graphs. Then, it learns a knowledge subgraph for each drug-pair to interpret the predicted DDI, where each of the edges is associated with a connection strength indicating the importance of a known DDI or resembling strength between a drug-pair whose connection is unknown. Thus, the lack of DDIs is implicitly compensated by the enriched drug representations and propagated drug similarities.   Resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRESS&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#22686;&#24378;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#20114;&#21160;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#23545;&#40784;&#21644;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#26041;&#38754;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.10081</link><description>&lt;p&gt;
DRESS&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25351;&#23548;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#21644;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRESS&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#22686;&#24378;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#20114;&#21160;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#23545;&#40784;&#21644;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#26041;&#38754;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DRESS&#65292;&#19968;&#31181;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#65292;&#23427;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;LVLM&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#26469;&#22686;&#24378;&#20854;&#23545;&#40784;&#21644;&#20114;&#21160;&#12290;&#39318;&#20808;&#65292;&#20808;&#21069;&#30340;LVLM&#36890;&#24120;&#20165;&#20381;&#36182;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#26469;&#22686;&#24378;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#22914;&#26524;&#19981;&#21152;&#20837;&#39069;&#22806;&#21453;&#39304;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#29983;&#25104;&#26080;&#29992;&#12289;&#34394;&#26500;&#25110;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#34429;&#28982;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#36890;&#24120;&#20197;&#22810;&#36718;&#23545;&#35805;&#26684;&#24335;&#32467;&#26500;&#21270;&#65292;&#20294;&#36830;&#32493;&#23545;&#35805;&#36718;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#20381;&#36182;&#20851;&#31995;&#36739;&#24369;&#12290;&#36825;&#38477;&#20302;&#20102;&#26377;&#25928;&#22810;&#36718;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;NLF&#20998;&#20026;&#20004;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#26032;&#39062;&#20998;&#31867;&#65306;&#25209;&#35780;&#21644;&#25913;&#36827;&#12290;&#25209;&#35780;&#24615;NLF&#35782;&#21035;&#21709;&#24212;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#29992;&#20110;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10081v2 Announce Type: replace-cross  Abstract: We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#36125;&#21494;&#26031; Tucker &#20998;&#35299;&#65288;FunBaT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558; Tucker &#20998;&#35299;&#25512;&#24191;&#21040;&#36830;&#32493;&#32034;&#24341;&#30340;&#24352;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#28508;&#22312;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.04829</link><description>&lt;p&gt;
&#36830;&#32493;&#32034;&#24341;&#24352;&#37327;&#25968;&#25454;&#30340;&#21151;&#33021;&#36125;&#21494;&#26031; Tucker &#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#36125;&#21494;&#26031; Tucker &#20998;&#35299;&#65288;FunBaT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558; Tucker &#20998;&#35299;&#25512;&#24191;&#21040;&#36830;&#32493;&#32034;&#24341;&#30340;&#24352;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#28508;&#22312;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tucker &#20998;&#35299;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24352;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26041;&#38754;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#23558;&#32593;&#26684;&#32467;&#26500;&#25968;&#25454;&#20998;&#35299;&#20026;&#26680;&#24352;&#37327;&#21644;&#19968;&#32452;&#23545;&#35937;&#34920;&#31034;&#65288;&#22240;&#23376;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#23637;&#31034;&#20302;&#31209;&#29305;&#24615;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#26159;&#27599;&#20010;&#26041;&#38754;&#25110;&#27169;&#24335;&#20013;&#37117;&#26377;&#26377;&#38480;&#30340;&#23545;&#35937;&#65292;&#23545;&#24212;&#20110;&#25968;&#25454;&#26465;&#30446;&#30340;&#31163;&#25955;&#32034;&#24341;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24448;&#24448;&#24182;&#38750;&#33258;&#28982;&#22320;&#21576;&#29616;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#12290;&#20363;&#22914;&#65292;&#22320;&#29702;&#25968;&#25454;&#20197;&#32428;&#24230;&#21644;&#32463;&#24230;&#22352;&#26631;&#30340;&#36830;&#32493;&#32034;&#24341;&#34920;&#31034;&#65292;&#26080;&#27861;&#30452;&#25509;&#36866;&#24212;&#24352;&#37327;&#27169;&#22411;&#12290;&#20026;&#20102;&#23558; Tucker &#20998;&#35299;&#25512;&#24191;&#21040;&#36825;&#31181;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21151;&#33021;&#36125;&#21494;&#26031; Tucker &#20998;&#35299;&#65288;FunBaT&#65289;&#12290;&#25105;&#20204;&#23558;&#36830;&#32493;&#32034;&#24341;&#25968;&#25454;&#35270;&#20026; Tucker &#26680;&#24515;&#21644;&#19968;&#32452;&#28508;&#22312;&#20989;&#25968;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20316;&#20026;&#21151;&#33021;&#20808;&#39564;&#26469;&#24314;&#27169;&#28508;&#22312;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#39640;&#26031;&#36807;&#31243;&#36716;&#25442;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04829v2 Announce Type: replace  Abstract: Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that there are finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, real-world data is often not naturally posed in this setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP) as functional priors to model the latent functions. Then, we convert each GP 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26694;&#26550;&#26469;&#35299;&#20915;&#39640;&#39057;&#21644;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#36890;&#36807;&#28789;&#27963;&#25429;&#25417;&#20027;&#23548;&#39057;&#29575;&#30340;&#26041;&#27861;&#65292;&#20272;&#35745;&#28151;&#21512;&#26435;&#37325;&#24182;&#33258;&#21160;&#35825;&#23548;&#31232;&#30095;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35889;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.04465</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#39640;&#39057;&#21644;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving High Frequency and Multi-Scale PDEs with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04465
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26694;&#26550;&#26469;&#35299;&#20915;&#39640;&#39057;&#21644;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#36890;&#36807;&#28789;&#27963;&#25429;&#25417;&#20027;&#23548;&#39057;&#29575;&#30340;&#26041;&#27861;&#65292;&#20272;&#35745;&#28151;&#21512;&#26435;&#37325;&#24182;&#33258;&#21160;&#35825;&#23548;&#31232;&#30095;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35889;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#29289;&#29702;&#27169;&#25311;&#21644;&#31185;&#23398;&#35745;&#31639;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#19968;&#20010;&#33879;&#21517;&#31034;&#20363;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#12290;&#28982;&#32780;&#65292;PINNs&#22312;&#35299;&#20915;&#39640;&#39057;&#21644;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#35889;&#20559;&#24046;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#28789;&#27963;&#22320;&#25429;&#25417;&#20027;&#23548;&#39057;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#23398;&#29983;$t$&#28151;&#21512;&#25110;&#39640;&#26031;&#28151;&#21512;&#26469;&#24314;&#27169;PDE&#35299;&#30340;&#21151;&#29575;&#35889;&#12290;&#25105;&#20204;&#24212;&#29992;&#36870;Fourier&#21464;&#25442;&#26469;&#33719;&#24471;&#21327;&#26041;&#24046;&#20989;&#25968;&#65288;&#36890;&#36807;Wiener-Khinchin&#23450;&#29702;&#65289;&#12290;&#20174;&#39640;&#26031;&#28151;&#21512;&#35889;&#23548;&#20986;&#30340;&#21327;&#26041;&#24046;&#23545;&#24212;&#20110;&#24050;&#30693;&#30340;&#35889;&#28151;&#21512;&#26680;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#23545;&#25968;&#22495;&#20013;&#20272;&#35745;&#28151;&#21512;&#26435;&#37325;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31561;&#25928;&#20110;&#25918;&#32622;Jeffreys&#20808;&#39564;&#12290;&#23427;&#33258;&#21160;&#35825;&#23548;&#31232;&#30095;&#24615;&#65292;&#20462;&#21098;&#36807;&#22810;&#30340;&#39057;&#29575;&#65292;&#24182;&#35843;&#25972;&#21097;&#19979;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04465v2 Announce Type: replace  Abstract: Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#21830;&#29992;AI/ML&#21152;&#36895;&#22120;&#30340;&#21021;&#27493;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#29305;&#28857;&#65292;&#20197;&#36776;&#21035;&#23427;&#20204;&#30340;&#21019;&#26032;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#20854;&#20182;&#35774;&#35745;&#20248;&#21270;&#65292;&#25215;&#35834;&#20026;AI/ML&#20219;&#21153;&#25552;&#20379;&#21331;&#36234;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.04417</link><description>&lt;p&gt;
&#35780;&#20272;&#26032;&#20852;&#30340;AI/ML&#21152;&#36895;&#22120;&#65306;IPU&#12289;RDU&#21644;NVIDIA/AMD GPU
&lt;/p&gt;
&lt;p&gt;
Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#21830;&#29992;AI/ML&#21152;&#36895;&#22120;&#30340;&#21021;&#27493;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#29305;&#28857;&#65292;&#20197;&#36776;&#21035;&#23427;&#20204;&#30340;&#21019;&#26032;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#20854;&#20182;&#35774;&#35745;&#20248;&#21270;&#65292;&#25215;&#35834;&#20026;AI/ML&#20219;&#21153;&#25552;&#20379;&#21331;&#36234;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#19981;&#26029;&#21457;&#23637;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#26085;&#30410;&#22797;&#26434;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#20256;&#32479;&#35745;&#31639;&#26550;&#26500;&#22522;&#20110;&#20911;&#183;&#35834;&#20234;&#26364;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#24403;&#20195;AI/ML&#31639;&#27861;&#30340;&#35201;&#27714;&#36229;&#36234;&#65292;&#23548;&#33268;&#20687;Graphcore Intelligence Processing Unit (IPU)&#12289;Sambanova Reconfigurable Datafl&#183;&#183;&#183;&#183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04417v2 Announce Type: replace-cross  Abstract: The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern t
&lt;/p&gt;</description></item><item><title>TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.02971</link><description>&lt;p&gt;
TabRepo&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#34920;&#26684;&#27169;&#22411;&#35780;&#20272;&#24211;&#21450;&#20854;AutoML&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02971
&lt;/p&gt;
&lt;p&gt;
TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TabRepo&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#22810;&#31181;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#27604;&#36739;&#36229;&#21442;&#25968;&#20248;&#21270;&#19982;&#24403;&#21069;AutoML&#31995;&#32479;&#20197;&#21450;&#22312;&#20351;&#29992;&#39044;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#30340;&#21516;&#26102;&#32771;&#34385;&#38598;&#25104;&#30340;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#25191;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#24212;&#29992;&#26631;&#20934;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#24310;&#36831;&#26041;&#38754;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02761</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#22312;&#26410;&#30693;&#25104;&#26412;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
One-Shot Strategic Classification Under Unknown Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#20998;&#31867;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#23545;&#31574;&#30053;&#36755;&#20837;&#25805;&#32437;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20915;&#31574;&#35268;&#21017;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#36825;&#20123;&#21709;&#24212;&#26159;&#24050;&#30693;&#30340;&#65307;&#32780;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22788;&#29702;&#26410;&#30693;&#21709;&#24212;&#65292;&#20294;&#23427;&#20204;&#19987;&#38376;&#30740;&#31350;&#37325;&#22797;&#27169;&#22411;&#37096;&#32626;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#25919;&#31574;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#28608;&#21169;&#29992;&#20363;&#20013;&#65292;&#22810;&#27425;&#37096;&#32626;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29978;&#33267;&#19968;&#20010;&#31967;&#31957;&#30340;&#36718;&#27425;&#37117;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#30340;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#27491;&#24335;&#30740;&#31350;&#65292;&#36825;&#38656;&#35201;&#22312;&#19968;&#27425;&#24615;&#36873;&#25321;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#30528;&#37325;&#20851;&#27880;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#23545;&#30495;&#23454;&#25104;&#26412;&#30340;&#23567;&#35823;&#24046;&#20063;&#21487;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#33267;&#26497;&#20302;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#26694;&#23450;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02761v2 Announce Type: replace  Abstract: The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use case$\unicode{x2014}$where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of one-shot strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, with the goal of identifying
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.02629</link><description>&lt;p&gt;
&#24102;&#26377;Q-Learning&#30340;&#25351;&#38024;&#32593;&#32476;&#29992;&#20110;OP&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pointer Networks with Q-Learning for OP Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Orienteering Problem&#65288;OP&#65289;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#20013;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#29289;&#27969;&#12289;&#20132;&#20184;&#21644;&#36816;&#36755;&#35268;&#21010;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;OP&#30340;NP-hard&#24615;&#36136;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#12290;&#23613;&#31649;&#25351;&#38024;&#32593;&#32476;&#65288;Ptr-Nets&#65289;&#22312;&#21508;&#31181;&#32452;&#21512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;OP&#19978;&#30340;&#34920;&#29616;&#20197;&#21450;&#38656;&#35201;&#19987;&#27880;&#20110;&#26410;&#26469;&#22238;&#25253;&#25110;&#25506;&#32034;&#30340;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#35748;&#35782;&#21040;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#19982;&#24207;&#21015;-&#24207;&#21015;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#33021;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#38024;Q&#32593;&#32476;&#65288;PQN&#65289;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;Ptr-Nets&#21644;Q-learning&#65292;&#30001;&#20110;&#20854;&#20165;&#20855;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#23427;&#22312;&#25429;&#33719;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#26377;&#25928;&#24212;&#23545;OP&#25552;&#20986;&#30340;&#20855;&#20307;&#25361;&#25112;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26550;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02629v2 Announce Type: replace  Abstract: The Orienteering Problem (OP) presents a unique challenge in Combinatorial Optimization (CO), emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP, and duties requiring focus on future return or exploration, leaves room for improvement. Recognizing the potency combining Reinforcement Learning (RL) methods with sequence-to-sequence models, this research unveils the Pointer Q-Network (PQN). This method combines Ptr-Nets and Q-learning, which, thanks to its critic only nature, outstands in its capability of capturing relationships within an embedded graph, a fundamental requirement in order to effectively address the specific challenges presented by OP. We explore the architecture and functionalit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;iable Euler Characteristic Transform&#65288;DECT&#65289;&#35745;&#31639;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;ECT&#65292;&#23637;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07630</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#27431;&#25289;&#29305;&#24449;&#21464;&#25442;&#29992;&#20110;&#24418;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Euler Characteristic Transforms for Shape Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07630
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;iable Euler Characteristic Transform&#65288;DECT&#65289;&#35745;&#31639;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;ECT&#65292;&#23637;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#25289;&#29305;&#24449;&#21464;&#25442;&#65288;ECT&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#21644;&#22270;&#24418;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;ECT&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#23618;&#65292;&#21487;&#20197;&#20351;ECT&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Differentiable Euler Characteristic Transform&#65288;DECT&#65289;&#65292;&#36895;&#24230;&#24555;&#65292;&#35745;&#31639;&#39640;&#25928;&#65292;&#21516;&#26102;&#22312;&#22270;&#24418;&#21644;&#28857;&#20113;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#30475;&#20284;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#19982;&#26356;&#22797;&#26434;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23618;&#30456;&#21516;&#30340;&#25299;&#25169;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07630v2 Announce Type: replace  Abstract: The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method, the Differentiable Euler Characteristic Transform (DECT), is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly simple statistic provides the same topological expressivity as more complex topological deep learning layers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;&#24555;&#36895;&#31616;&#21333;&#27714;&#35299;&#22120;&#65292;&#23558;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#21442;&#25968;&#21270;&#20026;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#65292;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2310.01174</link><description>&lt;p&gt;
&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;
&lt;/p&gt;
&lt;p&gt;
Light Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01174
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;&#24555;&#36895;&#31616;&#21333;&#27714;&#35299;&#22120;&#65292;&#23558;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#21442;&#25968;&#21270;&#20026;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#65292;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35745;&#31639;&#35874;&#23572;&#23486;&#26684;&#26725;&#65288;SB&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;SB&#27714;&#35299;&#22120;&#20173;&#28982;&#36807;&#37325;&#65292;&#24182;&#38656;&#35201;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22797;&#26434;&#20248;&#21270;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#20027;&#35201;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#20687;&#32858;&#31867;&#20013;&#30340;$k$-means&#26041;&#27861;&#12289;&#20998;&#31867;&#20013;&#30340;&#36923;&#36753;&#22238;&#24402;&#25110;&#31163;&#25955;&#26368;&#20248;&#36755;&#36816;&#20013;&#30340;Sinkhorn&#31639;&#27861;&#37027;&#26679;&#36215;&#21040;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#20316;&#29992;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24555;&#36895;&#31616;&#21333;&#30340;SB&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26368;&#36817;&#20986;&#29616;&#22312;&#35813;&#39046;&#22495;&#30340;&#20004;&#20010;&#35266;&#28857;&#30340;&#24039;&#22937;&#32467;&#21512;&#65306;&#65288;a&#65289;&#20351;&#29992;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#23545;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#65288;b&#65289;&#23558;&#23545;&#25968;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#36825;&#20123;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#19988;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21512;&#29702;&#30340;SB&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#31616;&#21333;&#30452;&#25509;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#23427;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01174v2 Announce Type: replace  Abstract: Despite the recent advances in the field of computational Schrodinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrodinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrodinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2309.10668</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#21363;&#20026;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Language Modeling Is Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.10668
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#20026;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#24050;&#30830;&#31435;&#20102;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#36716;&#21270;&#20026;&#26080;&#25439;&#21387;&#32553;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#38598;&#20013;&#31934;&#21147;&#35757;&#32451;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#65288;&#35821;&#35328;&#65289;&#27169;&#22411;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#26377;&#26395;&#25104;&#20026;&#24378;&#22823;&#30340;&#21387;&#32553;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#36890;&#36807;&#21387;&#32553;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#22823;&#22411;&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#30340;&#21387;&#32553;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#36890;&#29992;&#39044;&#27979;&#22120;&#65292;&#21387;&#32553;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20851;&#25193;&#23637;&#23450;&#24459;&#12289;&#26631;&#35760;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;Chinchilla 70B&#65292;&#34429;&#28982;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#23558;ImageNet&#30340;&#34917;&#19969;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;43.4%&#65292;&#23558;LibriSpeech&#26679;&#26412;&#21387;&#32553;&#20026;&#20854;&#21407;&#22987;&#22823;&#23567;&#30340;16.4%&#65292;&#36229;&#36234;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.10668v2 Announce Type: replace-cross  Abstract: It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors li
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#20449;&#24687;&#29942;&#39048;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#26469;&#36827;&#34892;&#22797;&#26434;&#31995;&#32479;&#20013;&#20449;&#24687;&#30340;&#20998;&#35299;</title><link>https://arxiv.org/abs/2307.04755</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#34892;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Information decomposition in complex systems via machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04755
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#20449;&#24687;&#29942;&#39048;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#26469;&#36827;&#34892;&#22797;&#26434;&#31995;&#32479;&#20013;&#20449;&#24687;&#30340;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#36808;&#20986;&#30340;&#22522;&#26412;&#27493;&#39588;&#20043;&#19968;&#26159;&#35782;&#21035;&#23545;&#23439;&#35266;&#23610;&#24230;&#34892;&#20026;&#26368;&#30456;&#20851;&#30340;&#31995;&#32479;&#32452;&#20214;&#23610;&#24230;&#19978;&#30340;&#21464;&#21270;&#12290;&#20114;&#20449;&#24687;&#25552;&#20379;&#20102;&#23558;&#31995;&#32479;&#21508;&#20010;&#23610;&#24230;&#19978;&#30340;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#30340;&#33258;&#28982;&#25163;&#27573;&#65292;&#22240;&#20026;&#23427;&#29420;&#31435;&#20110;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25551;&#36848;&#20449;&#24687;&#22914;&#20309;&#20998;&#24067;&#22312;&#19968;&#32452;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#26041;&#24335;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#38500;&#20102;&#23569;&#25968;&#20960;&#27425;&#27979;&#37327;&#22806;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20998;&#35299;&#19968;&#32452;&#27979;&#37327;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27599;&#20010;&#27979;&#37327;&#30340;&#26377;&#25439;&#21387;&#32553;&#12290;&#22312;&#20998;&#24067;&#24335;&#20449;&#24687;&#29942;&#39048;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#30340;&#24341;&#23548;&#19979;&#65292;&#20449;&#24687;&#20998;&#35299;&#35782;&#21035;&#20102;&#19982;&#25351;&#23450;&#30340;&#23439;&#35266;&#23610;&#24230;&#34892;&#20026;&#26368;&#30456;&#20851;&#30340;&#31995;&#32479;&#29366;&#24577;&#27979;&#37327;&#20013;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04755v2 Announce Type: replace  Abstract: One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale. Mutual information provides a natural means of linking variation across scales of a system due to its independence of functional relationship between observables. However, characterizing the manner in which information is distributed across a set of observables is computationally challenging and generally infeasible beyond a handful of measurements. Here we propose a practical and general methodology that uses machine learning to decompose the information contained in a set of measurements by jointly optimizing a lossy compression of each measurement. Guided by the distributed information bottleneck as a learning objective, the information decomposition identifies the variation in the measurements of the system state most relevant to specified mac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNN-ANN&#28151;&#21512;&#26550;&#26500;&#65292;&#29992;&#20110;&#20811;&#26381;SNN&#22312;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#20107;&#20214;&#39537;&#21160;&#30340;&#20809;&#27969;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2306.02960</link><description>&lt;p&gt;
&#20004;&#20840;&#20854;&#32654;&#65306;&#28151;&#21512;SNN-ANN&#26550;&#26500;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNN-ANN&#28151;&#21512;&#26550;&#26500;&#65292;&#29992;&#20110;&#20811;&#26381;SNN&#22312;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#20107;&#20214;&#39537;&#21160;&#30340;&#20809;&#27969;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#20107;&#20214;&#30456;&#26426;&#27491;&#22312;&#25104;&#20026;&#20256;&#32479;&#22522;&#20110;&#24103;&#30340;&#30456;&#26426;&#30340;&#20302;&#21151;&#32791;&#26367;&#20195;&#21697;&#65292;&#29992;&#20110;&#25429;&#25417;&#39640;&#36895;&#36816;&#21160;&#21644;&#39640;&#21160;&#24577;&#33539;&#22260;&#22330;&#26223;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#31232;&#30095;&#21644;&#24322;&#27493;&#20107;&#20214;&#36755;&#20986;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20197;&#20854;&#24322;&#27493;&#20107;&#20214;&#39537;&#21160;&#30340;&#35745;&#31639;&#65292;&#26174;&#31034;&#20986;&#20174;&#36825;&#20123;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26410;&#33021;&#26377;&#25928;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20855;&#26377;&#39069;&#22806;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65288;&#38408;&#20540;&#21644;&#27844;&#28431;&#65289;&#12289;&#22312;&#26356;&#28145;&#23618;&#27425;&#19978;&#28040;&#22833;&#30340;&#23574;&#23792;&#20197;&#21450;&#19981;&#21487;&#24494;&#20998;&#30340;&#20108;&#20540;&#28608;&#27963;&#20989;&#25968;&#65292;&#35757;&#32451;SNNs&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;SNNs&#20013;&#65292;&#36127;&#36131;&#36319;&#36394;&#26102;&#38388;&#20449;&#24687;&#30340;&#39069;&#22806;&#25968;&#25454;&#32467;&#26500;&#8212;&#8212;&#33180;&#30005;&#20301;&#65292;&#24517;&#39035;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#33719;&#21462;&#21644;&#26356;&#26032;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNN-ANN&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02960v2 Announce Type: replace-cross  Abstract: In the field of robotics, event-based cameras are emerging as a promising low-power alternative to traditional frame-based cameras for capturing high-speed motion and high dynamic range scenes. This is due to their sparse and asynchronous event outputs. Spiking Neural Networks (SNNs) with their asynchronous event-driven compute, show great potential for extracting the spatio-temporal features from these event streams. In contrast, the standard Analog Neural Networks (ANNs) fail to process event data effectively. However, training SNNs is difficult due to additional trainable parameters (thresholds and leaks), vanishing spikes at deeper layers, and a non-differentiable binary activation function. Furthermore, an additional data structure, membrane potential, responsible for keeping track of temporal information, must be fetched and updated at every timestep in SNNs. To overcome these challenges, we propose a novel SNN-ANN hybrid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2304.08260</link><description>&lt;p&gt;
&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#39044;&#27979;&#34892;&#20154;&#20114;&#21160;&#32467;&#26524;&#65306;&#31359;&#36234;&#25110;&#31561;&#24453;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cross or Wait? Predicting Pedestrian Interaction Outcomes at Unsignalized Crossings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34892;&#20154;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#34892;&#20026;&#26159;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#34892;&#20154;&#36807;&#39532;&#36335;&#30340;&#34892;&#20026;&#21463;&#21040;&#21508;&#31181;&#20114;&#21160;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#21040;&#36798;&#26102;&#38388;&#12289;&#34892;&#20154;&#31561;&#24453;&#26102;&#38388;&#12289;&#26001;&#39532;&#32447;&#30340;&#23384;&#22312;&#65292;&#20197;&#21450;&#34892;&#20154;&#21644;&#39550;&#39542;&#21592;&#30340;&#29305;&#24615;&#21644;&#20010;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#29992;&#20110;&#39044;&#27979;&#20114;&#21160;&#32467;&#26524;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#34892;&#20154;&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#19982;&#36710;&#36742;&#20114;&#21160;&#26102;&#30340;&#36807;&#39532;&#36335;&#34892;&#20026;&#65292;&#21253;&#25324;&#34892;&#20154;&#30340;&#36807;&#39532;&#36335;&#20915;&#31574;&#12289;&#36807;&#34903;&#24320;&#22987;&#26102;&#38388;&#65288;CIT&#65289;&#21644;&#36807;&#34903;&#25345;&#32493;&#26102;&#38388;&#65288;CD&#65289;&#12290;&#20998;&#24067;&#24335;&#27169;&#25311;&#25968;&#25454;&#34987;&#29992;&#20110;&#39044;&#27979;&#21644;&#20998;&#26512;&#36825;&#20123;&#20114;&#21160;&#22240;&#32032;&#12290;&#19982;&#36923;&#36753;&#22238;&#24402;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#25552;&#39640;&#20102;4.46%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08260v2 Announce Type: replace-cross  Abstract: Predicting pedestrian behavior when interacting with vehicles is one of the most critical challenges in the field of automated driving. Pedestrian crossing behavior is influenced by various interaction factors, including time to arrival, pedestrian waiting time, the presence of zebra crossing, and the properties and personality traits of both pedestrians and drivers. However, these factors have not been fully explored for use in predicting interaction outcomes. In this paper, we use machine learning to predict pedestrian crossing behavior including pedestrian crossing decision, crossing initiation time (CIT), and crossing duration (CD) when interacting with vehicles at unsignalized crossings. Distributed simulator data are utilized for predicting and analyzing the interaction factors. Compared with the logistic regression baseline model, our proposed neural network model improves the prediction accuracy and F1 score by 4.46% an
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24191;&#20041;&#25324;&#21495;&#25968;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#25163;&#30340;&#32422;&#26463;&#19982;&#31354;&#38388;&#22823;&#23567;&#65292;&#36890;&#36807;Follow-the-Perturbed-Leader&#31639;&#27861;&#23454;&#29616;&#20302;&#36951;&#25022;&#65292;&#20248;&#21270;&#35843;&#29992;&#20248;&#21270;Oracle&#30340;&#27425;&#25968;&#20197;&#23454;&#29616;&#36951;&#25022;&#22312;&#22810;&#20010;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2302.05430</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#27573;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#30340;Oracle&#39640;&#25928;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Oracle-Efficient Smoothed Online Learning for Piecewise Continuous Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05430
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24191;&#20041;&#25324;&#21495;&#25968;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#25163;&#30340;&#32422;&#26463;&#19982;&#31354;&#38388;&#22823;&#23567;&#65292;&#36890;&#36807;Follow-the-Perturbed-Leader&#31639;&#27861;&#23454;&#29616;&#20302;&#36951;&#25022;&#65292;&#20248;&#21270;&#35843;&#29992;&#20248;&#21270;Oracle&#30340;&#27425;&#25968;&#20197;&#23454;&#29616;&#36951;&#25022;&#22312;&#22810;&#20010;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#32531;&#35299;&#22312;&#20174;&#32463;&#20856;&#23398;&#20064;&#36716;&#21521;&#23545;&#25239;&#24615;&#23398;&#20064;&#26102;&#20135;&#29983;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#26174;&#33879;&#25439;&#22833;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24615;&#27010;&#24565;&#65292;&#21363;&#24191;&#20041;&#25324;&#21495;&#25968;&#65292;&#23558;&#23545;&#25163;&#30340;&#32422;&#26463;&#19982;&#31354;&#38388;&#22823;&#23567;&#30456;&#32467;&#21512;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;Follow-the-Perturbed-Leader&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#20302;&#36951;&#25022;&#65292;&#24182;&#19988;&#20248;&#21270;&#35843;&#29992;&#20248;&#21270;Oracle&#30340;&#27425;&#25968;&#20197;&#23454;&#29616;&#24179;&#22343;&#36951;&#25022;&#30340;&#26368;&#20339;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05430v2 Announce Type: replace-cross  Abstract: Smoothed online learning has emerged as a popular framework to mitigate the substantial loss in statistical and computational complexity that arises when one moves from classical to adversarial learning. Unfortunately, for some spaces, it has been shown that efficient algorithms suffer an exponentially worse regret than that which is minimax optimal, even when the learner has access to an optimization oracle over the space. To mitigate that exponential dependence, this work introduces a new notion of complexity, the generalized bracketing numbers, which marries constraints on the adversary to the size of the space, and shows that an instantiation of Follow-the-Perturbed-Leader can attain low regret with the number of calls to the optimization oracle scaling optimally with respect to average regret. We then instantiate our bounds in several problems of interest, including online prediction and planning of piecewise continuous fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#20998;&#27573;&#20223;&#23556;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#27169;&#25311;&#38382;&#39064;&#65292;&#22312;&#24369;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#20855;&#26377;&#22810;&#39033;&#24335;&#36951;&#25022;&#24230;&#65292;&#24182;&#19988;&#22312;&#35843;&#29992;&#20248;&#21270;&#39044;&#27979;&#27425;&#25968;&#26041;&#38754;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2301.11187</link><description>&lt;p&gt;
&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#29992;&#20110;&#20998;&#27573;&#20223;&#23556;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Smoothed Online Learning for Prediction in Piecewise Affine Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.11187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#20998;&#27573;&#20223;&#23556;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#27169;&#25311;&#38382;&#39064;&#65292;&#22312;&#24369;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#20855;&#26377;&#22810;&#39033;&#24335;&#36951;&#25022;&#24230;&#65292;&#24182;&#19988;&#22312;&#35843;&#29992;&#20248;&#21270;&#39044;&#27979;&#27425;&#25968;&#26041;&#38754;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#27573;&#20223;&#23556;&#65288;PWA&#65289;&#22238;&#24402;&#21644;&#35268;&#21010;&#38382;&#39064;&#23545;&#20110;&#22312;&#32447;&#23398;&#20064;&#12289;&#25511;&#21046;&#21644;&#26426;&#22120;&#20154;&#23398;&#30340;&#30740;&#31350;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#65292;&#23427;&#20026;&#30740;&#31350;&#31995;&#32479;&#21160;&#24577;&#21457;&#29983;&#24613;&#21095;&#21464;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31359;&#36234;&#19981;&#21516;&#8220;&#29255;&#27573;&#8221;&#26102;&#20986;&#29616;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23398;&#20064;&#22312;&#19968;&#33324;&#30340;&#39034;&#24207;&#35774;&#32622;&#20013;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#23454;&#38469;&#31639;&#27861;&#34987;&#36843;&#37319;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22312;&#24369;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#36951;&#25022;&#24230;&#24182;&#19988;&#22312;&#20248;&#21270;&#39044;&#27979;&#20013;&#39640;&#25928;&#30340;PWA&#31995;&#32479;&#39044;&#27979;&#21644;&#27169;&#25311;&#31639;&#27861;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#21040;&#19968;&#27493;&#39044;&#27979;&#21644;&#22810;&#27493;&#27169;&#25311;&#36951;&#25022;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.11187v2 Announce Type: replace-cross  Abstract: The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics. Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches. This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle. We further apply our results to the problems of one-step prediction and multi-step simulation regret i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23618;&#27425;&#21098;&#26525;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LTH&#21644;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#65292;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#21457;&#29616;&#20102;&#33719;&#32988;&#24425;&#31080;&#30340;&#23384;&#22312;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#35757;&#32451;&#31264;&#23494;&#32593;&#32476;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2301.10835</link><description>&lt;p&gt;
&#24403;&#23618;&#27425;&#25277;&#22870;&#26102;&#65292;&#25152;&#26377;&#31080;&#25454;&#22312;&#21021;&#22987;&#21270;&#26102;&#37117;&#33719;&#32988;
&lt;/p&gt;
&lt;p&gt;
When Layers Play the Lottery, all Tickets Win at Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23618;&#27425;&#21098;&#26525;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LTH&#21644;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#65292;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#21457;&#29616;&#20102;&#33719;&#32988;&#24425;&#31080;&#30340;&#23384;&#22312;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#35757;&#32451;&#31264;&#23494;&#32593;&#32476;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#20943;&#23569;&#28145;&#24230;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#35768;&#22810;&#21098;&#26525;&#20013;&#30340;&#36827;&#23637;&#21033;&#29992;&#20102;&#8220;&#24425;&#31080;&#20551;&#35774;&#8221;&#65288;LTH&#65289;&#30340;&#27010;&#24565;&#12290;LTH&#25581;&#31034;&#20102;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#31264;&#23494;&#32593;&#32476;&#20869;&#37096;&#23384;&#22312;&#30528;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#20934;&#30830;&#24230;&#65288;&#21363;&#36194;&#24471;&#24425;&#31080; - &#33719;&#32988;&#31080;&#65289;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;&#31080;&#65289;&#12290;&#22312;&#21021;&#22987;&#21270;&#26102;&#36827;&#34892;&#21098;&#26525;&#27880;&#37325;&#20110;&#25214;&#21040;&#33719;&#32988;&#31080;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#31264;&#23494;&#32593;&#32476;&#12290;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#30740;&#31350;&#34920;&#26126;&#23376;&#32593;&#32476;&#26469;&#33258;&#26435;&#37325;&#25110;&#28388;&#27874;&#22120;&#21098;&#26525;&#30340;&#36235;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#23618;&#27425;&#21098;&#26525;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;LTH&#21644;&#21021;&#22987;&#21270;&#26102;&#30340;&#21098;&#26525;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#24403;&#21098;&#26525;&#36807;&#31243;&#31227;&#38500;&#23618;&#26102;&#33719;&#32988;&#31080;&#30340;&#23384;&#22312;&#12290;&#20511;&#21161;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21021;&#22987;&#21270;&#26102;&#21457;&#29616;&#36825;&#20123;&#33719;&#32988;&#31080;&#65292;&#28040;&#38500;&#20102;&#35757;&#32451;&#21021;&#22987;&#65288;&#36807;&#21442;&#25968;&#21270;&#65289;&#31264;&#23494;&#32593;&#32476;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#30340;&#35201;&#27714;&#12290;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10835v2 Announce Type: replace  Abstract: Pruning is a standard technique for reducing the computational cost of deep networks. Many advances in pruning leverage concepts from the Lottery Ticket Hypothesis (LTH). LTH reveals that inside a trained dense network exists sparse subnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery - winning tickets). Pruning at initialization focuses on finding winning tickets without training a dense network. Studies on these concepts share the trend that subnetworks come from weight or filter pruning. In this work, we investigate LTH and pruning at initialization from the lens of layer pruning. First, we confirm the existence of winning tickets when the pruning process removes layers. Leveraged by this observation, we propose to discover these winning tickets at initialization, eliminating the requirement of heavy computational resources for training the initial (over-parameterized) dense network. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25490;&#21517;&#20113;ERP&#30340;&#37319;&#29992;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2301.00693</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;TOPSIS&#36827;&#34892;&#28145;&#24230;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Recurrent Learning Through Long Short Term Memory and TOPSIS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.00693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25490;&#21517;&#20113;ERP&#30340;&#37319;&#29992;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36164;&#28304;&#35268;&#21010;&#65288;ERP&#65289;&#36719;&#20214;&#23558;&#36164;&#28304;&#12289;&#25968;&#25454;&#27719;&#32858;&#22312;&#19968;&#36215;&#65292;&#20197;&#20445;&#25345;&#20225;&#19994;&#27969;&#31243;&#20013;&#30340;&#36719;&#20214;&#27969;&#30021;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#30340;&#24265;&#20215;&#12289;&#31616;&#20415;&#21644;&#24555;&#36895;&#31649;&#29702;&#25215;&#35834;&#20419;&#20351;&#20225;&#19994;&#25152;&#26377;&#32773;&#23558;&#21333;&#19968;&#20307;&#26550;&#26500;&#36716;&#21464;&#20026;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;/&#20113;&#30340;ERP&#12290;&#30001;&#20110;&#20113;ERP&#30340;&#24320;&#21457;&#28041;&#21450;&#19968;&#20010;&#24490;&#29615;&#36807;&#31243;&#65292;&#21363;&#35268;&#21010;&#12289;&#23454;&#26045;&#12289;&#27979;&#35797;&#21644;&#21319;&#32423;&#65292;&#20854;&#37319;&#29992;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;TOPSIS&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#21035;&#25490;&#21517;&#37319;&#29992;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#27169;&#22411;&#36890;&#36807;&#38416;&#36848;&#20851;&#38190;&#21442;&#19982;&#32773;&#12289;&#26381;&#21153;&#12289;&#26550;&#26500;&#12289;&#21151;&#33021;&#65292;&#22312;&#21442;&#32771;&#27169;&#22411;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32771;&#34385;&#25216;&#26415;&#12289;&#21019;&#26032;&#21644;&#25269;&#25239;&#38382;&#39064;&#65292;&#24320;&#23637;&#20102;&#19968;&#39033;&#23450;&#24615;&#35843;&#26597;&#65292;&#20197;&#23601;&#20851;&#38190;&#37319;&#29992;&#22240;&#32032;&#25552;&#20986;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.00693v2 Announce Type: replace-cross  Abstract: Enterprise resource planning (ERP) software brings resources, data together to keep software-flow within business processes in a company. However, cloud computing's cheap, easy and quick management promise pushes business-owners for a transition from monolithic to a data-center/cloud based ERP. Since cloud-ERP development involves a cyclic process, namely planning, implementing, testing and upgrading, its adoption is realized as a deep recurrent neural network problem. Eventually, a classification algorithm based on long short term memory (LSTM) and TOPSIS is proposed to identify and rank, respectively, adoption features. Our theoretical model is validated over a reference model by articulating key players, services, architecture, functionalities. Qualitative survey is conducted among users by considering technology, innovation and resistance issues, to formulate hypotheses on key adoption factors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Behave-XAI&#65292;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35299;&#20915;&#34892;&#20026;&#25366;&#25496;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29992;&#25143;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#34920;&#24449;&#25968;&#25454;&#30340;&#28145;&#24230;&#21487;&#35299;&#37322;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2301.00016</link><description>&lt;p&gt;
Behave-XAI&#65306;&#34892;&#20026;&#34920;&#24449;&#25968;&#25454;&#30340;&#28145;&#24230;&#21487;&#35299;&#37322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Behave-XAI: Deep Explainable Learning of Behavioral Representational Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Behave-XAI&#65292;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35299;&#20915;&#34892;&#20026;&#25366;&#25496;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29992;&#25143;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#34920;&#24449;&#25968;&#25454;&#30340;&#28145;&#24230;&#21487;&#35299;&#37322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;AI&#31995;&#32479;&#38656;&#35201;&#23601;&#20854;&#25552;&#20379;&#30340;&#19968;&#33324;&#24615;&#21644;&#20855;&#20307;&#24615;&#20915;&#31574;&#12289;&#26381;&#21153;&#36827;&#34892;&#28548;&#28165;&#12290;&#21482;&#26377;&#28040;&#36153;&#32773;&#23545;&#35299;&#37322;&#24863;&#21040;&#28385;&#24847;&#65292;&#20363;&#22914;&#20026;&#20160;&#20040;&#20219;&#20309;&#20998;&#31867;&#32467;&#26524;&#26159;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#30340;&#32467;&#26524;&#12290;&#36825;&#23454;&#38469;&#19978;&#28608;&#21169;&#25105;&#20204;&#22312;&#34892;&#20026;&#25366;&#25496;&#22330;&#26223;&#20013;&#20351;&#29992;&#21487;&#35299;&#37322;&#25110;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;AI&#65292;&#20854;&#20013;&#29992;&#25143;&#22312;&#25968;&#23383;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#24230;&#26159;&#20174;&#19978;&#19979;&#25991;&#65288;&#22914;&#24773;&#32490;&#12289;&#27963;&#21160;&#12289;&#22825;&#27668;&#31561;&#65289;&#20013;&#30830;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;AI&#31995;&#32479;&#30340;&#36755;&#20986;&#24182;&#19981;&#24635;&#26159;&#31995;&#32479;&#19978;&#27491;&#30830;&#30340;&#65292;&#36890;&#24120;&#26159;&#31995;&#32479;&#19978;&#27491;&#30830;&#30340;&#65292;&#20294;&#26126;&#26174;&#19981;&#23436;&#32654;&#65292;&#20174;&#32780;&#36896;&#25104;&#22256;&#24785;&#65292;&#20363;&#22914;&#65292;&#20026;&#20160;&#20040;&#20570;&#20986;&#36825;&#20010;&#20915;&#23450;&#65311;&#32972;&#21518;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#21046;&#23450;&#20102;&#34892;&#20026;&#25366;&#25496;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#20026;&#23384;&#22312;&#26469;&#33258;&#29992;&#25143;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.00016v2 Announce Type: replace  Abstract: According to the latest trend of artificial intelligence, AI-systems needs to clarify regarding general,specific decisions,services provided by it. Only consumer is satisfied, with explanation , for example, why any classification result is the outcome of any given time. This actually motivates us using explainable or human understandable AI for a behavioral mining scenario, where users engagement on digital platform is determined from context, such as emotion, activity, weather, etc. However, the output of AI-system is not always systematically correct, and often systematically correct, but apparently not-perfect and thereby creating confusions, such as, why the decision is given? What is the reason underneath? In this context, we first formulate the behavioral mining problem in deep convolutional neural network architecture. Eventually, we apply a recursive neural network due to the presence of time-series data from users physiolog
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;SCFL&#65292;&#19968;&#31181;&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2212.13992</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Social-Aware Clustered Federated Learning with Customized Privacy Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.13992
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;SCFL&#65292;&#19968;&#31181;&#20855;&#26377;&#23450;&#21046;&#38544;&#31169;&#20445;&#25252;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#20445;&#25252;&#31471;&#29992;&#25143;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#20013;&#20173;&#28982;&#23384;&#22312;&#36890;&#36807;&#20132;&#25442;&#26799;&#24230;&#21487;&#33021;&#23548;&#33268;&#30340;&#28508;&#22312;&#38544;&#31169;&#27844;&#28431;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#24494;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35745;&#31639;&#32467;&#26524;&#28155;&#21152;&#22122;&#22768;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24320;&#38144;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#20043;&#38388;&#30340;&#26222;&#36941;&#31038;&#20132;&#36830;&#25509;&#65292;&#24179;&#34913;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20132;&#24863;&#30693;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;SCFL&#65292;&#20854;&#20013;&#30456;&#20114;&#20449;&#20219;&#30340;&#20010;&#20307;&#21487;&#20197;&#33258;&#30001;&#32452;&#25104;&#19968;&#20010;&#31038;&#20132;&#38598;&#32676;&#65292;&#24182;&#22312;&#27599;&#20010;&#38598;&#32676;&#20869;&#32858;&#21512;&#20182;&#20204;&#30340;&#21407;&#22987;&#27169;&#22411;&#26356;&#26032;&#65288;&#20363;&#22914;&#26799;&#24230;&#65289;&#65292;&#28982;&#21518;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#12290;&#36890;&#36807;&#22312;&#31038;&#20132;&#32676;&#20307;&#20013;&#28151;&#21512;&#27169;&#22411;&#26356;&#26032;&#65292;&#23545;&#25163;&#21482;&#33021;&#31363;&#21548;&#31038;&#20132;&#23618;&#32452;&#21512;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#27861;&#31363;&#21548;&#21040;&#20010;&#20307;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.13992v2 Announce Type: replace-cross  Abstract: A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2211.10777</link><description>&lt;p&gt;
&#26080;&#30456;&#24178;&#31354;&#20013;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Non-Coherent Over-the-Air Decentralized Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35832;&#22914;&#36828;&#31243;&#24863;&#30693;&#12289;&#20998;&#24067;&#24335;&#25512;&#26029;&#12289;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#21508;&#31181;&#39046;&#22495;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#21463;&#21040;&#22122;&#22768;&#12289;&#34928;&#33853;&#21644;&#24102;&#23485;&#21463;&#38480;&#30340;&#26080;&#32447;&#31995;&#32479;&#19978;&#25191;&#34892;DGD&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#24230;&#20256;&#36755;&#20197;&#20943;&#36731;&#24178;&#25200;&#65292;&#24182;&#33719;&#21462;&#25299;&#25169;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36825;&#22312;&#26080;&#32447;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#26080;&#32447;&#31995;&#32479;&#23450;&#21046;&#30340;DGD&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#26080;&#38656;&#36827;&#34892;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#26080;&#30456;&#24178;&#31354;&#20013;&#65288;NCOTA&#65289;&#20849;&#35782;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#22122;&#22768;&#33021;&#37327;&#21472;&#21152;&#29305;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#21270;&#20256;&#36755;&#31574;&#30053;&#26469;&#36866;&#24212;&#21322;&#21452;&#24037;&#25805;&#20316;&#65292;&#21457;&#23556;&#26426;&#23558;&#20301;&#32622;&#26144;&#23556;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10777v2 Announce Type: replace-cross  Abstract: Decentralized Gradient Descent (DGD) is a popular algorithm used to solve decentralized optimization problems in diverse domains such as remote sensing, distributed inference, multi-agent coordination, and federated learning. Yet, executing DGD over wireless systems affected by noise, fading and limited bandwidth presents challenges, requiring scheduling of transmissions to mitigate interference and the acquisition of topology and channel state information -- complex tasks in wireless decentralized systems. This paper proposes a DGD algorithm tailored to wireless systems. Unlike existing approaches, it operates without inter-agent coordination, topology information, or channel state information. Its core is a Non-Coherent Over-The-Air (NCOTA) consensus scheme, exploiting a noisy energy superposition property of wireless channels. With a randomized transmission strategy to accommodate half-duplex operation, transmitters map loca
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;MKTFHE&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#21644;&#26356;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31192;&#23494;&#20998;&#20139;&#21644;&#35774;&#35745;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#23494;&#21327;&#35758;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#38750;&#32447;&#24615;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2211.09353</link><description>&lt;p&gt;
&#22522;&#20110;MKTFHE&#30340;&#26356;&#23433;&#20840;&#21644;&#26356;&#24555;&#36895;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SFPDML: Securer and Faster Privacy-Preserving Distributed Machine Learning based on MKTFHE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.09353
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;MKTFHE&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#21644;&#26356;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31192;&#23494;&#20998;&#20139;&#21644;&#35774;&#35745;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#23494;&#21327;&#35758;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#38750;&#32447;&#24615;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22810;&#23494;&#38053;&#21516;&#24577;&#21152;&#23494;&#65288;MKTFHE&#65289;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#26696;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;MKTFHE&#30340;&#35299;&#23494;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20851;&#20110;MKTFHE&#30340;&#26368;&#26032;&#24037;&#20316;&#20165;&#25903;&#25345;&#24067;&#23572;&#36816;&#31639;&#21644;&#32447;&#24615;&#36816;&#31639;&#65292;&#26080;&#27861;&#30452;&#25509;&#35745;&#31639;Sigmoid&#31561;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#20173;&#28982;&#24456;&#38590;&#22312;&#39640;&#24615;&#33021;&#19979;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#21457;&#29616;&#20102;&#38024;&#23545;&#29616;&#26377;MKTFHE&#20998;&#24067;&#24335;&#35299;&#23494;&#21327;&#35758;&#30340;&#21487;&#33021;&#25915;&#20987;&#65292;&#38543;&#21518;&#24341;&#20837;&#20102;&#31192;&#23494;&#20998;&#20139;&#20197;&#25552;&#20986;&#26356;&#23433;&#20840;&#30340;&#35299;&#23494;&#21327;&#35758;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;\emph{&#21516;&#24577;&#21270;&#22120;}&#21644;\emph{&#27604;&#36739;&#22235;&#37325;&#23376;}&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MKTFHE&#21451;&#22909;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.09353v2 Announce Type: replace-cross  Abstract: In recent years, distributed machine learning has garnered significant attention. However, privacy continues to be an unresolved issue within this field. Multi-key homomorphic encryption over torus (MKTFHE) is one of the promising candidates for addressing this concern. Nevertheless, there may be security risks in the decryption of MKTFHE. Moreover, to our best known, the latest works about MKTFHE only support Boolean operation and linear operation which cannot directly compute the non-linear function like Sigmoid. Therefore, it is still hard to perform common machine learning such as logistic regression and neural networks in high performance. In this paper, we first discover a possible attack on the existing distributed decryption protocol for MKTFHE and subsequently introduce secret sharing to propose a securer one. Next, we design a new MKTFHE-friendly activation function via \emph{homogenizer} and \emph{compare quads}. Fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#30446;&#26631;&#26799;&#24230;&#26657;&#27491;&#65288;MoCo&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#25910;&#25947;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#26799;&#24230;&#20559;&#24046;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.12624</link><description>&lt;p&gt;
&#32531;&#35299;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#24046;&#65306;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#38543;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#30446;&#26631;&#26799;&#24230;&#26657;&#27491;&#65288;MoCo&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#25910;&#25947;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#26799;&#24230;&#20559;&#24046;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#20989;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#36890;&#24120;&#20986;&#29616;&#22312;&#38656;&#35201;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#65288;&#22914;&#20844;&#24179;&#24615;&#65292;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#65289;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#65307;&#25110;&#32773;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#22810;&#20010;&#20219;&#21153;&#32852;&#21512;&#20248;&#21270;&#65292;&#20849;&#20139;&#23427;&#20204;&#20043;&#38388;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38543;&#26426;&#22810;&#30446;&#26631;&#26799;&#24230;&#26041;&#27861;&#21450;&#20854;&#21464;&#20307;&#65288;&#20363;&#22914;&#65292;MGDA&#65292;PCGrad&#65292;CAGrad&#31561;&#65289;&#37117;&#37319;&#29992;&#24102;&#20559;&#24046;&#30340;&#22122;&#22768;&#26799;&#24230;&#26041;&#21521;&#65292;&#23548;&#33268;&#32463;&#39564;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#38543;&#26426;&#22810;&#30446;&#26631;&#26799;&#24230;&#26657;&#27491;&#65288;MoCo&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#21363;&#20351;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20063;&#33021;&#20445;&#35777;&#25910;&#25947;&#32780;&#19981;&#22686;&#21152;&#25209;&#37327;&#22823;&#23567;&#12290;&#23545;&#22810;&#20219;&#21153;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12624v2 Announce Type: replace  Abstract: Machine learning problems with multiple objective functions appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic Multi-objective gradient Correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the non-convex setting. Simulations on multi-task supervised and reinforcement learning demonstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23884;&#22871;&#20989;&#25968;&#32452;&#21512;&#30340;Riemannian&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#27425;&#35843;&#29992;&#38543;&#26426;&#26799;&#24230;oracle&#25214;&#21040;&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2207.09350</link><description>&lt;p&gt;
Riemannian&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#23884;&#22871;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Riemannian Stochastic Gradient Method for Nested Composition Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09350
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23884;&#22871;&#20989;&#25968;&#32452;&#21512;&#30340;Riemannian&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#27425;&#35843;&#29992;&#38543;&#26426;&#26799;&#24230;oracle&#25214;&#21040;&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;Riemann&#27969;&#24418;&#19978;&#38024;&#23545;&#23884;&#22871;&#24418;&#24335;&#20013;&#21253;&#21547;&#26399;&#26395;&#30340;&#20989;&#25968;&#32452;&#21512;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#25110;&#20803;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#23450;&#21046;&#31561;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Riemann&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#65288;R-SCGD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20004;&#32423;&#32452;&#21512;&#20248;&#21270;&#65292;&#22312;$O(\epsilon^{-2})$&#27425;&#23545;&#22806;&#37096;&#20989;&#25968;&#30340;&#38543;&#26426;&#26799;&#24230;oracle&#20197;&#21450;&#20869;&#37096;&#20989;&#25968;&#30340;&#38543;&#26426;&#20989;&#25968;&#21644;&#26799;&#24230;oracles&#30340;&#35843;&#29992;&#20013;&#23547;&#25214;&#26399;&#26395;&#24179;&#26041;Riemann&#26799;&#24230;&#23567;&#20110;$\epsilon$&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09350v2 Announce Type: replace-cross  Abstract: This work considers optimization of composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\epsilon$, in $O(\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#36716;&#25442;&#31639;&#27861;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#37327;&#21270;&#35823;&#24046;&#12289;&#21098;&#20999;&#35823;&#24046;&#21644;&#27531;&#20313;&#33180;&#30005;&#20301;&#34920;&#31034;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#25439;&#22833;less&#30340;ANN-SNN&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#22312;&#36229;&#20302;&#24310;&#36831;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2205.07473</link><description>&lt;p&gt;
&#23454;&#29616;&#36229;&#20302;&#24310;&#36831;&#19979;&#25439;&#22833;less ANN-SNN&#36716;&#25442;&#30340;&#21452;&#30456;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.07473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#36716;&#25442;&#31639;&#27861;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#37327;&#21270;&#35823;&#24046;&#12289;&#21098;&#20999;&#35823;&#24046;&#21644;&#27531;&#20313;&#33180;&#30005;&#20301;&#34920;&#31034;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#25439;&#22833;less&#30340;ANN-SNN&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#22312;&#36229;&#20302;&#24310;&#36831;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#33033;&#20914;&#32593;&#32476; (SNNs) &#20197;&#24322;&#27493;&#31163;&#25955;&#20107;&#20214;&#25805;&#20316;&#65292;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#33021;&#32791;&#25928;&#29575;&#21644;&#31232;&#30095;&#35745;&#31639;&#12290;&#23454;&#29616;&#28145;&#24230; SNNs &#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#32467;&#21512; ANN-SNN &#36716;&#25442;&#26469;&#21516;&#26102;&#23454;&#29616; ANN &#30340;&#39640;&#25928;&#35757;&#32451;&#21644; SNN &#30340;&#39640;&#25928;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#31934;&#24230;&#25439;&#22833;&#36890;&#24120;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#37327;&#26102;&#38388;&#27493;&#19979;&#65292;&#36825;&#22823;&#22823;&#38480;&#21046;&#20102; SNN &#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#39318;&#20808;&#30830;&#23450;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#28304;&#33258;&#20110; SNNs &#20013;&#36127;&#38754;&#25110;&#28322;&#20986;&#27531;&#20313;&#33180;&#30005;&#20301;&#30340;&#38169;&#35823;&#34920;&#31034;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36716;&#25442;&#35823;&#24046;&#20998;&#35299;&#20026;&#19977;&#37096;&#20998;&#65306;&#37327;&#21270;&#35823;&#24046;&#12289;&#21098;&#20999;&#35823;&#24046;&#21644;&#27531;&#20313;&#33180;&#30005;&#20301;&#34920;&#31034;&#35823;&#24046;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36716;&#25442;&#31639;&#27861;&#65292;&#20998;&#21035;&#26368;&#23567;&#21270;&#36825;&#20123;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#27599;&#20010;&#38454;&#27573;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.07473v3 Announce Type: replace-cross  Abstract: Spiking neural networks (SNNs) operating with asynchronous discrete events show higher energy efficiency with sparse computation. A popular approach for implementing deep SNNs is ANN-SNN conversion combining both efficient training of ANNs and efficient inference of SNNs. However, the accuracy loss is usually non-negligible, especially under a few time steps, which restricts the applications of SNN on latency-sensitive edge devices greatly. In this paper, we first identify that such performance degradation stems from the misrepresentation of the negative or overflow residual membrane potential in SNNs. Inspired by this, we decompose the conversion error into three parts: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a two-stage conversion algorithm to minimize those errors respectively. Besides, We show each stage achieves significant performance gains i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15771</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#25968;&#25454;&#39537;&#21160;&#40065;&#26834;&#20248;&#21270;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36890;&#24120;&#28041;&#21450;&#20248;&#21270;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#38505;&#20934;&#21017;&#12290;&#39118;&#38505;&#36890;&#24120;&#26159;&#26681;&#25454;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#35745;&#31639;&#30340;&#65292;&#20294;&#30001;&#20110;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#21644;&#19981;&#22909;&#30340;&#26679;&#26412;&#22806;&#34920;&#29616;&#12290;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40065;&#26834;&#20934;&#21017;&#65292;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#65288;&#21363;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65289;&#29702;&#35770;&#21644;&#26368;&#36817;&#30340;&#24179;&#28369;&#27169;&#31946;&#35268;&#36991;&#20559;&#22909;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#30340;&#26032;&#36830;&#25509;&#65292;&#20854;&#20013;&#21253;&#25324;&#23725;&#22238;&#24402;&#21644;&#22871;&#32034;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#40065;&#26834;&#20248;&#21270;&#36807;&#31243;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#32479;&#35745;&#20445;&#35777;&#26041;&#38754;&#30340;&#26377;&#21033;&#24615;&#23384;&#22312;&#12290;&#23545;&#20110;&#23454;&#38469;&#23454;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#34920;&#31034;&#30340;&#21487;&#34892;&#36817;&#20284;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10191</link><description>&lt;p&gt;
&#20998;&#32780;&#19981;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#36873;&#25321;&#24615;&#35757;&#32451;&#19987;&#23478;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10191
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#27969;&#34892;&#65292;&#27169;&#22411;&#33021;&#22815;&#25299;&#23485;&#24212;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#24536;&#35760;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#36235;&#21183;&#26159;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25216;&#26415;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#20849;&#21516;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19987;&#23478;&#36890;&#24120;&#20250;&#19968;&#27425;&#24615;&#20351;&#29992;&#25972;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26679;&#20250;&#22686;&#21152;&#36951;&#24536;&#30340;&#39118;&#38505;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#12290;SEED&#20165;&#36873;&#25321;&#19968;&#20010;&#34987;&#35748;&#20026;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#22788;&#29702;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#23545;&#36825;&#20010;&#19987;&#23478;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#27599;&#20010;&#19987;&#23478;&#29992;&#39640;&#26031;&#20998;&#24067;&#34920;&#31034;&#27599;&#20010;&#31867;&#21035;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#36873;&#25321;&#26368;&#20248;&#19987;&#23478;&#12290;&#22240;&#27492;&#65292;SEED&#22312;&#20445;&#25345;&#38598;&#25104;&#26041;&#27861;&#30340;&#39640;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#22686;&#21152;&#20102;&#19987;&#23478;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SEED&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#22238;&#25910;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#31070;&#32463;&#31639;&#23376;&#22240;&#20854;&#39640;&#25512;&#29702;&#25928;&#29575;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#23376;&#38656;&#35201;&#29983;&#25104;&#22823;&#37327;&#24102;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#21363;PDE&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35299;&#20915;&#22823;&#37327;&#32447;&#24615;&#26041;&#31243;&#32452;&#20197;&#33719;&#24471;PDE&#30340;&#25968;&#20540;&#35299;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#23548;&#33268;&#35745;&#31639;&#26497;&#20854;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;(SKR)&#65292;&#20197;&#25552;&#39640;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SKR&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#25968;&#25454;&#29983;&#25104;&#32791;&#26102;&#24615;&#36136;&#30340;&#23581;&#35797;&#12290;SKR&#30340;&#26680;&#24515;&#26159;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08876</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#23427;&#20204;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#34920;&#31034;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#39044;&#27880;&#20876;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#21644;&#26174;&#31034;Top-1&#21644;Top-k&#39044;&#27979;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#30340;&#20934;&#30830;&#24615;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30456;&#24403;&#25110;&#31245;&#20302;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22270;&#20687;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#65292;&#39044;&#27979;&#38598;&#22312;&#36741;&#21161;&#20154;&#31867;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23454;&#36341;&#20013;&#24378;&#35843;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#12290;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#20219;&#20309;&#24378;&#24230;&#20026;&#22522;&#30784;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#20851;&#20110;&#24207;&#21015;&#21382;&#21490;&#26465;&#20214;&#19979;&#30340;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15045</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#30340;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#12290;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#20219;&#20309;&#24378;&#24230;&#20026;&#22522;&#30784;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#20851;&#20110;&#24207;&#21015;&#21382;&#21490;&#26465;&#20214;&#19979;&#30340;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#30340;&#32479;&#35745;&#21442;&#25968;&#27169;&#22411;&#24037;&#20855;&#31665;&#20013;&#65292;&#31070;&#32463;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#12290;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#27599;&#20010;&#20107;&#20214;&#19982;&#21333;&#20010;&#39033;&#30446;&#65288;&#21333;&#20010;&#20107;&#20214;&#31867;&#22411;&#25110;&#8220;&#26631;&#35760;&#8221;&#65289;&#30456;&#20851;&#32852;&#30340;&#24207;&#21015;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#27599;&#20010;&#20107;&#20214;&#19982;&#19968;&#32452;&#39033;&#30446;&#30456;&#20851;&#32852;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#38598;&#21512;&#25968;&#20540;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#19982;&#20219;&#20309;&#22522;&#20110;&#24378;&#24230;&#30340;&#36882;&#24402;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22238;&#31572;&#35832;&#22914;&#8220;&#22312;&#32771;&#34385;&#24207;&#21015;&#21382;&#21490;&#30340;&#26465;&#20214;&#19979;&#65292;&#39033;&#30446;A&#22312;&#39033;&#30446;B&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#8221;&#31561;&#27010;&#29575;&#26597;&#35810;&#38382;&#39064;&#12290;&#30001;&#20110;&#38382;&#39064;&#35774;&#32622;&#30340;&#36830;&#32493;&#26102;&#38388;&#24615;&#36136;&#21644;&#27599;&#20010;&#20107;&#20214;&#30340;&#28508;&#22312;&#32467;&#26524;&#31354;&#38388;&#30340;&#32452;&#21512;&#26497;&#22823;&#65292;&#23545;&#20110;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#65292;&#35745;&#31639;&#36825;&#20123;&#26597;&#35810;&#30340;&#31934;&#30830;&#31572;&#26696;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural marked temporal point processes have been a valuable addition to the existing toolbox of statistical parametric models for continuous-time event data. These models are useful for sequences where each event is associated with a single item (a single type of event or a "mark") -- but such models are not suited for the practical situation where each event is associated with a set of items. In this work, we develop a general framework for modeling set-valued data in continuous-time, compatible with any intensity-based recurrent neural point process model. In addition, we develop inference methods that can use such models to answer probabilistic queries such as "the probability of item $A$ being observed before item $B$," conditioned on sequence history. Computing exact answers for such queries is generally intractable for neural models due to both the continuous-time nature of the problem setting and the combinatorially-large space of potential outcomes for each event. To address th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.12784</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#29992;&#20110;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks. (arXiv:2312.12784v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#22312;&#20808;&#36827;&#21322;&#23548;&#20307;&#24037;&#33402;&#24320;&#21457;&#20013;&#23454;&#29616;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#26368;&#20339;&#21270;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#22312;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#33455;&#29255;&#32467;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#24037;&#33402;-&#30005;&#21387;-&#28201;&#24230;&#65288;PVT&#65289;&#35282;&#21644;&#25216;&#26415;&#21442;&#25968;&#19978;&#23637;&#31034;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;&#22312;512&#20010;&#26410;&#35265;&#36807;&#30340;&#24037;&#33402;&#35282;&#21644;&#19968;&#30334;&#19975;&#20010;&#27979;&#35797;&#25968;&#25454;&#28857;&#30340;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;33&#31181;&#31867;&#22411;&#30340;&#21333;&#20803;&#30340;&#24310;&#36831;&#12289;&#21151;&#29575;&#21644;&#36755;&#20837;&#24341;&#33050;&#30005;&#23481;&#20855;&#26377;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#22343;&#26041;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#8804; 0.95%&#65292;&#19982;SPICE&#20223;&#30495;&#30456;&#27604;&#21152;&#36895;&#20102;100&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31995;&#32479;&#32423;&#25351;&#26631;&#65292;&#22914;&#26368;&#24046;&#36127;&#26494;&#24347;&#65288;WNS&#65289;&#12289;&#28431;&#30005;&#21151;&#32791;&#21644;&#21160;&#24577;...
&lt;/p&gt;
&lt;p&gt;
Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.11730</link><description>&lt;p&gt;
&#24378;&#21270;&#22270;&#36716;&#25442;&#22120;&#19982;&#27491;&#21017;&#21270;&#20851;&#27880;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#29256;&#26412;&#65292;&#29992;&#20110;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23384;&#38382;&#39064;&#19978;&#23384;&#22312;&#30340;&#22256;&#25200;&#12290;&#19982;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;Graph Transformer&#30456;&#27604;&#65292;&#24212;&#29992;&#20102;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#20869;&#23384;&#28040;&#32791;&#22823;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26368;&#36817;&#21457;&#29616;&#65292;&#22522;&#20110;Transformer&#30340;GNN&#31216;&#20026;Graph Transformer&#22312;&#23384;&#22312;&#38271;&#31243;&#20381;&#36182;&#24615;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#25968;&#25454;&#21644;Transformer&#26550;&#26500;&#30456;&#32467;&#21512;&#23548;&#33268;&#20102;&#35760;&#24518;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#8221;&#30340;&#29256;&#26412;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;GT&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#28165;&#26970;&#22312;&#20301;&#32622;&#32534;&#30721;&#30340;&#22522;&#30784;&#19978;&#26159;&#21542;&#26377;&#36793;&#32536;&#27491;&#21017;&#21270;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#28982;&#32780;&#65292;&#26174;&#28982;&#65292;&#24212;&#29992;&#25105;&#20204;&#30340;&#36793;&#32536;&#27491;&#21017;&#21270;&#25216;&#26415;&#30830;&#23454;&#21487;&#20197;&#31283;&#23450;&#22320;&#25913;&#21892;GT&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;GT&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are notorious for its memory consumption. A recent Transformer-based GNN called Graph Transformer is shown to obtain superior performances when long range dependencies exist. However, combining graph data and Transformer architecture led to a combinationally worse memory issue. We propose a novel version of "edge regularization technique" that alleviates the need for Positional Encoding and ultimately alleviate GT's out of memory issue. We observe that it is not clear whether having an edge regularization on top of positional encoding is helpful. However, it seems evident that applying our edge regularization technique indeed stably improves GT's performance compared to GT without Positional Encoding.
&lt;/p&gt;</description></item><item><title>TrojFST&#26159;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...&#31561;&#27169;&#22359;&#26469;&#35299;&#20915;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.10467</link><description>&lt;p&gt;
TrojFST: &#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#23884;&#20837;&#21040;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#20013;
&lt;/p&gt;
&lt;p&gt;
TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10467
&lt;/p&gt;
&lt;p&gt;
TrojFST&#26159;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...&#31561;&#27169;&#22359;&#26469;&#35299;&#20915;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#20351;&#29992;&#26377;&#38480;&#36755;&#20837;&#26679;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#22788;&#29702;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#25104;&#21151;&#23548;&#33268;&#23545;&#25163;&#35797;&#22270;&#38024;&#23545;&#35813;&#25216;&#26415;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#12290;&#20043;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#25915;&#20987;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#38656;&#35201;&#20840;&#27169;&#22411;&#24494;&#35843;&#25110;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#65292;&#36825;&#28041;&#21450;&#20923;&#32467;PLM&#24182;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#36755;&#20837;&#26679;&#26412;&#19978;&#35843;&#20248;&#36719;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#27745;&#26579;&#25968;&#25454;&#38598;&#65292;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#19988;&#32570;&#20047;&#27880;&#24847;&#21147;&#24863;&#30693;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;TrojFST&#29992;&#20110;&#21518;&#38376;&#25915;&#20987;&#12290;TrojFST&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro
&lt;/p&gt;</description></item><item><title>MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18430</link><description>&lt;p&gt;
MCRAGE: &#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#21307;&#30103;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MCRAGE: Synthetic Healthcare Data for Fairness. (arXiv:2310.18430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18430
&lt;/p&gt;
&lt;p&gt;
MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24320;&#21457;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#21307;&#30103;&#36164;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#25968;&#25454;&#38598;&#22312;&#31181;&#26063;/&#27665;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#31561;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#22312;&#31867;&#19981;&#24179;&#34913;&#30340;EHR&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#65292;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#30340;&#20010;&#20307;&#32780;&#35328;&#65292;&#34920;&#29616;&#26174;&#33879;&#19981;&#22914;&#22810;&#25968;&#32676;&#20307;&#30340;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23569;&#25968;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#21307;&#30103;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30001;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;MCRAGE&#36807;&#31243;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#23569;&#25968;&#32676;&#20307;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#21512;&#25104;EHR&#26679;&#26412;&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDDPM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#19968;&#33268;&#24615;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;Polya&#29699;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#21487;&#29992;&#24615;&#21644;&#26356;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18108</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Transductive conformal inference with adaptive scores. (arXiv:2310.18108v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18108
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36716;&#23548;&#24335;&#19968;&#33268;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#19968;&#33268;&#24615;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;Polya&#29699;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#24471;&#20998;&#30340;&#21487;&#29992;&#24615;&#21644;&#26356;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#25512;&#26029;&#26159;&#19968;&#31181;&#22522;&#26412;&#19988;&#22810;&#29992;&#36884;&#30340;&#24037;&#20855;&#65292;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26080;&#20998;&#24067;&#20445;&#35777;&#12290;&#25105;&#20204;&#32771;&#34385;&#36716;&#23548;&#24335;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;$m$&#20010;&#26032;&#26679;&#26412;&#36827;&#34892;&#20915;&#31574;&#65292;&#20135;&#29983;$m$&#20010;&#19968;&#33268;&#25512;&#26029;$p$&#20540;&#12290;&#34429;&#28982;&#32463;&#20856;&#32467;&#26524;&#20165;&#28041;&#21450;&#20854;&#36793;&#38469;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#36981;&#24490;&#19968;&#20010;Polya&#29699;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#32463;&#39564;&#20998;&#24067;&#20989;&#25968;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20219;&#24847;&#21487;&#20132;&#25442;&#30340;&#24471;&#20998;&#65292;&#21253;&#25324;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#27979;&#35797;+&#26657;&#20934;&#26679;&#26412;&#30340;&#21327;&#21464;&#37327;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#8220;&#33258;&#36866;&#24212;&#8221;&#24471;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24403;&#21069;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#36716;&#23548;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#21306;&#38388;&#39044;&#27979;&#21644;&#22522;&#20110;&#20004;&#31867;&#20998;&#31867;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#65289;&#25552;&#20379;&#32479;&#19968;&#19988;&#22312;&#27010;&#29575;&#19978;&#30340;&#20445;&#35777;&#26469;&#28436;&#31034;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. {While classical results only concern their marginal distribution, we show that their joint distribution follows a P\'olya urn model, and establish a concentration inequality for their empirical distribution function.} The results hold for arbitrary exchangeable scores, including {\it adaptive} ones that can use the covariates of the test+calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;PGA&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#29289;&#20307;&#25235;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.12547</link><description>&lt;p&gt;
PGA: &#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#19982;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12547
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;PGA&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#29289;&#20307;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26465;&#20214;&#21270;&#26426;&#22120;&#20154;&#25235;&#21462;&#65288;LCRG&#65289;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#29289;&#20307;&#30340;&#25509;&#22320;&#21644;&#25235;&#21462;&#12290;&#34429;&#28982;&#33021;&#22815;&#35782;&#21035;&#20010;&#20154;&#29289;&#21697;&#22914;&#8220;&#25105;&#30340;&#38065;&#21253;&#8221;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#33258;&#28982;&#22320;&#19982;&#38750;&#19987;&#23478;&#29992;&#25143;&#20132;&#20114;&#65292;&#20294;&#24403;&#21069;&#30340;LCRG&#31995;&#32479;&#20027;&#35201;&#38480;&#21046;&#26426;&#22120;&#20154;&#21482;&#33021;&#29702;&#35299;&#19968;&#33324;&#34920;&#36798;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;GraspMine&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#20013;&#23398;&#20064;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;GraspMine&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#20256;&#25773;&#21040;&#20010;&#20154;&#29289;&#20307;&#19978;&#65292;&#36890;&#36807;Reminiscence-&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#19968;&#31995;&#21015;&#21407;&#22987;&#22270;&#20687;&#65292;&#33719;&#21462;&#20010;&#20154;&#29289;&#20307;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PGA&#36890;&#36807;&#29992;&#25143;&#23637;&#31034;&#24102;&#26377;&#30456;&#20851;&#25351;&#31034;&#22120;&#30340;&#20010;&#20154;&#29289;&#20307;&#65292;&#24182;&#20197;&#26059;&#36716;&#30340;&#26041;&#24335;&#26816;&#26597;&#29289;&#20307;&#26469;&#33719;&#21462;&#20010;&#20154;&#29289;&#20307;&#20449;&#24687;&#12290;&#26681;&#25454;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;PGA&#20026;&#29289;&#20307;&#36827;&#34892;&#20266;&#26631;&#31614;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like "my wallet" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#23427;&#21033;&#29992;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#30028;&#23450;&#38544;&#31169;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.15250</link><description>&lt;p&gt;
&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;&#21450;&#20854;&#22312;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Relative Gaussian Mechanism and its Application to Private Gradient Descent. (arXiv:2308.15250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#23427;&#21033;&#29992;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#30028;&#23450;&#38544;&#31169;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#26426;&#21046;(GM)&#26159;&#19968;&#31181;&#22312;&#21457;&#24067;&#20043;&#21069;&#21521;&#30690;&#37327;&#26597;&#35810;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#30340;&#26631;&#20934;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#26597;&#35810;&#28385;&#36275;&#26576;&#31181;L2&#25935;&#24863;&#24615;&#23646;&#24615;(&#20219;&#24847;&#20004;&#20010;&#30456;&#37051;&#36755;&#20837;&#19978;&#36755;&#20986;&#20043;&#38388;&#30340;L2&#36317;&#31163;&#26377;&#30028;)&#65292;GM&#20445;&#35777;&#20102;R&#233;nyi&#24046;&#20998;&#38544;&#31169;(RDP)&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31934;&#30830;&#22320;&#30028;&#23450;L2&#25935;&#24863;&#24615;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#20174;&#32780;&#23548;&#33268;&#26494;&#24347;&#30340;&#38544;&#31169;&#30028;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#20004;&#20010;&#26597;&#35810;&#36755;&#20986;&#20043;&#38388;&#30340;&#36317;&#31163;&#30028;&#38480;&#20063;&#21487;&#33021;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#33539;&#25968;&#12290;&#21033;&#29992;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#20854;&#20013;&#22122;&#22768;&#30340;&#26041;&#24046;&#21462;&#20915;&#20110;&#36755;&#20986;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#19979;&#35777;&#26126;&#20102;RDP&#21442;&#25968;&#30340;&#20005;&#26684;&#30028;&#38480;&#65292;&#24182;&#25551;&#36848;&#20102;&#22240;&#20351;&#29992;&#36755;&#20986;&#30456;&#20851;&#22122;&#22768;&#32780;&#20135;&#29983;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGM&#21487;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;&#28508;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a vector-valued query before releasing it, is a standard privacy protection mechanism. In particular, given that the query respects some L2 sensitivity property (the L2 distance between outputs on any two neighboring inputs is bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately, precisely bounding the L2 sensitivity can be hard, thus leading to loose privacy bounds. In this work, we consider a Relative L2 sensitivity assumption, in which the bound on the distance between two query outputs may also depend on their norm. Leveraging this assumption, we introduce the Relative Gaussian Mechanism (RGM), in which the variance of the noise depends on the norm of the output. We prove tight bounds on the RDP parameters under relative L2 sensitivity, and characterize the privacy loss incurred by using output-dependent noise. In particular, we show that RGM naturally adapts to a latent variable that would
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25197;&#26354;&#20960;&#20309;&#23398;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#25214;&#21040;&#20102;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.08305</link><description>&lt;p&gt;
&#22312;&#27431;&#20960;&#37324;&#24503;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#25197;&#26354;&#20960;&#20309;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Warped geometric information on the optimisation of Euclidean functions. (arXiv:2308.08305v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08305
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25197;&#26354;&#20960;&#20309;&#23398;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#25214;&#21040;&#20102;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#28508;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#32479;&#35745;&#25512;&#26029;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#25197;&#26354;&#40654;&#26364;&#20960;&#20309;&#27010;&#24565;&#65292;&#23558;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#30340;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#24102;&#26377;&#25197;&#26354;&#24230;&#37327;&#30340;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#22312;&#35813;&#27969;&#24418;&#19978;&#25214;&#21040;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#36873;&#25321;&#29992;&#20110;&#25628;&#32034;&#22495;&#30340;&#25197;&#26354;&#24230;&#37327;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#21451;&#22909;&#30340;&#24230;&#37327;&#24352;&#37327;&#65292;&#20351;&#24471;&#22312;&#27969;&#24418;&#19978;&#25214;&#21040;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#19982;&#27979;&#22320;&#32447;&#21464;&#24471;&#26356;&#23481;&#26131;&#35745;&#31639;&#12290;&#27839;&#27979;&#22320;&#32447;&#36827;&#34892;&#20248;&#21270;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#27969;&#24418;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#26512;&#22320;&#24471;&#21040;&#39640;&#36798;&#19977;&#38454;&#30340;&#27888;&#21202;&#36817;&#20284;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#23545;&#27979;&#22320;&#32447;&#30340;&#36817;&#20284;&#19981;&#20250;&#20301;&#20110;&#27969;&#24418;&#19978;&#65292;&#20294;&#25105;&#20204;&#26500;&#36896;&#20102;&#21512;&#36866;&#30340;&#22238;&#32553;&#26041;&#31243;&#23558;&#36825;&#20123;&#36817;&#20284;&#37325;&#26032;&#26144;&#23556;&#21040;&#27969;&#24418;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.08929</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#21270;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37117;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#34987;&#24191;&#27867;&#29992;&#20110;&#39044;&#27979;&#21644;&#27169;&#25311;&#38081;&#30005;&#21644;&#24347;&#35947;&#38081;&#30005;&#20307;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#22797;&#26434;&#65292;&#24456;&#38590;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;/&#25110;&#22797;&#26434;&#32452;&#20998;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#30340;&#21363;&#26102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#12290;&#21442;&#25968;&#21270;&#26159;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23436;&#25104;&#30340;&#65292;&#27599;&#19968;&#27493;&#39044;&#27979;&#33021;&#37327;&#12289;&#21147;&#21644;&#24212;&#21147;&#20197;&#21450;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#19981;&#30830;&#23450;&#24615;&#36739;&#22823;&#26102;&#65292;&#25191;&#34892;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20197;&#37325;&#26032;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35745;&#31639;&#20219;&#20309;&#25152;&#32771;&#34385;&#31995;&#32479;&#30340;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#20197;BaTiO3&#21644;Pb(Sc,Ta)O3&#20026;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06966</link><description>&lt;p&gt;
&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22810;&#27425;&#23545;&#20998;&#31163;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65307;&#26368;&#24120;&#35265;&#30340;&#32858;&#21512;&#26041;&#27861;&#26159;&#21442;&#25968;&#30340;&#31616;&#21333;&#24179;&#22343;&#12290;&#29702;&#35299;&#22312;&#38750;&#20984;&#35774;&#32622;&#65288;&#22914;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#32858;&#21512;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26377;&#25928;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#38459;&#30861;&#20102;&#33719;&#24471;&#39640;&#24615;&#33021;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;&#39057;&#32321;&#24179;&#22343;&#30340;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#35266;&#28857;&#26159;&#22312;&#29420;&#31435;&#35757;&#32451;&#26399;&#38388;&#65292;&#27169;&#22411;&#20250;&#30456;&#20114;&#28418;&#31227;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#26412;&#22320;&#21442;&#25968;&#26356;&#26032;&#21518;&#24179;&#22343;&#21487;&#33021;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20174;&#25439;&#22833;&#26354;&#38754;&#30340;&#35282;&#24230;&#26469;&#30475;&#65306;&#23545;&#20110;&#38750;&#20984;&#26354;&#38754;&#19978;&#30340;&#28857;&#65292;&#24179;&#22343;&#20540;&#21487;&#33021;&#21464;&#24471;&#20219;&#24847;&#31967;&#31957;&#12290;&#36890;&#24120;&#29992;&#20110;&#35299;&#37322;&#32852;&#37030;&#24179;&#22343;&#25104;&#21151;&#30340;&#23616;&#37096;&#20984;&#24615;&#20551;&#35774;&#19982;&#32463;&#39564;&#35777;&#25454;&#30456;&#30683;&#30462;&#65292;&#26174;&#31034;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#39640;&#25439;&#22833;&#38556;&#22721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
&lt;/p&gt;</description></item><item><title>T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03132</link><description>&lt;p&gt;
T-MARS&#65306;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#26469;&#25913;&#21892;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03132
&lt;/p&gt;
&lt;p&gt;
T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#32593;&#32476;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20026;&#23398;&#20064;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35782;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#22914;&#20309;&#31579;&#36873;&#36825;&#20123;&#26085;&#30410;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#36817;40%&#30340;LAION&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#19982;&#35828;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#25991;&#26412;&#12290;&#30452;&#35273;&#19978;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#28010;&#36153;&#36164;&#28304;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#32780;&#19981;&#26159;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#21435;&#38500;&#20063;&#21487;&#33021;&#28010;&#36153;&#65292;&#22240;&#20026;&#36825;&#20250;&#20002;&#24323;&#21253;&#21547;&#35270;&#35273;&#29305;&#24449;&#30340;&#22270;&#20687;&#65288;&#38500;&#20102;&#37325;&#21472;&#30340;&#25991;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#38750;&#38646;&#21644;&#21452;&#23618;&#20844;&#24335;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25915;&#20987;&#30456;&#21305;&#37197;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26631;&#20934;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.11035</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24212;&#34987;&#35270;&#20026;&#19968;&#20010;&#38750;&#38646;&#21644;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training Should Be Cast as a Non-Zero-Sum Game. (arXiv:2306.11035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#38750;&#38646;&#21644;&#21452;&#23618;&#20844;&#24335;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25915;&#20987;&#30456;&#21305;&#37197;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26631;&#20934;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#19968;&#20010;&#31361;&#20986;&#26041;&#27861;&#26159;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#20004;&#20010;&#29609;&#23478;&#38646;&#21644;&#33539;&#24335;&#65292;&#20854;&#20013;&#39044;&#27979;&#22120;&#34987;&#35757;&#32451;&#20197;&#23545;&#25239;&#24615;&#36873;&#25321;&#30340;&#25968;&#25454;&#25200;&#21160;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#26159;&#22522;&#20110;&#36825;&#31181;&#33539;&#24335;&#30340;&#31639;&#27861;&#24182;&#27809;&#26377;&#20135;&#29983;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36973;&#21463;&#30149;&#24577;&#34892;&#20026;&#65292;&#22914;&#24378;&#20581;&#30340;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#31181;&#32570;&#38519;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#24120;&#35265;&#22522;&#20110;&#20195;&#29702;&#30340;&#26494;&#24347;&#26041;&#27861;&#20351;&#25152;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#31283;&#20581;&#24615;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#38646;&#21644;&#21452;&#23618;&#23545;&#25239;&#35757;&#32451;&#20844;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#20248;&#21270;&#19981;&#21516;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#26631;&#20934;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation naturally yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial traini
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#23646;&#24615;&#39640;&#25928;&#30340;&#23398;&#20064;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20540;&#20989;&#25968;&#65292;&#24182;&#33021;&#22815;&#22312;&#22122;&#22768;&#19979;&#36827;&#34892;PAC&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.00673</link><description>&lt;p&gt;
&#23646;&#24615;&#39640;&#25928;&#30340;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20540;&#20989;&#25968;&#24102;&#22122;&#22768;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise. (arXiv:2306.00673v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#23646;&#24615;&#39640;&#25928;&#30340;&#23398;&#20064;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20540;&#20989;&#25968;&#65292;&#24182;&#33021;&#22815;&#22312;&#22122;&#22768;&#19979;&#36827;&#34892;PAC&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20540;&#20989;&#25968;&#65288;PTFs&#65289;&#30340;&#27010;&#24565;&#31867;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;$\mathbb{R}^n$&#19978;$K$&#31232;&#30095;&#24230;-$d$ PTFs&#30340;&#23646;&#24615;&#39640;&#25928;PAC&#23398;&#20064;&#65292;&#20854;&#20013;&#20219;&#20309;&#36825;&#26679;&#30340;&#27010;&#24565;&#20165;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;$K$&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#22312;&#39640;&#26031;&#36793;&#32536;&#20998;&#24067;&#19979;&#65292;&#21363;&#20351;&#26377;$O(\epsilon^d)$&#30340;$\eta$&#34987;&#24694;&#24847;&#22122;&#22768;Bshouty et al. (2002)&#30772;&#22351;&#65292;&#20063;&#21487;&#20197;&#22312;&#38169;&#35823;&#29575;$\epsilon$&#19979;&#20197;$O(\frac{K^{{4d}}}{\epsilon^{2d}}\cdot \log^{5d} n)$&#30340;&#26679;&#26412;PAC&#23398;&#20064;&#35813;&#31867;&#65292;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#20026;$({nd}/{\epsilon})^{O(d)}$&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#20165;&#20026;&#31232;&#30095;&#40784;&#27425;&#36229;&#24179;&#38754;&#30340;&#29305;&#27530;&#24773;&#20917;&#24314;&#31435;&#20102;&#23646;&#24615;&#39640;&#25928;&#30340;&#40065;&#26834;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#65306;1&#65289;&#23558;&#23646;&#24615;&#31232;&#30095;&#24615;&#36716;&#21270;&#20026;Hermite&#22810;&#39033;&#24335;&#22522;&#19979;chow&#21521;&#37327;&#30340;&#31232;&#30095;&#27169;&#24335;&#30340;&#32467;&#26500;&#32467;&#26524;&#65307;2&#65289;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#20197;&#21450;&#21033;&#29992;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;&#38408;&#20540;&#20989;&#25968;&#30340;&#30452;&#25509;&#21028;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of $K$-sparse degree-$d$ PTFs on $\mathbb{R}^n$, where any such concept depends only on $K$ out of $n$ attributes of the input. Our main contribution is a new algorithm that runs in time $({nd}/{\epsilon})^{O(d)}$ and under the Gaussian marginal distribution, PAC learns the class up to error rate $\epsilon$ with $O(\frac{K^{4d}}{\epsilon^{2d}} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01604</link><description>&lt;p&gt;
&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#26469;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#27979;&#36712;&#36857;&#12290;&#36890;&#36807;&#26816;&#26597;&#24213;&#23618;&#39640;&#32500;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30340;&#26377;&#25928;&#20302;&#32500;&#27969;&#24418;&#12290;&#20855;&#26377;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#32593;&#32476;&#22312;&#39044;&#27979;&#31354;&#38388;&#20869;&#20301;&#20110;&#21516;&#19968;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#27969;&#24418;&#30340;&#32454;&#33410;&#65292;&#21457;&#29616;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#32593;&#32476;&#36981;&#24490;&#21487;&#21306;&#20998;&#30340;&#36712;&#36857;&#65292;&#20294;&#20854;&#20182;&#22240;&#32032;&#24433;&#21709;&#26497;&#23567;; &#26356;&#22823;&#30340;&#32593;&#32476;&#27839;&#30528;&#19982;&#36739;&#23567;&#30340;&#32593;&#32476;&#30456;&#20284;&#30340;&#27969;&#24418;&#35757;&#32451;&#65292;&#21482;&#26159;&#26356;&#24555;; &#19981;&#21516;&#37096;&#20998;&#30340;&#21021;&#22987;&#21270;&#32593;&#32476;&#22312;&#30456;&#20284;&#30340;&#27969;&#24418;&#19978;&#21521;&#35299;&#20915;&#26041;&#26696;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03094</link><description>&lt;p&gt;
PopulAtion Parameter Averaging (PAPA)&#65288;&#20154;&#21475;&#21442;&#25968;&#24179;&#22343;&#65289;
&lt;/p&gt;
&lt;p&gt;
PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#25104;&#26412;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#24179;&#22343;&#26469;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#20010;&#65288;&#27169;&#22411;&#27748;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#27604;&#38598;&#25104;&#34920;&#29616;&#26356;&#24046;&#12290;&#24403;&#26435;&#37325;&#36275;&#22815;&#30456;&#20284;&#65288;&#22312;&#26435;&#37325;&#25110;&#29305;&#24449;&#31354;&#38388;&#20013;&#65289;&#21487;&#20197;&#24456;&#22909;&#22320;&#24179;&#22343;&#65292;&#20294;&#36275;&#22815;&#19981;&#21516;&#20197;&#20174;&#32452;&#21512;&#20013;&#21463;&#30410;&#26102;&#65292;&#26435;&#37325;&#24179;&#22343;&#25165;&#26159;&#26377;&#30410;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PopulAtion Parameter Averaging (PAPA)&#65292;&#19968;&#31181;&#23558;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;PAPA&#21033;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;&#22312;&#19981;&#21516;&#25968;&#25454;&#39034;&#24207;&#65292;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#19978;&#35757;&#32451;&#65289;&#30340;&#20154;&#21475;&#65292;&#32780;&#20598;&#23572;&#65288;&#19981;&#35201;&#22826;&#39057;&#32321;&#65292;&#20063;&#19981;&#35201;&#22826;&#31232;&#30095;&#65289;&#29992;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#20195;&#26367;&#20154;&#21475;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#12290;PAPA&#20943;&#23569;&#20102;&#24179;&#22343;&#20540;&#21644;&#38598;&#25104;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12923</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#65306;&#36829;&#21453;&#35268;&#21017;&#26159;&#21542;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12923
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#20223;&#32463;&#36807;&#35757;&#32451;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#30340;&#36719;&#27010;&#29575;&#26469;&#25552;&#39640;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#25104;&#36866;&#24212;&#25945;&#24072;&#30340;&#27010;&#29575;&#65292;&#23398;&#29983;&#19981;&#20165;&#26126;&#26174;&#20559;&#31163;&#36825;&#20123;&#27010;&#29575;&#65292;&#32780;&#19988;&#34920;&#29616;&#27604;&#25945;&#24072;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#30340;&#30830;&#20999;&#24615;&#36136;&#65292;&#24182;&#35770;&#35777;&#23427;&#20204;&#19982;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#20849;&#23384;&#26469;&#35299;&#20915;&#36825;&#19968;&#30475;&#20284;&#30683;&#30462;&#30340;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#20559;&#24046;&#23545;&#24212;&#20110;&#23398;&#29983;&#31995;&#32479;&#24615;&#22320;&#22840;&#22823;&#25945;&#24072;&#30340;&#33258;&#20449;&#27700;&#24179;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#24314;&#31435;&#20102;KD&#22312;&#25910;&#25947;&#26356;&#24555;&#30340;&#36807;&#31243;&#20013;&#22840;&#22823;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;</title><link>http://arxiv.org/abs/2212.03281</link><description>&lt;p&gt;
Copula&#32852;&#21512;&#39044;&#27979;&#29992;&#20110;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26159;&#26500;&#24314;&#24378;&#22823;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25311;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26080;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31639;&#27861;&#65292;&#22240;&#20854;&#26131;&#20110;&#23454;&#29616;&#12289;&#32479;&#35745;&#35206;&#30422;&#20445;&#35777;&#21644;&#23545;&#24213;&#23618;&#39044;&#27979;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;&#20165;&#38480;&#20110;&#21333;&#27493;&#39044;&#27979;&#65292;&#26410;&#32771;&#34385;&#26102;&#24207;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979; CopulaCPTS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; CopulaCPTS &#20855;&#26377;&#26377;&#38480;&#30340;&#26679;&#26412;*&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; CopulaCPTS &#30340;&#22810;&#27493;&#39044;&#27979;&#21487;&#20135;&#29983;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;"&#26368;&#20339;&#23454;&#36341;"&#24182;&#21521;&#20154;&#31867;&#20256;&#36798;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#34394;&#25311;&#21416;&#25151;&#31649;&#29702;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.08454</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Improving Human Sequential Decision-Making with Reinforcement Learning. (arXiv:2108.08454v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;"&#26368;&#20339;&#23454;&#36341;"&#24182;&#21521;&#20154;&#31867;&#20256;&#36798;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20154;&#31867;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#34394;&#25311;&#21416;&#25151;&#31649;&#29702;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#32773;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#23398;&#20064;&#22914;&#20309;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#19968;&#20010;&#32473;&#23450;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#20363;&#22914;&#65292;&#20915;&#31574;&#32467;&#26524;&#36890;&#24120;&#26159;&#38271;&#26399;&#30340;&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#20915;&#31574;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#23398;&#20064;&#33391;&#22909;&#30340;&#20915;&#31574;&#31574;&#30053;&#24456;&#22256;&#38590;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21487;&#20197;&#31616;&#27905;&#26126;&#20102;&#22320;&#34920;&#36798;&#12290;&#38024;&#23545;&#39034;&#24207;&#20915;&#31574;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#36861;&#36394;&#25968;&#25454;&#20013;&#25552;&#21462;&#8220;&#26368;&#20339;&#23454;&#36341;&#8221;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#8220;&#25552;&#31034;&#8221;&#30340;&#24418;&#24335;&#20256;&#36798;&#20854;&#35265;&#35299;&#32473;&#20154;&#31867;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#65292;&#20197;&#22635;&#34917;&#20154;&#31867;&#24037;&#20316;&#32773;&#37319;&#21462;&#30340;&#34892;&#21160;&#19982;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#32771;&#34385;&#21738;&#20123;&#34892;&#21160;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21442;&#19982;&#32773;&#31649;&#29702;&#19968;&#20010;&#34394;&#25311;&#21416;&#25151;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workers spend a significant amount of time learning how to make good decisions. Evaluating the efficacy of a given decision, however, can be complicated -- e.g., decision outcomes are often long-term and relate to the original decision in complex ways. Surprisingly, even though learning good decision-making strategies is difficult, they can often be expressed in simple and concise forms. Focusing on sequential decision-making, we design a novel machine learning algorithm that is capable of extracting "best practices" from trace data and conveying its insights to humans in the form of interpretable "tips". Our algorithm selects the tip that best bridges the gap between the actions taken by human workers and those taken by the optimal policy in a way that accounts for which actions are consequential for achieving higher performance. We evaluate our approach through a series of randomized controlled experiments where participants manage a virtual kitchen. Our experiments show that the tip
&lt;/p&gt;</description></item></channel></rss>