<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#37327;&#32423;&#20026;$d \:\text{polylog}(d)$&#30340;&#26679;&#26412;&#65292;&#23558;&#32593;&#32476;&#35757;&#32451;&#21040;&#20102;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#36825;&#26159;&#39318;&#27425;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;</title><link>http://arxiv.org/abs/2309.15111</link><description>&lt;p&gt;
SGD&#22312;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23547;&#25214;&#24182;&#35843;&#25972;&#29305;&#24449;&#65306;&#20197;XOR&#38382;&#39064;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#37327;&#32423;&#20026;$d \:\text{polylog}(d)$&#30340;&#26679;&#26412;&#65292;&#23558;&#32593;&#32476;&#35757;&#32451;&#21040;&#20102;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#36825;&#26159;&#39318;&#27425;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#20855;&#26377;&#20108;&#27425;&#30495;&#23454;&#20989;&#25968;&#20998;&#38548;&#25968;&#25454;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20174;$d$&#32500;&#24067;&#23572;&#36229;&#31435;&#26041;&#20307;&#20013;&#30001;&#20108;&#27425;&#8220;XOR&#8221;&#20989;&#25968;$y = -x_ix_j$&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#23567;&#25209;&#37327;SGD&#22312;&#36923;&#36753;&#25439;&#22833;&#19978;&#21516;&#26102;&#35757;&#32451;&#20004;&#23618;ReLU&#28608;&#27963;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;$d \:\text{polylog}(d)$&#20010;&#26679;&#26412;&#23558;&#20854;&#35757;&#32451;&#21040;&#20154;&#21475;&#35823;&#24046;&#20026;$o(1)$&#30340;&#31243;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#32473;&#20986;&#20102;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19978;&#20197;&#21450;&#26631;&#20934;&#35757;&#32451;&#19979;&#65292;&#23545;&#20110;&#22312;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#19978;&#39640;&#25928;&#23398;&#20064;XOR&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d)$&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#26159;&#23637;&#31034;&#32593;&#32476;&#28436;&#21270;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#19968;&#20010;&#8221;&#20449;&#21495;&#21457;&#29616;&#8220;&#38454;&#27573;&#65292;&#22312;&#27492;&#32593;&#32476;&#35268;&#27169;&#36739;&#23567;&#19988;&#35768;&#22810;&#31070;&#32463;&#20803;&#29420;&#31435;&#28436;&#21270;&#20197;&#23547;&#25214;&#29305;&#24449;&#65292;&#20197;&#21450;&#19968;&#20010;&#8221;&#20449;&#21495;&#23494;&#38598;&#8220;&#38454;&#27573;&#65292;&#20854;&#20013;&#35768;&#22810;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#20197;&#20248;&#21270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15098</link><description>&lt;p&gt;
&#28385;&#36275;&#20851;&#27880;&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#38169;&#35823;&#30340;&#32422;&#26463;&#28385;&#36275;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#25991;&#26412;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#26597;&#35810;&#24314;&#27169;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#19982;&#20107;&#23454;&#32422;&#26463;&#36827;&#34892;&#20869;&#37096;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20854;&#21709;&#24212;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#23384;&#22312;&#24378;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;11&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#24635;&#35745;&#36229;&#36807;40,000&#20010;&#25552;&#31034;&#30340;&#31934;&#24515;&#31574;&#21010;&#22871;&#35013;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Llama-2&#31995;&#21015;&#22312;&#25152;&#26377;&#35268;&#27169;&#65288;7B&#65292;13B&#65292;70B&#65289;&#19978;&#39044;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAT Probe&#65292;&#19968;&#31181;&#25506;&#26597;&#33258;&#27880;&#24847;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#12290;&#36825;&#19968;&#26041;&#27861;&#21644;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;LLM&#20013;&#20107;&#23454;&#24615;&#30340;&#26426;&#26800;&#29702;&#35299;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20004;&#20010;&#26041;&#21521;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#21644;&#36890;&#36807;&#20984;&#37325;&#22609;ReLU&#32593;&#32476;&#30340;&#20840;&#23616;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NTK&#30456;&#36830;&#30340;&#22810;&#26680;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;&#38376;&#25511;ReLU&#32593;&#32476;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#23631;&#34109;&#29305;&#24449;&#26144;&#23556;&#26469;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.15096</link><description>&lt;p&gt;
&#20462;&#22797;NTK&#65306;&#20174;&#31070;&#32463;&#32593;&#32476;&#32447;&#24615;&#21270;&#21040;&#31934;&#30830;&#30340;&#20984;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#26041;&#21521;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#21644;&#36890;&#36807;&#20984;&#37325;&#22609;ReLU&#32593;&#32476;&#30340;&#20840;&#23616;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NTK&#30456;&#36830;&#30340;&#22810;&#26680;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;&#38376;&#25511;ReLU&#32593;&#32476;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#23631;&#34109;&#29305;&#24449;&#26144;&#23556;&#26469;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#20998;&#26512;&#20027;&#35201;&#38598;&#20013;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#65306;1&#65289;&#36890;&#36807;&#22312;&#38544;&#34255;&#23618;&#23485;&#24230;&#26080;&#38480;&#22823;&#21644;&#23398;&#20064;&#29575;&#26080;&#31351;&#23567;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;SGD&#35757;&#32451;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#65288;&#20063;&#31216;&#20026;&#26799;&#24230;&#27969;&#65289;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#65307;2&#65289;&#36890;&#36807;&#38181;&#32422;&#26463;&#20984;&#37325;&#22609;ReLU&#32593;&#32476;&#30340;&#20840;&#23616;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#12290;&#21518;&#19968;&#31181;&#30740;&#31350;&#26041;&#21521;&#36824;&#25552;&#20379;&#20102;ReLU&#32593;&#32476;&#30340;&#21478;&#19968;&#31181;&#20844;&#24335;&#65292;&#31216;&#20026;&#38376;&#25511;ReLU&#32593;&#32476;&#65292;&#21487;&#36890;&#36807;&#39640;&#25928;&#30340;&#26080;&#32422;&#26463;&#20984;&#31243;&#24207;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38376;&#25511;ReLU&#32593;&#32476;&#30340;&#20984;&#38382;&#39064;&#35299;&#37322;&#20026;&#20855;&#26377;&#21152;&#26435;&#25968;&#25454;&#23631;&#34109;&#29305;&#24449;&#26144;&#23556;&#30340;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#27169;&#22411;&#65292;&#24182;&#19982;NTK&#24314;&#31435;&#20102;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#37027;&#20123;&#19982;&#23398;&#20064;&#30446;&#26631;&#26080;&#20851;&#30340;&#29305;&#23450;&#36873;&#25321;&#30340;&#25513;&#30721;&#26435;&#37325;&#65292;&#35813;&#26680;&#31561;&#25928;&#20110;&#38376;&#25511;ReLU&#32593;&#32476;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#30340;NTK&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;Spline&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20223;&#30495;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#23545;&#21307;&#30103;&#35774;&#22791;&#32452;&#35013;&#36807;&#31243;&#20013;&#30340;&#24555;&#25293;&#27969;&#31243;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#25903;&#25345;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15094</link><description>&lt;p&gt;
&#36890;&#36807;&#26367;&#20195;&#25216;&#26415;&#35782;&#21035;&#21307;&#30103;&#35774;&#22791;&#32452;&#35013;&#36807;&#31243;&#30340;&#20223;&#30495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process. (arXiv:2309.15094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;Spline&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20223;&#30495;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#23545;&#21307;&#30103;&#35774;&#22791;&#32452;&#35013;&#36807;&#31243;&#20013;&#30340;&#24555;&#25293;&#27969;&#31243;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#25903;&#25345;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31185;&#23398;&#35770;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#36924;&#36817;&#20223;&#30495;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#35774;&#22791;&#32452;&#35013;&#36807;&#31243;&#20013;&#20851;&#38190;&#30340;&#24555;&#25293;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#20223;&#30495;&#27169;&#22411;&#22312;&#25552;&#20379;&#24037;&#31243;&#24072;&#23545;&#24037;&#19994;&#36807;&#31243;&#30340;&#27934;&#23519;&#21147;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#32452;&#35013;&#20043;&#21069;&#36827;&#34892;&#23454;&#39564;&#21644;&#25925;&#38556;&#25490;&#38500;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#23548;&#33268;&#32791;&#26102;&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35782;&#21035;&#20223;&#30495;&#27169;&#22411;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#21033;&#29992;&#26679;&#26465;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#20855;&#26377;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20195;&#34920;&#24555;&#25293;&#36807;&#31243;&#24182;&#36866;&#24212;&#22810;&#31181;&#24773;&#26223;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#26377;&#26395;&#25552;&#39640;&#27969;&#31243;&#29702;&#35299;&#21644;&#20915;&#31574;&#25903;&#25345;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This scientific paper explores two distinct approaches for identifying and approximating the simulation model, particularly in the context of the snap process crucial to medical device assembly. Simulation models play a pivotal role in providing engineers with insights into industrial processes, enabling experimentation and troubleshooting before physical assembly. However, their complexity often results in time-consuming computations.  To mitigate this complexity, we present two distinct methods for identifying simulation models: one utilizing Spline functions and the other harnessing Machine Learning (ML) models. Our goal is to create adaptable models that accurately represent the snap process and can accommodate diverse scenarios. Such models hold promise for enhancing process understanding and aiding in decision-making, especially when data availability is limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#21338;&#22763;&#35770;&#25991;&#30340;&#26680;&#24515;&#35266;&#28857;&#26159;&#65292;&#21333;&#19968;&#29983;&#29289;&#31070;&#32463;&#20803;&#20855;&#26377;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#30340;&#31934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#30446;&#21069;&#22823;&#22810;&#25968;&#31070;&#32463;&#31185;&#23398;&#23478;&#30340;&#35266;&#28857;&#19981;&#21516;&#12290;&#36825;&#19968;&#28857;&#23545;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#33041;&#22238;&#36335;&#21644;&#31070;&#32463;&#27963;&#21160;&#30340;&#20449;&#24687;&#32534;&#30721;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15090</link><description>&lt;p&gt;
&#21333;&#19968;&#29983;&#29289;&#31070;&#32463;&#20803;&#20316;&#20026;&#31934;&#30830;&#30340;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers. (arXiv:2309.15090v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#21338;&#22763;&#35770;&#25991;&#30340;&#26680;&#24515;&#35266;&#28857;&#26159;&#65292;&#21333;&#19968;&#29983;&#29289;&#31070;&#32463;&#20803;&#20855;&#26377;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#30340;&#31934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#30446;&#21069;&#22823;&#22810;&#25968;&#31070;&#32463;&#31185;&#23398;&#23478;&#30340;&#35266;&#28857;&#19981;&#21516;&#12290;&#36825;&#19968;&#28857;&#23545;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#33041;&#22238;&#36335;&#21644;&#31070;&#32463;&#27963;&#21160;&#30340;&#20449;&#24687;&#32534;&#30721;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#21338;&#22763;&#35770;&#25991;&#30340;&#37325;&#28857;&#26159;&#35748;&#20026;&#22823;&#33041;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#24212;&#34987;&#35270;&#20026;&#20020;&#26102;&#31934;&#30830;&#19988;&#39640;&#24230;&#22797;&#26434;&#30340;&#26102;&#31354;&#27169;&#24335;&#35782;&#21035;&#22120;&#12290;&#19982;&#24403;&#20170;&#22823;&#22810;&#25968;&#31070;&#32463;&#31185;&#23398;&#23478;&#35748;&#20026;&#30340;&#29983;&#29289;&#31070;&#32463;&#20803;&#26159;&#31616;&#21333;&#19988;&#20027;&#35201;&#26159;&#31354;&#38388;&#27169;&#24335;&#35782;&#21035;&#22120;&#30340;&#35266;&#28857;&#30456;&#21453;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#23558;&#23581;&#35797;&#35777;&#26126;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21306;&#21035;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#19978;&#36848;&#35745;&#31639;&#29305;&#24615;&#23545;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#21508;&#31181;&#33041;&#22238;&#36335;&#20197;&#21450;&#31070;&#32463;&#27963;&#21160;&#22914;&#20309;&#32534;&#30721;&#20449;&#24687;&#37117;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#21363;&#36825;&#20123;&#21333;&#20010;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#8220;&#20302;&#32423;&#8221;&#32454;&#33410;&#23545;&#25972;&#20010;&#31995;&#32479;&#26377;&#30528;&#37325;&#22823;&#30340;&#24433;&#21709;&#12290;&#22312;&#24341;&#35328;&#20013;&#65292;&#25105;&#20204;&#23558;&#24378;&#35843;&#32452;&#25104;&#31070;&#32463;&#24494;&#30005;&#36335;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20174;&#31995;&#32479;&#30340;&#35282;&#24230;&#38416;&#26126;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This PhD thesis is focused on the central idea that single neurons in the brain should be regarded as temporally precise and highly complex spatio-temporal pattern recognizers. This is opposed to the prevalent view of biological neurons as simple and mainly spatial pattern recognizers by most neuroscientists today. In this thesis, I will attempt to demonstrate that this is an important distinction, predominantly because the above-mentioned computational properties of single neurons have far-reaching implications with respect to the various brain circuits that neurons compose, and on how information is encoded by neuronal activity in the brain. Namely, that these particular "low-level" details at the single neuron level have substantial system-wide ramifications. In the introduction we will highlight the main components that comprise a neural microcircuit that can perform useful computations and illustrate the inter-dependence of these components from a system perspective. In chapter 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36229;&#39069;&#39118;&#38505;&#26469;&#34913;&#37327;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#22330;&#26223;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#12290;&#34429;&#28982;&#20989;&#25968;&#31867;&#24456;&#22823;&#65292;&#20294;&#26080;&#32500;&#24230;&#36895;&#29575;&#26159;&#21487;&#33021;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.15075</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36229;&#39069;&#39118;&#38505;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36229;&#39069;&#39118;&#38505;&#26469;&#34913;&#37327;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#22330;&#26223;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#12290;&#34429;&#28982;&#20989;&#25968;&#31867;&#24456;&#22823;&#65292;&#20294;&#26080;&#32500;&#24230;&#36895;&#29575;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26356;&#32463;&#20856;&#30340;&#20998;&#31867;&#22120;&#65288;&#22914;SVM&#25110;boosting&#20998;&#31867;&#22120;&#65289;&#30456;&#27604;&#65292;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20854;&#36229;&#39069;&#39118;&#38505;&#26469;&#34913;&#37327;&#12290;&#19982;&#25991;&#29486;&#20013;&#25152;&#35268;&#23450;&#30340;&#20856;&#22411;&#26465;&#20214;&#30456;&#27604;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#22330;&#26223;&#65292;&#23427;&#22312;&#20004;&#20010;&#26041;&#38754;&#19982;&#23454;&#38469;&#24212;&#29992;&#31867;&#20284;&#65306;&#39318;&#20808;&#65292;&#35201;&#36817;&#20284;&#30340;&#20989;&#25968;&#31867;&#21253;&#25324;&#20102;Barron&#20989;&#25968;&#20316;&#20026;&#27491;&#23376;&#38598;&#65307;&#20854;&#27425;&#65292;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#32780;&#19981;&#26159;0-1&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#32771;&#34385;&#30340;&#20989;&#25968;&#31867;&#38750;&#24120;&#22823;&#65292;&#26368;&#20248;&#36895;&#29575;&#19981;&#33021;&#36229;&#36807;$n^{-\frac{1}{3}}$&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26080;&#32500;&#24230;&#36895;&#29575;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat
&lt;/p&gt;</description></item><item><title>QUILT&#26159;&#19968;&#20010;&#38024;&#23545;&#24403;&#21069;&#23481;&#26131;&#20986;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#38598;&#21512;&#65292;&#22312;MNIST&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#39640;&#36798;85%&#30340;&#22810;&#31867;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.15056</link><description>&lt;p&gt;
QUILT&#65306;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#38598;&#21512;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#22810;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers. (arXiv:2309.15056v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15056
&lt;/p&gt;
&lt;p&gt;
QUILT&#26159;&#19968;&#20010;&#38024;&#23545;&#24403;&#21069;&#23481;&#26131;&#20986;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#38598;&#21512;&#65292;&#22312;MNIST&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#39640;&#36798;85%&#30340;&#22810;&#31867;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#27604;&#32463;&#20856;&#35745;&#31639;&#26426;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#20294;&#30001;&#20110;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#26377;&#38480;&#19988;&#23481;&#26131;&#20986;&#38169;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#36817;&#26399;&#21457;&#23637;&#21463;&#21040;&#38480;&#21046;&#12290;Quilt&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24403;&#21069;&#23481;&#26131;&#20986;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#26377;&#25928;&#25191;&#34892;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#12290;Quilt&#20351;&#29992;&#30495;&#23454;&#30340;&#37327;&#23376;&#26426;&#22120;&#20197;&#21450;&#26410;&#26469;&#22122;&#38899;&#27700;&#24179;&#30340;&#35780;&#20272;&#65292;&#38543;&#30528;&#37327;&#23376;&#26426;&#22120;&#21464;&#24471;&#26356;&#21152;&#22122;&#38899;&#23569;&#12290;&#22312;&#20116;&#27604;&#29305;&#31995;&#32479;&#19978;&#65292;Quilt&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#39640;&#36798;85%&#30340;&#22810;&#31867;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computers can theoretically have significant acceleration over classical computers; but, the near-future era of quantum computing is limited due to small number of qubits that are also error prone. Quilt is a framework for performing multi-class classification task designed to work effectively on current error-prone quantum computers. Quilt is evaluated with real quantum machines as well as with projected noise levels as quantum machines become more noise-free. Quilt demonstrates up to 85% multi-class classification accuracy with the MNIST dataset on a five-qubit system.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15038</link><description>&lt;p&gt;
HPCR: &#22522;&#20110;&#20195;&#29702;&#30340;&#32508;&#21512;&#23545;&#27604;&#37325;&#25918;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning. (arXiv:2309.15038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15038
&lt;/p&gt;
&lt;p&gt;
HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#22312;&#32447;&#25968;&#25454;&#27969;&#20256;&#36882;&#25345;&#32493;&#23398;&#20064;&#26032;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#36890;&#24120;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#20195;&#29702;&#20026;&#22522;&#30784;&#25110;&#23545;&#27604;&#20026;&#22522;&#30784;&#30340;&#37325;&#25918;&#26041;&#24335;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#37325;&#25918;&#26041;&#24335;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#31216;&#20026;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;PCR&#65289;&#65292;&#23427;&#23558;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#38170;&#28857;-&#26679;&#26412;&#23545;&#26367;&#25442;&#20026;&#38170;&#28857;-&#20195;&#29702;&#23545;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#12290;&#22522;&#20110;PCR&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;HPCR&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#12290;&#23545;&#27604;&#32452;&#20214;&#22312;PCR&#30340;&#22522;&#30784;&#19978;&#26465;&#20214;&#24615;&#22320;&#23558;&#38170;&#28857;-&#26679;&#26412;&#23545;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35757;&#32451;&#25209;&#27425;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26159;&#37325;&#25918;&#32452;&#20214;&#65292;&#23427;&#22312;&#26679;&#26412;&#36873;&#25321;&#19978;&#37319;&#29992;&#20102;&#22810;&#26679;&#24615;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#25968;&#25454;&#19982;&#24403;&#21069;&#20219;&#21153;&#20855;&#26377;&#26356;&#39640;&#30340;&#20851;&#32852;&#24615;&#12290;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#27491;&#21017;&#21270;&#32452;&#20214;&#65292;&#36890;&#36807;&#32553;&#23567;&#26679;&#26412;&#31354;&#38388;&#65292;&#20419;&#36827;&#23398;&#20064;&#27169;&#22411;&#23545;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#26356;&#22909;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#26041;&#27861;&#22312;&#22810;&#20010;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream. It generally suffers from the catastrophic forgetting issue. Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner. In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting. Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components. The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch. The sec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>Synthia's melody&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38899;&#39057;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#20026;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#21508;&#31181;&#26059;&#24459;&#65292;&#24182;&#35780;&#20272;&#22768;&#23398;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15024</link><description>&lt;p&gt;
Synthia&#30340;&#26059;&#24459;&#65306;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38899;&#39057;&#22522;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio. (arXiv:2309.15024v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15024
&lt;/p&gt;
&lt;p&gt;
Synthia's melody&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38899;&#39057;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#20026;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#21508;&#31181;&#26059;&#24459;&#65292;&#24182;&#35780;&#20272;&#22768;&#23398;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38899;&#39057;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20173;&#28982;&#30456;&#23545;&#26410;&#24320;&#21457;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Synthia&#30340;&#26059;&#24459;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#25311;&#20855;&#26377;&#29992;&#25143;&#25351;&#23450;&#28151;&#28102;&#32467;&#26500;&#30340;&#26080;&#38480;&#22810;&#31181;4&#31186;&#26059;&#24459;&#65292;&#36825;&#20123;&#32467;&#26500;&#30001;&#38899;&#20048;&#38190;&#12289;&#38899;&#33394;&#21644;&#38899;&#37327;&#29305;&#24449;&#21270;&#12290;&#19982;&#35266;&#27979;&#35774;&#32622;&#19979;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;Synthia&#30340;&#26059;&#24459;&#27809;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#20102;&#23454;&#39564;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#27604;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;-&#39046;&#22495;&#36716;&#31227;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#36716;&#31227;&#19979;&#22768;&#23398;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Synthia&#30340;&#26059;&#24459;&#20026;&#26816;&#39564;&#36825;&#20123;&#27169;&#22411;&#23545;&#19981;&#21516;&#20998;&#24067;&#32423;&#21035;&#30340;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15004</link><description>&lt;p&gt;
&#20174;&#25945;&#32946;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24335;&#27963;&#21160;&#65288;QBA&#65289;&#22312;&#25945;&#32946;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20256;&#32479;&#19978;&#26159;&#23398;&#20064;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#12290;&#36890;&#36807;&#23545;104&#21517;&#25945;&#24072;&#30340;&#19987;&#23478;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;QBA&#30340;&#38656;&#27714;&#65292;&#20316;&#20026;&#19968;&#20010;&#33021;&#22815;&#26174;&#33879;&#20943;&#36731;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#20419;&#36827;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#30340;&#24037;&#20855;&#12290;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#38382;&#39064;&#29983;&#25104;&#12289;&#27491;&#30830;&#31572;&#26696;&#39044;&#27979;&#21644;&#24178;&#25200;&#39033;&#21046;&#23450;&#30340;&#19981;&#21516;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#21644;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#24182;&#19988;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#32654;&#22269;&#26159;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#26368;&#39640;&#30340;&#22269;&#23478;&#65292;&#32780;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.14994</link><description>&lt;p&gt;
&#38024;&#23545;&#24070;&#33337;&#20215;&#26684;&#21644;&#29305;&#24449;&#20197;&#21450;&#21306;&#22495;&#22320;&#21306;&#30340;&#27979;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Measurement Models For Sailboats Price vs. Features And Regional Areas. (arXiv:2309.14994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14994
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#21644;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#24182;&#19988;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#32654;&#22269;&#26159;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#26368;&#39640;&#30340;&#22269;&#23478;&#65292;&#32780;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#19982;&#20854;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#21253;&#25324;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#21507;&#27700;&#12289;&#25490;&#27700;&#37327;&#12289;&#24070;&#38754;&#31215;&#21644;&#27700;&#32447;&#31561;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24070;&#33337;&#20215;&#26684;&#12290;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20135;&#29983;&#20102;&#26368;&#20302;&#30340;MSE&#21644;MAE&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#32780;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36739;&#20302;&#30340;&#21507;&#27700;&#19982;&#36739;&#39640;&#30340;&#25346;&#29260;&#20215;&#26684;&#26377;&#20851;&#32852;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21306;&#22495;&#23450;&#20215;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#32654;&#22269;&#22312;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#19978;&#23621;&#39318;&#65292;&#20854;&#27425;&#26159;&#27431;&#27954;&#12289;&#39321;&#28207;&#21644;&#21152;&#21202;&#27604;&#22320;&#21306;&#12290;&#19982;&#25105;&#20204;&#26368;&#21021;&#30340;&#20551;&#35774;&#30456;&#21453;&#65292;&#19968;&#20010;&#22269;&#23478;&#30340;GDP&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the relationship between sailboat technical specifications and their prices, as well as regional pricing influences. Utilizing a dataset encompassing characteristics like length, beam, draft, displacement, sail area, and waterline, we applied multiple machine learning models to predict sailboat prices. The gradient descent model demonstrated superior performance, producing the lowest MSE and MAE. Our analysis revealed that monohulled boats are generally more affordable than catamarans, and that certain specifications such as length, beam, displacement, and sail area directly correlate with higher prices. Interestingly, lower draft was associated with higher listing prices. We also explored regional price determinants and found that the United States tops the list in average sailboat prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our initial hypothesis, a country's GDP showed no direct correlation with sailboat prices. Utilizing a 50
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;"&#26102;&#38388;&#21516;&#27493;"&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22681;&#38047;&#26102;&#38388;&#32780;&#19981;&#26159;&#24773;&#33410;&#36827;&#23637;&#26469;&#23454;&#29616;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14989</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33410;&#22863;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;"&#26102;&#38388;&#21516;&#27493;"&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22681;&#38047;&#26102;&#38388;&#32780;&#19981;&#26159;&#24773;&#33410;&#36827;&#23637;&#26469;&#23454;&#29616;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#20808;&#25105;&#20204;&#25552;&#20986;&#24182;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#8220;&#26102;&#38388;&#21516;&#27493;&#8221;&#38382;&#39064;&#65292;&#36825;&#26159;&#38459;&#30861;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#29616;&#23454;&#20013;&#65292;&#29615;&#22659;&#30340;&#21464;&#21270;&#26159;&#25353;&#29031;&#22681;&#38047;&#26102;&#38388;&#65288;$\mathfrak{t}$&#65289;&#32780;&#19981;&#26159;&#25353;&#29031;&#24773;&#33410;&#36827;&#23637;&#65288;$k$&#65289;&#21457;&#29983;&#30340;&#65292;&#20854;&#20013;&#22681;&#38047;&#26102;&#38388;&#34920;&#31034;&#22266;&#23450;&#25345;&#32493;&#26102;&#38388;$\mathfrak{t} \in [0, T]$&#20869;&#23454;&#38469;&#27969;&#36893;&#30340;&#26102;&#38388;&#12290;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#22312;&#24773;&#33410;$k$&#26102;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#19968;&#20010;&#36712;&#36857;&#24182;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#65292;&#28982;&#21518;&#36716;&#20837;&#24773;&#33410;$k+1$&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#19981;&#21516;&#27493;&#30340;&#29615;&#22659;&#19979;&#65292;&#26234;&#33021;&#20307;&#22312;&#26102;&#38388;$\mathfrak{t}_k$&#20998;&#37197;$\Delta \mathfrak{t}$&#29992;&#20110;&#36712;&#36857;&#29983;&#25104;&#21644;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$&#26102;&#21051;&#36716;&#20837;&#19979;&#19968;&#20010;&#24773;&#33410;&#12290;&#23613;&#31649;&#24773;&#33410;&#24635;&#25968;&#22266;&#23450;&#65288;$K$&#65289;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#30456;&#20114;&#20316;&#29992;&#26102;&#38388;&#30340;&#36873;&#25321;&#65288;$\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$&#65289;&#31215;&#32047;&#19981;&#21516;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \textit{interaction times} ($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26410;&#30693;&#37327;&#23376;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21487;&#34892;&#23450;&#29702;&#12290;&#24403;&#25439;&#22833;&#20540;&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#65292;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#27010;&#29575;&#38543;&#30528;&#37327;&#23376;&#27604;&#29305;&#25968;&#25351;&#25968;&#32423;&#20943;&#23569;&#65292;&#32780;&#38543;&#30528;&#30005;&#36335;&#28145;&#24230;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#26354;&#29575;&#38598;&#20013;&#22312;&#37327;&#23376;Fisher&#20449;&#24687;&#20056;&#20197;&#25439;&#22833;&#30456;&#20851;&#24120;&#25968;&#19978;&#65292;&#35813;&#24120;&#25968;&#34920;&#24449;&#36755;&#20986;&#24577;&#30456;&#23545;&#20110;QNN&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14980</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37327;&#23376;&#24577;&#23398;&#20064;&#36807;&#31243;&#30340;&#32479;&#35745;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks. (arXiv:2309.14980v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26410;&#30693;&#37327;&#23376;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21487;&#34892;&#23450;&#29702;&#12290;&#24403;&#25439;&#22833;&#20540;&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#65292;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#27010;&#29575;&#38543;&#30528;&#37327;&#23376;&#27604;&#29305;&#25968;&#25351;&#25968;&#32423;&#20943;&#23569;&#65292;&#32780;&#38543;&#30528;&#30005;&#36335;&#28145;&#24230;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#26354;&#29575;&#38598;&#20013;&#22312;&#37327;&#23376;Fisher&#20449;&#24687;&#20056;&#20197;&#25439;&#22833;&#30456;&#20851;&#24120;&#25968;&#19978;&#65292;&#35813;&#24120;&#25968;&#34920;&#24449;&#36755;&#20986;&#24577;&#30456;&#23545;&#20110;QNN&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNN)&#26159;&#36861;&#27714;&#36817;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#35768;&#22810;&#24212;&#29992;&#21487;&#20197;&#30475;&#20316;&#23398;&#20064;&#32534;&#30721;&#26377;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#24577;&#12290;&#20316;&#20026;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#37327;&#23376;&#24577;&#23398;&#20064;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#37117;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#20351;&#29992;QNN&#23398;&#20064;&#26410;&#30693;&#37327;&#23376;&#24577;&#30340;&#38382;&#39064;&#25552;&#20986;&#19968;&#20010;&#19981;&#21487;&#34892;&#23450;&#29702;&#65292;&#21363;&#20351;&#20174;&#39640;&#20445;&#30495;&#24230;&#30340;&#21021;&#22987;&#24577;&#24320;&#22987;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25439;&#22833;&#20540;&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#65292;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#27010;&#29575;&#38543;&#30528;&#37327;&#23376;&#27604;&#29305;&#25968;&#25351;&#25968;&#32423;&#20943;&#23569;&#65292;&#32780;&#38543;&#30528;&#30005;&#36335;&#28145;&#24230;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#26354;&#29575;&#38598;&#20013;&#22312;&#37327;&#23376;Fisher&#20449;&#24687;&#20056;&#20197;&#25439;&#22833;&#30456;&#20851;&#24120;&#25968;&#19978;&#65292;&#35813;&#24120;&#25968;&#34920;&#24449;&#36755;&#20986;&#24577;&#30456;&#23545;&#20110;QNN&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20219;&#20309;&#30005;&#36335;&#32467;&#26500;&#12289;&#21021;&#22987;&#21270;&#31574;&#30053;&#20197;&#21450;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) have been a promising framework in pursuing near-term quantum advantage in various fields, where many applications can be viewed as learning a quantum state that encodes useful data. As a quantum analog of probability distribution learning, quantum state learning is theoretically and practically essential in quantum machine learning. In this paper, we develop a no-go theorem for learning an unknown quantum state with QNNs even starting from a high-fidelity initial state. We prove that when the loss value is lower than a critical threshold, the probability of avoiding local minima vanishes exponentially with the qubit count, while only grows polynomially with the circuit depth. The curvature of local minima is concentrated to the quantum Fisher information times a loss-dependent constant, which characterizes the sensibility of the output state with respect to parameters in QNNs. These results hold for any circuit structures, initialization strategies, and 
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#33322;&#31354;&#22120;&#22320;&#38754;&#33322;&#36857;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#39134;&#34892;&#21592;&#34892;&#20026;&#21644;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#30340;&#39134;&#34892;&#21306;&#22495;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#32321;&#24537;&#30340;&#31354;&#22495;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14957</link><description>&lt;p&gt;
&#29992;&#20110;&#33322;&#31354;&#22120;&#22320;&#38754;&#33322;&#36857;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Generative Models for Prediction of Aircraft Ground Tracks. (arXiv:2309.14957v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#33322;&#31354;&#22120;&#22320;&#38754;&#33322;&#36857;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#39134;&#34892;&#21592;&#34892;&#20026;&#21644;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#30340;&#39134;&#34892;&#21306;&#22495;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#32321;&#24537;&#30340;&#31354;&#22495;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#36857;&#39044;&#27979;&#22312;&#25903;&#25345;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#30340;&#20915;&#31574;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#26159;&#30830;&#23450;&#24615;&#30340;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#65292;&#20351;&#29992;&#20840;&#29699;&#25910;&#38598;&#30340;&#33322;&#31354;&#30417;&#27979;&#25968;&#25454;&#36827;&#34892;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24573;&#35270;&#20102;&#39134;&#34892;&#21592;&#21644;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#30340;&#24847;&#22270;&#65292;&#36825;&#21487;&#33021;&#23545;&#35266;&#27979;&#21040;&#30340;&#33322;&#36857;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#27178;&#21521;&#24179;&#38754;&#19978;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27178;&#21521;&#33322;&#36857;&#39044;&#27979;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26469;&#24314;&#27169;&#26469;&#33258;&#39134;&#34892;&#21592;&#34892;&#20026;&#21644;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24847;&#22270;&#26410;&#30693;&#24433;&#21709;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#29305;&#23450;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#39134;&#34892;&#21306;&#22495;&#65292;&#21487;&#20197;&#24314;&#27169;&#26412;&#22320;&#30340;&#21327;&#35843;&#36827;&#20986;&#28857;&#31561;&#31243;&#24207;&#12290;&#20351;&#29992;&#20102;&#19968;&#21608;&#26102;&#38388;&#30340;&#36890;&#36807;&#33521;&#22269;&#19978;&#23618;&#31354;&#22495;&#19968;&#20010;&#32321;&#24537;&#21306;&#22495;&#30340;&#33322;&#31354;&#30417;&#27979;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction (TP) plays an important role in supporting the decision-making of Air Traffic Controllers (ATCOs). Traditional TP methods are deterministic and physics-based, with parameters that are calibrated using aircraft surveillance data harvested across the world. These models are, therefore, agnostic to the intentions of the pilots and ATCOs, which can have a significant effect on the observed trajectory, particularly in the lateral plane. This work proposes a generative method for lateral TP, using probabilistic machine learning to model the effect of the epistemic uncertainty arising from the unknown effect of pilot behaviour and ATCO intentions. The models are trained to be specific to a particular sector, allowing local procedures such as coordinated entry and exit points to be modelled. A dataset comprising a week's worth of aircraft surveillance data, passing through a busy sector of the United Kingdom's upper airspace, was used to train and test the models. Specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#19978;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#24402;&#19968;&#21270;&#23618;&#26469;&#36866;&#24212;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.14949</link><description>&lt;p&gt;
&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65306;&#20855;&#26377;&#24179;&#34913;&#24402;&#19968;&#21270;&#30340;&#19977;&#32593;&#32476;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization. (arXiv:2309.14949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#19978;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#24402;&#19968;&#21270;&#23618;&#26469;&#36866;&#24212;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26088;&#22312;&#23558;&#28304;&#22495;&#27169;&#22411;&#36866;&#24212;&#21040;&#25512;&#26029;&#38454;&#27573;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#65292;&#22312;&#36866;&#24212;&#21040;&#26410;&#35265;&#36807;&#30340;&#30772;&#25439;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#36825;&#20123;&#23581;&#35797;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#27969;&#21644;&#25345;&#32493;&#30340;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#26469;&#34917;&#20805;&#29616;&#26377;&#30340;&#30495;&#23454;&#19990;&#30028;TTA&#21327;&#35758;&#12290;&#25105;&#20204;&#35777;&#26126;&#25226;&#25152;&#26377;&#35774;&#32622;&#32467;&#21512;&#36215;&#26469;&#23545;&#29616;&#26377;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#26368;&#20808;&#22833;&#36133;&#30340;&#29616;&#26377;&#26041;&#27861;&#26159;&#22240;&#20026;&#19981;&#21152;&#36873;&#25321;&#22320;&#23558;&#24402;&#19968;&#21270;&#23618;&#36866;&#24212;&#21040;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#26367;&#25442;&#21407;&#26469;&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#12290;&#26032;&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#33021;&#22815;&#36866;&#24212;&#32780;&#19981;&#20559;&#21521;&#22810;&#25968;&#31867;&#21035;&#12290;&#25105;&#20204;&#21463;&#21040;&#33258;&#23398;&#20064;&#65288;ST&#65289;&#22312;&#26080;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training~(ST) in learning from unlabeled 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38647;&#36798;&#25968;&#25454;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25856;&#21319;&#39134;&#26426;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20462;&#27491;&#25512;&#21147;&#30340;&#20989;&#25968;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#21253;&#25324;&#65306;&#19982;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#21040;&#36798;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#20943;&#23569;&#20102;66.3%&#65307;&#29983;&#25104;&#30340;&#36712;&#36857;&#19982;&#27979;&#35797;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#30495;&#23454;&#65307;&#24182;&#19988;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.14941</link><description>&lt;p&gt;
&#20174;&#38647;&#36798;&#25968;&#25454;&#23398;&#20064;&#25856;&#21319;&#39134;&#26426;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Generative Models for Climbing Aircraft from Radar Data. (arXiv:2309.14941v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38647;&#36798;&#25968;&#25454;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25856;&#21319;&#39134;&#26426;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20462;&#27491;&#25512;&#21147;&#30340;&#20989;&#25968;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#21253;&#25324;&#65306;&#19982;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#21040;&#36798;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#20943;&#23569;&#20102;66.3%&#65307;&#29983;&#25104;&#30340;&#36712;&#36857;&#19982;&#27979;&#35797;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#30495;&#23454;&#65307;&#24182;&#19988;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25856;&#21319;&#39134;&#26426;&#30340;&#20934;&#30830;&#36712;&#36857;&#39044;&#27979;&#21463;&#21040;&#26426;&#36733;&#35774;&#22791;&#25805;&#20316;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#36712;&#36857;&#19982;&#35266;&#27979;&#36712;&#36857;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20462;&#27491;&#25512;&#21147;&#30340;&#20989;&#25968;&#26469;&#20016;&#23500;&#26631;&#20934;&#30340;&#39134;&#26426;&#22522;&#30784;&#25968;&#25454;&#65288;BADA&#65289;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#29305;&#28857;&#65306;&#19982;BADA&#30456;&#27604;&#65292;&#21040;&#36798;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#20943;&#23569;&#20102;66.3%&#65307;&#29983;&#25104;&#30340;&#36712;&#36857;&#19982;&#27979;&#35797;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#30495;&#23454;&#65307;&#24182;&#19988;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate trajectory prediction (TP) for climbing aircraft is hampered by the presence of epistemic uncertainties concerning aircraft operation, which can lead to significant misspecification between predicted and observed trajectories. This paper proposes a generative model for climbing aircraft in which the standard Base of Aircraft Data (BADA) model is enriched by a functional correction to the thrust that is learned from data. The method offers three features: predictions of the arrival time with 66.3% less error when compared to BADA; generated trajectories that are realistic when compared to test data; and a means of computing confidence bounds for minimal computational cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30446;&#26631;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#21270;&#26435;&#37325;&#36827;&#34892;&#26631;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14936</link><description>&lt;p&gt;
&#24179;&#34892;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#20248;&#21270;&#19982;&#32479;&#19968;&#24402;&#19968;&#21270;&#19982;&#26377;&#30028;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives. (arXiv:2309.14936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30446;&#26631;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#21270;&#26435;&#37325;&#36827;&#34892;&#26631;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#37197;&#32622;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#23545;&#20110;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#20934;&#30830;&#24230;&#26159;&#19968;&#20010;&#24120;&#29992;&#30340;&#24615;&#33021;&#30446;&#26631;&#65292;&#20294;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20165;&#20934;&#30830;&#24230;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#35768;&#22810;&#35774;&#32622;&#19979;&#65292;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20934;&#30830;&#24230;&#12289;&#32622;&#20449;&#24230;&#12289;&#20844;&#24179;&#24615;&#12289;&#26657;&#20934;&#12289;&#38544;&#31169;&#12289;&#24310;&#36831;&#21644;&#20869;&#23384;&#28040;&#32791;&#31561;&#22810;&#20010;&#30446;&#26631;&#23545;&#20854;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21363;&#31995;&#32479;&#22320;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#21333;&#19968;&#30446;&#26631;&#24050;&#32463;&#24456;&#20855;&#25361;&#25112;&#24615;&#20102;&#65292;&#23545;&#20110;&#22810;&#20010;&#30446;&#26631;&#21017;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#30446;&#26631;&#23610;&#24230;&#30340;&#24046;&#24322;&#12289;&#22833;&#36133;&#21644;&#24322;&#24120;&#20540;&#30340;&#23384;&#22312;&#20351;&#38382;&#39064;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;(MoBO)&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30446;&#26631;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#21270;&#26435;&#37325;&#36827;&#34892;&#26631;&#37327;&#21270;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#26045;&#21152;&#32422;&#26463;&#26469;&#25552;&#39640;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods offer a wide range of configurable hyperparameters that have a significant influence on their performance. While accuracy is a commonly used performance objective, in many settings, it is not sufficient. Optimizing the ML models with respect to multiple objectives such as accuracy, confidence, fairness, calibration, privacy, latency, and memory consumption is becoming crucial. To that end, hyperparameter optimization, the approach to systematically optimize the hyperparameters, which is already challenging for a single objective, is even more challenging for multiple objectives. In addition, the differences in objective scales, the failures, and the presence of outlier values in objectives make the problem even harder. We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization. We increase the efficiency of our approach by imposing constra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.14928</link><description>&lt;p&gt;
&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#26377;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#27880;&#65292;&#36825;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;NtUA&#20316;&#20026;&#19968;&#20010;&#38190;&#20540;&#32531;&#23384;&#65292;&#23558;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#39044;&#27979;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#38190;&#20540;&#23545;&#36827;&#34892;&#24314;&#27169;&#12290;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#35774;&#35745;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#65292;&#36890;&#36807;&#26681;&#25454;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#23545;&#38190;&#20540;&#23545;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23545;&#25239;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#26159;&#20266;&#26631;&#31614;&#20462;&#27491;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38190;&#20540;&#23545;&#30340;&#26435;&#37325;&#26469;&#20462;&#27491;&#20266;&#26631;&#31614;&#20197;&#21450;&#32531;&#23384;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14907</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#26631;&#31614;&#35299;&#21367;&#31215;&#20197;&#25269;&#25239;&#23398;&#20064;&#20559;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias. (arXiv:2309.14907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24102;&#23646;&#24615;&#30340;&#22270;&#20013;&#65292;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#23545;&#35768;&#22810;&#37325;&#35201;&#30340;&#19979;&#28216;&#20219;&#21153;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#21516;&#26102;&#32534;&#30721;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#36827;&#34892;&#25972;&#21512;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#33410;&#28857;&#32534;&#30721;&#22120;(NEs)&#26469;&#32534;&#30721;&#23646;&#24615;&#12290;&#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#21516;&#26102;&#35757;&#32451;&#22823;&#22411;NEs&#21644;GNNs&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#26041;&#27861;&#25552;&#20986;&#20102;&#20998;&#21035;&#35757;&#32451;NEs&#21644;GNNs&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;NEs&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;GNNs&#20013;&#30340;&#29305;&#24449;&#21367;&#31215;&#65292;&#23548;&#33268;&#20102;&#19982;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#30340;&#26174;&#33879;&#23398;&#20064;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26631;&#31614;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#26631;&#31614;&#35299;&#21367;&#31215;(LD)&#65292;&#36890;&#36807;&#23545;GNNs&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#26032;&#39062;&#19988;&#39640;&#24230;&#21487;&#20280;&#32553;&#30340;&#36817;&#20284;&#65292;&#20197;&#20943;&#36731;&#23398;&#20064;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning on attributed graphs -- whose nodes are associated with rich attributes (e.g., texts and protein sequences) -- plays a crucial role in many important downstream tasks. To encode the attributes and graph structures simultaneously, recent studies integrate pre-trained models with graph neural networks (GNNs), where pre-trained models serve as node encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs on large-scale graphs suffers from severe scalability issues, many methods propose to train NEs and GNNs separately. Consequently, they do not take feature convolutions in GNNs into consideration in the training phase of NEs, leading to a significant learning bias from that by the joint training. To address this challenge, we propose an efficient label regularization technique, namely Label Deconvolution (LD), to alleviate the learning bias by a novel and highly scalable approximation to the inverse mapping of GNNs. The inverse mapping l
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;FDLS&#65292;&#19968;&#31181;&#29992;&#20110;&#21046;&#20316;&#21697;&#36136;&#12289;&#21487;&#25511;&#21644;&#21487;&#37325;&#23450;&#21521;&#30340;&#38754;&#37096;&#34920;&#28436;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;FDLS&#37319;&#29992;&#20174;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20154;&#24037;&#21442;&#19982;&#65292;&#20801;&#35768;&#22312;&#27714;&#35299;&#36807;&#31243;&#20013;&#39564;&#35777;&#21644;&#32534;&#36753;&#27714;&#35299;&#21518;&#30340;&#34920;&#28436;&#12290;</title><link>http://arxiv.org/abs/2309.14897</link><description>&lt;p&gt;
FDLS&#65306;&#19968;&#31181;&#29992;&#20110;&#21046;&#20316;&#21697;&#36136;&#12289;&#21487;&#25511;&#21644;&#21487;&#37325;&#23450;&#21521;&#30340;&#38754;&#37096;&#34920;&#28436;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
FDLS: A Deep Learning Approach to Production Quality, Controllable, and Retargetable Facial Performances. (arXiv:2309.14897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14897
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;FDLS&#65292;&#19968;&#31181;&#29992;&#20110;&#21046;&#20316;&#21697;&#36136;&#12289;&#21487;&#25511;&#21644;&#21487;&#37325;&#23450;&#21521;&#30340;&#38754;&#37096;&#34920;&#28436;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;FDLS&#37319;&#29992;&#20174;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20154;&#24037;&#21442;&#19982;&#65292;&#20801;&#35768;&#22312;&#27714;&#35299;&#36807;&#31243;&#20013;&#39564;&#35777;&#21644;&#32534;&#36753;&#27714;&#35299;&#21518;&#30340;&#34920;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25928;&#26524;&#36890;&#24120;&#38656;&#35201;&#21019;&#24314;&#36924;&#30495;&#30340;&#21512;&#25104;&#20154;&#29289;&#65292;&#24182;&#23558;&#28436;&#21592;&#30340;&#34920;&#28436;&#37325;&#26032;&#23450;&#21521;&#21040;&#31867;&#20284;&#22806;&#26143;&#20154;&#21644;&#24618;&#29289;&#30340;&#20154;&#24418;&#35282;&#33394;&#12290;&#23454;&#29616;&#23089;&#20048;&#19994;&#25152;&#38656;&#30340;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#34920;&#28436;&#38656;&#35201;&#25805;&#20316;&#20855;&#26377;&#25968;&#30334;&#20010;&#21442;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#12290;&#23436;&#20840;&#21019;&#20316;&#25511;&#21046;&#38656;&#35201;&#22312;&#21046;&#20316;&#30340;&#20219;&#20309;&#38454;&#27573;&#23545;&#20854;&#36827;&#34892;&#32534;&#36753;&#65292;&#36825;&#31105;&#27490;&#20102;&#20351;&#29992;&#23436;&#20840;&#33258;&#21160;&#19988;&#24102;&#26377;&#19981;&#21487;&#35299;&#37322;&#21442;&#25968;&#30340;&#8220;&#40657;&#31665;&#8221;&#35299;&#20915;&#26041;&#26696;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29992;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#21046;&#20316;&#36924;&#30495;&#30340;&#21160;&#30011;&#26159;&#22256;&#38590;&#21644;&#36153;&#26102;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FDLS&#65288;Facial Deep Learning Solver&#65289;&#65292;&#36825;&#26159;Weta Digital&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;FDLS&#37319;&#29992;&#20174;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20154;&#24037;&#21442;&#19982;&#65292;&#20801;&#35768;&#22312;&#27714;&#35299;&#36807;&#31243;&#30340;&#22810;&#20010;&#38454;&#27573;&#39564;&#35777;&#21644;&#32534;&#36753;&#27714;&#35299;&#21518;&#30340;&#34920;&#28436;&#12290;&#20026;&#20102;&#35757;&#32451;FDLS&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21407;&#22987;&#30340;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#25442;&#20026;&#31283;&#20581;&#30340;&#22270;&#24418;&#29305;&#24449;&#65292;&#28982;&#21518;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual effects commonly requires both the creation of realistic synthetic humans as well as retargeting actors' performances to humanoid characters such as aliens and monsters. Achieving the expressive performances demanded in entertainment requires manipulating complex models with hundreds of parameters. Full creative control requires the freedom to make edits at any stage of the production, which prohibits the use of a fully automatic ``black box'' solution with uninterpretable parameters. On the other hand, producing realistic animation with these sophisticated models is difficult and laborious. This paper describes FDLS (Facial Deep Learning Solver), which is Weta Digital's solution to these challenges. FDLS adopts a coarse-to-fine and human-in-the-loop strategy, allowing a solved performance to be verified and edited at several stages in the solving process. To train FDLS, we first transform the raw motion-captured data into robust graph features. Secondly, based on the observatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36816;&#21160;&#21407;&#35821;&#26500;&#25104;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#27169;&#22411;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20013;&#23454;&#26102;&#29983;&#25104;&#21487;&#39564;&#35777;&#30340;&#34892;&#20026;&#65292;&#20026;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#27169;&#22411;&#22312;&#25506;&#32034;&#20219;&#21153;&#21644;&#22788;&#29702;&#39063;&#31890;&#29366;&#20171;&#36136;&#30340;&#23454;&#38469;&#24773;&#22659;&#19979;&#20855;&#26377;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14894</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#23398;&#20064;&#34892;&#20026;&#36890;&#36807;&#36816;&#21160;&#21407;&#35821;&#26500;&#25104;&#65306;&#24212;&#29992;&#20110;&#39063;&#31890;&#29366;&#20171;&#36136;&#30340;&#38130;&#21462;
&lt;/p&gt;
&lt;p&gt;
Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media. (arXiv:2309.14894v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#21407;&#35821;&#26500;&#25104;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#27169;&#22411;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20013;&#23454;&#26102;&#29983;&#25104;&#21487;&#39564;&#35777;&#30340;&#34892;&#20026;&#65292;&#20026;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#27169;&#22411;&#22312;&#25506;&#32034;&#20219;&#21153;&#21644;&#22788;&#29702;&#39063;&#31890;&#29366;&#20171;&#36136;&#30340;&#23454;&#38469;&#24773;&#22659;&#19979;&#20855;&#26377;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#21487;&#38752;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20013;&#29983;&#25104;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21152;&#24555;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#37319;&#29992;&#65292;&#22240;&#20026;&#23427;&#22686;&#24378;&#20102;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#22120;&#21019;&#24314;&#30340;&#23398;&#20064;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#36827;&#34892;&#39564;&#35777;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#36816;&#21160;&#21407;&#35821;&#21644;&#27010;&#29575;&#39564;&#35777;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#34892;&#20026;&#25688;&#35201;&#22120;&#65292;&#36890;&#36807;&#21512;&#25104;&#25152;&#25552;&#20379;&#30340;&#36816;&#21160;&#21407;&#35821;&#30340;&#26377;&#21521;&#22270;&#29983;&#25104;&#34892;&#20026;&#12290;&#22914;&#26524;&#36825;&#20123;&#32452;&#25104;&#30340;&#36816;&#21160;&#21407;&#35821;&#31526;&#21512;&#25105;&#20204;&#25351;&#23450;&#30340;&#26631;&#20934;&#65292;&#25152;&#24471;&#21040;&#30340;&#34892;&#20026;&#20855;&#26377;&#27010;&#29575;&#19978;&#30340;&#21487;&#39564;&#35777;&#24615;&#12290;&#25105;&#20204;&#22312;&#25506;&#32034;&#20219;&#21153;&#30340;&#27169;&#25311;&#21644;&#20351;&#29992;&#26426;&#22120;&#20154;&#22312;&#30828;&#20214;&#19978;&#38130;&#21462;&#39063;&#31890;&#29366;&#20171;&#36136;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#21487;&#39564;&#35777;&#30340;&#34892;&#20026;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#21355;&#26143;&#22270;&#20687;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#23616;&#37096;&#20445;&#25345;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21355;&#26143;&#22270;&#20687;&#29305;&#26377;&#30340;&#22823;&#23610;&#24230;&#21644;&#35889;&#21464;&#24322;&#24615;&#65292;&#24182;&#24674;&#22797;&#35299;&#37322;&#24615;&#26041;&#21521;&#65292;&#20174;&#32780;&#29992;&#20110;&#26377;&#23548;&#21521;&#21512;&#25104;&#12290;&#36890;&#36807;&#20445;&#25345;&#23616;&#37096;&#24615;&#65292;&#24471;&#21040;&#30340;&#21521;&#37327;&#26356;&#40065;&#26834;&#65292;&#33021;&#26356;&#22909;&#22320;&#20445;&#25345;&#31867;&#21035;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.14883</link><description>&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#23616;&#37096;&#20445;&#25345;&#26041;&#21521;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs. (arXiv:2309.14883v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14883
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#21355;&#26143;&#22270;&#20687;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#23616;&#37096;&#20445;&#25345;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21355;&#26143;&#22270;&#20687;&#29305;&#26377;&#30340;&#22823;&#23610;&#24230;&#21644;&#35889;&#21464;&#24322;&#24615;&#65292;&#24182;&#24674;&#22797;&#35299;&#37322;&#24615;&#26041;&#21521;&#65292;&#20174;&#32780;&#29992;&#20110;&#26377;&#23548;&#21521;&#21512;&#25104;&#12290;&#36890;&#36807;&#20445;&#25345;&#23616;&#37096;&#24615;&#65292;&#24471;&#21040;&#30340;&#21521;&#37327;&#26356;&#40065;&#26834;&#65292;&#33021;&#26356;&#22909;&#22320;&#20445;&#25345;&#31867;&#21035;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22522;&#20110;&#23567;&#27874;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#35299;&#37322;&#28508;&#22312;&#31354;&#38388;&#30340;&#23616;&#37096;&#24863;&#30693;&#26041;&#27861;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#21355;&#26143;&#22270;&#20687;&#29305;&#26377;&#30340;&#22823;&#23610;&#24230;&#21644;&#35889;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#20445;&#25345;&#23616;&#37096;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#35299;&#39044;&#35757;&#32451;GAN&#30340;&#26435;&#37325;&#31354;&#38388;&#65292;&#24182;&#24674;&#22797;&#19982;&#39640;&#32423;&#35821;&#20041;&#27010;&#24565;&#65288;&#22914;&#22478;&#24066;&#21270;&#12289;&#32467;&#26500;&#23494;&#24230;&#12289;&#26893;&#34987;&#23384;&#22312;&#65289;&#30456;&#23545;&#24212;&#30340;&#21487;&#35299;&#37322;&#26041;&#21521;&#65292;&#20174;&#32780;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#26377;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#36890;&#24120;&#37319;&#29992;&#30340;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25429;&#25417;&#26435;&#37325;&#31354;&#38388;&#21464;&#24322;&#24615;&#30340;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;PCA&#65289;&#65292;&#25105;&#20204;&#26174;&#31034;&#20445;&#25345;&#23616;&#37096;&#24615;&#21487;&#20197;&#24471;&#21040;&#19981;&#21516;&#35282;&#24230;&#30340;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#23545;&#20266;&#24433;&#26356;&#21152;&#40065;&#26834;&#65292;&#33021;&#26356;&#22909;&#22320;&#20445;&#25345;&#31867;&#21035;&#20449;&#24687;&#12290;&#36890;&#36807;&#19968;&#32452;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#20363;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a locality-aware method for interpreting the latent space of wavelet-based Generative Adversarial Networks (GANs), that can well capture the large spatial and spectral variability that is characteristic to satellite imagery. By focusing on preserving locality, the proposed method is able to decompose the weight-space of pre-trained GANs and recover interpretable directions that correspond to high-level semantic concepts (such as urbanization, structure density, flora presence) - that can subsequently be used for guided synthesis of satellite imagery. In contrast to typically used approaches that focus on capturing the variability of the weight-space in a reduced dimensionality space (i.e., based on Principal Component Analysis, PCA), we show that preserving locality leads to vectors with different angles, that are more robust to artifacts and can better preserve class information. Via a set of quantitative and qualitative examples, we further show that the proposed approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#21333;&#31867;&#20998;&#31867;&#26041;&#27861;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#24182;&#20855;&#22791;&#39044;&#27979;&#21644;&#23545;&#25239;&#26410;&#30693;&#27450;&#35784;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14880</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#21333;&#31867;&#20998;&#31867;&#26041;&#27861;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Credit Card Fraud Detection with Subspace Learning-based One-Class Classification. (arXiv:2309.14880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#21333;&#31867;&#20998;&#31867;&#26041;&#27861;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#24182;&#20855;&#22791;&#39044;&#27979;&#21644;&#23545;&#25239;&#26410;&#30693;&#27450;&#35784;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#25968;&#23383;&#21270;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#20449;&#29992;&#21345;&#27450;&#35784;&#30340;&#28608;&#22686;&#21644;&#27450;&#35784;&#25216;&#26415;&#30340;&#19981;&#26029;&#28436;&#21464;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#36130;&#21153;&#25439;&#22833;&#12290;&#33258;&#21160;&#21270;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26159;&#21152;&#36895;&#26816;&#27979;&#12289;&#20943;&#23569;&#21709;&#24212;&#26102;&#38388;&#21644;&#26368;&#23567;&#21270;&#28508;&#22312;&#36130;&#21153;&#25439;&#22833;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#24615;&#65292;&#21363;&#30495;&#23454;&#20132;&#26131;&#36828;&#36828;&#22810;&#20110;&#27450;&#35784;&#20132;&#26131;&#65292;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#29305;&#24449;&#38598;&#21512;&#20013;&#30340;&#39640;&#32500;&#24230;&#25968;&#37327;&#24341;&#21457;&#20102;&#8220;&#32500;&#24230;&#28798;&#38590;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20197;&#21333;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#20026;&#26680;&#24515;&#30340;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20855;&#26377;&#39044;&#35265;&#21644;&#23545;&#25239;&#23578;&#26410;&#21457;&#26126;&#30340;&#27450;&#35784;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;OCC&#31639;&#27861;&#30340;&#28508;&#21147;&#26469;&#31361;&#20986;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an increasingly digitalized commerce landscape, the proliferation of credit card fraud and the evolution of sophisticated fraudulent techniques have led to substantial financial losses. Automating credit card fraud detection is a viable way to accelerate detection, reducing response times and minimizing potential financial losses. However, addressing this challenge is complicated by the highly imbalanced nature of the datasets, where genuine transactions vastly outnumber fraudulent ones. Furthermore, the high number of dimensions within the feature set gives rise to the ``curse of dimensionality". In this paper, we investigate subspace learning-based approaches centered on One-Class Classification (OCC) algorithms, which excel in handling imbalanced data distributions and possess the capability to anticipate and counter the transactions carried out by yet-to-be-invented fraud techniques. The study highlights the potential of subspace learning-based OCC algorithms by investigating th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14859</link><description>&lt;p&gt;
&#23548;&#33322;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#65306;&#20174;LyCORIS&#24494;&#35843;&#21040;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#32473;&#26032;&#26041;&#27861;&#30340;&#25972;&#21512;&#21644;&#31995;&#32479;&#35780;&#20272;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65288;Lora beYond Conventional methods&#65292;Other Rank adaptation Implementations for Stable diffusion&#65289;[https://github.com/KohakuBlueleaf/LyCORIS]&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#26631;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24494;&#35843;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22312;&#19981;&#21516;&#27010;&#24565;&#31867;&#21035;&#19979;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2309.14857</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#27969;&#24418;&#25237;&#24433;&#36827;&#34892;&#32858;&#31867;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#26159;&#21487;&#35270;&#21270;&#25506;&#32034;&#39640;&#32500;&#25968;&#25454;&#21644;&#21457;&#29616;&#20854;&#22312;&#20108;&#32500;&#25110;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#20043;&#19968;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#22823;&#37096;&#20998;&#38477;&#32500;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#23454;&#36341;&#32773;&#21487;&#33021;&#23545;&#25152;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#19981;&#20165;&#25490;&#38500;&#19982;&#20808;&#39564;&#30693;&#35782;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#32780;&#19988;&#26088;&#22312;&#25581;&#31034;&#20219;&#20309;&#21097;&#20313;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#30446;&#26631;&#30340;&#32447;&#24615;&#32452;&#21512;&#65306;&#39318;&#20808;&#26159;&#23545;&#27604;PCA&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#20854;&#27425;&#26159;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#24471;&#21040;&#30340;&#23884;&#20837;&#20013;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#23450;&#20041;&#20026;&#27969;&#24418;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#32771;&#34385;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#65292;&#20197;&#25903;&#25345;&#28921;&#39274;&#26426;&#22120;&#20154;&#22312;&#29038;&#40481;&#34507;&#36807;&#31243;&#20013;&#23545;&#40481;&#34507;&#29366;&#24577;&#30340;&#24863;&#30693;&#21644;&#25605;&#25292;&#21160;&#20316;&#30340;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2309.14837</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#23454;&#26102;&#21160;&#20316;&#29983;&#25104;&#21644;&#20027;&#21160;&#24863;&#30693;&#30340;&#28921;&#39274;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot. (arXiv:2309.14837v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14837
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#65292;&#20197;&#25903;&#25345;&#28921;&#39274;&#26426;&#22120;&#20154;&#22312;&#29038;&#40481;&#34507;&#36807;&#31243;&#20013;&#23545;&#40481;&#34507;&#29366;&#24577;&#30340;&#24863;&#30693;&#21644;&#25605;&#25292;&#21160;&#20316;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20154;&#31867;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#33258;&#20027;&#23398;&#20064;&#65292;&#36866;&#24212;&#29289;&#20307;&#21644;&#29615;&#22659;&#65292;&#24182;&#25191;&#34892;&#36866;&#24403;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#30495;&#23454;&#30340;&#39135;&#26448;&#29038;&#28818;&#40481;&#34507;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#23454;&#26102;&#24863;&#30693;&#40481;&#34507;&#30340;&#29366;&#24577;&#24182;&#35843;&#25972;&#25605;&#25292;&#21160;&#20316;&#65292;&#21516;&#26102;&#40481;&#34507;&#34987;&#21152;&#28909;&#19988;&#29366;&#24577;&#19981;&#26029;&#21464;&#21270;&#12290;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22788;&#29702;&#21464;&#21270;&#30340;&#29289;&#20307;&#34987;&#21457;&#29616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24863;&#30693;&#20449;&#24687;&#21253;&#25324;&#21160;&#24577;&#30340;&#12289;&#37325;&#35201;&#25110;&#22024;&#26434;&#30340;&#20449;&#24687;&#65292;&#32780;&#19988;&#27599;&#27425;&#24212;&#35813;&#20851;&#27880;&#30340;&#27169;&#24577;&#19981;&#26029;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26435;&#34913;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#21306;&#20998;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#12290;&#27169;&#22411;&#36890;&#36807;&#31034;&#33539;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20801;&#35768;&#19981;&#26029;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
To support humans in their daily lives, robots are required to autonomously learn, adapt to objects and environments, and perform the appropriate actions. We tackled on the task of cooking scrambled eggs using real ingredients, in which the robot needs to perceive the states of the egg and adjust stirring movement in real time, while the egg is heated and the state changes continuously. In previous works, handling changing objects was found to be challenging because sensory information includes dynamical, both important or noisy information, and the modality which should be focused on changes every time, making it difficult to realize both perception and motion generation in real time. We propose a predictive recurrent neural network with an attention mechanism that can weigh the sensor input, distinguishing how important and reliable each modality is, that realize quick and efficient perception and motion generation. The model is trained with learning from the demonstration, and allow
&lt;/p&gt;</description></item><item><title>OS-net&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21608;&#26399;&#21160;&#21147;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#21644;&#20276;&#38543;&#26041;&#27861;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#30830;&#20445;&#32593;&#32476;&#26435;&#37325;&#30340;&#31283;&#23450;&#24615;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21457;&#29616;R\"{o}ssler&#21644;Sprott&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.14822</link><description>&lt;p&gt;
OS-net: &#36712;&#36947;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
OS-net: Orbitally Stable Neural Networks. (arXiv:2309.14822v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14822
&lt;/p&gt;
&lt;p&gt;
OS-net&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21608;&#26399;&#21160;&#21147;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#21644;&#20276;&#38543;&#26041;&#27861;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#30830;&#20445;&#32593;&#32476;&#26435;&#37325;&#30340;&#31283;&#23450;&#24615;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21457;&#29616;R\"{o}ssler&#21644;Sprott&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;OS-net&#65288;&#36712;&#36947;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21608;&#26399;&#21160;&#21147;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;OS-net&#26159;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#30340;&#29305;&#20363;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#22522;&#20110;&#20276;&#38543;&#26041;&#27861;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#12290;&#21033;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#25152;&#24471;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23558;OS-net&#24212;&#29992;&#20110;&#21457;&#29616;R\"{o}ssler&#21644;Sprott&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20004;&#20010;&#21160;&#21147;&#31995;&#32479;&#22240;&#20854;&#20493;&#39057;&#21560;&#24341;&#23376;&#21644;&#28151;&#27788;&#34892;&#20026;&#32780;&#38395;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OS-net (Orbitally Stable neural NETworks), a new family of neural network architectures specifically designed for periodic dynamical data. OS-net is a special case of Neural Ordinary Differential Equations (NODEs) and takes full advantage of the adjoint method based backpropagation method. Utilizing ODE theory, we derive conditions on the network weights to ensure stability of the resulting dynamics. We demonstrate the efficacy of our approach by applying OS-net to discover the dynamics underlying the R\"{o}ssler and Sprott's systems, two dynamical systems known for their period doubling attractors and chaotic behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20154;&#21475;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#22823;&#33041;&#24180;&#40836;&#22238;&#24402;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14816</link><description>&lt;p&gt;
&#20154;&#21475;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#33041;&#24180;&#40836;&#22238;&#24402;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression. (arXiv:2309.14816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20154;&#21475;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#22823;&#33041;&#24180;&#40836;&#22238;&#24402;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#20197;&#25104;&#20026;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#37325;&#35201;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#22823;&#33041;&#24180;&#40836;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#32435;&#20837;&#35813;&#20272;&#35745;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#21475;&#22270;&#65292;&#23427;&#32467;&#21512;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#24182;&#25429;&#25417;&#20102;&#20154;&#32676;&#20869;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#20154;&#21475;&#22270;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#26395;&#30340;&#32467;&#26524;&#65292;&#20027;&#35201;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22270;&#32467;&#26500;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#38745;&#24577;&#12290;&#28982;&#32780;&#65292;&#25552;&#21462;&#20154;&#21475;&#22270;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;GNNs&#23545;&#20110;&#22270;&#32467;&#26500;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#26500;&#24314;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#20154;&#21475;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#20110;&#22823;&#33041;&#24180;&#40836;&#20272;&#35745;&#20013;GNN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difference between the chronological and biological brain age of a subject can be an important biomarker for neurodegenerative diseases, thus brain age estimation can be crucial in clinical settings. One way to incorporate multimodal information into this estimation is through population graphs, which combine various types of imaging data and capture the associations among individuals within a population. In medical imaging, population graphs have demonstrated promising results, mostly for classification tasks. In most cases, the graph structure is pre-defined and remains static during training. However, extracting population graphs is a non-trivial task and can significantly impact the performance of Graph Neural Networks (GNNs), which are sensitive to the graph structure. In this work, we highlight the importance of a meaningful graph construction and experiment with different population-graph construction methods and their effect on GNN performance on brain age estimation. We us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14808</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#20998;&#31867;&#22120;&#20351;&#29992;Softmax&#20989;&#25968;&#26469;&#23398;&#20064;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#25351;&#20986;&#20854;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#31163;&#32676;&#20540;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#65292;&#36890;&#24120;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#22266;&#26377;&#38480;&#21046;&#36824;&#38480;&#21046;&#20102;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36873;&#25321;&#20309;&#26102;&#24536;&#35760;&#21644;&#20445;&#30041;&#20808;&#21069;&#35757;&#32451;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#30340;&#20934;&#30830;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25513;&#30721;Softmax&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#26082;&#31616;&#21333;&#21448;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#32622;&#20449;&#24230;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#31283;&#23450;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#37325;&#25918;&#30340;&#31867;-&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#36275;&#22815;&#22823;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2309.14807</link><description>&lt;p&gt;
&#35780;&#20272;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65306;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating Soccer Match Prediction Models: A Deep Learning Approach and Feature Optimization for Gradient-Boosted Trees. (arXiv:2309.14807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#22320;&#29992;&#20110;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20351;&#24471;&#27169;&#22411;&#35780;&#20272;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;2023&#24180;&#36275;&#29699;&#39044;&#27979;&#25361;&#25112;&#35201;&#27714;&#39318;&#20808;&#39044;&#27979;&#27599;&#25903;&#29699;&#38431;&#30340;&#20934;&#30830;&#36827;&#29699;&#25968;&#65292;&#20854;&#27425;&#39044;&#27979;&#32988;&#36127;&#24179;&#30340;&#27010;&#29575;&#12290;&#31454;&#36187;&#25552;&#20379;&#20102;&#21407;&#22987;&#30340;&#35757;&#32451;&#38598;&#21644;&#29305;&#24449;&#65292;&#20294;&#36824;&#22686;&#21152;&#20102;&#22312;2023&#24180;4&#26376;4&#26085;&#33267;4&#26376;13&#26085;&#26399;&#38388;&#36827;&#34892;&#30340;&#39069;&#22806;&#27604;&#36187;&#65292;&#36825;&#20195;&#34920;&#20102;&#35757;&#32451;&#38598;&#25130;&#27490;&#26085;&#26399;&#21040;&#39318;&#27425;&#39044;&#27979;&#27604;&#36187;&#20043;&#38388;&#30340;&#26102;&#26399;&#65288;&#29992;&#20110;&#35780;&#20272;&#24615;&#33021;&#65289;&#12290;&#20351;&#29992;pi-ratings&#20316;&#20026;&#29305;&#24449;&#30340;CatBoost&#27169;&#22411;&#34987;&#24212;&#29992;&#65292;&#26368;&#21021;&#34987;&#30830;&#23450;&#20026;&#35745;&#31639;&#32988;&#36127;&#24179;&#27010;&#29575;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20010;&#29305;&#23450;&#20219;&#21153;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have become increasingly popular for predicting the results of soccer matches, however, the lack of publicly-available benchmark datasets has made model evaluation challenging. The 2023 Soccer Prediction Challenge required the prediction of match results first in terms of the exact goals scored by each team, and second, in terms of the probabilities for a win, draw, and loss. The original training set of matches and features, which was provided for the competition, was augmented with additional matches that were played between 4 April and 13 April 2023, representing the period after which the training set ended, but prior to the first matches that were to be predicted (upon which the performance was evaluated). A CatBoost model was employed using pi-ratings as the features, which were initially identified as the optimal choice for calculating the win/draw/loss probabilities. Notably, deep learning models have frequently been disregarded in this particular task. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14779</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#25506;&#32034;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#25928;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#25163;&#21160;&#26631;&#35760;&#30340;&#39640;&#25104;&#26412;&#65292;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#38754;&#20020;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#20294;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65292;&#23567;&#20110;10&#20159;&#20010;&#21442;&#25968;&#65289;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#23450;&#21046;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#31526;&#21512;&#24037;&#19994;&#32422;&#26463;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20855;&#26377;220M&#21442;&#25968;&#30340;&#20856;&#22411;SLM T5-base&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19978;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65288;&#36798;&#21040;&#23436;&#25972;&#25968;&#25454;&#30340;15%&#65289;&#65292;&#26174;&#31034;&#20986;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;MarchOn&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#36827;&#34892;&#36845;&#20195;&#65292;&#20855;&#26377;&#26368;&#20339;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14775</link><description>&lt;p&gt;
&#25968;&#25454;&#32852;&#21512;&#19978;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#38236;&#20687;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Markov Chain Mirror Descent On Data Federation. (arXiv:2309.14775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;MarchOn&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#36827;&#34892;&#36845;&#20195;&#65292;&#20855;&#26377;&#26368;&#20339;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#22914;&#38236;&#20687;&#19979;&#38477;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#36890;&#24120;&#36798;&#21040;&#20102;&#20122;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#21487;&#33021;&#36807;&#20110;&#24378;&#22823;&#21644;&#19981;&#20999;&#23454;&#38469;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#20174;&#39532;&#23572;&#31185;&#22827;&#38142;&#20013;&#25277;&#21462;&#23454;&#20363;&#26102;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#26469;&#35828;&#65292;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#29256;&#26412;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65292;&#31216;&#20043;&#20026;MarchOn&#12290;&#22312;&#19968;&#20010;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#65292;&#27169;&#22411;&#20250;&#20174;&#19968;&#20010;&#33410;&#28857;&#36845;&#20195;&#22320;&#38543;&#26426;&#31227;&#21160;&#21040;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;MarchOn&#65292;&#20026;&#20984;&#12289;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#25552;&#20379;&#20102;&#26368;&#20339;&#25910;&#25947;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;MarchOn&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization methods such as mirror descent have wide applications due to low computational cost. Those methods have been well studied under assumption of the independent and identical distribution, and usually achieve sublinear rate of convergence. However, this assumption may be too strong and unpractical in real application scenarios. Recent researches investigate stochastic gradient descent when instances are sampled from a Markov chain. Unfortunately, few results are known for stochastic mirror descent. In the paper, we propose a new version of stochastic mirror descent termed by MarchOn in the scenario of the federated learning. Given a distributed network, the model iteratively travels from a node to one of its neighbours randomly. Furthermore, we propose a new framework to analyze MarchOn, which yields best rates of convergence for convex, strongly convex, and non-convex loss. Finally, we conduct empirical studies to evaluate the convergence of MarchOn, and validate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#21482;&#35843;&#25972;&#38468;&#21152;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14774</link><description>&lt;p&gt;
BLIP-Adapter&#65306;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#21482;&#35843;&#25972;&#38468;&#21152;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26377;&#25928;&#30340;&#35843;&#25972;&#26041;&#27861;&#26469;&#22788;&#29702;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#31227;&#21160;&#35774;&#22791;&#23631;&#24149;&#25130;&#22270;&#30340;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26696;&#20363;&#20013;&#23545;&#20135;&#21697;&#25130;&#23631;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#25551;&#36848;&#30456;&#23545;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#38656;&#35201;&#32771;&#34385;&#21040;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#27169;&#22411;&#20013;&#22823;&#37327;&#21442;&#25968;&#30340;&#26102;&#38388;&#12289;&#35745;&#31639;&#21147;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#21482;&#38656;&#35201;&#35843;&#25972;&#27169;&#22411;&#20013;&#30340;&#38468;&#21152;&#27169;&#22359;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35270;&#35273;&#25110;&#35821;&#35328;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#26631;&#39064;&#29983;&#25104;&#20013;&#30340;&#31867;&#20284;&#25361;&#25112;&#12290;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#26631;&#39064;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#35757;&#32451;&#20854;&#20182;&#27169;&#22359;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#20013;&#20449;&#24687;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.14757</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20154;&#26426;&#32676;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach. (arXiv:2309.14757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#20013;&#20449;&#24687;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#36890;&#20449;&#22330;&#26223;&#20013;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#38656;&#35201;&#30001;&#33021;&#22815;&#38752;&#36817;&#29289;&#32852;&#32593;&#35774;&#22791;&#24182;&#20943;&#23569;&#19978;&#34892;&#33021;&#37327;&#28040;&#32791;&#30340;&#21160;&#24577;&#21333;&#20803;&#36827;&#34892;&#35206;&#30422;&#12290;&#19968;&#31181;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37096;&#32626;&#22823;&#37327;&#26080;&#20154;&#26426;&#65288;&#26080;&#20154;&#26426;&#32676;&#65289;&#25552;&#20379;&#35206;&#30422;&#24182;&#20026;&#29289;&#32852;&#32593;&#32593;&#32476;&#25552;&#20379;&#26356;&#22909;&#30340;&#35270;&#32447;&#36830;&#36890;&#24615;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#26381;&#21153;&#21333;&#20803;&#30340;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#22330;&#26223;&#20250;&#24341;&#23548;&#20986;&#20855;&#26377;&#39640;&#22797;&#26434;&#24615;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#30001;&#37096;&#32626;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#24341;&#36215;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23558;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#32988;&#36807;&#39640;&#22797;&#26434;&#24615;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26080;&#33021;&#20026;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many massive IoT communication scenarios, the IoT devices require coverage from dynamic units that can move close to the IoT devices and reduce the uplink energy consumption. A robust solution is to deploy a large number of UAVs (UAV swarm) to provide coverage and a better line of sight (LoS) for the IoT network. However, the study of these massive IoT scenarios with a massive number of serving units leads to high dimensional problems with high complexity. In this paper, we apply multi-agent deep reinforcement learning to address the high-dimensional problem that results from deploying a swarm of UAVs to collect fresh information from IoT devices. The target is to minimize the overall age of information in the IoT network. The results reveal that both cooperative and partially cooperative multi-agent deep reinforcement learning approaches are able to outperform the high-complexity centralized deep reinforcement learning approach, which stands helpless in large-scale networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#35299;&#20915;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.14727</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#19982;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization. (arXiv:2309.14727v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#35299;&#20915;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#36830;&#32493;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#65288;MACDPP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#33021;&#21147;&#26377;&#38480;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#21040;&#20013;&#24515;&#21270;&#35757;&#32451;&#19982;&#20998;&#25955;&#25191;&#34892;&#65288;CTDE&#65289;&#26694;&#26550;&#20013;&#30340;Actor-Critic&#65288;AC&#65289;&#32467;&#26500;&#65292;&#23427;&#32531;&#35299;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#21253;&#25324;OpenAI&#22522;&#20934;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#22312;&#20869;&#30340;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;MACDPP&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#30456;&#20851;&#30340;&#22810;&#26234;&#33021;&#20307;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#26234;&#33021;&#20307;&#22522;&#32447;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#22240;&#27492;&#25193;&#23637;&#20102;MARL&#22312;&#26377;&#25928;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14717</link><description>&lt;p&gt;
QA-LoRA: &#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;LLMs&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;QA-LoRA&#65289;&#31639;&#27861;&#12290;&#21160;&#26426;&#22312;&#20110;&#37327;&#21270;&#21644;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#19981;&#24179;&#34913;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#32452;&#20869;&#36816;&#31639;&#31526;&#65292;&#22686;&#21152;&#37327;&#21270;&#30340;&#33258;&#30001;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#12290;QA-LoRA&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#20351;&#21407;&#22987;&#30340;LoRA&#20855;&#22791;&#20102;&#20004;&#20010;&#33021;&#21147;&#65306;&#65288;i&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLM&#30340;&#26435;&#37325;&#34987;&#37327;&#21270;&#65288;&#20363;&#22914;&#36716;&#25442;&#20026;INT4&#65289;&#65292;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#65288;ii&#65289;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;LLM&#21644;&#36741;&#21161;&#26435;&#37325;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;QA-LoRA&#24212;&#29992;&#21040;LLaMA&#21644;LLaMA2&#27169;&#22411;&#23478;&#26063;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#28145;&#24230;&#20154;&#33080;&#31639;&#27861;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20154;&#33080;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#20026;AI&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23454;&#29992;&#20154;&#33080;&#21487;&#35270;&#21270;&#30340;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.14715</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35270;&#21270;&#35299;&#37322;&#28145;&#24230;&#20154;&#33080;&#31639;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explaining Deep Face Algorithms through Visualization: A Survey. (arXiv:2309.14715v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#28145;&#24230;&#20154;&#33080;&#31639;&#27861;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20154;&#33080;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#20026;AI&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23454;&#29992;&#20154;&#33080;&#21487;&#35270;&#21270;&#30340;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#30340;&#20154;&#33080;&#20219;&#21153;&#28145;&#24230;&#27169;&#22411;&#22312;&#26576;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#25105;&#20204;&#24182;&#19981;&#20102;&#35299;&#23427;&#20204;&#26159;&#22914;&#20309;&#24037;&#20316;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26080;&#27861;&#39044;&#27979;&#23427;&#23545;&#26032;&#36755;&#20837;&#30340;&#21453;&#24212;&#65292;&#23548;&#33268;&#31639;&#27861;&#20986;&#29616;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#21644;&#19981;&#24076;&#26395;&#30340;&#20559;&#35265;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26377;&#21161;&#20110;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#30446;&#21069;&#38024;&#23545;&#20154;&#33080;&#30340;&#21487;&#35270;&#21270;&#31639;&#27861;&#38750;&#24120;&#23569;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#36827;&#34892;&#20102;&#20154;&#33080;&#39046;&#22495;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#36890;&#29992;&#21487;&#35270;&#21270;&#31639;&#27861;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#20154;&#33080;&#39046;&#22495;&#30340;&#32454;&#24494;&#24046;&#21035;&#21644;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#36890;&#36807;&#23545;&#27969;&#34892;&#20154;&#33080;&#27169;&#22411;&#36827;&#34892;&#35745;&#31639;&#21487;&#35270;&#21270;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#20154;&#33080;&#32593;&#32476;&#32467;&#26500;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#21487;&#20026;AI&#20174;&#19994;&#32773;&#25552;&#20379;&#30340;&#23454;&#29992;&#20154;&#33080;&#21487;&#35270;&#21270;&#30340;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although current deep models for face tasks surpass human performance on some benchmarks, we do not understand how they work. Thus, we cannot predict how it will react to novel inputs, resulting in catastrophic failures and unwanted biases in the algorithms. Explainable AI helps bridge the gap, but currently, there are very few visualization algorithms designed for faces. This work undertakes a first-of-its-kind meta-analysis of explainability algorithms in the face domain. We explore the nuances and caveats of adapting general-purpose visualization algorithms to the face domain, illustrated by computing visualizations on popular face models. We review existing face explainability works and reveal valuable insights into the structure and hierarchy of face networks. We also determine the design considerations for practical face visualizations accessible to AI practitioners by conducting a user study on the utility of various explainability algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20108;&#38454;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31867;&#26377;&#30028;&#26102;&#38388;&#30340;&#20108;&#38454;RNN&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36716;&#31227;&#34920;&#32534;&#30721;&#21040;&#20854;&#24490;&#29615;&#26435;&#37325;&#20013;&#23454;&#29616;&#26377;&#30028;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14691</link><description>&lt;p&gt;
&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24418;&#24335;&#23618;&#27425;&#30340;&#20108;&#38454;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. (arXiv:2309.14691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20108;&#38454;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31867;&#26377;&#30028;&#26102;&#38388;&#30340;&#20108;&#38454;RNN&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36716;&#31227;&#34920;&#32534;&#30721;&#21040;&#20854;&#24490;&#29615;&#26435;&#37325;&#20013;&#23454;&#29616;&#26377;&#30028;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24490;&#29615;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;(TC)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#26435;&#37325;&#26080;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;ANNs&#20063;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#25110;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#25165;&#33021;&#35782;&#21035;TC&#35821;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22266;&#23450;&#25110;&#26377;&#30028;&#31934;&#24230;&#31070;&#32463;&#20803;&#21644;&#26102;&#38388;&#30340;&#32422;&#26463;&#19979;&#65292;&#26080;&#35760;&#24518;&#30340;ANNs&#34987;&#35777;&#26126;&#24456;&#38590;&#35782;&#21035;&#29978;&#33267;&#26159;&#19978;&#19979;&#25991;&#33258;&#30001;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20108;&#38454;&#24490;&#29615;&#32593;&#32476;($2^{nd}$ RNN)&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#23384;&#22312;&#19968;&#31867;&#26377;&#30028;&#26102;&#38388;&#30340;$2^{nd}$ RNN&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#23558;&#36716;&#31227;&#34920;&#32534;&#30721;&#21040;&#20854;&#24490;&#29615;&#26435;&#37325;&#20013;&#65292;&#23454;&#29616;&#26377;&#30028;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#26377;&#30028;&#26435;&#37325;&#21644;&#26102;&#38388;&#32422;&#26463;&#19979;&#65292;&#26080;&#35760;&#24518;&#30340;&#20108;&#38454;RNNs&#22312;&#35782;&#21035;&#19978;&#20248;&#20110;&#29616;&#20195;&#27169;&#22411;&#65292;&#22914;&#22522;&#26412;RNNs&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) with recurrence and self-attention have been shown to be Turing-complete (TC). However, existing work has shown that these ANNs require multiple turns or unbounded computation time, even with unbounded precision in weights, in order to recognize TC grammars. However, under constraints such as fixed or bounded precision neurons and time, ANNs without memory are shown to struggle to recognize even context-free languages. In this work, we extend the theoretical foundation for the $2^{nd}$-order recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$ RNN that is Turing-complete with bounded time. This model is capable of directly encoding a transition table into its recurrent weights, enabling bounded time computation and is interpretable by design. We also demonstrate that $2$nd order RNNs, without memory, under bounded weights and time constraints, outperform modern-day models such as vanilla RNNs and gated recurrent units in recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>FedCompass&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#31471;&#20351;&#29992;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14675</link><description>&lt;p&gt;
FedCompass&#65306;&#22522;&#20110;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#22791;&#30340;&#39640;&#25928;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler. (arXiv:2309.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14675
&lt;/p&gt;
&lt;p&gt;
FedCompass&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#31471;&#20351;&#29992;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20026;&#21327;&#21516;&#35757;&#32451;&#40065;&#26834;&#19988;&#27867;&#21270;&#30340;AI&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#65292;&#22914;&#21307;&#30103;&#12289;&#37329;&#34701;&#20197;&#21450;&#32570;&#20047;&#38598;&#20013;&#24335;&#25968;&#25454;&#35774;&#26045;&#30340;&#31185;&#23398;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#23458;&#25143;&#31471;&#65288;&#21363;&#35774;&#22791;&#24322;&#26500;&#24615;&#65289;&#20043;&#38388;&#30340;&#35745;&#31639;&#36164;&#28304;&#24046;&#24322;&#65292;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#31561;&#24453;&#38459;&#22622;&#23458;&#25143;&#31471;&#26102;&#25928;&#29575;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#38750;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30001;&#20110;&#36807;&#26102;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#23458;&#25143;&#31471;&#20559;&#31227;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21644;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#30340;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedCompass&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#30340;&#21019;&#26032;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power aware scheduler on the server side, which adaptive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14674</link><description>&lt;p&gt;
&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#22522;&#20110;UPTST&#30340;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#36275;&#21475;&#30149;&#65288;HFMD&#65289;&#29190;&#21457;&#19982;&#20005;&#37325;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#20799;&#31185;HFMD&#24739;&#32773;&#30340;&#27599;&#26085;&#20303;&#38498;&#20154;&#25968;&#23545;&#20110;&#21327;&#21161;&#21307;&#38498;&#24212;&#23545;&#28508;&#22312;&#30340;&#29190;&#21457;&#21644;&#20943;&#23569;&#21307;&#38498;&#20869;&#20256;&#25773;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;U-net&#24418;&#29366;&#65292;&#24182;&#21033;&#29992;&#20102;&#19982;HFMD&#23494;&#20999;&#30456;&#20851;&#30340;&#33133;&#21693;&#21475;&#28814;&#30340;&#35265;&#35299;&#12290;&#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#24341;&#20837;&#37325;&#26500;&#25439;&#22833;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#26469;&#25972;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;UPTST&#27169;&#22411;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;HFMD&#38271;&#30701;&#33218;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#24615;&#30340;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#20986;&#20102;&#20256;&#26579;&#30149;&#30340;&#39044;&#27979;&#65292;&#25552;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
&lt;/p&gt;</description></item><item><title>ALEX&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#22270;&#20256;&#36755;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#24179;&#34913;&#26631;&#31614;&#20998;&#24067;&#30340;&#23376;&#22270;&#26500;&#24314;&#26041;&#27861;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.14673</link><description>&lt;p&gt;
ALEX: &#26397;&#21521;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26377;&#25928;&#22270;&#20256;&#36755;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ALEX: Towards Effective Graph Transfer Learning with Noisy Labels. (arXiv:2309.14673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14673
&lt;/p&gt;
&lt;p&gt;
ALEX&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#22270;&#20256;&#36755;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#24179;&#34913;&#26631;&#31614;&#20998;&#24067;&#30340;&#23376;&#22270;&#26500;&#24314;&#26041;&#27861;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#22312;&#21508;&#31181;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#37117;&#26159;&#20351;&#29992;&#23436;&#20840;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#23548;&#33268;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#30340;&#22270;&#20256;&#36755;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23558;&#30693;&#35782;&#20174;&#24102;&#26377;&#22122;&#22768;&#30340;&#28304;&#22270;&#20256;&#36755;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Balance Alignment and Information-aware Examination (ALEX)&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;ALEX&#39318;&#20808;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#29983;&#25104;&#20855;&#26377;&#20851;&#38190;&#32467;&#26500;&#35821;&#20041;&#30340;&#19981;&#21516;&#35270;&#22270;&#65292;&#21033;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#20026;&#20102;&#20943;&#36731;&#26631;&#31614;&#20559;&#31227;&#21644;&#39046;&#22495;&#20559;&#31227;&#65292;&#25105;&#20204;&#20272;&#35745;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#26469;&#26500;&#24314;&#20855;&#26377;&#24179;&#34913;&#26631;&#31614;&#20998;&#24067;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have garnered considerable interest due to their exceptional performance in a wide range of graph machine learning tasks. Nevertheless, the majority of GNN-based approaches have been examined using well-annotated benchmark datasets, leading to suboptimal performance in real-world graph learning scenarios. To bridge this gap, the present paper investigates the problem of graph transfer learning in the presence of label noise, which transfers knowledge from a noisy source graph to an unlabeled target graph. We introduce a novel technique termed Balance Alignment and Information-aware Examination (ALEX) to address this challenge. ALEX first employs singular value decomposition to generate different views with crucial structural semantics, which help provide robust node representations using graph contrastive learning. To mitigate both label shift and domain shift, we estimate a prior distribution to build subgraphs with balanced label distributions. Building o
&lt;/p&gt;</description></item><item><title>DONNAv2&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#12290;&#23427;&#37319;&#29992;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#24335;&#25506;&#32034;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#30340;&#39640;&#25928;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#31934;&#24230;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2309.14670</link><description>&lt;p&gt;
DONNAv2-&#36731;&#37327;&#32423;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks. (arXiv:2309.14670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14670
&lt;/p&gt;
&lt;p&gt;
DONNAv2&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#12290;&#23427;&#37319;&#29992;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#24335;&#25506;&#32034;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#30340;&#39640;&#25928;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#31934;&#24230;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35270;&#35273;&#24212;&#29992;&#21644;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#24320;&#21457;&#23545;&#30828;&#20214;&#21451;&#22909;&#30340;&#20307;&#31995;&#32467;&#26500;&#24182;&#22312;&#35774;&#22791;&#37096;&#32626;&#26399;&#38388;&#20445;&#25345;&#24615;&#33021;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#20197;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#24335;&#25506;&#32034;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#21457;&#29616;&#29992;&#20110;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#30340;&#39640;&#25928;&#20307;&#31995;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#25928;&#29575;&#30340;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#33976;&#39311;&#30340;&#19979;&#19968;&#20195;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;-DONNAv2&#12290;&#20256;&#32479;&#30340;NAS&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#35745;&#31639;&#23494;&#38598;&#30340;&#38454;&#27573;&#65292;&#22312;&#35813;&#38454;&#27573;&#20013;&#65292;&#23398;&#20064;&#31934;&#24230;&#39044;&#27979;&#22120;&#20197;&#39044;&#20272;&#25628;&#32034;&#31354;&#38388;&#20869;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24314;&#31435;&#31934;&#24230;&#39044;&#27979;&#22120;&#26377;&#21161;&#20110;&#39044;&#27979;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#24314;&#31435;&#31934;&#24230;&#39044;&#27979;&#22120;&#24182;&#23558;DONNA&#25193;&#23637;&#21040;&#35745;&#31639;&#25928;&#29575;&#30340;&#35774;&#32622;&#20013;&#12290;&#24418;&#25104;&#32593;&#32476;&#30340;&#20010;&#20307;&#22359;&#30340;&#25439;&#22833;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
With the growing demand for vision applications and deployment across edge devices, the development of hardware-friendly architectures that maintain performance during device deployment becomes crucial. Neural architecture search (NAS) techniques explore various approaches to discover efficient architectures for diverse learning tasks in a computationally efficient manner. In this paper, we present the next-generation neural architecture design for computationally efficient neural architecture distillation - DONNAv2 . Conventional NAS algorithms rely on a computationally extensive stage where an accuracy predictor is learned to estimate model performance within search space. This building of accuracy predictors helps them predict the performance of models that are not being finetuned. Here, we have developed an elegant approach to eliminate building the accuracy predictor and extend DONNA to a computationally efficient setting. The loss metric of individual blocks forming the network s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZiCo-BC&#30340;&#20462;&#27491;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;ZiCo-BC&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#19978;&#37117;&#33021;&#25104;&#21151;&#25628;&#32034;&#20986;&#20248;&#21270;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.14666</link><description>&lt;p&gt;
ZiCo-BC&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#20462;&#27491;&#20559;&#24046;&#30340;&#38646;&#26679;&#26412;NAS
&lt;/p&gt;
&lt;p&gt;
ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks. (arXiv:2309.14666v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZiCo-BC&#30340;&#20462;&#27491;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;ZiCo-BC&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#19978;&#37117;&#33021;&#25104;&#21151;&#25628;&#32034;&#20986;&#20248;&#21270;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#38646;&#26679;&#26412;&#20195;&#29702;&#65292;&#20197;&#22823;&#24133;&#20943;&#23569;&#19982;&#20256;&#32479;&#22522;&#20110;&#35757;&#32451;&#30340;NAS&#30456;&#27604;&#30340;&#25628;&#32034;&#26102;&#38388;&#12290;&#23613;&#31649;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#31561;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#65292;&#38646;&#26679;&#26412;&#20195;&#29702;&#30340;&#26377;&#25928;&#24615;&#24456;&#23569;&#24471;&#21040;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#34987;&#35777;&#26126;&#23545;&#29305;&#23450;&#27169;&#22411;&#29305;&#24449;&#20855;&#26377;&#20559;&#21521;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#38646;&#26679;&#26412;&#20195;&#29702;ZiCo&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#20559;&#24046;&#65292;&#21457;&#29616;ZiCo&#23545;&#20110;&#26356;&#30246;&#26356;&#28145;&#30340;&#32593;&#32476;&#26377;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZiCo-BC&#30340;&#23545;ZiCo&#36827;&#34892;&#20559;&#24046;&#20462;&#27491;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65288;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#25628;&#32034;&#20986;&#20248;&#21270;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Neural Architecture Search (NAS) approaches propose novel training-free metrics called zero-shot proxies to substantially reduce the search time compared to the traditional training-based NAS. Despite the success on image classification, the effectiveness of zero-shot proxies is rarely evaluated on complex vision tasks such as semantic segmentation and object detection. Moreover, existing zero-shot proxies are shown to be biased towards certain model characteristics which restricts their broad applicability. In this paper, we empirically study the bias of state-of-the-art (SOTA) zero-shot proxy ZiCo across multiple vision tasks and observe that ZiCo is biased towards thinner and deeper networks, leading to sub-optimal architectures. To solve the problem, we propose a novel bias correction on ZiCo, called ZiCo-BC. Our extensive experiments across various vision tasks (image classification, object detection and semantic segmentation) show that our approach can successfully sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;RuBERT&#27169;&#22411;&#21644;Transformer&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#21672;&#35810;&#30340;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19987;&#23478;&#29305;&#38271;&#65292;&#34920;&#29616;&#20986;&#36229;&#36807;92%&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.14662</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21307;&#23398;&#21672;&#35810;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#19982;&#19987;&#23478;&#29305;&#38271;&#30456;&#20851;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation. (arXiv:2309.14662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;RuBERT&#27169;&#22411;&#21644;Transformer&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#21672;&#35810;&#30340;&#29992;&#25143;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19987;&#23478;&#29305;&#38271;&#65292;&#34920;&#29616;&#20986;&#36229;&#36807;92%&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21307;&#30103;&#26102;&#20195;&#65292;&#23545;&#20110;&#29087;&#32451;&#30340;&#21307;&#30103;&#25903;&#25345;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;RuBERT&#27169;&#22411;&#65292;&#23558;&#21307;&#23398;&#21672;&#35810;&#39046;&#22495;&#30340;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#30528;&#37325;&#20851;&#27880;&#19987;&#23478;&#30340;&#29305;&#38271;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;RuBERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26597;&#35810;&#19982;&#29305;&#23450;&#21307;&#23398;&#19987;&#38271;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#21449;&#39564;&#35777;&#21644;&#20256;&#32479;&#30340;&#27979;&#35797;&#21644;&#35757;&#32451;&#38598;&#21010;&#20998;&#19979;&#22343;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;F1&#24471;&#20998;&#36229;&#36807;92%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24515;&#33039;&#30149;&#23398;&#12289;&#31070;&#32463;&#30149;&#23398;&#21644;&#30382;&#32932;&#31185;&#31561;&#21307;&#23398;&#39046;&#22495;&#30340;&#27867;&#21270;&#24615;&#33021;&#20063;&#38750;&#24120;&#20986;&#33394;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#38469;&#30410;&#22788;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#24341;&#23548;&#33267;&#36866;&#24403;&#30340;&#19987;&#23478;&#20197;&#33719;&#24471;&#21450;&#26102;&#32780;&#26377;&#38024;&#23545;&#24615;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23427;&#36824;&#25552;&#39640;&#20102;&#21307;&#30103;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#20174;&#19994;&#32773;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for skilled medical support is growing in the era of digital healthcare. This research presents an innovative strategy, utilising the RuBERT model, for categorising user inquiries in the field of medical consultation with a focus on expert specialisation. By harnessing the capabilities of transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset, which facilitates precise correspondence between queries and particular medical specialisms. Using a comprehensive dataset, we have demonstrated our approach's superior performance with an F1-score of over 92%, calculated through both cross-validation and the traditional split of test and train datasets. Our approach has shown excellent generalisation across medical domains such as cardiology, neurology and dermatology. This methodology provides practical benefits by directing users to appropriate specialists for prompt and targeted medical advice. It also enhances healthcare system efficiency, reduces practitioner 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#24182;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24555;&#36895;&#25509;&#36817;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14648</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#30830;&#23450;&#25511;&#21046;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;: &#38750;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis. (arXiv:2309.14648v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#36890;&#36807;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#24182;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24555;&#36895;&#25509;&#36817;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#38656;&#35201;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#33258;&#36866;&#24212;/&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#20363;&#22914;&#22312;&#32447;&#40065;&#26834;&#31283;&#23450;&#25511;&#21046;&#21644;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;&#38750;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#26377;&#30028;&#12289;&#29420;&#31435;&#21516;&#20998;&#24067;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#30001;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#20272;&#35745;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30452;&#24452;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#30001;&#38598;&#21512;&#25104;&#21592;&#36523;&#20221;&#26356;&#26032;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#40065;&#26834;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#35813;&#25511;&#21046;&#22120;&#19982;&#31163;&#32447;&#26368;&#20248;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#24555;&#36895;&#25509;&#36817;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#32622;&#20449;&#21306;&#22495;&#30340;&#25511;&#21046;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Set-membership estimation is commonly used in adaptive/learning-based control algorithms that require robustness over the model uncertainty sets, e.g., online robustly stabilizing control and robust adaptive model predictive control. Despite having broad applications, non-asymptotic estimation error bounds in the stochastic setting are limited. This paper provides such a non-asymptotic bound on the diameter of the uncertainty sets generated by set membership estimation on linear dynamical systems under bounded, i.i.d. disturbances. Further, this result is applied to robust adaptive model predictive control with uncertainty sets updated by set membership. We numerically demonstrate the performance of the robust adaptive controller, which rapidly approaches the performance of the offline optimal model predictive controller, in comparison with the control design based on least square estimation's confidence regions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21516;&#19968;&#32929;&#31080;&#24066;&#22330;&#36827;&#34892;&#20132;&#26131;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#28784;&#30418;&#26041;&#27861;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#26131;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#20132;&#26131;&#20195;&#29702;&#21463;&#21040;&#23545;&#25163;&#25805;&#32437;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14615</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#20195;&#29702;&#30340;&#28784;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents. (arXiv:2309.14615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21516;&#19968;&#32929;&#31080;&#24066;&#22330;&#36827;&#34892;&#20132;&#26131;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#28784;&#30418;&#26041;&#27861;&#23545;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#26131;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#20132;&#26131;&#20195;&#29702;&#21463;&#21040;&#23545;&#25163;&#25805;&#32437;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;Deep RL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;&#22797;&#26434;&#28216;&#25103;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#35768;&#22810;&#31995;&#32479;&#20013;&#65292;&#20854;&#20013;&#19968;&#20010;&#26377;&#36259;&#30340;&#24212;&#29992;&#26696;&#20363;&#26159;&#23558;&#20854;&#20316;&#20026;&#33258;&#21160;&#21270;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#20219;&#20309;&#33258;&#21160;&#21270;&#20132;&#26131;&#20195;&#29702;&#37117;&#23481;&#26131;&#21463;&#21040;&#20132;&#26131;&#29615;&#22659;&#20013;&#30340;&#23545;&#25163;&#30340;&#25805;&#32437;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#40065;&#26834;&#24615;&#23545;&#20110;&#20854;&#23454;&#36341;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#30740;&#31350;RL&#40065;&#26834;&#24615;&#30340;&#20856;&#22411;&#26426;&#21046;&#65292;&#21363;&#22522;&#20110;&#30333;&#30418;&#26799;&#24230;&#22522;&#30784;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#25216;&#26415;&#65288;&#22914;FGSM&#65289;&#65292;&#23545;&#20110;&#36825;&#31181;&#29992;&#20363;&#26469;&#35828;&#24050;&#32463;&#36807;&#26102;&#65292;&#22240;&#20026;&#27169;&#22411;&#21463;&#21040;&#23433;&#20840;&#30340;&#22269;&#38469;&#20132;&#26131;&#25152;API&#30340;&#20445;&#25252;&#65292;&#22914;&#32435;&#26031;&#36798;&#20811;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#8220;&#28784;&#30418;&#8221;&#26041;&#27861;&#21487;&#20197;&#25915;&#20987;&#22522;&#20110;Deep RL&#30340;&#20132;&#26131;&#20195;&#29702;&#65292;&#20165;&#36890;&#36807;&#22312;&#21516;&#19968;&#32929;&#31080;&#24066;&#22330;&#36827;&#34892;&#20132;&#26131;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25509;&#35302;&#20132;&#26131;&#20195;&#29702;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;&#25163;&#20195;&#29702;&#20351;&#29992;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a "gray-box" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#21442;&#25968;&#21270;&#21464;&#20998;&#25298;&#32477;&#37319;&#26679;&#65288;VRS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#21270;&#30340;&#25552;&#35758;&#20998;&#24067;&#19982;&#25298;&#32477;&#37319;&#26679;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26063;&#65292;&#26126;&#30830;&#21033;&#29992;&#24050;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#20026;&#20855;&#26377;&#36830;&#32493;&#28508;&#21464;&#37327;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25512;&#26029;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.14612</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#21464;&#20998;&#25298;&#32477;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Reparameterized Variational Rejection Sampling. (arXiv:2309.14612v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#21442;&#25968;&#21270;&#21464;&#20998;&#25298;&#32477;&#37319;&#26679;&#65288;VRS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#21270;&#30340;&#25552;&#35758;&#20998;&#24067;&#19982;&#25298;&#32477;&#37319;&#26679;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26063;&#65292;&#26126;&#30830;&#21033;&#29992;&#24050;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#20026;&#20855;&#26377;&#36830;&#32493;&#28508;&#21464;&#37327;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25512;&#26029;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#20381;&#36182;&#20110;&#21442;&#25968;&#21270;&#30340;&#21464;&#20998;&#20998;&#24067;&#26063;&#65292;&#36873;&#25321;&#30340;&#20998;&#24067;&#26063;&#22312;&#30830;&#23450;&#21518;&#39564;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#31616;&#21333;&#30340;mean-field&#20998;&#24067;&#26063;&#36890;&#24120;&#23548;&#33268;&#36739;&#24046;&#30340;&#36817;&#20284;&#65292;&#32780;&#20687;&#24402;&#19968;&#21270;&#27969;&#36825;&#26679;&#30340;&#20016;&#23500;&#20998;&#24067;&#26063;&#24448;&#24448;&#38590;&#20197;&#20248;&#21270;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#21253;&#21547;&#24050;&#30693;&#30446;&#26631;&#20998;&#24067;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;&#20854;&#26159;&#40657;&#31665;&#30340;&#12290;&#20026;&#20102;&#25193;&#23637;&#28789;&#27963;&#30340;&#21464;&#20998;&#20998;&#24067;&#26063;&#31354;&#38388;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#21464;&#20998;&#25298;&#32477;&#37319;&#26679;&#65288;VRS&#65289;[Grover et al., 2018]&#65292;&#23427;&#23558;&#21442;&#25968;&#21270;&#25552;&#35758;&#20998;&#24067;&#19982;&#25298;&#32477;&#37319;&#26679;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26063;&#65292;&#26126;&#30830;&#21033;&#29992;&#24050;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25552;&#35758;&#20998;&#24067;&#21442;&#25968;&#30340;&#20302;&#26041;&#24046;&#37325;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#20351;VRS&#25104;&#20026;&#20855;&#26377;&#36830;&#32493;&#28508;&#21464;&#37327;&#30340;&#21560;&#24341;&#20154;&#30340;&#25512;&#26029;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>Neuro-Visualizer&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#22320;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#39640;&#32500;&#25439;&#22833;&#22320;&#24418;&#65292;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#23454;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#22320;&#24418;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14601</link><description>&lt;p&gt;
&#31070;&#32463;&#35270;&#35273;&#21270;: &#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#22320;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method. (arXiv:2309.14601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14601
&lt;/p&gt;
&lt;p&gt;
Neuro-Visualizer&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#22320;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#39640;&#32500;&#25439;&#22833;&#22320;&#24418;&#65292;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#23454;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#22320;&#24418;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#22320;&#24418;&#36827;&#34892;&#21487;&#35270;&#21270;&#24863;&#20852;&#36259;&#12290;&#32447;&#24615;&#30340;&#22320;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#30452;&#35266;&#22320;&#24110;&#21161;&#30740;&#31350;&#32773;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#28789;&#27963;&#24615;&#21644;&#20302;&#20445;&#30495;&#24230;&#20197;&#34920;&#31034;&#39640;&#32500;&#22320;&#24418;&#32780;&#21463;&#21040;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#22320;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;Neuro-Visualizer&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#22320;&#24418;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#35782;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#24212;&#29992;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;Neuro-Visualizer&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22522;&#20934;&#65292;&#24182;&#26377;&#21161;&#20110;&#35777;&#23454;&#65292;&#24182;&#26377;&#26102;&#23545;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#30340;&#20027;&#24352;&#25552;&#20986;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in visualizing the loss landscape of neural networks. Linear landscape visualization methods, such as principal component analysis, have become widely used as they intuitively help researchers study neural networks and their training process. However, these linear methods suffer from limitations and drawbacks due to their lack of flexibility and low fidelity at representing the high dimensional landscape. In this paper, we present a novel auto-encoder-based non-linear landscape visualization method called Neuro-Visualizer that addresses these shortcoming and provides useful insights about neural network loss landscapes. To demonstrate its potential, we run experiments on a variety of problems in two separate applications of knowledge-guided machine learning (KGML). Our findings show that Neuro-Visualizer outperforms other linear and non-linear baselines and helps corroborate, and sometime challenge, claims proposed by machine learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14597</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#23545;&#22238;&#25253;&#26223;&#35266;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#22833;&#36133;&#21306;&#22495;&#21644;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#24615;&#33021;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#20174;&#30740;&#31350;&#31574;&#30053;&#21644;&#22238;&#25253;&#20043;&#38388;&#30340;&#26144;&#23556;&#21363;&#22238;&#25253;&#26223;&#35266;&#30340;&#35282;&#24230;&#20026;&#36825;&#20123;&#34892;&#20026;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#31639;&#27861;&#22312;&#36825;&#20010;&#26223;&#35266;&#30340;&#22122;&#22768;&#37051;&#22495;&#20013;&#31359;&#34892;&#65292;&#19968;&#20010;&#31574;&#30053;&#21442;&#25968;&#30340;&#21333;&#27425;&#26356;&#26032;&#20250;&#23548;&#33268;&#22238;&#25253;&#22312;&#24456;&#22823;&#33539;&#22260;&#20869;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22238;&#25253;&#36827;&#34892;&#20998;&#24067;&#22788;&#29702;&#65292;&#25105;&#20204;&#23545;&#26223;&#35266;&#36827;&#34892;&#20102;&#26144;&#23556;&#65292;&#25551;&#36848;&#20102;&#31574;&#30053;&#31354;&#38388;&#20013;&#23481;&#26131;&#20135;&#29983;&#22833;&#36133;&#30340;&#21306;&#22495;&#65292;&#24182;&#25581;&#31034;&#20102;&#31574;&#30053;&#21697;&#36136;&#30340;&#38544;&#34255;&#32500;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26223;&#35266;&#30340;&#24778;&#20154;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#25214;&#21040;&#31616;&#21333;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#24320;&#22122;&#22768;&#37051;&#22495;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20248;&#21270;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#39034;&#24207;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#23545;&#23569;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#35757;&#32451;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;&#36807;&#25311;&#21512;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#27010;&#24565;&#28418;&#31227;&#31561;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.14591</link><description>&lt;p&gt;
&#24212;&#29992;&#39034;&#24207;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Applications of Sequential Learning for Medical Image Classification. (arXiv:2309.14591v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#39034;&#24207;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#23545;&#23569;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#35757;&#32451;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;&#36807;&#25311;&#21512;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#27010;&#24565;&#28418;&#31227;&#31561;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23569;&#37327;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#24182;&#21019;&#24314;&#22312;&#32570;&#20047;&#39564;&#35777;&#38598;&#25110;&#27979;&#35797;&#38598;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35757;&#32451;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#22238;&#39038;&#24615;&#30340;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23545;&#21307;&#23398;&#22270;&#20687;&#30340;&#23567;&#25209;&#37327;&#36827;&#34892;&#35757;&#32451;&#21644;&#25345;&#32493;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;PyTorch&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#23398;MNIST&#21644;NIH&#33016;&#37096;X&#23556;&#32447;&#25104;&#20687;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#39034;&#24207;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#27010;&#24565;&#28418;&#31227;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#20004;&#31181;&#39034;&#24207;&#35757;&#32451;&#30340;CNN&#26041;&#27861;&#65306;&#26377;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#26080;&#22522;&#30784;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#20004;&#31181;&#29420;&#29305;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#25968;&#25454;&#25307;&#21215;&#26041;&#27861;&#65292;&#20197;&#20272;&#35745;&#23436;&#25972;&#20449;&#24687;&#30340;&#25552;&#21462;&#32780;&#19981;&#20250;&#36807;&#25311;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#30475;&#24453;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: The aim of this work is to develop a neural network training framework for continual training of small amounts of medical imaging data and create heuristics to assess training in the absence of a hold-out validation or test set.  Materials and Methods: We formulated a retrospective sequential learning approach that would train and consistently update a model on mini-batches of medical images over time. We address problems that impede sequential learning such as overfitting, catastrophic forgetting, and concept drift through PyTorch convolutional neural networks (CNN) and publicly available Medical MNIST and NIH Chest X-Ray imaging datasets. We begin by comparing two methods for a sequentially trained CNN with and without base pre-training. We then transition to two methods of unique training and validation data recruitment to estimate full information extraction without overfitting. Lastly, we consider an example of real-life data that shows how our approach would see mainstre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14587</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20855;&#26377;&#30072;&#21464;&#29575;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20934;&#30830;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24726;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24212;&#35813;&#36890;&#36807;&#35757;&#32451;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#32780;&#19981;&#26159;&#30001;&#32593;&#32476;&#32422;&#26463;&#25152;&#20915;&#23450;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#30001;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#24182;&#20998;&#26512;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#25968;&#25454;&#21644;&#30072;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#20808;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
&lt;/p&gt;</description></item><item><title>DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.14585</link><description>&lt;p&gt;
DifAttack: &#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14585
&lt;/p&gt;
&lt;p&gt;
DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#22522;&#20110;&#20998;&#25968;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;DifAttack&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DifAttack&#39318;&#20808;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#29305;&#24449;&#35299;&#32806;&#20026;&#23545;&#25239;&#29305;&#24449;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20854;&#20013;&#21069;&#32773;&#20027;&#23548;&#22270;&#20687;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#32780;&#21518;&#32773;&#20027;&#35201;&#20915;&#23450;&#20854;&#35270;&#35273;&#22806;&#35266;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#21487;&#29992;&#26367;&#20195;&#27169;&#22411;&#36890;&#36807;&#30333;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#30340;&#24178;&#20928;&#22270;&#20687;&#21644;&#20854;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#32806;&#12290;&#26368;&#32456;&#65292;DifAttack&#26681;&#25454;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#65292;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#65292;&#30452;&#21040;&#25104;&#21151;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#29305;&#24449;&#19981;&#21464;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36991;&#20813;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CWCL&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#36328;&#27169;&#24577;&#36801;&#31227;&#20013;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#23545;&#27604;&#35757;&#32451;&#65292;CWCL&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14580</link><description>&lt;p&gt;
CWCL&#65306;&#36830;&#32493;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#19979;&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss. (arXiv:2309.14580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CWCL&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#36328;&#27169;&#24577;&#36801;&#31227;&#20013;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#23545;&#27604;&#35757;&#32451;&#65292;CWCL&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#36827;&#34892;&#36328;&#27169;&#24577;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#21518;&#19968;&#20010;&#39046;&#22495;&#20013;&#23398;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#24341;&#36215;&#30456;&#24403;&#20851;&#27880;&#30340;&#8220;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#8221;&#21644;&#8220;&#38145;&#23450;&#22270;&#20687;&#35843;&#25972;&#65288;LiT&#65289;&#8221;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#26041;&#27861;&#65288;&#21253;&#25324;CLIP&#21644;LiT&#65289;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#35757;&#32451;&#30446;&#26631;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#30456;&#20284;&#21644;&#39537;&#25955;&#19981;&#30456;&#20284;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20855;&#26377;&#26356;&#36830;&#32493;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#8220;&#38750;&#20108;&#36827;&#21046;&#8221;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#65288;CWCL&#65289;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#20351;&#29992;CWCL&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to ``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning (LiT)'' that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2309.14566</link><description>&lt;p&gt;
&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles. (arXiv:2309.14566v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#20056;&#23458;&#36710;&#36742;&#65288;APV&#65289;&#30340;&#36947;&#36335;&#36712;&#36857;&#35268;&#21010;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#36712;&#36857;&#35268;&#21010;&#26088;&#22312;&#20026;APV&#29983;&#25104;&#20840;&#23616;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#32771;&#34385;&#21040;&#35832;&#22914;&#36710;&#36742;&#21160;&#21147;&#23398;&#12289;&#32422;&#26463;&#21644;&#26816;&#27979;&#21040;&#30340;&#38556;&#30861;&#29289;&#31561;&#21508;&#31181;&#22240;&#32032;&#12290;&#20256;&#32479;&#25216;&#26415;&#28041;&#21450;&#37319;&#26679;&#26041;&#27861;&#19982;&#20248;&#21270;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21069;&#32773;&#30830;&#20445;&#20840;&#23616;&#24863;&#30693;&#65292;&#21518;&#32773;&#20248;&#21270;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32422;&#26463;&#36845;&#20195;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;CILQR&#65289;&#20248;&#21270;&#31639;&#27861;&#26368;&#36817;&#20986;&#29616;&#65292;&#38024;&#23545;APV&#31995;&#32479;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24378;&#35843;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36710;&#36742;&#33258;&#34892;&#36710;&#36816;&#21160;&#27169;&#22411;&#30340;&#29616;&#26377;&#23454;&#29616;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#21487;&#25511;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#39033;&#65292;&#21253;&#25324;&#26354;&#29575;&#21644;&#32437;&#21521;&#21152;&#36895;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#65292;&#26469;&#22686;&#24378;&#36825;&#20010;&#27169;&#22411;&#12290;&#36825;&#31181;&#21253;&#21547;&#26377;&#21161;&#20110;&#22312;&#25104;&#26412;&#21644;&#32422;&#26463;&#35774;&#35745;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the advancements in on-road trajectory planning for Autonomous Passenger Vehicles (APV). Trajectory planning aims to produce a globally optimal route for APVs, considering various factors such as vehicle dynamics, constraints, and detected obstacles. Traditional techniques involve a combination of sampling methods followed by optimization algorithms, where the former ensures global awareness and the latter refines for local optima. Notably, the Constrained Iterative Linear Quadratic Regulator (CILQR) optimization algorithm has recently emerged, adapted for APV systems, emphasizing improved safety and comfort. However, existing implementations utilizing the vehicle bicycle kinematic model may not guarantee controllable trajectories. We augment this model by incorporating higher-order terms, including the first and second-order derivatives of curvature and longitudinal jerk. This inclusion facilitates a richer representation in our cost and constraint design. We also
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#32479;&#35745;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#25112;&#32988;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;&#24182;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14563</link><description>&lt;p&gt;
&#38754;&#21521;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#32479;&#35745;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#32479;&#35745;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#25112;&#32988;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;&#24182;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22823;&#23567;&#20026;N&#30340;&#26679;&#26412;&#65292;&#36873;&#25321;&#19968;&#20010;&#26356;&#23567;&#30340;&#22823;&#23567;n&lt;N&#30340;&#23376;&#26679;&#26412;&#29992;&#20110;&#32479;&#35745;&#20272;&#35745;&#25110;&#23398;&#20064;&#36890;&#24120;&#26159;&#26377;&#29992;&#30340;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#36873;&#25321;&#27493;&#39588;&#26377;&#21161;&#20110;&#20943;&#23569;&#25968;&#25454;&#26631;&#35760;&#30340;&#35201;&#27714;&#21644;&#23398;&#20064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#32473;&#23450;&#20102;N&#20010;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;{x_i}&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#8220;&#26367;&#20195;&#27169;&#22411;&#8221;&#65292;&#23427;&#21487;&#20197;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#22320;&#39044;&#27979;&#26631;&#31614;y_i&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#19968;&#20010;&#23376;&#26679;&#26412;&#38598;{&#119857;_i}&#65292;&#20854;&#22823;&#23567;&#20026;|G|=n&lt;N&#12290;&#28982;&#21518;&#25105;&#20204;&#20026;&#36825;&#20010;&#38598;&#21512;&#33719;&#21462;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#28151;&#21512;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#28176;&#36817;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#25512;&#23548;&#65292;&#25105;&#20204;&#35777;&#26126;&#65306;(i) &#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20987;&#36133;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#65307;(ii) &#22312;&#25968;&#25454;&#36873;&#25321;&#26041;&#38754;&#65292;&#26576;&#20123;&#27969;&#34892;&#30340;&#36873;&#25321;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a sample of size $N$, it is often useful to select a subsample of smaller size $n&lt;N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n&lt;N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35748;&#30693;&#25968;&#23383;&#20379;&#24212;&#38142;&#21452;&#32990;&#32974;&#26694;&#26550;&#20013;&#36827;&#34892;&#25171;&#20081;&#26816;&#27979;&#65292;&#20197;&#22686;&#24378;&#20379;&#24212;&#38142;&#30340;&#38887;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#26102;&#30340;&#25171;&#20081;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24110;&#21161;&#20915;&#31574;&#32773;&#21644;&#20379;&#24212;&#38142;&#23454;&#36341;&#32773;&#20570;&#20986;&#36866;&#24403;&#30340;&#20915;&#31574;&#65292;&#20197;&#26368;&#23567;&#21270;&#25171;&#20081;&#20107;&#20214;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14557</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35748;&#30693;&#25968;&#23383;&#20379;&#24212;&#38142;&#21452;&#32990;&#32974;&#30340;&#25171;&#20081;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning. (arXiv:2309.14557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35748;&#30693;&#25968;&#23383;&#20379;&#24212;&#38142;&#21452;&#32990;&#32974;&#26694;&#26550;&#20013;&#36827;&#34892;&#25171;&#20081;&#26816;&#27979;&#65292;&#20197;&#22686;&#24378;&#20379;&#24212;&#38142;&#30340;&#38887;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#26102;&#30340;&#25171;&#20081;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24110;&#21161;&#20915;&#31574;&#32773;&#21644;&#20379;&#24212;&#38142;&#23454;&#36341;&#32773;&#20570;&#20986;&#36866;&#24403;&#30340;&#20915;&#31574;&#65292;&#20197;&#26368;&#23567;&#21270;&#25171;&#20081;&#20107;&#20214;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36817;&#26399;&#30340;&#25171;&#20081;&#20107;&#20214;&#65292;&#22914;COVID-19&#21644;&#20420;&#20044;&#20914;&#31361;&#65292;&#23545;&#20840;&#29699;&#20379;&#24212;&#38142;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#25968;&#23383;&#20379;&#24212;&#38142;&#21452;&#32990;&#32974;&#34987;&#25552;&#20986;&#65292;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24037;&#20855;&#26469;&#20943;&#36731;&#25171;&#20081;&#24433;&#21709;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35748;&#30693;&#25968;&#23383;&#20379;&#24212;&#38142;&#21452;&#32990;&#32974;&#26694;&#26550;&#20013;&#36827;&#34892;&#25171;&#20081;&#26816;&#27979;&#65292;&#20197;&#22686;&#24378;&#20379;&#24212;&#38142;&#30340;&#38887;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#25171;&#20081;&#26816;&#27979;&#27169;&#22359;&#21033;&#29992;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20102;&#38271;&#30701;&#26102;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#21463;&#21040;&#25171;&#20081;&#24433;&#21709;&#30340;&#38454;&#23618;&#24182;&#39044;&#27979;&#24674;&#22797;&#26102;&#38388;&#12290;&#32467;&#26524;&#65306;&#25152;&#25552;&#20986;&#26041;&#27861;&#33719;&#21462;&#30340;&#20449;&#24687;&#23558;&#24110;&#21161;&#20915;&#31574;&#32773;&#21644;&#20379;&#24212;&#38142;&#23454;&#36341;&#32773;&#26681;&#25454;&#23454;&#26102;&#30340;&#25171;&#20081;&#24773;&#20917;&#20570;&#20986;&#36866;&#24403;&#30340;&#20915;&#31574;&#65292;&#20197;&#26368;&#23567;&#21270;&#25171;&#20081;&#20107;&#20214;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Recent disruptive events, such as COVID-19 and Russia-Ukraine conflict, had a significant impact of global supply chains. Digital supply chain twins have been proposed in order to provide decision makers with an effective and efficient tool to mitigate disruption impact. Methods: This paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module utilises a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect. Results: The obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14552</link><description>&lt;p&gt;
&#31283;&#23450;&#25918;&#32622;&#30340;&#22806;&#37096;&#25509;&#35302;&#22359;&#30340;&#35302;&#35273;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#20316;&#25216;&#33021;&#26469;&#35828;&#65292;&#20934;&#30830;&#24863;&#30693;&#25509;&#35302;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#26426;&#22120;&#20154;&#35774;&#35745;&#21453;&#39304;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26426;&#22120;&#20154;&#24517;&#39035;&#23398;&#20064;&#23558;&#22797;&#26434;&#24418;&#29366;&#30340;&#29289;&#20307;&#22534;&#21472;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#38750;&#24120;&#36731;&#24494;&#30340;&#25509;&#35302;&#20132;&#20114;&#26469;&#25512;&#29702;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26681;&#25454;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#35835;&#25968;&#26469;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21147;&#21644;&#35302;&#35273;&#35266;&#27979;&#26469;&#20272;&#35745;&#25235;&#21462;&#29289;&#20307;&#21644;&#20854;&#29615;&#22659;&#20043;&#38388;&#30340;&#25509;&#35302;&#21306;&#22495;&#65292;&#20174;&#32780;&#20272;&#35745;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#25509;&#35302;&#21306;&#22495;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#37322;&#25918;&#25235;&#21462;&#21518;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#27454;&#38750;&#24120;&#27969;&#34892;&#30340;&#26827;&#30424;&#28216;&#25103;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#29289;&#20307;&#23545;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20809;&#32447;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#23567;&#21151;&#29575;&#25439;&#22833;&#30340;&#31363;&#21548;&#20107;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20809;&#24615;&#33021;&#30417;&#27979;&#25968;&#25454;&#21487;&#20197;&#26816;&#27979;&#36825;&#31181;&#24494;&#23567;&#30340;&#31363;&#21548;&#25439;&#22833;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23450;&#20301;&#36825;&#31867;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.14541</link><description>&lt;p&gt;
&#20809;&#38142;&#36335;&#20013;&#30340;&#31363;&#21548;&#35782;&#21035;&#21644;&#23450;&#20301;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cluster-based Method for Eavesdropping Identification and Localization in Optical Links. (arXiv:2309.14541v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20809;&#32447;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#23567;&#21151;&#29575;&#25439;&#22833;&#30340;&#31363;&#21548;&#20107;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20809;&#24615;&#33021;&#30417;&#27979;&#25968;&#25454;&#21487;&#20197;&#26816;&#27979;&#36825;&#31181;&#24494;&#23567;&#30340;&#31363;&#21548;&#25439;&#22833;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23450;&#20301;&#36825;&#31867;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#20301;&#20809;&#32447;&#31995;&#32479;&#20013;&#23567;&#21151;&#29575;&#25439;&#22833;&#30340;&#31363;&#21548;&#20107;&#20214;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#24615;&#33021;&#30417;&#27979;&#65288;OPM&#65289;&#25968;&#25454;&#20165;&#36890;&#36807;&#25509;&#25910;&#22120;&#25910;&#38598;&#23601;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#24494;&#23567;&#30340;&#31363;&#21548;&#25439;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#32447;OPM&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#23545;&#36825;&#31867;&#20107;&#20214;&#30340;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22278;&#29615;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#65292;&#32780;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#23545;&#20110;&#24052;&#22763;&#12289;&#27773;&#36710;&#21644;&#21345;&#36710;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20986;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.14540</link><description>&lt;p&gt;
&#22278;&#29615;&#35774;&#35745;&#23545;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning. (arXiv:2309.14540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22278;&#29615;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#65292;&#32780;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#23545;&#20110;&#24052;&#22763;&#12289;&#27773;&#36710;&#21644;&#21345;&#36710;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20986;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22278;&#29615;&#30340;&#24615;&#33021;&#24182;&#30740;&#31350;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#12289;&#23481;&#37327;&#21644;&#29615;&#22659;&#20248;&#21183;&#20197;&#21450;&#20026;&#36807;&#22659;&#21644;&#25972;&#21512;&#25552;&#20379;&#23433;&#20840;&#21644;&#27969;&#30021;&#36710;&#36742;&#27969;&#21160;&#65292;&#22278;&#29615;&#22312;&#22269;&#23478;&#38388;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#12289;&#20837;&#21475;&#36895;&#24230;&#20197;&#21450;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#65288;&#24052;&#22763;&#12289;&#27773;&#36710;&#12289;&#21345;&#36710;&#65289;&#39550;&#39542;&#21592;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#65292;&#24182;&#23558;&#20854;&#34892;&#20026;&#20998;&#20026;&#20445;&#23432;&#22411;&#12289;&#27491;&#24120;&#22411;&#21644;&#20405;&#30053;&#22411;&#12290;&#39044;&#27979;&#21644;&#35782;&#21035;&#39550;&#39542;&#21592;&#34892;&#20026;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22278;&#29615;&#23545;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to evaluate the performance of the rotors and study the behavior of the human driver in interacting with the rotors. In recent years, rotors have been increasingly used between countries due to their safety, capacity, and environmental advantages, and because they provide safe and fluid flows of vehicles for transit and integration. It turns out that roundabouts can significantly reduce speed at twisting intersections, entry speed and the resulting effect on speed depends on the rating of road users. In our research, (bus, car, truck) drivers were given special attention and their behavior was categorized into (conservative, normal, aggressive). Anticipating and recognizing driver behavior is an important challenge. Therefore, the aim of this research is to study the effect of roundabouts on these classifiers and to develop a method for predicting the behavior of road users at roundabout intersections. Safety is primarily due to two inherent features of the rotor. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14518</link><description>&lt;p&gt;
Detach-ROCKET: &#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22914;&#21307;&#23398;&#12289;&#37329;&#34701;&#12289;&#29615;&#22659;&#31185;&#23398;&#21644;&#21046;&#36896;&#19994;&#65292;&#21487;&#20197;&#23454;&#29616;&#30142;&#30149;&#35786;&#26029;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32929;&#20215;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;InceptionTime&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#38656;&#27714;&#30340;&#32321;&#37325;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;Rocket&#21450;&#20854;&#34893;&#29983;&#27169;&#22411;&#31561;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#38543;&#26426;&#29983;&#25104;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#31616;&#21270;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#36136;&#65292;&#29983;&#25104;&#30340;&#22823;&#37096;&#20998;&#29305;&#24449;&#26159;&#20887;&#20313;&#25110;&#38750;&#20449;&#24687;&#24615;&#30340;&#65292;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#36127;&#36733;&#24182;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#65288;SFD&#65289;&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#21644;&#20462;&#21098;&#36825;&#20123;&#38750;&#20027;&#35201;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;SFD&#21033;&#29992;&#27169;&#22411;&#31995;&#25968;&#26469;&#20272;&#35745;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is essential in many fields, such as medicine, finance, environmental science, and manufacturing, enabling tasks like disease diagnosis, anomaly detection, and stock price prediction. Machine learning models like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive training requirements. To address this, random convolutional kernel models such as Rocket and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14509</link><description>&lt;p&gt;
DeepSpeed Ulysses&#65306;&#29992;&#20110;&#35757;&#32451;&#26497;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#30340;&#31995;&#32479;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#25209;&#37327;&#22823;&#23567;&#12289;&#38544;&#34255;&#32500;&#24230;&#12289;&#23618;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#26469;&#25551;&#36848;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21152;&#36895;LLM&#35757;&#32451;&#30340;&#31995;&#32479;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21069;&#19977;&#20010;&#32500;&#24230;&#19978;&#65306;&#25209;&#37327;&#22823;&#23567;&#30340;&#25968;&#25454;&#24182;&#34892;&#21270;&#12289;&#38544;&#34255;&#23610;&#23544;&#30340;&#24352;&#37327;&#24182;&#34892;&#21270;&#20197;&#21450;&#27169;&#22411;&#28145;&#24230;&#25110;&#23618;&#25968;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#21270;&#12290;&#36825;&#20123;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#24182;&#34892;&#24418;&#24335;&#24182;&#19981;&#38024;&#23545;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#37492;&#20110;&#38271;&#24207;&#21015;LLM&#22312;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#19978;&#30340;&#37325;&#35201;&#24615;&#65292;&#24207;&#21015;&#24182;&#34892;&#21270;&#24341;&#36215;&#20102;&#37325;&#26032;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24207;&#21015;&#24182;&#34892;&#21270;&#24037;&#20316;&#21463;&#21040;&#20869;&#23384;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38271;&#24207;&#21015;&#22823;&#27169;&#22411;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#20415;&#25658;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26045;&#36317;&#31163;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#26679;&#26412;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#30456;&#20284;&#26102;&#26816;&#27979;&#24182;&#25552;&#20379;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;Spallation Neutron Source (SNS)&#21152;&#36895;&#22120;&#30340;&#38169;&#35823;&#26463;&#27969;&#39044;&#27979;&#65288;&#20998;&#31867;&#65289;&#21644;Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex&#65288;&#22238;&#24402;&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14502</link><description>&lt;p&gt;
&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Aware Deep Learning for Particle Accelerators. (arXiv:2309.14502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26045;&#36317;&#31163;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#26679;&#26412;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#30456;&#20284;&#26102;&#26816;&#27979;&#24182;&#25552;&#20379;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;Spallation Neutron Source (SNS)&#21152;&#36895;&#22120;&#30340;&#38169;&#35823;&#26463;&#27969;&#39044;&#27979;&#65288;&#20998;&#31867;&#65289;&#21644;Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex&#65288;&#22238;&#24402;&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#25429;&#25417;&#22797;&#26434;&#31995;&#32479;&#21160;&#24577;&#38750;&#24120;&#29702;&#24819;&#65292;&#20294;&#26159;&#24403;&#36755;&#20837;&#26679;&#26412;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#30456;&#20284;&#26102;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#21487;&#33021;&#20250;&#20219;&#24847;&#19981;&#20934;&#30830;&#12290;&#36890;&#36807;&#23454;&#29616;&#36317;&#31163;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#26816;&#27979;&#36825;&#20123;&#24773;&#20917;&#24182;&#25552;&#20379;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#20449;&#24515;&#27700;&#24179;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#36817;&#20284;&#65288;DGPA&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;Spallation Neutron Source (SNS)&#21152;&#36895;&#22120;&#30340;&#38169;&#35823;&#26463;&#27969;&#65288;&#20998;&#31867;&#65289;&#65292;&#24182;&#20026;Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex&#65288;&#22238;&#24402;&#65289;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard deep learning models for classification and regression applications are ideal for capturing complex system dynamics. However, their predictions can be arbitrarily inaccurate when the input samples are not similar to the training data. Implementation of distance aware uncertainty estimation can be used to detect these scenarios and provide a level of confidence associated with their predictions. In this paper, we present results from using Deep Gaussian Process Approximation (DGPA) methods for errant beam prediction at Spallation Neutron Source (SNS) accelerator (classification) and we provide an uncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex (regression).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;</title><link>http://arxiv.org/abs/2309.14495</link><description>&lt;p&gt;
&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#20998;&#31867;&#26631;&#35760;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Classifying token frequencies using angular Minkowski $p$-distance. (arXiv:2309.14495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14495
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#26159;&#19968;&#31181;&#20351;&#29992;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#26367;&#20195;&#20313;&#24358;&#30456;&#20284;&#24230;&#23450;&#20041;&#30340;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#20313;&#24358;&#30456;&#20284;&#24230;&#32463;&#24120;&#29992;&#20110;&#21253;&#21547;&#26631;&#35760;&#39057;&#29575;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#32780;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#33021;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;20-newsgroups&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#21152;&#26435;&#26368;&#36817;&#37051;&#21644;&#27169;&#31946;&#31895;&#31961;&#26368;&#36817;&#37051;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36229;&#21442;&#25968;$p$&#65292;&#25968;&#25454;&#38598;&#32500;&#25968;$m$&#65292;&#37051;&#23621;&#25968;&#37327;$k$&#65292;&#26435;&#37325;&#36873;&#25321;&#21644;&#20998;&#31867;&#22120;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#20351;&#29992;&#21512;&#36866;&#30340;$p$&#20540;&#65292;&#20351;&#29992;&#35282;&#24230;&#26126;&#21487;&#22827;&#26031;&#22522;$p$-&#36317;&#31163;&#21487;&#20197;&#33719;&#24471;&#27604;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24230;&#26356;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by replacing Euclidean distance in the definition of cosine dissimilarity with other Minkowski $p$-distances. Cosine dissimilarity is frequently used with datasets containing token frequencies, and angular Minkowski $p$-distance may potentially be an even better choice for certain tasks. In a case study based on the 20-newsgroups dataset, we evaluate clasification performance for classical weighted nearest neighbours, as well as fuzzy rough nearest neighbours. In addition, we analyse the relationship between the hyperparameter $p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the choice of weights and the choice of classifier. We conclude that it is possible to obtain substantially higher classification performance with angular Minkowski $p$-distance with suitable values for $p$ than with classical cosine dissimilarity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#32852;&#21512;NLU&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#20854;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#25193;&#23637;&#36866;&#29992;&#20110;&#20854;&#20182;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.14485</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#35821;&#38899;&#21161;&#25163;&#21450;&#20854;&#23427;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond. (arXiv:2309.14485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14485
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#32852;&#21512;NLU&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#20854;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#25193;&#23637;&#36866;&#29992;&#20110;&#20854;&#20182;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#22635;&#20805;&#65292;&#20063;&#31216;&#20026;&#32852;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65292;&#23545;&#20110;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#21508;&#31181;&#25216;&#26415;&#25552;&#39640;&#20934;&#30830;&#24615;&#19978;&#12290;&#21487;&#35299;&#37322;&#24615;&#26080;&#30097;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#32852;&#21512;NLU&#27169;&#22411;&#12290;&#22914;&#26524;&#27809;&#26377;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#23545;&#22806;&#30028;&#26469;&#35828;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#23481;&#26131;&#32570;&#20047;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#23436;&#25972;&#30340;&#32852;&#21512;NLU&#27169;&#22411;&#36716;&#21270;&#20026;&#22312;&#32454;&#31890;&#24230;&#19978;&#8220;&#20869;&#22312;&#22320;&#8221;&#21487;&#35299;&#37322;&#30340;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#30340;&#32852;&#21512;NLU&#27169;&#22411;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25193;&#23637;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#20854;&#20182;&#19968;&#33324;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be `inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36817;&#36793;&#21306;&#22495;&#22826;&#38451;&#32768;&#26001;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22522;&#20110;AlexNet&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#20339;&#39044;&#27979;&#25935;&#24863;&#24615;&#65292;&#21487;&#20026;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.14483</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36817;&#36793;&#21306;&#22495;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions. (arXiv:2309.14483v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36817;&#36793;&#21306;&#22495;&#22826;&#38451;&#32768;&#26001;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22522;&#20110;AlexNet&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#20339;&#39044;&#27979;&#25935;&#24863;&#24615;&#65292;&#21487;&#20026;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;24&#23567;&#26102;&#20869;$\geq$M&#32423;&#22826;&#38451;&#32768;&#26001;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#27599;&#23567;&#26102;&#37319;&#26679;&#30340;&#20840;&#29699;&#35270;&#32447;&#30913;&#22242;&#22270;&#20687;&#65292;&#22312;&#29305;&#21035;&#20851;&#27880;&#24120;&#34987;&#24573;&#35270;&#30340;&#20301;&#20110;&#36817;&#36793;&#21306;&#22495;&#65288;&#22826;&#38451;&#30424;&#30340;$\pm$70$^{\circ}$&#20043;&#22806;&#65289;&#23545;&#24212;&#30340;&#32768;&#26001;&#20107;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35757;&#32451;&#20102;&#19977;&#31181;&#30693;&#21517;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;--AlexNet&#12289;VGG16&#21644;ResNet34&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25216;&#33021;&#32479;&#35745;&#20540;&#65288;TSS&#65289;&#21644;Heidke&#25216;&#33021;&#35780;&#20998;&#65288;HSS&#65289;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35745;&#31639;&#20102;&#23545;X&#31867;&#21644;M&#31867;&#32768;&#26001;&#30340;&#20013;&#22830;&#21644;&#36817;&#36793;&#21306;&#22495;&#30340;&#39044;&#27979;&#25935;&#24863;&#24615;&#30340;&#21484;&#22238;&#29575;&#12290;&#20197;&#19979;&#26159;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#27010;&#25324;&#65306;&#65288;1&#65289;&#22522;&#20110;AlexNet&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24179;&#22343;TSS&#20026;0.53&#65292;&#24179;&#22343;HSS&#20026;0.37&#65307;&#65288;2&#65289;&#27492;&#22806;&#65292;&#21484;&#22238;&#29575;&#30340;&#31354;&#38388;&#20998;&#26512;&#25581;&#31034;&#65292;&#23545;&#20110;&#22826;&#38451;&#32768;&#26001;&#30340;&#20013;&#22830;&#21644;&#36817;&#36793;&#21306;&#22495;&#65292;AlexNet&#27169;&#22411;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#39044;&#27979;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to evaluate the performance of deep learning models in predicting $\geq$M-class solar flares with a prediction window of 24 hours, using hourly sampled full-disk line-of-sight (LoS) magnetogram images, particularly focusing on the often overlooked flare events corresponding to the near-limb regions (beyond $\pm$70$^{\circ}$ of the solar disk). We trained three well-known deep learning architectures--AlexNet, VGG16, and ResNet34 using transfer learning and compared and evaluated the overall performance of our models using true skill statistics (TSS) and Heidke skill score (HSS) and computed recall scores to understand the prediction sensitivity in central and near-limb regions for both X- and M-class flares. The following points summarize the key findings of our study: (1) The highest overall performance was observed with the AlexNet-based model, which achieved an average TSS$\sim$0.53 and HSS$\sim$0.37; (2) Further, a spatial analysis of recall scores disclosed that for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LogGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;LogGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.14482</link><description>&lt;p&gt;
LogGPT&#65306;&#36890;&#36807;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LogGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;LogGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26085;&#24535;&#25968;&#25454;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30830;&#20445;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#26085;&#24535;&#24207;&#21015;&#24314;&#27169;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#26102;&#24207;&#27169;&#22411;&#65288;&#22914;LSTM&#25110;Transformer&#65289;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26469;&#32534;&#30721;&#26085;&#24535;&#24207;&#21015;&#20013;&#30340;&#27491;&#24120;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#24314;&#27169;&#19982;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#65292;&#22240;&#20026;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#35757;&#32451;&#26102;&#24207;&#27169;&#22411;&#30340;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#19981;&#30452;&#25509;&#30456;&#20851;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;GPT&#36827;&#34892;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;LogGPT&#26681;&#25454;&#21069;&#24207;&#24207;&#21015;&#39044;&#27979;&#19979;&#19968;&#20010;&#26085;&#24535;&#26465;&#30446;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LogGPT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#19987;&#38376;&#20026;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#32452;&#20214;&#30340;&#28151;&#21512;&#31574;&#30053;&#24182;&#30001;&#20998;&#24320;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290; (&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#28151;&#21512;&#31574;&#30053;&#65292;&#20998;&#24320;&#30340;&#32593;&#32476;&#35780;&#20272;)</title><link>http://arxiv.org/abs/2309.14471</link><description>&lt;p&gt;
&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36866;&#24212;&#21452;Q-Learning
&lt;/p&gt;
&lt;p&gt;
Adapting Double Q-Learning for Continuous Reinforcement Learning. (arXiv:2309.14471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#32452;&#20214;&#30340;&#28151;&#21512;&#31574;&#30053;&#24182;&#30001;&#20998;&#24320;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290; (&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#28151;&#21512;&#31574;&#30053;&#65292;&#20998;&#24320;&#30340;&#32593;&#32476;&#35780;&#20272;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#25511;&#21046;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#30340;&#26159;&#36807;&#39640;&#20272;&#35745;&#30340;&#32467;&#26524;&#65292;&#32780;&#38750;&#20854;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#21452;Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#30001;&#20004;&#20010;&#32452;&#25104;&#25104;&#20998;&#26500;&#25104;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#27599;&#20010;&#31574;&#30053;&#25104;&#20998;&#30001;&#20998;&#21035;&#26368;&#22823;&#21270;&#21644;&#35780;&#20272;&#30340;&#32593;&#32476;&#22788;&#29702;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.
&lt;/p&gt;</description></item><item><title>FARSEC&#26159;&#19968;&#31181;&#21487;&#37325;&#29616;&#30340;&#22522;&#20110;&#20132;&#36890;&#25668;&#20687;&#22836;&#30340;&#33258;&#21160;&#23454;&#26102;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#36947;&#36335;&#27573;&#38271;&#24230;&#30340;&#26032;&#39062;&#25216;&#26415;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14468</link><description>&lt;p&gt;
FARSEC:&#19968;&#31181;&#21487;&#37325;&#29616;&#30340;&#22522;&#20110;&#20132;&#36890;&#25668;&#20687;&#22836;&#30340;&#33258;&#21160;&#23454;&#26102;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras. (arXiv:2309.14468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14468
&lt;/p&gt;
&lt;p&gt;
FARSEC&#26159;&#19968;&#31181;&#21487;&#37325;&#29616;&#30340;&#22522;&#20110;&#20132;&#36890;&#25668;&#20687;&#22836;&#30340;&#33258;&#21160;&#23454;&#26102;&#36710;&#36742;&#36895;&#24230;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#36947;&#36335;&#27573;&#38271;&#24230;&#30340;&#26032;&#39062;&#25216;&#26415;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20132;&#36890;&#25668;&#20687;&#22836;&#20272;&#35745;&#36710;&#36742;&#36895;&#24230;&#26159;&#20132;&#36890;&#30417;&#25511;&#21644;&#31649;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20248;&#21270;&#30340;&#20132;&#36890;&#27969;&#37327;&#12289;&#25913;&#21892;&#36947;&#36335;&#23433;&#20840;&#21644;&#38477;&#20302;&#29615;&#22659;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#36825;&#20010;&#39046;&#22495;&#25253;&#21578;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#20294;&#20854;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#32570;&#20047;&#21487;&#37325;&#29616;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#23454;&#26102;&#36710;&#36742;&#36895;&#24230;&#35745;&#31639;&#65292;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#20844;&#24320;&#20132;&#36890;&#25668;&#20687;&#22836;&#30340;&#26356;&#22810;&#31181;&#31867;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#22270;&#39044;&#27979;&#26469;&#20272;&#35745;&#36947;&#36335;&#27573;&#30340;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#22788;&#29702;&#23454;&#38469;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#26426;&#36816;&#21160;&#21644;&#19981;&#21516;&#30340;&#35270;&#39057;&#27969;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#19977;&#20010;&#30693;&#21517;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the speed of vehicles using traffic cameras is a crucial task for traffic surveillance and management, enabling more optimal traffic flow, improved road safety, and lower environmental impact. Transportation-dependent systems, such as for navigation and logistics, have great potential to benefit from reliable speed estimation. While there is prior research in this area reporting competitive accuracy levels, their solutions lack reproducibility and robustness across different datasets. To address this, we provide a novel framework for automatic real-time vehicle speed calculation, which copes with more diverse data from publicly available traffic cameras to achieve greater robustness. Our model employs novel techniques to estimate the length of road segments via depth map prediction. Additionally, our framework is capable of handling realistic conditions such as camera movements and different video stream inputs automatically. We compare our model to three well-known models i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DefGoalNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#30452;&#25509;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.14463</link><description>&lt;p&gt;
DefGoalNet&#65306;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#26102;&#30340;&#19978;&#19979;&#25991;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation. (arXiv:2309.14463v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DefGoalNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#30452;&#25509;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#20282;&#26381;&#26159;&#19968;&#31181;&#25511;&#21046;&#29289;&#20307;&#21040;&#36798;&#39044;&#26399;&#30446;&#26631;&#24418;&#29366;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#23545;&#20110;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25805;&#20316;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#24418;&#29366;&#30340;&#35268;&#23450;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#30446;&#26631;&#24418;&#29366;&#36890;&#24120;&#36890;&#36807;&#32321;&#29712;&#30340;&#39046;&#22495;&#30693;&#35782;&#24037;&#31243;&#36807;&#31243;&#33719;&#21462;&#65292;&#25110;&#32773;&#36890;&#36807;&#25163;&#21160;&#25805;&#20316;&#29289;&#20307;&#21040;&#36798;&#25152;&#38656;&#24418;&#29366;&#24182;&#25429;&#33719;&#35813;&#29305;&#23450;&#26102;&#21051;&#30340;&#30446;&#26631;&#24418;&#29366;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;DefGoalNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#20174;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25163;&#26415;&#25764;&#36864;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;10&#20010;&#31034;&#33539;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#20013;&#20540;&#20063;&#25509;&#36817;90%&#12290;&#36825;&#20123;&#32467;&#26524;&#26631;&#24535;&#30528;&#22312;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20013;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method's effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#28369;&#38634;&#36339;&#21488;&#36816;&#21160;&#20013;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#36827;&#34892;&#24615;&#33021;&#20998;&#26512;&#21644;&#29983;&#29289;&#21453;&#39304;&#12290;&#36890;&#36807;&#27979;&#37327;&#28369;&#38634;&#38772;&#38795;&#22443;&#19978;&#30340;&#33050;&#21387;&#21147;&#65292;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#32473;&#25945;&#32451;&#25913;&#21892;&#21453;&#39304;&#21644;&#36816;&#21160;&#21592;&#21363;&#26102;&#30340;&#34892;&#21160;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2309.14455</link><description>&lt;p&gt;
Skilog&#65306;&#28369;&#38634;&#36339;&#21488;&#36816;&#21160;&#20013;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#29992;&#20110;&#24615;&#33021;&#20998;&#26512;&#21644;&#29983;&#29289;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping. (arXiv:2309.14455v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#28369;&#38634;&#36339;&#21488;&#36816;&#21160;&#20013;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#36827;&#34892;&#24615;&#33021;&#20998;&#26512;&#21644;&#29983;&#29289;&#21453;&#39304;&#12290;&#36890;&#36807;&#27979;&#37327;&#28369;&#38634;&#38772;&#38795;&#22443;&#19978;&#30340;&#33050;&#21387;&#21147;&#65292;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#32473;&#25945;&#32451;&#25913;&#21892;&#21453;&#39304;&#21644;&#36816;&#21160;&#21592;&#21363;&#26102;&#30340;&#34892;&#21160;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28369;&#38634;&#36339;&#21488;&#36816;&#21160;&#20013;&#65292;&#36339;&#36291;&#30340;&#37325;&#22797;&#29575;&#20302;&#38480;&#21046;&#20102;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#27599;&#27425;&#21333;&#29420;&#36339;&#36291;&#30340;&#23398;&#20064;&#29575;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#36816;&#21160;&#21592;&#35757;&#32451;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#36816;&#21160;&#23398;&#20064;&#65292;&#30740;&#31350;&#34920;&#26126;&#21453;&#39304;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#23398;&#20064;&#12290;&#29305;&#21035;&#26159;&#22312;&#36895;&#24230;&#19978;&#22369;&#26102;&#65292;&#23545;&#37325;&#24515;&#30340;&#31934;&#32454;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#26159;&#22240;&#20026;&#23454;&#38469;&#36215;&#36339;&#21457;&#29983;&#22312;&#30504;&#30524;&#38388;&#65288;&#32422;300&#27627;&#31186;&#65289;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#22369;&#38454;&#27573;&#20219;&#20309;&#19981;&#24179;&#34913;&#30340;&#36523;&#20307;&#23039;&#21183;&#37117;&#20250;&#24433;&#21709;&#39134;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#12289;&#32039;&#20945;&#12289;&#33410;&#33021;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#31995;&#32479;&#65292;&#29992;&#20110;&#28369;&#38634;&#36339;&#21488;&#36816;&#21160;&#20013;&#30340;&#23454;&#26102;&#24615;&#33021;&#20998;&#26512;&#21644;&#29983;&#29289;&#21453;&#39304;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#22312;&#28369;&#38634;&#38772;&#38795;&#22443;&#19978;&#30340;&#19977;&#20010;&#19981;&#21516;&#28857;&#27979;&#37327;&#33050;&#21387;&#21147;&#65292;&#20197;100Hz&#30340;&#39057;&#29575;&#36827;&#34892;&#25805;&#20316;&#12290;&#33050;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#30452;&#25509;&#21457;&#36865;&#32473;&#25945;&#32451;&#20197;&#25913;&#21892;&#21453;&#39304;&#65292;&#25110;&#32773;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#28369;&#38634;&#38772;&#20013;&#30340;&#25391;&#21160;&#39532;&#36798;&#32473;&#36816;&#21160;&#21592;&#25552;&#20379;&#21363;&#26102;&#30340;&#34892;&#21160;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ski jumping, low repetition rates of jumps limit the effectiveness of training. Thus, increasing learning rate within every single jump is key to success. A critical element of athlete training is motor learning, which has been shown to be accelerated by feedback methods. In particular, a fine-grained control of the center of gravity in the in-run is essential. This is because the actual takeoff occurs within a blink of an eye ($\sim$300ms), thus any unbalanced body posture during the in-run will affect flight. This paper presents a smart, compact, and energy-efficient wireless sensor system for real-time performance analysis and biofeedback during ski jumping. The system operates by gauging foot pressures at three distinct points on the insoles of the ski boot at 100Hz. Foot pressure data can either be directly sent to coaches to improve their feedback, or fed into a ML model to give athletes instantaneous in-action feedback using a vibration motor in the ski boot. In the biofeedba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#20301;&#38169;&#21160;&#21147;&#23398;&#30340;&#36801;&#31227;&#35268;&#24459;&#12290;&#36890;&#36807;&#23558;&#36801;&#31227;&#35268;&#24459;&#24314;&#27169;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;BCC&#38056;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31163;&#25955;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#37325;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#25289;&#20280;/&#21387;&#32553;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.14450</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#20301;&#38169;&#21160;&#21147;&#23398;&#30340;&#36801;&#31227;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Learning dislocation dynamics mobility laws from large-scale MD simulations. (arXiv:2309.14450v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#20301;&#38169;&#21160;&#21147;&#23398;&#30340;&#36801;&#31227;&#35268;&#24459;&#12290;&#36890;&#36807;&#23558;&#36801;&#31227;&#35268;&#24459;&#24314;&#27169;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;BCC&#38056;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31163;&#25955;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#37325;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#25289;&#20280;/&#21387;&#32553;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20301;&#38169;&#21160;&#21147;&#23398;&#65288;DDD&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#34987;&#29992;&#20316;&#30495;&#27491;&#30340;&#21407;&#23376;&#23610;&#24230;&#26230;&#26684;&#20301;&#38169;&#21160;&#21147;&#23398;&#30340;&#31895;&#31890;&#21270;&#27169;&#22411;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30001;&#20301;&#38169;&#30340;&#38598;&#20307;&#34892;&#20026;&#24341;&#36215;&#30340;&#37329;&#23646;&#22609;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20316;&#20026;&#19968;&#31181;&#20171;&#35266;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;DDD&#27169;&#22411;&#20013;&#20301;&#38169;&#30340;&#36816;&#21160;&#36890;&#36807;&#36801;&#31227;&#35268;&#24459;&#26469;&#39044;&#23450;&#65307;&#36801;&#31227;&#35268;&#24459;&#26159;&#25351;&#20301;&#38169;&#32447;&#24212;&#22914;&#20309;&#21709;&#24212;&#39537;&#21160;&#21147;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#36801;&#31227;&#35268;&#24459;&#30340;&#24320;&#21457;&#21487;&#33021;&#26159;&#19968;&#20010;&#40635;&#28902;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#28041;&#21450;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#30340;&#31616;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#20197;&#31616;&#21270;&#25968;&#25454;&#39537;&#21160;&#30340;&#36801;&#31227;&#35268;&#24459;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#35268;&#24459;&#34987;&#27169;&#25311;&#20026;&#22312;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#30340;&#26230;&#20307;&#22609;&#24615;&#19978;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#20197;BCC&#38056;&#20026;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;DDS&#27169;&#25311;&#20013;&#23454;&#29616;&#30340;GNN&#36801;&#31227;&#35268;&#24459;&#33021;&#22815;&#20934;&#30830;&#22320;&#37325;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25289;&#20280;/&#21387;&#32553;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational method of discrete dislocation dynamics (DDD), used as a coarse-grained model of true atomistic dynamics of lattice dislocations, has become of powerful tool to study metal plasticity arising from the collective behavior of dislocations. As a mesoscale approach, motion of dislocations in the DDD model is prescribed via the mobility law; a function which specifies how dislocation lines should respond to the driving force. However, the development of traditional hand-crafted mobility laws can be a cumbersome task and may involve detrimental simplifications. Here we introduce a machine-learning (ML) framework to streamline the development of data-driven mobility laws which are modeled as graph neural networks (GNN) trained on large-scale Molecular Dynamics (MD) simulations of crystal plasticity. We illustrate our approach on BCC tungsten and demonstrate that our GNN mobility implemented in large-scale DDD simulations accurately reproduces the challenging tension/compress
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.14425</link><description>&lt;p&gt;
&#33258;&#24674;&#22797;&#25552;&#31034;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#33258;&#24674;&#22797;&#30340;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#65288;GPSR&#65289;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#39640;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#31995;&#32479;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#26412;&#25991;&#39318;&#20808;&#22522;&#20110;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#39030;&#23618;GPSR&#31995;&#32479;&#65292;&#29992;&#20110;&#20840;&#29699;&#31454;&#36187;&#65288;RoboCup@Home 2023&#65289;&#12290;&#35813;&#31995;&#32479;&#26082;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#21464;&#21270;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#27599;&#20010;&#27169;&#22411;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512;&#25152;&#24320;&#21457;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;GPSR&#24212;&#29992;&#35774;&#32622;&#20013;&#23384;&#22312;&#19977;&#31181;&#22833;&#36133;&#31867;&#22411;&#65306;&#20449;&#24687;&#19981;&#36275;&#12289;&#38169;&#35823;&#30340;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24674;&#22797;&#25552;&#31034;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#25506;&#32034;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#24182;&#20462;&#25913;&#20854;&#25552;&#31034;&#26469;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#20855;&#26377;&#33258;&#24674;&#22797;&#26426;&#21046;&#30340;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#22833;&#36133;&#26696;&#20363;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#20379;&#34917;&#20805;&#30340;&#35270;&#39057;&#21487;&#22312;https://sites.google.com/view/srgpsr&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26680;&#26041;&#27861;&#26159;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#20219;&#20309;&#26680;&#20989;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;</title><link>http://arxiv.org/abs/2309.14419</link><description>&lt;p&gt;
&#20851;&#20110;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the expressivity of embedding quantum kernels. (arXiv:2309.14419v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14419
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#26159;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#36890;&#36807;&#24341;&#20837;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#20219;&#20309;&#26680;&#20989;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#65292;&#37327;&#23376;&#26680;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26368;&#33258;&#28982;&#30340;&#32852;&#31995;&#12290;&#26680;&#26041;&#27861;&#20381;&#36182;&#20110;&#20869;&#31215;&#29305;&#24449;&#21521;&#37327;&#65292;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#23384;&#22312;&#20110;&#22823;&#22411;&#29305;&#24449;&#31354;&#38388;&#20013;&#12290;&#37327;&#23376;&#26680;&#36890;&#24120;&#36890;&#36807;&#26174;&#24335;&#26500;&#36896;&#37327;&#23376;&#29305;&#24449;&#24577;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#20869;&#31215;&#26469;&#35780;&#20272;&#65292;&#36825;&#37324;&#31216;&#20026;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;&#30001;&#20110;&#32463;&#20856;&#26680;&#36890;&#24120;&#22312;&#19981;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#30340;&#34920;&#36798;&#33021;&#21147;&#22914;&#20309;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#26159;&#21542;&#25152;&#26377;&#30340;&#37327;&#23376;&#26680;&#37117;&#21487;&#20197;&#34920;&#36798;&#20026;&#37327;&#23376;&#29305;&#24449;&#24577;&#30340;&#20869;&#31215;&#65311;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#26159;&#32943;&#23450;&#30340;&#65306;&#36890;&#36807;&#35843;&#29992;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#20219;&#20309;&#26680;&#20989;&#25968;&#65292;&#24635;&#26159;&#23384;&#22312;&#23545;&#24212;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#21644;&#23884;&#20837;&#24335;&#37327;&#23376;&#26680;&#12290;&#28982;&#32780;&#65292;&#38382;&#39064;&#26356;&#20851;&#27880;&#30340;&#26159;&#26377;&#25928;&#30340;&#26500;&#36896;&#26041;&#24335;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;
&lt;/p&gt;
&lt;p&gt;
One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#37327;&#23376;&#23398;&#20064;&#22120;&#21644;&#22522;&#20110;Grover&#31639;&#27861;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#27169;&#24335;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#21487;&#34892;&#30340;&#20248;&#21183;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#20998;&#31867;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14406</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#37327;&#23376;&#23398;&#20064;&#22120;&#21644;&#22522;&#20110;Grover&#31639;&#27861;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover's algorithm. (arXiv:2309.14406v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#37327;&#23376;&#23398;&#20064;&#22120;&#21644;&#22522;&#20110;Grover&#31639;&#27861;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#27169;&#24335;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#21487;&#34892;&#30340;&#20248;&#21183;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#20998;&#31867;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27491;&#22312;&#21162;&#21147;&#23547;&#25214;&#37327;&#23376;&#23398;&#20064;&#38382;&#39064;&#30340;&#37327;&#23376;&#21152;&#36895;&#12290;&#26368;&#36817;&#65292;&#21016;&#36920;&#31561;&#20154;&#22312;&#33258;&#28982;&#29289;&#29702;&#26434;&#24535;&#19978;&#35777;&#26126;&#20102;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#21033;&#29992;Shor&#31639;&#27861;&#30340;&#21152;&#36895;&#21487;&#20197;&#23454;&#29616;&#25351;&#25968;&#32423;&#21152;&#36895;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#25193;&#23637;&#65292;&#21033;&#29992;Grover&#31639;&#27861;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#26680;&#20013;&#23454;&#29616;&#21152;&#36895;&#12290;&#20026;&#20102;&#23637;&#31034;&#26680;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#19982;&#27169;&#24335;&#21305;&#37197;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#21487;&#34892;&#19988;&#21487;&#35777;&#26126;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#32467;&#21512;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#20998;&#31867;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an ongoing effort to find quantum speedups for learning problems. Recently, [Y. Liu et al., Nat. Phys. $\textbf{17}$, 1013--1017 (2021)] have proven an exponential speedup for quantum support vector machines by leveraging the speedup of Shor's algorithm. We expand upon this result and identify a speedup utilizing Grover's algorithm in the kernel of a support vector machine. To show the practicality of the kernel structure we apply it to a problem related to pattern matching, providing a practical yet provable advantage. Moreover, we show that combining quantum computation in a preprocessing step with classical methods for classification further improves classifier performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pLMFPPred&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;&#25216;&#26415;&#65292;&#20934;&#30830;&#39044;&#27979;&#21151;&#33021;&#32957;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pLMFPPred&#22312;&#39044;&#27979;&#21151;&#33021;&#32957;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14404</link><description>&lt;p&gt;
pLMFPPred: &#19968;&#31181;&#20934;&#30830;&#39044;&#27979;&#21151;&#33021;&#32957;&#30340;&#26032;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning. (arXiv:2309.14404v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pLMFPPred&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;&#25216;&#26415;&#65292;&#20934;&#30830;&#39044;&#27979;&#21151;&#33021;&#32957;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pLMFPPred&#22312;&#39044;&#27979;&#21151;&#33021;&#32957;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#32957;&#20855;&#26377;&#27835;&#30103;&#21508;&#31181;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;&#23427;&#20204;&#20855;&#26377;&#33391;&#22909;&#30340;&#27835;&#30103;&#25928;&#26524;&#21644;&#20302;&#27602;&#24615;&#65292;&#26159;&#29702;&#24819;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#20174;&#34507;&#30333;&#36136;&#24207;&#21015;&#38598;&#21512;&#20013;&#24555;&#36895;&#35782;&#21035;&#26032;&#30340;&#21151;&#33021;&#32957;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#19981;&#21516;&#21151;&#33021;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#65288;ESM-2&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;pLMFPPred&#65288;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#32957;&#39044;&#27979;&#24037;&#20855;&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#21151;&#33021;&#32957;&#24182;&#35782;&#21035;&#26377;&#27602;&#32957;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SMOTE-TOMEK&#25968;&#25454;&#21512;&#25104;&#37319;&#26679;&#21644;Shapley&#20540;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#12290;&#22312;&#32463;&#36807;&#39564;&#35777;&#30340;&#29420;&#31435;&#27979;&#35797;&#38598;&#19978;&#65292;pLMFPPred&#30340;&#20934;&#30830;&#29575;&#12289;&#25509;&#21463;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#21644;F1&#24471;&#20998;&#20998;&#21035;&#36798;&#21040;0.974&#12289;0.99&#21644;0.974&#12290;&#23545;&#27604;&#23454;&#39564;&#34920;&#26126;&#65292;pLMFPPred&#22312;&#39044;&#27979;&#21151;&#33021;&#32957;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional peptides have the potential to treat a variety of diseases. Their good therapeutic efficacy and low toxicity make them ideal therapeutic agents. Artificial intelligence-based computational strategies can help quickly identify new functional peptides from collections of protein sequences and discover their different functions.Using protein language model-based embeddings (ESM-2), we developed a tool called pLMFPPred (Protein Language Model-based Functional Peptide Predictor) for predicting functional peptides and identifying toxic peptides. We also introduced SMOTE-TOMEK data synthesis sampling and Shapley value-based feature selection techniques to relieve data imbalance issues and reduce computational costs. On a validated independent test set, pLMFPPred achieved accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score values of 0.974, 0.99, and 0.974, respectively. Comparative experiments show that pLMFPPred outperforms current methods for predicti
&lt;/p&gt;</description></item><item><title>DECORAIT&#26159;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#27880;&#20876;&#34920;&#65292;&#20026;&#20869;&#23481;&#21019;&#20316;&#32773;&#25552;&#20379;&#36873;&#25321;&#21152;&#20837;&#25110;&#36864;&#20986;AI&#35757;&#32451;&#30340;&#26435;&#21033;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20998;&#23618;&#32858;&#31867;&#21644;on/off-chain&#23384;&#20648;&#30340;&#26041;&#24335;&#26469;&#36861;&#36394;GenAI&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#65292;&#20197;&#30830;&#23450;&#35757;&#32451;&#21516;&#24847;&#24182;&#22870;&#21169;&#36129;&#29486;&#35813;&#25968;&#25454;&#30340;&#21019;&#24847;&#20154;&#12290;</title><link>http://arxiv.org/abs/2309.14400</link><description>&lt;p&gt;
DECORAIT - &#29992;&#20110;AI&#35757;&#32451;&#30340;&#21435;&#20013;&#24515;&#21270;&#36873;&#25321;&#21152;&#20837;/&#36864;&#20986;&#27880;&#20876;&#34920;
&lt;/p&gt;
&lt;p&gt;
DECORAIT -- DECentralized Opt-in/out Registry for AI Training. (arXiv:2309.14400v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14400
&lt;/p&gt;
&lt;p&gt;
DECORAIT&#26159;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#27880;&#20876;&#34920;&#65292;&#20026;&#20869;&#23481;&#21019;&#20316;&#32773;&#25552;&#20379;&#36873;&#25321;&#21152;&#20837;&#25110;&#36864;&#20986;AI&#35757;&#32451;&#30340;&#26435;&#21033;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20998;&#23618;&#32858;&#31867;&#21644;on/off-chain&#23384;&#20648;&#30340;&#26041;&#24335;&#26469;&#36861;&#36394;GenAI&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#65292;&#20197;&#30830;&#23450;&#35757;&#32451;&#21516;&#24847;&#24182;&#22870;&#21169;&#36129;&#29486;&#35813;&#25968;&#25454;&#30340;&#21019;&#24847;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;DECORAIT&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#27880;&#20876;&#34920;&#65292;&#36890;&#36807;&#35813;&#27880;&#20876;&#34920;&#65292;&#20869;&#23481;&#21019;&#20316;&#32773;&#21487;&#20197;&#34920;&#26126;&#20182;&#20204;&#36873;&#25321;&#21152;&#20837;&#25110;&#36864;&#20986;AI&#35757;&#32451;&#30340;&#26435;&#21033;&#65292;&#24182;&#20026;&#20182;&#20204;&#30340;&#36129;&#29486;&#33719;&#24471;&#22870;&#21169;&#12290;&#29983;&#25104;&#24335;AI&#65288;GenAI&#65289;&#20351;&#29992;&#22312;&#20174;&#20844;&#20849;&#26469;&#28304;&#20013;&#25235;&#21462;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#21512;&#25104;&#22270;&#20687;&#12290;&#36825;&#23545;&#20110;&#24076;&#26395;&#20844;&#24320;&#20998;&#20139;&#20854;&#20316;&#21697;&#32780;&#19981;&#35748;&#21487;&#20854;&#29992;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#20869;&#23481;&#21019;&#20316;&#32773;&#26469;&#35828;&#65292;&#26159;&#19968;&#20010;&#25968;&#25454;&#27835;&#29702;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;GenAI&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#23545;&#20110;&#21019;&#24847;&#20154;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#65292;&#20197;&#30830;&#20445;&#23545;&#20182;&#20204;&#30340;&#20351;&#29992;&#24471;&#21040;&#20844;&#24179;&#30340;&#35748;&#21487;&#21644;&#22870;&#21169;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;DECORAIT&#30340;&#21407;&#22411;&#65292;&#35813;&#21407;&#22411;&#36890;&#36807;&#25506;&#32034;&#20998;&#23618;&#32858;&#31867;&#21644;on/off-chain&#23384;&#20648;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21435;&#20013;&#24515;&#21270;&#27880;&#20876;&#34920;&#65292;&#20197;&#36861;&#36394;GenAI&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#65292;&#20197;&#30830;&#23450;&#35757;&#32451;&#21516;&#24847;&#24182;&#22870;&#21169;&#36129;&#29486;&#35813;&#25968;&#25454;&#30340;&#21019;&#24847;&#20154;&#12290;DECORAIT&#23558;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;&#65288;DLT&#65289;&#19982;&#35270;&#35273;&#25351;&#32441;&#35782;&#21035;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DECORAIT; a decentralized registry through which content creators may assert their right to opt in or out of AI training as well as receive reward for their contributions. Generative AI (GenAI) enables images to be synthesized using AI models trained on vast amounts of data scraped from public sources. Model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. Further, establishing the provenance of GenAI training data is important to creatives to ensure fair recognition and reward for their such use. We report a prototype of DECORAIT, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of GenAI training data in order to determine training consent and reward creatives who contribute that data. DECORAIT combines distributed ledger technology (DLT) with visual fingerprinting, lever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>&#20083;&#33146;&#30284;&#30340;&#21457;&#23637;&#19981;&#20165;&#19982;&#36951;&#20256;&#22240;&#32032;&#26377;&#20851;&#65292;&#29615;&#22659;&#22240;&#32032;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#24433;&#21709;&#20083;&#33146;&#30284;&#39118;&#38505;&#12289;&#21457;&#30149;&#29575;&#21644;&#32467;&#23616;&#30340;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#65292;&#21253;&#25324;&#29983;&#27963;&#26041;&#24335;&#20915;&#31574;&#21644;&#29615;&#22659;&#27745;&#26579;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.14397</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#29615;&#22659;&#23545;&#20083;&#33146;&#30284;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Predicting environment effects on breast cancer by implementing machine learning. (arXiv:2309.14397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14397
&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#30340;&#21457;&#23637;&#19981;&#20165;&#19982;&#36951;&#20256;&#22240;&#32032;&#26377;&#20851;&#65292;&#29615;&#22659;&#22240;&#32032;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#24433;&#21709;&#20083;&#33146;&#30284;&#39118;&#38505;&#12289;&#21457;&#30149;&#29575;&#21644;&#32467;&#23616;&#30340;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#65292;&#21253;&#25324;&#29983;&#27963;&#26041;&#24335;&#20915;&#31574;&#21644;&#29615;&#22659;&#27745;&#26579;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#36880;&#28176;&#25104;&#20026;&#36896;&#25104;&#22899;&#24615;&#27515;&#20129;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36229;&#36807;&#20102;&#24515;&#33039;&#30149;&#12290;&#34429;&#28982;&#36951;&#20256;&#22240;&#32032;&#22312;&#20083;&#33146;&#30284;&#30340;&#21457;&#23637;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#29615;&#22659;&#22240;&#32032;&#22312;&#20854;&#21457;&#29983;&#21644;&#21457;&#23637;&#20013;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#23545;&#21487;&#33021;&#24433;&#21709;&#20083;&#33146;&#30284;&#39118;&#38505;&#12289;&#21457;&#30149;&#29575;&#21644;&#32467;&#23616;&#30340;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#30740;&#31350;&#20174;&#29983;&#27963;&#26041;&#24335;&#20915;&#31574;&#24320;&#22987;&#65292;&#22914;&#39278;&#39135;&#20064;&#24815;&#12289;&#36816;&#21160;&#20064;&#24815;&#21644;&#39278;&#37202;&#65292;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#28608;&#32032;&#22833;&#34913;&#21644;&#28814;&#30151;&#65292;&#36825;&#26159;&#20083;&#33146;&#30284;&#21457;&#23637;&#30340;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20892;&#33647;&#12289;&#20869;&#20998;&#27852;&#24178;&#25200;&#29289;&#65288;EDCs&#65289;&#21644;&#24037;&#19994;&#24223;&#27668;&#31561;&#29615;&#22659;&#27745;&#26579;&#29289;&#22312;&#20083;&#33146;&#30284;&#21457;&#23637;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#36825;&#20123;&#29289;&#36136;&#36890;&#36807;&#24178;&#25200;&#28608;&#32032;&#20449;&#21495;&#20256;&#23548;&#21644;DNA&#25439;&#20260;&#32780;&#19982;&#20083;&#33146;&#30284;&#39118;&#38505;&#22686;&#21152;&#30456;&#20851;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#29615;&#22659;&#23545;&#20083;&#33146;&#30284;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biggest Breast cancer is increasingly a major factor in female fatalities, overtaking heart disease. While genetic factors are important in the growth of breast cancer, new research indicates that environmental factors also play a substantial role in its occurrence and progression. The literature on the various environmental factors that may affect breast cancer risk, incidence, and outcomes is thoroughly reviewed in this study report. The study starts by looking at how lifestyle decisions, such as eating habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two important factors driving the development of breast cancer. Additionally, it explores the part played by environmental contaminants such pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. Algorithms for machine lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32763;&#35793;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#65292;&#20197;&#32553;&#30701;&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#30340;&#26102;&#38388;&#21644;&#24037;&#31243;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.14396</link><description>&lt;p&gt;
&#29468;&#27979;&#19982;&#32472;&#22270;&#65306;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;
&lt;/p&gt;
&lt;p&gt;
Guess &amp; Sketch: Language Model Guided Transpilation. (arXiv:2309.14396v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#36716;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32763;&#35793;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#65292;&#20197;&#32553;&#30701;&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#30340;&#26102;&#38388;&#21644;&#24037;&#31243;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25252;&#36951;&#30041;&#36719;&#20214;&#38656;&#35201;&#22823;&#37327;&#30340;&#36719;&#20214;&#21644;&#31995;&#32479;&#24037;&#31243;&#26102;&#38388;&#12290;&#27719;&#32534;&#20195;&#30721;&#31243;&#24207;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#29305;&#21035;&#38590;&#20197;&#20998;&#26512;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#35745;&#31639;&#26426;&#26426;&#22120;&#29366;&#24577;&#38656;&#35201;&#20302;&#32423;&#21035;&#30340;&#25511;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#21464;&#37327;&#21517;&#31216;&#12290;&#29616;&#26377;&#30340;&#20256;&#32479;&#31243;&#24207;&#36716;&#25442;&#22120;&#20445;&#35777;&#27491;&#30830;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#25163;&#24037;&#24037;&#31243;&#35774;&#35745;&#30340;&#12290;&#23398;&#20064;&#24335;&#36716;&#35793;&#65292;&#21363;&#20195;&#30721;&#30340;&#33258;&#21160;&#32763;&#35793;&#65292;&#25552;&#20379;&#20102;&#25163;&#21160;&#37325;&#20889;&#21644;&#24037;&#31243;&#21162;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#33258;&#21160;&#21270;&#30340;&#31526;&#21495;&#31243;&#24207;&#36716;&#25442;&#26041;&#27861;&#20445;&#35777;&#20102;&#27491;&#30830;&#24615;&#65292;&#20294;&#26159;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24456;&#38590;&#25193;&#23637;&#21040;&#36739;&#38271;&#30340;&#31243;&#24207;&#12290;&#23427;&#20204;&#21018;&#24615;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#21482;&#33021;&#25512;&#29702;&#20986;&#19968;&#23567;&#37096;&#20998;&#31243;&#24207;&#31354;&#38388;&#12290;&#27010;&#29575;&#24615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#65292;&#20294;&#26159;&#20197;&#30830;&#20445;&#27491;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#20102;LMs&#21644;&#31526;&#21495;&#21270;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#20844;&#36335;&#19978;&#31361;&#21457;&#36335;&#38556;&#24773;&#20917;&#19979;&#26234;&#33021;&#36710;&#36742;&#30340;&#34892;&#36710;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14395</link><description>&lt;p&gt;
&#38544;&#24615;&#24863;&#30693;&#22312;&#20132;&#36890;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques. (arXiv:2309.14395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#20844;&#36335;&#19978;&#31361;&#21457;&#36335;&#38556;&#24773;&#20917;&#19979;&#26234;&#33021;&#36710;&#36742;&#30340;&#34892;&#36710;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#31361;&#28982;&#36335;&#38556;&#30001;&#20110;&#36947;&#36335;&#32500;&#25252;&#12289;&#20107;&#25925;&#21644;&#27773;&#36710;&#32500;&#20462;&#31561;&#21407;&#22240;&#26159;&#25105;&#20204;&#20960;&#20046;&#27599;&#22825;&#37117;&#20250;&#36935;&#21040;&#30340;&#24773;&#20917;&#12290;&#37197;&#22791;&#21487;&#20197;&#33719;&#21462;&#36710;&#36742;&#21160;&#24577;&#20449;&#24687;&#65288;&#22914;&#36895;&#24230;&#12289;&#21152;&#36895;&#24230;&#21644;&#20301;&#32622;&#65289;&#30340;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#21487;&#20197;&#22312;&#21040;&#36798;&#36335;&#38556;&#20043;&#21069;&#20570;&#20986;&#26234;&#33021;&#20915;&#31574;&#26469;&#21464;&#25442;&#36710;&#36947;&#12290;&#35768;&#22810;&#25991;&#29486;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#21644;&#21464;&#36947;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38598;&#25104;&#30340;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#26377;&#28508;&#21147;&#27169;&#25311;&#23454;&#38469;&#30340;&#39550;&#39542;&#25805;&#32437;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#23558;&#36827;&#34892;&#31361;&#21457;&#26045;&#24037;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#23558;&#24773;&#26223;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#37319;&#29992;&#30528;&#21517;&#30340;DQN&#31639;&#27861;&#26469;&#35757;&#32451;RL&#20195;&#29702;&#20197;&#21046;&#23450;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A sudden roadblock on highways due to many reasons such as road maintenance, accidents, and car repair is a common situation we encounter almost daily. Autonomous Vehicles (AVs) equipped with sensors that can acquire vehicle dynamics such as speed, acceleration, and location can make intelligent decisions to change lanes before reaching a roadblock. A number of literature studies have examined car-following models and lane-changing models. However, only a few studies proposed an integrated car-following and lane-changing model, which has the potential to model practical driving maneuvers. Hence, in this paper, we present an integrated car-following and lane-changing decision-control system based on Deep Reinforcement Learning (DRL) to address this issue. Specifically, we consider a scenario where sudden construction work will be carried out along a highway. We model the scenario as a Markov Decision Process (MDP) and employ the well-known DQN algorithm to train the RL agent to make the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;MRI&#37325;&#24314;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#23376;&#32452;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#20559;&#24046;&#65292;&#24182;&#25581;&#31034;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#35757;&#32451;&#27495;&#35270;&#19981;&#26159;&#20559;&#35265;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.14392</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#20110;&#33041;MRI&#37325;&#24314;&#20013;&#30340;&#20844;&#24179;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction. (arXiv:2309.14392v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;MRI&#37325;&#24314;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#23376;&#32452;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#20559;&#24046;&#65292;&#24182;&#25581;&#31034;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#35757;&#32451;&#27495;&#35270;&#19981;&#26159;&#20559;&#35265;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#29305;&#21035;&#26159;&#22312;MRI&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#22270;&#20687;&#20445;&#30495;&#24230;&#30340;&#25552;&#39640;&#21644;&#37319;&#38598;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;&#22312;&#31070;&#32463;&#25104;&#20687;&#20013;&#65292;DL&#26041;&#27861;&#21487;&#20197;&#20174;&#37319;&#26679;&#19981;&#36275;&#30340;&#25968;&#25454;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#27665;&#26063;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#22312;DL&#31639;&#27861;&#20013;&#32771;&#34385;&#20844;&#24179;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;DL&#30340;&#33041;MRI&#37325;&#24314;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#23454;&#26045;&#22522;&#32447;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#21644;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#26469;&#25506;&#32034;&#20844;&#24179;&#24615;&#30340;&#23384;&#22312;&#21644;&#26469;&#28304;&#12290;&#20351;&#29992;&#22270;&#20687;&#37325;&#24314;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24615;&#21035;&#21644;&#24180;&#40836;&#23376;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#33021;&#20559;&#24046;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#35757;&#32451;&#27495;&#35270;&#19981;&#26159;&#20559;&#35265;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#39033;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;DL&#22270;&#20687;&#37325;&#24314;&#20013;&#20844;&#24179;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#26088;&#22312;&#25913;&#21892;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equit
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.14391</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems. (arXiv:2309.14391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14391
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (Deep RL) &#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21160;&#24577;&#26381;&#21153;&#32452;&#21512;&#12289;&#20316;&#19994;&#35843;&#24230;&#12289;&#21368;&#36733;&#20197;&#21450;&#26381;&#21153;&#36866;&#24212;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#23398;&#21040;&#30340;&#20915;&#31574;&#31574;&#30053;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24110;&#21161;&#26381;&#21153;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#35843;&#35797;&#12289;&#25903;&#25345;&#26381;&#21153;&#25552;&#20379;&#21830;&#36981;&#23432;&#30456;&#20851;&#27861;&#24459;&#26694;&#26550;&#20197;&#21450;&#24110;&#21161;&#26381;&#21153;&#20351;&#29992;&#32773;&#24314;&#31435;&#20449;&#20219;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Chat4XAI&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#20419;&#36827;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;&#19982;&#35270;&#35273;&#35299;&#37322;&#30456;&#27604;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#25253;&#21578;&#20248;&#28857;&#21253;&#25324;&#38750;&#25216;&#26415;&#29992;&#25143;&#26356;&#22909;&#30340;&#21487;&#29702;&#35299;&#24615;&#12289;&#29992;&#25143;&#30340;&#25509;&#21463;&#24230;&#21644;&#20449;&#20219;&#24230;&#25552;&#39640;&#65292;&#20197;&#21450;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#29992;&#25143;&#26089;&#26399;&#27969;&#22833;&#30340;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#19994;&#21153;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.14390</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;-&#20135;&#21697;&#20114;&#21160;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#26089;&#26399;&#27969;&#22833;
&lt;/p&gt;
&lt;p&gt;
Early Churn Prediction from Large Scale User-Product Interaction Time Series. (arXiv:2309.14390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#29992;&#25143;&#26089;&#26399;&#27969;&#22833;&#30340;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#19994;&#21153;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#27969;&#22833;&#65292;&#22312;&#21508;&#31181;&#38754;&#21521;&#23458;&#25143;&#30340;&#19994;&#21153;&#22330;&#26223;&#20013;&#65292;&#20197;&#32467;&#26463;&#19982;&#20225;&#19994;&#20851;&#31995;&#20026;&#29305;&#24449;&#65292;&#23545;&#32463;&#27982;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#22312;&#35768;&#22810;&#31995;&#32479;&#23545;&#29992;&#25143;&#30340;&#34892;&#21160;&#20013;&#65292;&#22914;&#20419;&#38144;&#25240;&#25187;&#21644;&#30041;&#23384;&#27963;&#21160;&#65292;&#39044;&#27979;&#28508;&#22312;&#30340;&#27969;&#22833;&#23458;&#25143;&#26159;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#21464;&#21160;&#21095;&#28872;&#30340;&#39046;&#22495;&#65292;&#22914;&#24187;&#24819;&#20307;&#32946;&#65292;&#19981;&#21487;&#39044;&#27979;&#30340;&#22240;&#32032;&#65292;&#22914;&#22269;&#38469;&#20307;&#32946;&#36187;&#20107;&#65292;&#29978;&#33267;&#21487;&#20197;&#24433;&#21709;&#21040;&#24120;&#35268;&#30340;&#28040;&#36153;&#20064;&#24815;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;&#20132;&#26131;&#21382;&#21490;&#21644;&#29992;&#25143;-&#20135;&#21697;&#20114;&#21160;&#23545;&#20110;&#39044;&#27979;&#27969;&#22833;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#12290;&#27492;&#22806;&#65292;&#27969;&#22833;&#39044;&#27979;&#31995;&#32479;&#30340;&#29305;&#24449;&#24320;&#21457;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;2&#20159;&#22810;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#65292;&#25512;&#29702;&#27969;&#27700;&#32447;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#24449;&#24037;&#31243;&#19978;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#23458;&#25143;&#27969;&#22833;&#30340;&#21487;&#33021;&#24615;&#65292;&#20419;&#36827;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
User churn, characterized by customers ending their relationship with a business, has profound economic consequences across various Business-to-Customer scenarios. For numerous system-to-user actions, such as promotional discounts and retention campaigns, predicting potential churners stands as a primary objective. In volatile sectors like fantasy sports, unpredictable factors such as international sports events can influence even regular spending habits. Consequently, while transaction history and user-product interaction are valuable in predicting churn, they demand deep domain knowledge and intricate feature engineering. Additionally, feature development for churn prediction systems can be resource-intensive, particularly in production settings serving 200m+ users, where inference pipelines largely focus on feature engineering. This paper conducts an exhaustive study on predicting user churn using historical data. We aim to create a model forecasting customer churn likelihood, facil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14387</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#38543;&#26426;&#26597;&#35810;&#25506;&#32034;&#26426;&#22120;&#20154;&#24418;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring Robot Morphology Spaces through Breadth-First Search and Random Query. (arXiv:2309.14387v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14387
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#20026;&#35774;&#35745;&#21644;&#36827;&#21270;&#26426;&#22120;&#20154;&#24418;&#24577;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#23588;&#20854;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#22240;&#22411;&#21040;&#34920;&#22411;&#26144;&#23556;&#36807;&#31243;&#20013;&#65292;&#26597;&#35810;&#26426;&#21046;&#30340;&#20316;&#29992;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#26597;&#35810;&#26426;&#21046;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26597;&#35810;&#26426;&#21046;&#8212;&#8212;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#65292;&#22312;&#20351;&#29992;CPPN&#36827;&#21270;&#26426;&#22120;&#20154;&#24418;&#24577;&#21644;&#20351;&#29992;&#24352;&#37327;&#36827;&#21270;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#22312;&#20004;&#31181;&#36827;&#21270;&#26694;&#26550;&#65288;&#25289;&#39532;&#20811;&#21644;&#36798;&#23572;&#25991;&#31995;&#32479;&#65289;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#23545;&#36827;&#21270;&#32467;&#26524;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#36523;&#20307;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#24418;&#24577;&#26234;&#33021;&#12289;&#22810;&#26679;&#24615;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;BFS&#26356;&#21152;&#26377;&#25928;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Evolutionary robotics offers a powerful framework for designing and evolving robot morphologies, particularly in the context of modular robots. However, the role of query mechanisms during the genotype-to-phenotype mapping process has been largely overlooked. This research addresses this gap by conducting a comparative analysis of query mechanisms in the brain-body co-evolution of modular robots. Using two different query mechanisms, Breadth-First Search (BFS) and Random Query, within the context of evolving robot morphologies using CPPNs and robot controllers using tensors, and testing them in two evolutionary frameworks, Lamarckian and Darwinian systems, this study investigates their influence on evolutionary outcomes and performance. The findings demonstrate the impact of the two query mechanisms on the evolution and performance of modular robot bodies, including morphological intelligence, diversity, and morphological traits. This study suggests that BFS is both more effective and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#26679;-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;-&#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#26694;&#26550;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14385</link><description>&lt;p&gt;
&#37319;&#26679; - &#21464;&#20998;&#33258;&#32534;&#30721;&#22120; - &#38598;&#25104;&#65306;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25506;&#32034;&#20013;
&lt;/p&gt;
&lt;p&gt;
Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence. (arXiv:2309.14385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#26679;-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;-&#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#26694;&#26550;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#26041;&#27861;&#25110;&#36884;&#24452;&#26469;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#19968;&#20010;&#31995;&#32479;&#21644;&#26377;&#20957;&#32858;&#21147;&#30340;&#26694;&#26550;&#24050;&#32463;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#25972;&#21512;&#26032;&#30340;&#25216;&#26415;&#65292;&#22914;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#26694;&#26550;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21363;&#37319;&#26679; - &#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289; - &#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#65292;&#20026;XAI&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#28151;&#21512;&#26550;&#26500;&#65292;&#20854;&#20013;VAE&#19982;&#38598;&#25104;&#22534;&#21472;&#21644;SHapley&#21487;&#21152;&#24615;&#35299;&#37322;&#65288;SHAP&#65289;&#29992;&#20110;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#36824;&#32467;&#21512;&#25490;&#21015;&#37325;&#35201;&#24615;&#21644;&#20869;&#37096;&#19982;&#22806;&#37096;&#37325;&#35201;&#24615;&#30340;SHAP&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) models have recently attracted a great deal of interest from a variety of application sectors. Despite significant developments in this area, there are still no standardized methods or approaches for understanding AI model outputs. A systematic and cohesive framework is also increasingly necessary to incorporate new techniques like discriminative and generative models to close the gap. This paper contributes to the discourse on XAI by presenting an empirical evaluation based on a novel framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble stacking and SHapley Additive exPlanations are used for imbalanced classification. The finding reveals that combining ensemble stacking, VAE, and SHAP can. not only lead to better model performance but also provide an easily explainable framework. This work has used SHAP combined with Permutation Importance and In
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#21683;&#22013;&#35786;&#26029;&#21628;&#21560;&#36947;&#30142;&#30149;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#30740;&#31350;&#21683;&#22013;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#21487;&#20197;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.14383</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#21683;&#22013;&#35786;&#26029;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey. (arXiv:2309.14383v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14383
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#21683;&#22013;&#35786;&#26029;&#21628;&#21560;&#36947;&#30142;&#30149;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#30740;&#31350;&#21683;&#22013;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#21487;&#20197;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21683;&#22013;&#22768;&#38899;&#21253;&#21547;&#30528;&#21628;&#21560;&#31995;&#32479;&#30149;&#29702;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#20247;&#22810;&#37325;&#35201;&#20449;&#24687;&#12290;&#36890;&#36807;&#30740;&#31350;&#28508;&#22312;&#30340;&#21683;&#22013;&#29305;&#24449;&#20197;&#21450;&#30142;&#30149;&#35786;&#26029;&#65292;&#21487;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#21683;&#22013;&#20107;&#20214;&#22312;&#24674;&#22797;&#21307;&#30103;&#23454;&#36341;&#20013;&#21457;&#25381;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36817;&#26399;&#24212;&#29992;&#21644;&#27867;&#22312;&#35745;&#31639;&#30340;&#36827;&#27493;&#20026;&#21628;&#21560;&#36947;&#30142;&#30149;&#39044;&#27979;&#24320;&#21019;&#20102;&#26377;&#21033;&#30340;&#36235;&#21183;&#21644;&#26080;&#25968;&#26410;&#26469;&#21487;&#33021;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#21683;&#22013;&#35786;&#26029;&#31639;&#27861;&#30340;&#36805;&#36895;&#20986;&#29616;&#24050;&#32463;&#24320;&#22987;&#21033;&#29992;&#21683;&#22013;&#29305;&#24449;&#12290;&#22823;&#37327;&#20851;&#20110;&#22522;&#20110;&#21683;&#22013;&#30340;AI&#31639;&#27861;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#21307;&#30103;&#19987;&#23478;&#26469;&#35828;&#65292;&#20197;&#35814;&#23613;&#30340;&#26041;&#24335;&#25910;&#38598;&#25152;&#26377;&#30456;&#20851;&#30740;&#31350;&#30340;&#20449;&#24687;&#26159;&#38750;&#24120;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cough acoustics contain multitudes of vital information about pathomorphological alterations in the respiratory system. Reliable and accurate detection of cough events by investigating the underlying cough latent features and disease diagnosis can play an indispensable role in revitalizing the healthcare practices. The recent application of Artificial Intelligence (AI) and advances of ubiquitous computing for respiratory disease prediction has created an auspicious trend and myriad of future possibilities in the medical domain. In particular, there is an expeditiously emerging trend of Machine learning (ML) and Deep Learning (DL)-based diagnostic algorithms exploiting cough signatures. The enormous body of literature on cough-based AI algorithms demonstrate that these models can play a significant role for detecting the onset of a specific respiratory disease. However, it is pertinent to collect the information from all relevant studies in an exhaustive manner for the medical experts a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14374</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30417;&#31649;&#25991;&#20214;&#25110;&#24314;&#31569;&#27861;&#35268;&#35299;&#37322;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#30340;&#26684;&#24335;&#23545;&#20110;&#26234;&#33021;&#35774;&#35745;&#21644;&#24314;&#36896;&#24314;&#31569;&#21644;&#22522;&#30784;&#35774;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#33258;&#21160;&#21270;&#35268;&#21017;&#35299;&#37322;&#65288;ARI&#65289;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#22810;&#24180;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#24314;&#31569;&#27861;&#35268;&#20013;&#26089;&#26399;&#21644;&#25163;&#21160;&#31579;&#36873;&#21487;&#35299;&#37322;&#26465;&#27454;&#12290; &#34429;&#28982;&#20854;&#20013;&#23569;&#25968;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#36825;&#20195;&#34920;&#20102;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#26684;&#24335;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#21333;&#20010;&#26465;&#27454;&#21644;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#31867;&#21035;&#65292;&#20197;&#32771;&#34385;&#23545;&#35268;&#21017;&#35299;&#37322;&#30340;&#35201;&#27714;&#23545;&#27599;&#20010;&#24314;&#31569;&#27861;&#35268;&#26465;&#27454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#39640;&#25928;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them highly depend on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clause and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14372</link><description>&lt;p&gt;
&#20154;&#31867;&#36716;&#24405;&#36136;&#37327;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Human Transcription Quality Improvement. (arXiv:2309.14372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#36716;&#24405;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#19994;&#32423;&#25968;&#25454;&#25910;&#38598;&#31649;&#36947;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#20247;&#21253;&#36716;&#24405;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#35821;&#38899;&#36716;&#24405;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#26469;&#25913;&#21892;&#36716;&#24405;&#36136;&#37327;&#65306;&#22312;&#26631;&#27880;&#38454;&#27573;&#22522;&#20110;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#30340;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;LibriCrowd - &#19968;&#20010;&#21253;&#21547;100&#23567;&#26102;&#33521;&#35821;&#35821;&#38899;&#36716;&#24405;&#30340;&#22823;&#35268;&#27169;&#20247;&#21253;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#24378;&#30456;&#20851;&#24615;&#12290;&#36716;&#24405;&#36136;&#37327;&#30340;&#25552;&#21319;&#20351;ASR&#27169;&#22411;&#30340;WER&#30456;&#23545;&#20943;&#23569;&#20102;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#36896;&#31119;&#30740;&#31350;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#65292;&#24182;&#33021;&#22815;&#31934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#19982;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#28436;&#31034;&#20102;&#30001;&#22810;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;</title><link>http://arxiv.org/abs/2309.14366</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#21333;&#27425;&#36845;&#20195;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#29992;&#20110;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Unitary Weights Based One-Iteration Quantum Perceptron Algorithm for Non-Ideal Training Sets. (arXiv:2309.14366v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#65292;&#24182;&#33021;&#22815;&#31934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#19982;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#28436;&#31034;&#20102;&#30001;&#22810;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#65288;&#21363;&#19981;&#23436;&#25972;&#25110;&#36807;&#23436;&#22791;&#38598;&#65289;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#12290;&#23545;&#37327;&#23376;&#38376;{H&#65292;S&#65292;T&#65292;CNOT&#65292;Toffoli&#65292;Fredkin}&#30340;&#31034;&#20363;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#27425;&#36845;&#20195;&#20013;&#20934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#36824;&#28436;&#31034;&#20102;&#30001;&#20960;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve the problem of non-ideal training sets (i.e., the less-complete or over-complete sets) and implement one-iteration learning, a novel efficient quantum perceptron algorithm based on unitary weights is proposed, where the singular value decomposition of the total weight matrix from the training set is calculated to make the weight matrix to be unitary. The example validation of quantum gates {H, S, T, CNOT, Toffoli, Fredkin} shows that our algorithm can accurately implement arbitrary quantum gates within one iteration. The performance comparison between our algorithm and other quantum perceptron algorithms demonstrates the advantages of our algorithm in terms of applicability, accuracy, and availability. For further validating the applicability of our algorithm, a quantum composite gate which consists of several basic quantum gates is also illustrated.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#24341;&#23548;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DACDM&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#30446;&#26631;&#22495;&#26679;&#26412;&#65292;&#24110;&#21161;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#26356;&#36731;&#26494;&#30340;&#36801;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14360</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#30340;&#39046;&#22495;&#24341;&#23548;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation. (arXiv:2309.14360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#24341;&#23548;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DACDM&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#30446;&#26631;&#22495;&#26679;&#26412;&#65292;&#24110;&#21161;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#26356;&#36731;&#26494;&#30340;&#36801;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#26102;&#65292;&#21463;&#38480;&#30340;&#21487;&#36801;&#31227;&#24615;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#36890;&#36807;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UDA&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#22823;&#33539;&#22260;&#30340;&#39046;&#22495;&#20559;&#31227;&#21644;&#26377;&#38480;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#24341;&#23548;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DACDM&#65289;&#65292;&#29992;&#20110;&#20026;&#30446;&#26631;&#22495;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#26679;&#26412;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;DACDM&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#31867;&#21035;&#20449;&#24687;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#26679;&#26412;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#22312;DACDM&#20013;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#20998;&#31867;&#22120;&#26469;&#25351;&#23548;&#30446;&#26631;&#22495;&#30340;&#29983;&#25104;&#26679;&#26412;&#12290;&#29983;&#25104;&#30340;&#26679;&#26412;&#24110;&#21161;&#29616;&#26377;&#30340;UDA&#26041;&#27861;&#26356;&#36731;&#26494;&#22320;&#20174;&#28304;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36801;&#31227;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DACDM&#24102;&#26469;&#20102;&#36739;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited transferability hinders the performance of deep learning models when applied to new application scenarios. Recently, Unsupervised Domain Adaptation (UDA) has achieved significant progress in addressing this issue via learning domain-invariant features. However, the performance of existing UDA methods is constrained by the large domain shift and limited target domain data. To alleviate these issues, we propose DomAin-guided Conditional Diffusion Model (DACDM) to generate high-fidelity and diversity samples for the target domain. In the proposed DACDM, by introducing class information, the labels of generated samples can be controlled, and a domain classifier is further introduced in DACDM to guide the generated samples for the target domain. The generated samples help existing UDA methods transfer from the source domain to the target domain more easily, thus improving the transfer performance. Extensive experiments on various benchmarks demonstrate that DACDM brings a large impr
&lt;/p&gt;</description></item><item><title>COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14356</link><description>&lt;p&gt;
COCO-Counterfactuals:&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14356
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#24050;&#35777;&#26126;&#23545;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#20363;&#22312;NLP&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29992;&#65292;&#20294;&#30001;&#20110;&#21019;&#24314;&#26368;&#23567;&#21453;&#20107;&#23454;&#21464;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38590;&#24230;&#65292;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;COCO-Counterfactuals&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;MS-COCO&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#26631;&#39064;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;COCO-Counterfactuals&#22312;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#28436;&#35762;&#20013;&#36827;&#34892;&#35821;&#35328;&#26631;&#35760;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#26469;&#26816;&#27979;&#21644;&#37327;&#21270;&#27665;&#31929;&#20027;&#20041;&#30340;&#26680;&#24515;&#32500;&#24230;&#65292;&#24182;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2309.14355</link><description>&lt;p&gt;
PopBERT. &#26816;&#27979;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#30340;&#27665;&#31929;&#20027;&#20041;&#21450;&#20854;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
PopBERT. Detecting populism and its host ideologies in the German Bundestag. (arXiv:2309.14355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#28436;&#35762;&#20013;&#36827;&#34892;&#35821;&#35328;&#26631;&#35760;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#26469;&#26816;&#27979;&#21644;&#37327;&#21270;&#27665;&#31929;&#20027;&#20041;&#30340;&#26680;&#24515;&#32500;&#24230;&#65292;&#24182;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#31929;&#20027;&#20041;&#30340;&#23835;&#36215;&#24341;&#36215;&#20102;&#35768;&#22810;&#25919;&#27835;&#23398;&#23478;&#21644;&#20174;&#19994;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23545;&#20854;&#28508;&#22312;&#35821;&#35328;&#30340;&#26816;&#27979;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#12289;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27665;&#31929;&#20027;&#20041;&#31435;&#22330;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#35758;&#20250;&#28436;&#35762;(2013&#24180;&#33267;2021&#24180;)&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#36981;&#24490;&#27665;&#31929;&#20027;&#20041;&#30340;&#27010;&#24565;&#23450;&#20041;&#65292;&#25105;&#20204;&#23558;&#23545;&#39640;&#23578;&#20154;&#27665;&#25110;&#33104;&#36133;&#31934;&#33521;&#30340;&#36947;&#24503;&#24341;&#29992;&#26631;&#35760;&#20026;&#27665;&#31929;&#20027;&#20041;&#35821;&#35328;&#30340;&#26680;&#24515;&#32500;&#24230;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30830;&#23450;&#27665;&#31929;&#20027;&#20041;&#34180;&#24847;&#35782;&#24418;&#24577;&#30340;&#24418;&#25104;&#26041;&#24335;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#27665;&#31929;&#20027;&#20041;&#38472;&#36848;&#22914;&#20309;&#19982;&#24038;&#32764;&#25110;&#21491;&#32764;&#30340;&#20027;&#23548;&#24847;&#35782;&#24418;&#24577;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;PopBERT&#65289;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#37327;&#21270;&#27599;&#20010;&#32500;&#24230;&#12290;&#19968;&#31995;&#21015;&#39564;&#35777;&#26816;&#26597;&#26174;&#31034;&#35813;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#24378;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38754;&#37096;&#26377;&#25928;&#24615;&#65292;&#19982;&#19987;&#23478;&#35843;&#26597;&#30340;&#20826;&#27966;&#25490;&#21517;&#30456;&#21563;&#21512;&#65292;&#24182;&#26816;&#27979;&#20986;-of-sa
&lt;/p&gt;
&lt;p&gt;
The rise of populism concerns many political scientists and practitioners, yet the detection of its underlying language remains fragmentary. This paper aims to provide a reliable, valid, and scalable approach to measure populist stances. For that purpose, we created an annotated dataset based on parliamentary speeches of the German Bundestag (2013 to 2021). Following the ideational definition of populism, we label moralizing references to the virtuous people or the corrupt elite as core dimensions of populist language. To identify, in addition, how the thin ideology of populism is thickened, we annotate how populist statements are attached to left-wing or right-wing host ideologies. We then train a transformer-based model (PopBERT) as a multilabel classifier to detect and quantify each dimension. A battery of validation checks reveals that the model has a strong predictive accuracy, provides high qualitative face validity, matches party rankings of expert surveys, and detects out-of-sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#28040;&#24687;&#20132;&#25442;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;D-ADMM&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.14353</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#20998;&#24067;&#24335;ADMM&#23454;&#29616;&#26377;&#38480;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Limited Communications Distributed Optimization via Deep Unfolded Distributed ADMM. (arXiv:2309.14353v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#28040;&#24687;&#20132;&#25442;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;D-ADMM&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20248;&#21270;&#26159;&#22312;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#21327;&#20316;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#25805;&#20316;&#34987;&#24314;&#27169;&#20026;&#20849;&#21516;&#26368;&#23567;&#21270;&#19968;&#20010;&#20849;&#20139;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#26412;&#22320;&#25910;&#38598;&#30340;&#35266;&#27979;&#12290;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#24120;&#35265;&#30340;D-ADMM&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32467;&#21512;&#26412;&#22320;&#35745;&#31639;&#21644;&#28040;&#24687;&#20132;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#65288;&#29305;&#21035;&#26159;D-ADMM&#65289;&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#36890;&#20449;&#65292;&#21363;&#20195;&#29702;&#20043;&#38388;&#20132;&#25442;&#30340;&#28040;&#24687;&#65292;&#20197;&#36798;&#25104;&#20849;&#35782;&#12290;&#36825;&#20351;&#24471;D-ADMM&#22312;&#21151;&#32791;&#12289;&#24310;&#36831;&#21644;&#36890;&#36947;&#36164;&#28304;&#26041;&#38754;&#21464;&#24471;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#65292;&#23427;&#37319;&#29992;&#26032;&#20852;&#30340;&#28145;&#24230;&#23637;&#24320;&#26041;&#27861;&#65292;&#20351;D-ADMM&#33021;&#22815;&#21487;&#38752;&#22320;&#36890;&#36807;&#27599;&#20010;&#26234;&#33021;&#20307;&#20107;&#20808;&#23450;&#20041;&#30340;&#23569;&#37327;&#28040;&#24687;&#36827;&#34892;&#25805;&#20316;&#12290;&#23637;&#24320;&#30340;D-ADMM&#23436;&#20840;&#20445;&#30041;D-ADMM&#30340;&#25805;&#20316;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization is a fundamental framework for collaborative inference and decision making in decentralized multi-agent systems. The operation is modeled as the joint minimization of a shared objective which typically depends on observations gathered locally by each agent. Distributed optimization algorithms, such as the common D-ADMM, tackle this task by iteratively combining local computations and message exchanges. One of the main challenges associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications, i.e., messages exchanged between the agents, to reach consensus. This can make D-ADMM costly in power, latency, and channel resources. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20223;&#30495;&#25968;&#25454;&#21644;&#21355;&#26143;&#27979;&#39640;&#20202;&#22521;&#35757;&#20102;&#29992;&#20110;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#30340;&#31070;&#32463;&#26144;&#23556;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#28023;&#27915;&#27169;&#25311;&#25968;&#25454;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14350</link><description>&lt;p&gt;
&#20351;&#29992;&#20223;&#30495;&#25968;&#25454;&#22521;&#35757;&#21355;&#26143;&#27979;&#39640;&#30340;&#31070;&#32463;&#26144;&#23556;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Training neural mapping schemes for satellite altimetry with simulation data. (arXiv:2309.14350v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20223;&#30495;&#25968;&#25454;&#21644;&#21355;&#26143;&#27979;&#39640;&#20202;&#22521;&#35757;&#20102;&#29992;&#20110;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#30340;&#31070;&#32463;&#26144;&#23556;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#28023;&#27915;&#27169;&#25311;&#25968;&#25454;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#27979;&#39640;&#19982;&#25968;&#25454;&#21516;&#21270;&#21644;&#26368;&#20248;&#25554;&#20540;&#26041;&#26696;&#32467;&#21512;&#65292;&#28145;&#21051;&#25552;&#21319;&#20102;&#25105;&#20204;&#30417;&#27979;&#28023;&#27915;&#34920;&#38754;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#24050;&#25104;&#20026;&#35299;&#20915;&#26102;&#31354;&#25554;&#20540;&#38382;&#39064;&#30340;&#21560;&#24341;&#20154;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#21355;&#26143;&#27979;&#39640;&#25968;&#25454;&#38598;&#22312;&#28023;&#27915;&#34920;&#38754;&#26102;&#31354;&#35206;&#30422;&#26041;&#38754;&#31232;&#32570;&#65292;&#36825;&#21046;&#32422;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26041;&#26696;&#22312;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#19978;&#30340;&#35757;&#32451;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#28023;&#27915;&#21160;&#21147;&#23398;&#30340;&#27169;&#25311;&#21644;&#21355;&#26143;&#27979;&#39640;&#20202;&#26469;&#35757;&#32451;&#22522;&#20110;&#20223;&#30495;&#30340;&#31070;&#32463;&#26144;&#23556;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#28023;&#27915;&#34920;&#38754;&#39640;&#24230;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#30495;&#23454;&#21355;&#26143;&#27979;&#39640;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#28023;&#27915;&#27169;&#25311;&#25968;&#25454;&#38598;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#23454;&#39564;&#24615;&#20998;&#26512;&#28085;&#30422;&#20102;&#28065;&#27969;-&#23384;&#22312;&#37197;&#32622;&#21040;&#28065;&#27969;&#20016;&#23500;&#37197;&#32622;&#30340;&#20998;&#36776;&#29575;&#65292;&#20351;&#29992;&#25968;&#25454;&#21516;&#21270;&#21644;&#26080;&#28526;&#35299;&#26512;&#27169;&#25311;&#30340;&#24378;&#21046;&#27169;&#25311;&#19982;&#20877;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite altimetry combined with data assimilation and optimal interpolation schemes have deeply renewed our ability to monitor sea surface dynamics. Recently, deep learning (DL) schemes have emerged as appealing solutions to address space-time interpolation problems. The scarcity of real altimetry dataset, in terms of space-time coverage of the sea surface, however impedes the training of state-of-the-art neural schemes on real-world case-studies. Here, we leverage both simulations of ocean dynamics and satellite altimeters to train simulation-based neural mapping schemes for the sea surface height and demonstrate their performance for real altimetry datasets. We analyze further how the ocean simulation dataset used during the training phase impacts this performance. This experimental analysis covers both the resolution from eddy-present configurations to eddy-rich ones, forced simulations vs. reanalyses using data assimilation and tide-free vs. tide-resolving simulations. Our benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#21457;&#23637;&#32972;&#26223;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.14349</link><description>&lt;p&gt;
&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Corporate Credit Rating: A Survey. (arXiv:2309.14349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#21457;&#23637;&#32972;&#26223;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#65288;CCR&#65289;&#22312;&#24403;&#20195;&#32463;&#27982;&#21644;&#31038;&#20250;&#21457;&#23637;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#22914;&#20309;&#20351;&#29992;&#20449;&#29992;&#35780;&#32423;&#26041;&#27861;&#23545;&#20225;&#19994;&#36827;&#34892;&#35780;&#20272;&#19968;&#30452;&#26159;&#19968;&#20010;&#20540;&#24471;&#35752;&#35770;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22269;&#20869;&#22806;&#30456;&#20851;&#25991;&#29486;&#30340;&#38405;&#35835;&#21644;&#30740;&#31350;&#65292;&#26412;&#35770;&#25991;&#23545;CCR&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#26412;&#35770;&#25991;&#20174;&#32479;&#35745;&#27169;&#22411;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#19977;&#20010;&#23618;&#38754;&#25972;&#29702;&#20102;CCR&#26041;&#27861;&#21457;&#23637;&#30340;&#32972;&#26223;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#24120;&#35265;&#25968;&#25454;&#38598;&#65292;&#24182;&#28145;&#20837;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;&#19982;&#29616;&#26377;&#30340;CCR&#32508;&#36848;&#30456;&#27604;&#65292;&#26412;&#35770;&#25991;&#35814;&#32454;&#38416;&#36848;&#21644;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corporate credit rating (CCR) plays a very important role in the process of contemporary economic and social development. How to use credit rating methods for enterprises has always been a problem worthy of discussion. Through reading and studying the relevant literature at home and abroad, this paper makes a systematic survey of CCR. This paper combs the context of the development of CCR methods from the three levels: statistical models, machine learning models and neural network models, summarizes the common databases of CCR, and deeply compares the advantages and disadvantages of the models. Finally, this paper summarizes the problems existing in the current research and prospects the future of CCR. Compared with the existing review of CCR, this paper expounds and analyzes the progress of neural network model in this field in recent years.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#36873;&#25321;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;PS-DES&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#24230;&#37327;&#25351;&#26631;&#35780;&#20272;&#22810;&#31181;DES&#25216;&#26415;&#36873;&#25321;&#30340;&#38598;&#25104;&#65292;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14307</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#30340;&#21518;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A post-selection algorithm for improving dynamic ensemble selection methods. (arXiv:2309.14307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#36873;&#25321;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;PS-DES&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#24230;&#37327;&#25351;&#26631;&#35780;&#20272;&#22810;&#31181;DES&#25216;&#26415;&#36873;&#25321;&#30340;&#38598;&#25104;&#65292;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;DES&#65289;&#26159;&#19968;&#31181;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#65288;MCS&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#36873;&#25321;&#38454;&#27573;&#20026;&#27599;&#20010;&#26597;&#35810;&#26679;&#26412;&#36873;&#25321;&#19968;&#20010;&#38598;&#25104;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;DES&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#27809;&#26377;&#29305;&#23450;&#30340;DES&#25216;&#26415;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36873;&#25321;&#27599;&#20010;&#26597;&#35810;&#31034;&#20363;&#30340;&#26368;&#20339;DES&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21518;&#36873;&#25321;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;PS-DES&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21518;&#36873;&#25321;&#26041;&#26696;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#24230;&#37327;&#25351;&#26631;&#35780;&#20272;&#30001;&#20960;&#31181;DES&#25216;&#26415;&#36873;&#25321;&#30340;&#38598;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20934;&#30830;&#24615;&#20316;&#20026;&#36873;&#25321;&#38598;&#25104;&#30340;&#25351;&#26631;&#65292;PS-DES&#30340;&#34920;&#29616;&#20248;&#20110;&#21333;&#20010;DES&#25216;&#26415;&#12290;PS-DES&#28304;&#20195;&#30721;&#21487;&#22312;GitHub&#23384;&#20648;&#24211;&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository
&lt;/p&gt;</description></item><item><title>MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.14236</link><description>&lt;p&gt;
MoDem-V2: &#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#35270;&#35273;-&#36816;&#21160;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14236
&lt;/p&gt;
&lt;p&gt;
MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#26426;&#36733;&#20256;&#24863;&#22120;&#30452;&#25509;&#24863;&#30693;&#19990;&#30028;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#23398;&#20064;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20687;&#32032;&#30340;&#38544;&#24335;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#28040;&#38500;&#29615;&#22659;&#35013;&#32622;&#30340;&#38656;&#27714;&#65292;&#20294;&#20165;&#20165;&#20381;&#38752;&#31232;&#30095;&#30340;&#35270;&#35273;&#22870;&#21169;&#20449;&#21495;&#22312;&#25509;&#35302;&#20016;&#23500;&#30340;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#26174;&#33879;&#21152;&#21095;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#27169;&#25311;&#25110;&#20005;&#26684;&#24037;&#31243;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#31264;&#23494;&#22870;&#21169;&#30340;&#25351;&#23548;&#19979;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20195;&#29702;&#30340;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#21644;&#37325;&#22823;&#23433;&#20840;&#25925;&#38556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MoDem-V2&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#30452;&#25509;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#12290;&#22312;&#26368;&#26032;&#30340;&#31639;&#27861;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;
&lt;/p&gt;
&lt;p&gt;
Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
&lt;/p&gt;</description></item><item><title>&#20266;&#26631;&#31614;&#36873;&#25321;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#20915;&#31574;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;BPLS&#26694;&#26550;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#36873;&#25321;&#20013;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13926</link><description>&lt;p&gt;
&#20266;&#26631;&#31614;&#36873;&#25321;&#26159;&#19968;&#20010;&#20915;&#31574;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Pseudo Label Selection is a Decision Problem. (arXiv:2309.13926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13926
&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#36873;&#25321;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#20915;&#31574;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;BPLS&#26694;&#26550;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#36873;&#25321;&#20013;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#36873;&#25321;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#38656;&#35201;&#19968;&#20123;&#20934;&#21017;&#26469;&#25351;&#23548;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#20934;&#21017;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#24037;&#20316;&#24471;&#30456;&#24403;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#21462;&#20915;&#20110;&#26631;&#35760;&#25968;&#25454;&#19978;&#21021;&#22987;&#27169;&#22411;&#30340;&#25311;&#21512;&#24773;&#20917;&#12290;&#26089;&#26399;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#34987;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#22312;&#20004;&#39033;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#21487;&#20197;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#20915;&#31574;&#29702;&#35770;&#20013;&#12290;&#36825;&#20026;BPLS&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21487;&#20197;&#32531;&#35299;&#30830;&#35748;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#20934;&#21017;&#65306;&#20266;&#26679;&#26412;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#35299;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#36825;&#20010;&#8220;&#20266;POS&#8221;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#26469;&#25512;&#23548;&#20986;&#36825;&#20010;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo pos
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#31934;&#24230;&#19979;&#19977;&#31181;&#26550;&#26500;&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#24615;&#33021;&#65292;&#21457;&#29616;&#26657;&#20934;&#36136;&#37327;&#19982;&#37327;&#21270;&#36136;&#37327;&#30456;&#20851;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20302;&#31934;&#24230;&#19979;&#24615;&#33021;&#21644;&#26657;&#20934;&#36136;&#37327;&#22343;&#21464;&#24046;&#12290;GhostNet-VGG&#34920;&#29616;&#20986;&#26368;&#39640;&#31283;&#20581;&#24615;&#65292;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#25913;&#21892;&#37327;&#21270;&#32593;&#32476;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13866</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;&#37327;&#21270;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Calibration of Modern Quantized Efficient Neural Networks. (arXiv:2309.13866v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13866
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#31934;&#24230;&#19979;&#19977;&#31181;&#26550;&#26500;&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#24615;&#33021;&#65292;&#21457;&#29616;&#26657;&#20934;&#36136;&#37327;&#19982;&#37327;&#21270;&#36136;&#37327;&#30456;&#20851;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20302;&#31934;&#24230;&#19979;&#24615;&#33021;&#21644;&#26657;&#20934;&#36136;&#37327;&#22343;&#21464;&#24046;&#12290;GhostNet-VGG&#34920;&#29616;&#20986;&#26368;&#39640;&#31283;&#20581;&#24615;&#65292;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#25913;&#21892;&#37327;&#21270;&#32593;&#32476;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#26550;&#26500;&#65288;ShuffleNetv2&#12289;GhostNet-VGG&#21644;MobileOne&#65289;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CIFAR-100&#21644;PathMNIST&#65289;&#22312;&#19981;&#21516;&#31934;&#24230;&#19979;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#36136;&#37327;&#19982;&#37327;&#21270;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#65307;&#24050;&#26377;&#25991;&#29486;&#35777;&#26126;&#38543;&#30528;&#31934;&#24230;&#38477;&#20302;&#65292;&#24615;&#33021;&#20250;&#21464;&#24471;&#26356;&#24046;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26657;&#20934;&#36136;&#37327;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#20851;&#32852;&#12290;&#36825;&#22312;4&#20301;&#28608;&#27963;&#21306;&#38388;&#23588;&#20026;&#20005;&#37325;&#12290;GhostNet-VGG&#22312;&#26356;&#20302;&#31934;&#24230;&#19979;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#25913;&#21892;&#37327;&#21270;&#32593;&#32476;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#20294;&#38656;&#35201;&#27880;&#24847;&#19968;&#20123;&#32454;&#33410;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#21021;&#27493;&#30340;&#35266;&#23519;&#33021;&#22815;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#38752;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#26356;&#22810;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore calibration properties at various precisions for three architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets: CIFAR-100 and PathMNIST. The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration. This becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown to be the most robust to overall performance drop at lower precision. We find that temperature scaling can improve calibration error for quantized networks, with some caveats. We hope that these preliminary insights can lead to more opportunities for explainable and reliable EdgeML.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.13781</link><description>&lt;p&gt;
ICU &#37325;&#26032;&#20837;&#38498;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#21307;&#38498;&#29615;&#22659;&#65292;&#21307;&#29983;&#30340;&#20915;&#31574;&#23545;&#24739;&#32773;&#30340;&#29983;&#21629;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#24517;&#39035;&#36981;&#24490;&#19968;&#26465;&#20840;&#38754;&#30340;&#25252;&#29702;&#36335;&#24452;&#26469;&#20943;&#23569;&#24182;&#21457;&#30151;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#12289;&#31454;&#20105;&#24615;&#21644;&#38750;&#35745;&#21010;&#24615;&#30340;&#22240;&#32032;&#22686;&#21152;&#20102;&#32479;&#19968;&#23454;&#26045;&#25252;&#29702;&#36335;&#24452;&#30340;&#22256;&#38590;&#12290;&#20877;&#20837;&#38498;&#26159;&#35813;&#36335;&#24452;&#30340;&#22256;&#38590;&#20043;&#19968;&#65292;&#21363;&#24739;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#20877;&#27425;&#20837;&#20303;ICU&#65292;&#23548;&#33268;&#39640;&#27515;&#20129;&#29575;&#21644;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24739;&#32773;&#30340;&#21307;&#30103;&#20449;&#24687;&#26469;&#39044;&#27979;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#20877;&#20837;&#38498;&#26102;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#23545;&#20877;&#20837;&#38498;&#39044;&#27979;&#36827;&#34892;&#36866;&#24403;&#30340;&#35780;&#20272;&#12289;&#25551;&#36848;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#65288;&#21363;&#21253;&#21547;166,355&#21517;&#24739;&#32773;&#30340;eICU&#38431;&#21015;&#65292;200,859&#21517;...&#65289;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#21452;&#24179;&#38754;X&#23556;&#32447;&#21040;3D&#24418;&#29366;&#37325;&#24314;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#35780;&#20272;&#19982;&#30495;&#23454;&#20020;&#24202;&#24773;&#20917;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;8&#20010;&#27169;&#22411;&#30340;&#21442;&#32771;&#23454;&#29616;&#21644;6&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#25105;&#20204;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35780;&#20272;&#20854;&#23545;&#30495;&#23454;&#19990;&#30028;&#20020;&#24202;&#24773;&#26223;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13587</link><description>&lt;p&gt;
&#23545;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#21452;&#24179;&#38754;X&#23556;&#32447;&#21040;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction. (arXiv:2309.13587v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13587
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21452;&#24179;&#38754;X&#23556;&#32447;&#21040;3D&#24418;&#29366;&#37325;&#24314;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#35780;&#20272;&#19982;&#30495;&#23454;&#20020;&#24202;&#24773;&#20917;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;8&#20010;&#27169;&#22411;&#30340;&#21442;&#32771;&#23454;&#29616;&#21644;6&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#25105;&#20204;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35780;&#20272;&#20854;&#23545;&#30495;&#23454;&#19990;&#30028;&#20020;&#24202;&#24773;&#26223;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#20174;&#20004;&#20010;&#27491;&#20132;&#65288;&#21452;&#24179;&#38754;&#65289;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;3D&#39592;&#24418;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35299;&#21078;&#12289;&#38431;&#21015;&#21644;&#65288;&#36890;&#24120;&#26159;&#31169;&#26377;&#30340;&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#36824;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#24120;&#24120;&#20248;&#21270;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#21106;&#24230;&#37327;&#65292;&#20363;&#22914;Dice&#20998;&#25968;&#65292;&#22312;2D-3D&#39592;&#24418;&#37325;&#24314;&#20013;&#20272;&#35745;&#20020;&#24202;&#21442;&#25968;&#30340;&#24433;&#21709;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#26356;&#25509;&#36817;&#20020;&#24202;&#36716;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35780;&#20272;&#19982;&#30495;&#23454;&#20020;&#24202;&#24773;&#26223;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#37325;&#24314;&#26029;&#39592;&#12289;&#26893;&#20837;&#29289;&#39592;&#65292;&#23545;&#20154;&#32676;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20272;&#35745;&#20020;&#24202;&#21442;&#25968;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#24179;&#21488;&#25552;&#20379;&#20102;8&#20010;&#27169;&#22411;&#30340;&#21442;&#32771;&#23454;&#29616;&#65288;&#20854;&#20013;&#35768;&#22810;&#23454;&#29616;&#20197;&#21069;&#19981;&#21487;&#20844;&#24320;&#33719;&#24471;&#65289;&#65292;API&#21487;&#36731;&#26494;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;6&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#23454;&#35777;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images. However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets. Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known. To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters. Our open-source platform provides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13575</link><description>&lt;p&gt;
&#27010;&#29575;&#26435;&#37325;&#22266;&#23450;&#65306;&#29992;&#20110;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#33021;&#37327;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26041;&#27861;&#24120;&#24120;&#22522;&#20110;&#26435;&#37325;&#20540;&#26412;&#36523;&#36827;&#34892;&#20551;&#35774;&#65292;&#24182;&#24573;&#35270;&#20102;&#26435;&#37325;&#20301;&#32622;&#22312;&#20854;&#20013;&#25198;&#28436;&#30340;&#29420;&#29305;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#26681;&#25454;&#21333;&#20010;&#26435;&#37325;&#30340;&#20301;&#32622;&#29305;&#23450;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26469;&#30830;&#23450;&#21487;&#20197;&#23558;&#21738;&#20123;&#26435;&#37325;&#31227;&#21160;&#21040;&#21738;&#20010;&#32858;&#31867;&#20013;&#24515;&#20197;&#21450;&#31227;&#21160;&#21040;&#20160;&#20040;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21021;&#22987;&#21270;&#35774;&#32622;&#21644;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;-&#27169;&#22411;&#32452;&#21512;&#19979;&#35757;&#32451;BNNs&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#21040;&#30340;&#26435;&#37325;&#20540;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#30340;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BLASTNet 2.0&#65292;&#21253;&#21547;&#19977;&#32500;&#39640;&#20445;&#30495;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#35268;&#27169;&#12289;&#25104;&#26412;&#21644;&#26550;&#26500;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.13457</link><description>&lt;p&gt;
&#28966;&#28857;&#20013;&#30340;&#28237;&#27969;&#65306;&#29992;BLASTNet 2.0&#25968;&#25454;&#22522;&#20934;&#27979;&#37327;&#19977;&#32500;&#20307;&#31215;&#36229;&#20998;&#36776;&#29575;&#30340;&#32553;&#25918;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data. (arXiv:2309.13457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BLASTNet 2.0&#65292;&#21253;&#21547;&#19977;&#32500;&#39640;&#20445;&#30495;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#35268;&#27169;&#12289;&#25104;&#26412;&#21644;&#26550;&#26500;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#30340;&#20998;&#26512;&#23545;&#25512;&#36827;&#12289;&#33021;&#28304;&#29983;&#25104;&#21644;&#29615;&#22659;&#30456;&#20851;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BLASTNet 2.0&#65292;&#23427;&#26159;&#19968;&#20010;&#21253;&#21547;744&#20010;&#23436;&#25972;&#22495;&#26679;&#26412;&#26469;&#33258;34&#20010;&#39640;&#20445;&#30495;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#30340;2.2TB&#25968;&#25454;&#38598;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#19977;&#32500;&#39640;&#20445;&#30495;&#21453;&#24212;&#21644;&#38750;&#21453;&#24212;&#21387;&#32553;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#25968;&#25454;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20102;49&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20116;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#36827;&#31185;&#23398;&#25104;&#20687;&#12289;&#27169;&#25311;&#12289;&#28237;&#27969;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#31070;&#32463;&#32553;&#25918;&#20998;&#26512;&#65292;&#20197;&#26816;&#26597;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20004;&#31181;&#31185;&#23398;ML&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;i&#65289;&#39044;&#27979;&#24615;&#33021;&#21487;&#20197;&#38543;&#27169;&#22411;&#35268;&#27169;&#21644;&#25104;&#26412;&#32780;&#25193;&#23637;&#65292;&#65288;ii&#65289;&#26550;&#26500;&#23588;&#20854;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20197;&#21450;&#65288;iii&#65289;b...
&lt;/p&gt;
&lt;p&gt;
Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the b
&lt;/p&gt;</description></item><item><title>InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13064</link><description>&lt;p&gt;
InvestLM&#65306;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13064
&lt;/p&gt;
&lt;p&gt;
InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37329;&#34701;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InvestLM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19982;&#37329;&#34701;&#25237;&#36164;&#30456;&#20851;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#23545;LLaMA-65B&#36827;&#34892;&#35843;&#20248;&#12290;&#21463;&#21040;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#26082;&#23567;&#21448;&#22810;&#26679;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#38382;&#39064;&#21040;SEC&#25991;&#20214;&#21644;Stackexchange&#37327;&#21270;&#37329;&#34701;&#35752;&#35770;&#30340;&#24191;&#27867;&#37329;&#34701;&#30456;&#20851;&#20027;&#39064;&#12290;InvestLM&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#21253;&#25324;&#23545;&#20914;&#22522;&#37329;&#32463;&#29702;&#21644;&#30740;&#31350;&#20998;&#26512;&#24072;&#22312;&#20869;&#30340;&#37329;&#34701;&#19987;&#23478;&#23558;InvestLM&#30340;&#22238;&#31572;&#35780;&#20215;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#65288;GPT-3.5&#12289;GPT-4&#21644;Claude-2&#65289;&#21487;&#23218;&#32654;&#12290;&#23545;&#19968;&#32452;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20174;&#30740;&#31350;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#36827;&#34892;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#23558;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#30340;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#22312;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12694</link><description>&lt;p&gt;
&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Recurrent Temporal Revision Graph Networks. (arXiv:2309.12694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#23558;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#30340;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#22312;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#38745;&#24577;&#22270;&#30456;&#27604;&#65292;&#26102;&#38388;&#22270;&#33021;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#37051;&#23621;&#32858;&#21512;&#26159;&#22270;&#32593;&#32476;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23545;&#20110;&#26102;&#38388;&#22270;&#26469;&#35828;&#65292;&#30446;&#21069;&#26159;&#20174;&#38745;&#24577;&#22270;&#30452;&#25509;&#25299;&#23637;&#32780;&#26469;&#30340;&#12290;&#24403;&#22312;&#36825;&#31181;&#32858;&#21512;&#36807;&#31243;&#20013;&#28041;&#21450;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#26102;&#65292;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#38750;&#24120;&#39640;&#26114;&#12290;&#23454;&#38469;&#19978;&#65292;&#36890;&#24120;&#21482;&#28041;&#21450;&#26368;&#36817;&#37051;&#23621;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23376;&#25277;&#26679;&#20250;&#23548;&#33268;&#37051;&#23621;&#20449;&#24687;&#19981;&#23436;&#25972;&#21644;&#26377;&#20559;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#37051;&#23621;&#32858;&#21512;&#65292;&#23427;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#30340;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;+9.6%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;AUC Gap&#30340;&#25351;&#26631;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;AI/ML&#27169;&#22411;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20026;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#31574;&#30053;&#20998;&#20139;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.12371</link><description>&lt;p&gt;
Fairness Hub&#25216;&#26415;&#31616;&#25253;: AUC Gap
&lt;/p&gt;
&lt;p&gt;
Fairness Hub Technical Briefs: AUC Gap. (arXiv:2309.12371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;AUC Gap&#30340;&#25351;&#26631;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;AI/ML&#27169;&#22411;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20026;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#31574;&#30053;&#20998;&#20139;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27979;&#37327;&#20559;&#35265;&#65292;&#25105;&#20204;&#40723;&#21169;&#22242;&#38431;&#32771;&#34385;&#20351;&#29992;AUC Gap&#65306;&#23376;&#32676;&#20307;&#65288;&#20363;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;SES&#12289;&#20808;&#21069;&#30693;&#35782;&#65289;&#30340;&#26368;&#39640;&#21644;&#26368;&#20302;&#27979;&#35797;AUC&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;AI/ML&#31639;&#27861;&#65292;&#24182;&#25429;&#25417;&#27169;&#22411;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20363;&#22914;&#38024;&#23545;&#20132;&#21449;&#36523;&#20221;&#32676;&#20307;&#12290;LEVI&#22242;&#38431;&#22312;&#36861;&#27714;&#22312;&#20302;&#25910;&#20837;&#20013;&#23398;&#20013;&#23558;&#25968;&#23398;&#25104;&#23601;&#32763;&#20493;&#30340;&#20849;&#21516;&#30446;&#26631;&#26102;&#65292;&#20351;&#29992;&#21508;&#31181;AI/ML&#27169;&#22411;&#12290;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#32972;&#26223;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#19981;&#24341;&#20837;&#25110;&#25918;&#22823;&#20559;&#35265;&#65292;&#23545;&#20110;&#23454;&#29616;LEVI&#30446;&#26631;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#27169;&#22411;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#20379;&#25152;&#26377;LEVI&#22242;&#38431;&#20351;&#29992;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#20934;&#21644;&#20998;&#26512;&#22522;&#30784;&#65292;&#29992;&#20110;&#20849;&#20139;&#19981;&#21516;&#22242;&#38431;&#24050;&#32463;&#37319;&#21462;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
To measure bias, we encourage teams to consider using AUC Gap: the absolute difference between the highest and lowest test AUC for subgroups (e.g., gender, race, SES, prior knowledge). It is agnostic to the AI/ML algorithm used and it captures the disparity in model performance for any number of subgroups, which enables non-binary fairness assessments such as for intersectional identity groups. The LEVI teams use a wide range of AI/ML models in pursuit of a common goal of doubling math achievement in low-income middle schools. Ensuring that the models, which are trained on datasets collected in many different contexts, do not introduce or amplify biases is important for achieving the LEVI goal. We offer here a versatile and easy-to-compute measure of model bias for all LEVI teams in order to create a common benchmark and an analytical basis for sharing what strategies have worked for different teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#19982;&#28508;&#22312;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.11932</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning-oriented Survey on Tiny Machine Learning. (arXiv:2309.11932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#19982;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#20986;&#29616;&#36890;&#36807;&#25512;&#21160;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#30828;&#20214;&#35774;&#22791;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#36719;&#20214;&#26550;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#31215;&#26497;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;TinyML&#22312;&#31532;&#22235;&#21644;&#31532;&#20116;&#27425;&#24037;&#19994;&#38761;&#21629;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24110;&#21161;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#20010;&#20154;&#24212;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#34701;&#20837;&#35745;&#31639;&#25216;&#26415;&#65288;&#22914;&#26234;&#24935;&#22478;&#24066;&#12289;&#27773;&#36710;&#21644;&#21307;&#30103;&#26426;&#22120;&#20154;&#65289;&#12290;&#30001;&#20110;&#20854;&#22810;&#23398;&#31185;&#24615;&#36136;&#65292;TinyML&#39046;&#22495;&#24050;&#32463;&#20174;&#35768;&#22810;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#26412;&#32508;&#36848;&#24076;&#26395;&#25552;&#20379;&#19968;&#20010;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#20110;TinyML&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#25152;&#26377;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#32508;&#36848;&#22522;&#20110;&#31995;&#32479;&#32508;&#36848;&#21644;&#20803;&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#65288;PRISMA&#65289;&#26041;&#27861;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21644;&#23436;&#25972;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#25105;&#20204;&#23558;&#30740;&#31350;&#23454;&#29616;TinyML&#22522;&#30784;&#35299;&#20915;&#26041;&#26696;&#30340;&#19977;&#31181;&#19981;&#21516;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-bas
&lt;/p&gt;</description></item><item><title>TMac&#26159;&#19968;&#20010;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22270;&#23398;&#20064;&#30340;&#26041;&#24335;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11845</link><description>&lt;p&gt;
TMac&#65306;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification. (arXiv:2309.11845v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11845
&lt;/p&gt;
&lt;p&gt;
TMac&#26159;&#19968;&#20010;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22270;&#23398;&#20064;&#30340;&#26041;&#24335;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25968;&#23383;&#26102;&#20195;&#65292;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#38543;&#22788;&#21487;&#35265;&#65292;&#36825;&#23545;&#20110;&#23545;&#23427;&#20204;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;&#35201;&#27714;&#12290;&#26377;&#25928;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#26356;&#22909;&#30340;&#38899;&#39057;&#35270;&#35273;&#27169;&#22411;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#33258;&#28982;&#20855;&#26377;&#26102;&#38388;&#23646;&#24615;&#65292;&#20363;&#22914;&#35270;&#39057;&#20013;&#27599;&#19968;&#24103;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#36825;&#20123;&#25968;&#25454;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#33258;&#28982;&#24418;&#25104;&#22810;&#27169;&#24577;&#65292;&#24182;&#19988;&#20005;&#26684;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#22810;&#27169;&#24577;&#22768;&#38899;&#20107;&#20214;&#24314;&#27169;&#20013;&#65292;&#26102;&#24577;&#20449;&#24687;&#23545;&#20110;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#37117;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#22788;&#29702;&#27599;&#20010;&#27169;&#24577;&#29305;&#24449;&#65292;&#20165;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24573;&#35270;&#20102;&#26102;&#24577;&#20851;&#31995;&#30340;&#25366;&#25496;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#30340;&#26102;&#24577;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;TMac&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#23545;&#36825;&#31181;&#26102;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11798</link><description>&lt;p&gt;
&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30740;&#31350;&#26174;&#33879;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#21151;&#33021;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#23558;&#39030;&#28857;&#21010;&#20998;&#20026;&#20855;&#26377;&#24378;&#20869;&#37096;&#36830;&#25509;&#21644;&#36739;&#24369;&#36830;&#25509;&#30340;&#38598;&#32676;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38416;&#36848;&#65292;&#21253;&#25324;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31038;&#21306;&#26816;&#27979;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#20197;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#24182;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.11722</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning. (arXiv:2309.11722v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#20197;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#24182;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25913;&#36827;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#21442;&#19982;&#32773;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#19988;&#20182;&#20204;&#23558;&#33719;&#24471;&#20840;&#23616;&#27169;&#22411;&#21644;&#25903;&#20184;&#12290;&#29702;&#24615;&#30340;&#21442;&#19982;&#32773;&#35797;&#22270;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#20010;&#20307;&#25928;&#29992;&#65292;&#38500;&#38750;&#20182;&#20204;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#25903;&#20184;&#65292;&#21542;&#21017;&#20182;&#20204;&#23558;&#19981;&#20250;&#30495;&#23454;&#22320;&#36755;&#20837;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#21463;&#30410;&#20110;&#21442;&#19982;&#32773;&#30340;&#21512;&#20316;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#26082;&#28608;&#21169;&#30495;&#23454;&#36755;&#20837;&#25968;&#25454;&#21448;&#20419;&#36827;&#31283;&#23450;&#21512;&#20316;&#30340;&#28608;&#21169;&#26426;&#21046;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#20849;&#20139;&#21338;&#24328;&#27169;&#22411;&#65292;&#24182;&#36816;&#29992;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#19968;&#20010;&#36873;&#26680;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#19968;&#20010;&#27969;&#34892;&#27010;&#24565;&#65292;&#21363;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09866</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization with Fourier Transform and Soft Thresholding. (arXiv:2309.09866v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#20010;&#28304;&#39046;&#22495;&#65292;&#24182;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26041;&#27861;&#22240;&#20026;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#21644;&#35268;&#24459;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#23545;&#39046;&#22495;&#36716;&#31227;&#26356;&#21152;&#40065;&#26834;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#20027;&#27969;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#22312;&#28304;&#22270;&#20687;&#21644;&#30446;&#26631;&#22270;&#20687;&#20043;&#38388;&#20132;&#25442;&#20613;&#37324;&#21494;&#24133;&#24230;&#35889;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20301;&#35889;&#12290;&#28982;&#32780;&#65292;&#23427;&#24573;&#30053;&#20102;&#24133;&#24230;&#35889;&#20013;&#30340;&#32972;&#26223;&#24178;&#25200;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#20613;&#37324;&#21494;&#22495;&#24341;&#20837;&#20102;&#19968;&#20010;&#36719;&#38408;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#23545;&#20110;&#35786;&#26029;&#30524;&#31185;&#30142;&#30149;&#24456;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06169</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35299;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#37319;&#26679;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#36890;&#36807;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#30334;&#25110;&#25968;&#21315;&#27425;&#36830;&#32493;&#20989;&#25968;&#35780;&#20272;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#30456;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#37319;&#26679;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388; SDE&#65288;ER SDE&#65289;&#65292;&#23558;&#20043;&#21069;&#23545;ODE&#21644;SDE&#30340;&#25506;&#32034;&#32479;&#19968;&#36215;&#26469;&#12290;&#21033;&#29992;ER SDE&#35299;&#30340;&#21322;&#32447;&#24615;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;VP SDE&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#20219;&#24847;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#20026;VE SDE&#25552;&#20379;&#20102;&#39640;&#38454;&#36817;&#20284;&#35299;&#12290;&#22522;&#20110;ER SDE&#30340;&#35299;&#31354;&#38388;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ODE&#27714;&#35299;&#22120;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;VP SDE&#27714;&#35299;&#22120;&#19982;&#20854;VE SDE&#27714;&#35299;&#22120;&#22312;&#24615;&#33021;&#19978;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#24207;&#29305;&#24615;&#30340;&#24863;&#30693;&#27969;&#20013;&#30340;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#24207;&#39044;&#27979;&#25439;&#22833;&#65288;TPL&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26159;&#19968;&#31181;&#26356;&#24120;&#35265;&#30340;&#24863;&#30693;&#24212;&#29992;&#20013;&#30340;AL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05517</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#24207;&#39044;&#27979;&#25439;&#22833;&#30340;&#24863;&#30693;&#27969;&#20013;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss. (arXiv:2309.05517v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#24207;&#29305;&#24615;&#30340;&#24863;&#30693;&#27969;&#20013;&#30340;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#24207;&#39044;&#27979;&#25439;&#22833;&#65288;TPL&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26159;&#19968;&#31181;&#26356;&#24120;&#35265;&#30340;&#24863;&#30693;&#24212;&#29992;&#20013;&#30340;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#36890;&#36807;&#26234;&#33021;&#36873;&#25321;&#35201;&#26631;&#35760;&#30340;&#23454;&#20363;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27744;&#30340;AL&#35201;&#27714;&#25152;&#26377;&#25968;&#25454;&#37117;&#38598;&#20013;&#22312;&#25968;&#25454;&#20013;&#24515;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#26102;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#26426;&#22120;&#20154;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#19978;&#36827;&#34892;&#24863;&#30693;&#20256;&#24863;&#22120;&#27969;&#30340;&#25968;&#25454;&#36807;&#28388;&#21518;&#20877;&#21040;&#36798;&#25968;&#25454;&#20013;&#24515;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#21033;&#29992;&#20102;&#36825;&#31181;&#22270;&#20687;&#27969;&#30340;&#26102;&#24207;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#24207;&#39044;&#27979;&#25439;&#22833;&#65288;TPL&#65289;&#26041;&#27861;&#12290;&#20026;&#20102;&#27491;&#30830;&#35780;&#20272;&#22522;&#20110;&#27969;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GTA V&#34903;&#36947;&#21644;A2D2&#34903;&#36947;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#22522;&#20110;&#27744;&#30340;&#26041;&#27861;&#22312;&#24863;&#30693;&#24212;&#29992;&#20013;&#26356;&#20026;&#24120;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#22522;&#20110;&#27744;&#21644;&#22522;&#20110;&#27969;&#30340;AL&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) reduces the amount of labeled data needed to train a machine learning model by intelligently choosing which instances to label. Classic pool-based AL requires all data to be present in a datacenter, which can be challenging with the increasing amounts of data needed in deep learning. However, AL on mobile devices and robots, like autonomous cars, can filter the data from perception sensor streams before reaching the datacenter. We exploited the temporal properties for such image streams in our work and proposed the novel temporal predicted loss (TPL) method. To evaluate the stream-based setting properly, we introduced the GTA V streets and the A2D2 streets dataset and made both publicly available. Our experiments showed that our approach significantly improves the diversity of the selection while being an uncertainty-based method. As pool-based approaches are more common in perception applications, we derived a concept for comparing pool-based and stream-based AL, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04737</link><description>&lt;p&gt;
Spiking Neural Network&#32852;&#21512;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#65292;&#36880;&#28176;&#24341;&#20837;&#38590;&#24230;&#30340;&#27010;&#24565;&#26159;&#20154;&#31867;&#23398;&#20064;&#30340;&#33258;&#28982;&#36807;&#31243;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#20294;&#30446;&#21069;&#30340;SNNs&#27169;&#22411;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#24179;&#31561;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#30340;&#21407;&#21017;&#19981;&#31526;&#65292;&#24182;&#24573;&#35270;&#20102;SNNs&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#24341;&#20837;SNNs&#30340;CL-SNN&#27169;&#22411;&#65292;&#20351;SNNs&#26356;&#20687;&#20154;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;CL&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#25552;&#20513;&#22312;&#36880;&#28176;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#20043;&#21069;&#21521;&#27169;&#22411;&#23637;&#31034;&#26356;&#23481;&#26131;&#30340;&#25968;&#25454;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20449;&#24515;&#24863;&#30693;&#30340;&#25439;&#22833;&#26469;&#34913;&#37327;&#21644;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#65292;&#27169;&#22411;&#33258;&#21160;&#20943;&#23569;&#20102;&#38590;&#26679;&#26412;&#23545;&#21442;&#25968;&#20248;&#21270;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
&lt;/p&gt;</description></item><item><title>RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.05345</link><description>&lt;p&gt;
RTLLM&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;RTL&#29983;&#25104;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05345
&lt;/p&gt;
&lt;p&gt;
RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#37319;&#29992;LLMs&#36827;&#34892;&#25935;&#25463;&#30828;&#20214;&#35774;&#35745;&#65292;&#20363;&#22914;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#30446;&#26631;&#35774;&#35745;&#37117;&#30456;&#23545;&#31616;&#21333;&#19988;&#35268;&#27169;&#36739;&#23567;&#65292;&#24182;&#30001;&#20316;&#32773;&#33258;&#24049;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;LLMs&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#35774;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#35780;&#20272;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#30340;&#35774;&#35745;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RTLLM&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19977;&#20010;&#28176;&#36827;&#30446;&#26631;&#65292;&#21363;&#35821;&#27861;&#30446;&#26631;&#12289;&#21151;&#33021;&#30446;&#26631;&#21644;&#35774;&#35745;&#36136;&#37327;&#30446;&#26631;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#33258;&#21160;&#25552;&#20379;&#23545;&#20219;&#20309;&#32473;&#23450;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisin
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#22522;&#20934;&#32447;&#22312;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#21462;&#24471;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35813;&#22522;&#20934;&#32447;&#24182;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15422</link><description>&lt;p&gt;
&#19968;&#20010;Epoch&#23601;&#36275;&#22815;&#36827;&#34892;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?. (arXiv:2307.15422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15422
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20934;&#32447;&#22312;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#21462;&#24471;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35813;&#22522;&#20934;&#32447;&#24182;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#24494;&#35843;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;MF-HPO&#65289;&#21033;&#29992;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#20934;&#30830;&#24615;&#32423;&#21035;&#65292;&#24182;&#22312;&#23398;&#20064;&#26089;&#26399;&#20002;&#24323;&#20302;&#24615;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#22522;&#20934;&#25968;&#25454;&#19978;&#23558;&#21508;&#31181;&#20195;&#34920;&#24615;&#30340;MF-HPO&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20934;&#32447;&#26159;&#22312;&#35757;&#32451;&#20102;&#20165;&#19968;&#20010;Epoch&#21518;&#20002;&#24323;&#38500;Top-K&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#35757;&#32451;&#20197;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20010;&#22522;&#20934;&#32447;&#19982;&#20854;&#23545;&#24212;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#20998;&#26512;&#22522;&#20934;&#25968;&#25454;&#30340;&#23398;&#20064;&#26354;&#32447;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#20960;&#20010;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#36825;&#35299;&#37322;&#20102;&#25105;&#20204;&#22522;&#20934;&#32447;&#30340;&#25104;&#21151;&#12290;&#36825;&#34920;&#26126;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#65288;1&#65289;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20351;&#29992;&#24314;&#35758;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#65288;2&#65289;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models but can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on. We compared various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-K after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation. Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27969;&#24418;&#38468;&#36817;&#23545;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26102;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15288</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders. (arXiv:2307.15288v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27969;&#24418;&#38468;&#36817;&#23545;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26102;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#38477;&#38454;&#24314;&#27169;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20302;&#32500;&#27969;&#24418;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#36807;&#28193;&#21518;&#26465;&#20214;&#20013;&#23545;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#21021;&#22987;&#26465;&#20214;&#21644;&#20854;&#20182;&#24178;&#25200;&#30340;&#25928;&#24212;&#24050;&#32463;&#34928;&#20943;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#23454;&#26102;&#25511;&#21046;&#21644;&#39044;&#27979;&#24212;&#29992;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26469;&#35828;&#65292;&#24555;&#36895;&#21160;&#24577;&#21644;&#38750;&#27491;&#24120;&#25935;&#24863;&#26426;&#21046;&#30340;&#24433;&#21709;&#20351;&#24471;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#24320;&#22987;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#30340;&#19968;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#25237;&#24433;&#31867;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20351;&#29992;&#21487;&#36870;&#28608;&#27963;&#20989;&#25968;&#21644;&#23545;&#20598;&#26435;&#37325;&#30697;&#38453;&#65292;&#20197;&#30830;&#20445;&#32534;&#30721;&#22120;&#26159;&#35299;&#30721;&#22120;&#30340;&#24038;&#36870;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#20855;&#26377;&#21160;&#24577;&#24863;&#30693;&#24615;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#20419;&#36827;&#23398;&#20064;&#32771;&#34385;&#26012;&#25237;&#24433;&#32420;&#32500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed reduced-order modeling techniques aim to approximate nonlinear dynamical systems on low-dimensional manifolds learned from data. This is an effective approach for modeling dynamics in a post-transient regime where the effects of initial conditions and other disturbances have decayed. However, modeling transient dynamics near an underlying manifold, as needed for real-time control and forecasting applications, is complicated by the effects of fast dynamics and nonnormal sensitivity mechanisms. To begin to address these issues, we introduce a parametric class of nonlinear projections described by constrained autoencoder neural networks in which both the manifold and the projection fibers are learned from data. Our architecture uses invertible activation functions and biorthogonal weight matrices to ensure that the encoder is a left inverse of the decoder. We also introduce new dynamics-aware cost functions that promote learning of oblique projection fibers that account
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;</title><link>http://arxiv.org/abs/2307.05374</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#25552;&#39640;&#30456;&#24178;&#20809;&#31995;&#32479;&#20013;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05374
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#12290;&#19982;&#24120;&#35268;&#25968;&#23383;&#26102;&#38047;&#24674;&#22797;&#26041;&#27861;&#30456;&#27604;&#65292;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;
For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#20174;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#65292;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#25512;&#33616;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.03332</link><description>&lt;p&gt;
ACDNet&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#29992;&#20110;&#26377;&#25928;&#30340;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#20174;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#65292;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#22797;&#26434;&#30340;&#21307;&#30103;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20174;&#24739;&#32773;EHR&#20013;&#25552;&#21462;&#32437;&#21521;&#20449;&#24687;&#20197;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#24739;&#32773;&#34920;&#31034;&#65292;&#24182;&#24573;&#35270;&#20102;&#32771;&#34385;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACDNet&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#23618;&#38754;&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#12290;ACDNet&#36824;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#21033;&#29992;&#33647;&#29289;&#35760;&#24405;&#19982;&#33647;&#29289;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20419;&#36827;&#25512;&#33616;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22823;&#22411;&#21307;&#30103;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ACDNet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patient's medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, cle
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.03190</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#29983;&#25104;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;&#12290;&#22312;&#22788;&#29702;&#34394;&#26500;&#20803;&#32032;&#21644;&#33402;&#26415;&#39118;&#26684;&#30340;&#25552;&#31034;&#26102;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#21160;&#20316;&#30340;&#22797;&#26434;&#24615;&#12290;&#29616;&#26377;&#30340;&#21333;&#22270;&#21160;&#30011;&#26041;&#27861;&#22312;&#33402;&#26415;&#24615;&#36755;&#20837;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26041;&#27861;&#24120;&#24120;&#24341;&#20837;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#38590;&#20197;&#20351;&#26576;&#20123;&#21306;&#22495;&#20445;&#25345;&#38745;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#30340;&#24605;&#24819;&#65292;&#21363;&#33402;&#26415;&#22270;&#20687;&#21644;&#20854;&#19982;&#20687;&#32032;&#23545;&#40784;&#30340;&#33258;&#28982;&#22806;&#35266;&#37197;&#23545;&#12290;&#34429;&#28982;&#33402;&#26415;&#22270;&#20687;&#25551;&#32472;&#20102;&#25105;&#20204;&#22312;&#25991;&#26412;&#25552;&#31034;&#20013;&#35814;&#32454;&#25551;&#36848;&#30340;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20294;&#30495;&#23454;&#30340;&#23545;&#24212;&#22270;&#20687;&#22823;&#22823;&#31616;&#21270;&#20102;&#24067;&#23616;&#21644;&#21160;&#20316;&#20998;&#26512;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#20986;&#30495;&#23454;&#22270;&#20687;&#24182;&#26681;&#25454;&#35821;&#20041;&#20449;&#24687;&#39044;&#27979;&#20986;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.14131</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing. (arXiv:2306.14131v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#23545;&#20110;&#27979;&#35797;&#21644;&#39564;&#35777;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#22312;&#32500;&#24230;&#35781;&#21650;&#21644;&#25628;&#32034;&#31354;&#38388;&#30340;&#38480;&#21046;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#22330;&#26223;&#65292;&#20363;&#22914;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#25110;&#20462;&#25913;&#29616;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#21253;&#21547;&#39118;&#38505;&#21644;&#21487;&#34892;&#24615;&#30446;&#26631;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#34892;&#24615;&#30446;&#26631;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#30340;&#21487;&#33021;&#24615;&#65307;&#23427;&#24809;&#32602;&#29983;&#25104;&#19981;&#22826;&#21487;&#33021;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;</title><link>http://arxiv.org/abs/2306.08094</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807; ChatGPT &#23454;&#29616;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65311;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#22810;&#65292;&#21516;&#26102;&#20063;&#20984;&#26174;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22797;&#26434;&#30340;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#22686;&#21152;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2306.02426</link><description>&lt;p&gt;
&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#23427;&#20204;&#24517;&#39035;&#28385;&#36275;&#22810;&#20010;&#35201;&#27714;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#24809;&#32602;&#26469;&#38544;&#24335;&#22320;&#26045;&#21152;&#65292;&#25110;&#32773;&#36890;&#36807;&#22522;&#20110;Lagrangian&#23545;&#20598;&#30340;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#26174;&#24335;&#22320;&#26045;&#21152;&#12290;&#26080;&#35770;&#21738;&#31181;&#26041;&#24335;&#65292;&#25351;&#23450;&#35201;&#27714;&#37117;&#21463;&#21040;&#22949;&#21327;&#21644;&#26377;&#38480;&#30340;&#26377;&#20851;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#23454;&#38469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#26469;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21516;&#26102;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#30340;&#21516;&#26102;&#35843;&#25972;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#23427;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#20102;&#23398;&#20064;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#20855;&#26377;&#24377;&#24615;&#30340;&#32422;&#26463;&#23398;&#20064;&#65292;&#36825;&#26159;&#23545;&#29992;&#20110;&#25551;&#36848;&#29983;&#24577;&#31995;&#32479;&#30340;&#26415;&#35821;&#30340;&#19968;&#31181;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18427</link><description>&lt;p&gt;
GRD: &#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#29983;&#25104;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#21738;&#20123;&#29366;&#24577;-&#34892;&#21160;&#23545;&#24212;&#35813;&#23545;&#26410;&#26469;&#30340;&#20998;&#27493;&#22870;&#21169;&#36127;&#36131;&#12290;Return Decomposition&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#35266;&#27979;&#24207;&#21015;&#20013;&#30340;&#22870;&#21169;&#26469;&#20445;&#25345;&#31574;&#30053;&#19981;&#21464;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20197;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26500;&#24314;&#22870;&#21169;&#20877;&#20998;&#37197;&#65292;&#20294;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#22240;&#26524;&#36879;&#35270;&#26469;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#36129;&#29486;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#22312;&#36820;&#22238;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25551;&#36848;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22522;&#20110;&#36712;&#36857;&#30340;&#38271;&#26399;&#22238;&#25253;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRD&#39318;&#20808;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#28982;&#21518;&#21033;&#29992;&#30830;&#23450;&#30340;&#22240;&#26524;&#27169;&#22411;&#35745;&#31639;&#21487;&#35266;&#27979;&#22870;&#21169;&#30340;&#26399;&#26395;&#65292;&#36827;&#32780;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02725</link><description>&lt;p&gt;
FMG-Net&#21644;W-Net&#65306;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02725
&lt;/p&gt;
&lt;p&gt;
FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#23545;&#20110;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#21307;&#30103;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#38754;&#20020;&#30528;&#22788;&#29702;&#32454;&#31890;&#24230;&#29305;&#24449;&#21644;&#22270;&#20687;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#29305;&#21035;&#26126;&#26174;&#65292;&#20363;&#22914;BraTS&#22810;&#26631;&#31614;&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;&#36187;&#20013;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#31934;&#30830;&#22320;&#20998;&#21106;&#19981;&#21516;&#30340;&#32959;&#30244;&#20122;&#32452;&#20998;&#65292;&#22312;&#22823;&#23567;&#21644;&#24418;&#29366;&#19978;&#37117;&#26377;&#26174;&#33879;&#21464;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20063;&#20250;&#20135;&#29983;&#37325;&#22823;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26550;&#26500;&#65292;FMG-Net&#21644;W-Net&#65292;&#23427;&#20204;&#23558;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#21407;&#29702;&#32435;&#20837;CNN&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FMG-Net&#21644;W-Net&#37117;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;U-Net&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#20013;&#30340;tum&#30340;&#31934;&#24230;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#25110;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540;&#34987;&#38480;&#21046;&#22312; Lipschitz &#24120;&#25968;&#20869;&#65292;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01828</link><description>&lt;p&gt;
&#23398;&#20064;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Stable and Robust Linear Parameter-Varying State-Space Models. (arXiv:2304.01828v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01828
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31283;&#23450;&#21644;&#40065;&#26834;&#30340;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#25110;&#36890;&#36807;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540;&#34987;&#38480;&#21046;&#22312; Lipschitz &#24120;&#25968;&#20869;&#65292;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#31283;&#23450;&#21644;&#40065;&#26834;&#32447;&#24615;&#21442;&#25968;&#21487;&#21464;&#29366;&#24577;&#31354;&#38388; (LPV-SS) &#27169;&#22411;&#12290;&#27169;&#22411;&#21442;&#25968;&#21270;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#35777;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#27169;&#22411;&#37117;&#26159;&#31283;&#23450;&#30340;&#65292;&#20855;&#26377;&#25910;&#32553;&#24847;&#20041;&#65292;&#25110;&#20854; Lipschitz &#24120;&#25968;&#34987;&#29992;&#25143;&#23450;&#20041;&#30340;&#20540; $\gamma$ &#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21442;&#25968;&#21270;&#26159;&#30452;&#25509;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#26080;&#32422;&#26463;&#20248;&#21270;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#23646;&#20110; LPV-SS &#31867;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#36827;&#19968;&#27493;&#30340;&#20984;&#20998;&#26512;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312; LPV &#35782;&#21035;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two direct parameterizations of stable and robust linear parameter-varying state-space (LPV-SS) models. The model parametrizations guarantee a priori that for all parameter values during training, the allowed models are stable in the contraction sense or have their Lipschitz constant bounded by a user-defined value $\gamma$. Furthermore, since the parametrizations are direct, the models can be trained using unconstrained optimization. The fact that the trained models are of the LPV-SS class makes them useful for, e.g., further convex analysis or controller design. The effectiveness of the approach is demonstrated on an LPV identification problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#24212;&#29992;&#20102;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.01525</link><description>&lt;p&gt;
&#24102;&#23545;&#25163;&#30340;&#22312;&#32447;&#23398;&#20064;&#65306;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Adversaries: A Differential Inclusion Analysis. (arXiv:2304.01525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#24212;&#29992;&#20102;&#24494;&#20998;&#21253;&#23481;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#27979;&#37327;&#27169;&#22411; $Y = AX$&#65292;&#20854;&#20013; $X$ &#21644; $Y$ &#26159;&#38543;&#26426;&#21464;&#37327;&#65292;$A$ &#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#39640;&#30697;&#38453;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#23454;&#20363;&#65292;&#21487;&#20197;&#33719;&#24471; $Y$ &#30340;&#19968;&#20010;&#22352;&#26631;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#36890;&#36807;&#36825;&#20123;&#26679;&#26412;&#20272;&#35745; $\mu := \mathbb{E}[X]$&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#65306;&#23567;&#20294;&#26410;&#30693;&#30340; $Y$ &#30340;&#22352;&#26631;&#23376;&#38598;&#30001;&#23545;&#25163;&#25511;&#21046;&#65292;&#24182;&#20855;&#26377;&#26080;&#38480;&#30340;&#33021;&#21147;&#65306;&#27599;&#27425;&#26597;&#35810;&#26679;&#26412;&#26102;&#65292;&#20182;&#20204;&#21487;&#20197;&#36820;&#22238;&#20219;&#20309;&#23454;&#25968;&#12290;&#23545;&#20110;&#36825;&#31181;&#23545;&#25239;&#24615;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20960;&#20046;&#30830;&#23450;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040; $\mu$ &#30340;&#24322;&#27493;&#22312;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#20998;&#21253;&#23481;&#22522;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20004;&#20010;&#20851;&#38190;&#20142;&#28857;&#21253;&#25324;&#65306;(a) &#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340; Lyapunov &#20989;&#25968;&#26469;&#35777;&#26126; $\mu$ &#26159;&#25105;&#20204;&#31639;&#27861;&#26497;&#38480;&#21160;&#24577;&#30340;&#21807;&#19968;&#20840;&#23616;&#21560;&#24341;&#23376;&#65292;(b) &#20351;&#29992;&#38789;&#21644;&#20572;&#26102;&#29702;&#35770;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are random variables and $A$ is an a priori known tall matrix. At each time instance, a sample of one of $Y$'s coordinates is available, and the goal is to estimate $\mu := \mathbb{E}[X]$ via these samples. However, the challenge is that a small but unknown subset of $Y$'s coordinates are controlled by adversaries with infinite power: they can return any real number each time they are queried for a sample. For such an adversarial setting, we propose the first asynchronous online algorithm that converges to $\mu$ almost surely. We prove this result using a novel differential inclusion based two-timescale analysis. Two key highlights of our proof include: (a) the use of a novel Lyapunov function for showing that $\mu$ is the unique global attractor for our algorithm's limiting dynamics, and (b) the use of martingale and stopping time theory to show that our algorithm's iterates are almost surely bounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#36890;&#29992;&#24191;&#20041;&#32447;&#24615;&#23545;&#25239;&#24615;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#21338;&#23572;&#36798;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#8220;&#20808;&#25506;&#32034;&#20877;&#25191;&#34892;&#8221;&#31639;&#27861;&#36991;&#20813;&#20102;&#22256;&#38590;&#30340;&#21518;&#24724;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.08816</link><description>&lt;p&gt;
Borda Regret Minimization for Generalized Linear Dueling Bandits (&#36890;&#29992;&#24191;&#20041;&#32447;&#24615;&#23545;&#25239;&#24615;&#25490;&#21517;&#38382;&#39064;&#30340;&#21338;&#23572;&#36798;&#21518;&#24724;&#26368;&#23567;&#21270;&#31639;&#27861;)
&lt;/p&gt;
&lt;p&gt;
Borda Regret Minimization for Generalized Linear Dueling Bandits. (arXiv:2303.08816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36890;&#29992;&#24191;&#20041;&#32447;&#24615;&#23545;&#25239;&#24615;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#21338;&#23572;&#36798;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#8220;&#20808;&#25506;&#32034;&#20877;&#25191;&#34892;&#8221;&#31639;&#27861;&#36991;&#20813;&#20102;&#22256;&#38590;&#30340;&#21518;&#24724;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25490;&#21517;&#38382;&#39064;(Dueling bandits)&#24120;&#34987;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#25490;&#21517;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#23545;&#25239;&#24615;&#25490;&#21517;&#38382;&#39064;&#20013;&#21338;&#23572;&#36798;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#30830;&#23450;&#20855;&#26377;&#26368;&#39640;&#21338;&#23572;&#36798;&#24471;&#20998;&#30340;&#39033;&#30446;&#65292;&#24182;&#21516;&#26102;&#26368;&#23567;&#21270;&#32047;&#35745;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#36890;&#29992;&#24191;&#20041;&#32447;&#24615;&#23545;&#25239;&#24615;&#25490;&#21517;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21338;&#23572;&#36798;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#26159;&#22256;&#38590;&#30340;&#12290; &#25105;&#20204;&#35777;&#26126;&#20102;&#28176;&#36817;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#21518;&#24724;&#19979;&#38480;&#26159;$\Omega(d^{2/3} T^{2/3})$&#65292;&#20854;&#20013;$d$&#26159;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#32500;&#25968;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;&#20026;&#20102;&#36798;&#21040;&#19979;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#20808;&#25506;&#32034;&#20877;&#25191;&#34892;"&#30340;&#31639;&#27861;&#65292;&#23427;&#20855;&#26377;&#20960;&#20046;&#21305;&#37197;&#30340;&#19978;&#38480;&#22238;&#24402;&#35823;&#24046;$\tilde{O}(d^{2/3} T^{2/3})$&#12290;&#24403;&#39033;&#30446;&#25968;&#37327;$K$&#24456;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#36229;&#21442;&#25968;&#20197;&#36798;&#21040;&#26356;&#23567;&#30340;&#21518;&#24724;$\tilde{O}((d\log K)^{1/3}T^{2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Dueling bandits are widely used to model preferential feedback that is prevalent in machine learning applications such as recommendation systems and ranking. In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret. We propose a new and highly expressive generalized linear dueling bandits model, which covers many existing models. Surprisingly, the Borda regret minimization problem turns out to be difficult, as we prove a regret lower bound of order $\Omega(d^{2/3} T^{2/3})$, where $d$ is the dimension of contextual vectors and $T$ is the time horizon. To attain the lower bound, we propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound $\tilde{O}(d^{2/3} T^{2/3})$. When the number of items/arms $K$ is small, our algorithm can achieve a smaller regret $\tilde{O}( (d \log K)^{1/3} T^{2/3})$ with proper choices of hyperparamete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06280</link><description>&lt;p&gt;
&#25506;&#31350;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.
&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#26497;&#20026;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#65292;&#35797;&#22270;&#38450;&#24481;&#26356;&#21463;&#38480;&#21046;&#30340;&#40657;&#30418;&#25915;&#20987;&#32773;&#12290;&#36825;&#20123;&#38450;&#24481;&#36890;&#36807;&#36319;&#36394;&#20256;&#20837;&#27169;&#22411;&#26597;&#35810;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#25298;&#32477;&#37027;&#20123;&#21487;&#30097;&#22320;&#30456;&#20284;&#30340;&#26597;&#35810;&#26469;&#25805;&#20316;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;Blacklight&#26159;&#22312;USENIX Security '22&#19978;&#25552;&#20986;&#30340;&#65292;&#22768;&#31216;&#21487;&#20197;&#38450;&#27490;&#20960;&#20046;100&#65285;&#30340;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#35843;&#25972;&#29616;&#26377;&#40657;&#30418;&#25915;&#20987;&#30340;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#21463;Blacklight&#20445;&#25252;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;CIFAR10&#19978;&#20174;82.2&#65285;&#38477;&#33267;6.4&#65285;&#65289;&#12290;&#21463;&#21040;&#36825;&#19968;&#24778;&#20154;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#31995;&#32479;&#21270;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#20250;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;82.2&#65285;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;76.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.00196</link><description>&lt;p&gt;
&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#21487;&#20197;&#24110;&#21161;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#22810;&#36890;&#36947;&#25968;&#25454;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36716;&#25442;&#22495;&#20013;&#30340;&#20302;&#31209;&#24615;&#65292;&#21363;&#36716;&#25442;&#30340;&#20302;&#31209;&#24615;&#65292;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#22312;&#22810;&#36890;&#36947;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#24182;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#20989;&#25968;&#34920;&#31034;&#65292;&#22914;&#20855;&#26377;t-&#20056;&#31215;&#23618;&#65288;t-NNs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;t-SVD&#29702;&#35770;&#19978;&#22914;&#20309;&#24433;&#21709;t-NNs&#30340;&#23398;&#20064;&#34892;&#20026;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#36890;&#36807;&#25512;&#23548;&#26631;&#20934;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;t-NNs&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23613;&#31649;t-NNs&#24456;&#23569;&#20855;&#26377;&#23436;&#20840;&#36716;&#25442;&#30340;&#20302;&#31209;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#27969;&#65288;GF&#65289;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;t-NNs&#20855;&#26377;ReLU
&lt;/p&gt;
&lt;p&gt;
Achieving efficient and robust multi-channel data learning is a challenging task in data science. By exploiting low-rankness in the transformed domain, i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has achieved extensive success in multi-channel data representation and has recently been extended to function representation such as Neural Networks with t-product layers (t-NNs). However, it still remains unclear how t-SVD theoretically affects the learning behavior of t-NNs. This paper is the first to answer this question by deriving the upper bounds of the generalization error of both standard and adversarially trained t-NNs. It reveals that the t-NNs compressed by exact transformed low-rank parameterization can achieve a sharper adversarial generalization bound. In practice, although t-NNs rarely have exactly transformed low-rank weights, our analysis further shows that by adversarial training with gradient flow (GF), the over-parameterized t-NNs with ReLU 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.14040</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#22815;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#65288;NFN&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#32593;&#32476;&#32534;&#36753;&#21644;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#35774;&#35745;&#22788;&#29702;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#30340;&#26377;&#25928;&#26550;&#26500;&#30340;&#32479;&#19968;&#21407;&#21017;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31216;&#24615;&#30340;&#35270;&#35282;&#26469;&#35774;&#35745;&#31070;&#32463;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20851;&#27880;&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#26435;&#37325;&#20013;&#20986;&#29616;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#22240;&#20026;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#27809;&#26377;&#22266;&#26377;&#39034;&#24207;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#26469;&#32422;&#26463;&#20026;&#32622;&#25442;&#31561;&#21464;&#30340;NF-Layers&#65288;&#31070;&#32463;&#21151;&#33021;&#23618;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#22312;&#22270;&#20687;&#20013;&#30452;&#25509;&#25511;&#21046;&#23545;&#35937;&#30340;&#25918;&#32622;&#12290;&#36890;&#36807;&#35266;&#23519;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#24341;&#20837;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#29305;&#23450;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22330;&#26223;&#32452;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.13153</link><description>&lt;p&gt;
&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;: &#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#23454;&#29616;&#23545;&#35937;&#25918;&#32622;&#30340;&#30452;&#25509;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#23548;&#21521;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880;&#24341;&#23548;&#22312;&#22270;&#20687;&#20013;&#30452;&#25509;&#25511;&#21046;&#23545;&#35937;&#30340;&#25918;&#32622;&#12290;&#36890;&#36807;&#35266;&#23519;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#24341;&#20837;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#29305;&#23450;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22330;&#26223;&#32452;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;DALLE-2&#12289;Imagen&#21644;&#31283;&#23450;&#25193;&#25955;&#65289;&#33021;&#22815;&#20165;&#36890;&#36807;&#25551;&#36848;&#25152;&#38656;&#22270;&#20687;&#20869;&#23481;&#30340;&#31616;&#30701;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21508;&#31181;&#24418;&#24335;&#30340;&#22270;&#20687;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#32452;&#21512;&#21253;&#21547;&#22810;&#20010;&#20851;&#38190;&#23545;&#35937;&#65288;&#22914;&#25351;&#23450;&#20301;&#32622;&#20851;&#31995;&#20013;&#30340;&#23383;&#31526;&#65289;&#30340;&#22330;&#26223;&#12290;&#22312;&#35762;&#36848;&#25925;&#20107;&#20013;&#65292;&#33021;&#22815;&#8220;&#30452;&#25509;&#8221;&#25351;&#23548;&#20154;&#29289;&#21644;&#23545;&#35937;&#30340;&#25918;&#32622;&#65292;&#26080;&#35770;&#26159;&#22312;&#22270;&#20687;&#20869;&#36824;&#26159;&#36328;&#22270;&#20687;&#20869;&#65292;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19968;&#28857;&#22312;&#30005;&#24433;&#21644;&#21160;&#30011;&#29702;&#35770;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29305;&#21035;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25552;&#20379;&#25152;&#38656;&#30340;&#25351;&#23548;&#12290;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#25552;&#31034;&#35789;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#21453;&#26144;&#20986;&#30001;&#36825;&#20123;&#35789;&#25152;&#34920;&#31034;&#30340;&#23545;&#35937;&#30340;&#31354;&#38388;&#24067;&#23616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#36825;&#20123;&#20132;&#21449;&#27880;&#24847;&#21147;&#26144;&#23556;&#20013;&#30340;&#26399;&#26395;&#20301;&#32622;&#20135;&#29983;&#8220;&#28608;&#27963;&#8221;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#26159;&#36890;&#21521;&#27867;&#21270;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to "direct" the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.09167</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#30340;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#19982;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;&#22312;&#32531;&#35299;&#24403;&#21069;&#25317;&#22581;&#31243;&#24230;&#26041;&#38754;&#24050;&#32463;&#22833;&#25928;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#36827;&#34892;&#20132;&#36890;&#25511;&#21046;&#30340;&#24819;&#27861;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#32423;&#21035;&#33258;&#20027;&#24615;&#36710;&#36742;&#30340;&#19981;&#26029;&#28044;&#29616;&#12290;&#36825;&#24341;&#36215;&#20102;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36710;&#36742;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35843;&#33410;&#20154;&#39550;&#39542;&#36710;&#36742;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;1&#65289;&#22270;&#20687;&#36890;&#36807;&#21355;&#26143;&#22270;&#20687;&#12289;&#36710;&#20869;&#25668;&#20687;&#31995;&#32479;&#21644;&#20132;&#36890;&#30417;&#25511;&#31995;&#32479;&#26222;&#36941;&#23384;&#22312;&#65307;2&#65289;&#22270;&#20687;&#19981;&#38656;&#35201;&#26356;&#26032;&#29616;&#26377;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#21521;&#21487;&#33021;&#19981;&#24895;&#24847;&#37197;&#21512;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#20256;&#36882;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;(DPMs)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#21644;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23569;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2302.04867</link><description>&lt;p&gt;
UniPC&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. (arXiv:2302.04867v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;(DPMs)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#21644;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23569;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21435;&#22122;&#32593;&#32476;&#30340;&#22810;&#27425;&#35780;&#20272;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;DPM&#20013;&#36827;&#34892;&#37319;&#26679;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#27492;&#21152;&#36895;DPM&#37319;&#26679;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#35774;&#35745;&#20986;&#20102;&#24555;&#36895;&#37319;&#26679;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#26080;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#24212;&#29992;&#26356;&#38738;&#30544;&#20110;&#36739;&#23569;&#30340;&#27493;&#39588;(&#22914;&lt;10)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#29616;&#26377;&#30340;DPM&#37319;&#26679;&#22120;&#20043;&#21518;&#24212;&#29992;&#65292;&#25552;&#39640;&#31934;&#30830;&#24230;&#30340;&#38454;&#25968;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#25903;&#25345;&#20219;&#24847;&#38454;&#25968;&#30340;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#20316;&#20026;&#21103;&#20135;&#21697;&#12290;&#36890;&#36807;&#32467;&#21512;UniP&#21644;UniC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;DPMs&#65292;&#23427;&#20855;&#26377;&#20219;&#24847;&#38454;&#25968;&#30340;&#32479;&#19968;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $&lt;$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#23558;&#31169;&#23494;&#29992;&#25143;&#25968;&#25454;&#36879;&#38706;&#32473;&#36828;&#31243;&#26381;&#21153;&#22120;&#12290;&#36890;&#36807;&#21152;&#36895;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;&#24182;&#19982;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20849;&#21516;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.10904</link><description>&lt;p&gt;
&#22522;&#20110;GPU&#30340;&#35774;&#22791;&#19978;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#30340;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPU-based Private Information Retrieval for On-Device Machine Learning Inference. (arXiv:2301.10904v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#23558;&#31169;&#23494;&#29992;&#25143;&#25968;&#25454;&#36879;&#38706;&#32473;&#36828;&#31243;&#26381;&#21153;&#22120;&#12290;&#36890;&#36807;&#21152;&#36895;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;&#24182;&#19982;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20849;&#21516;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#21487;&#20197;&#22312;&#19981;&#23558;&#29992;&#25143;&#25968;&#25454;&#36879;&#38706;&#32473;&#36828;&#31243;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#31169;&#23494;&#29992;&#25143;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32431;&#35774;&#22791;&#19978;&#30340;&#31169;&#23494;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#35768;&#22810;&#20381;&#36182;&#20110;&#23884;&#20837;&#24335;&#34920;&#30340;&#24212;&#29992;&#26469;&#35828;&#26159;&#19981;&#23454;&#38469;&#30340;&#65292;&#36825;&#20123;&#23884;&#20837;&#24335;&#34920;&#30340;&#22823;&#23567;&#22826;&#22823;&#26080;&#27861;&#23384;&#20648;&#22312;&#35774;&#22791;&#19978;&#12290;&#29305;&#21035;&#26159;&#65292;&#25512;&#33616;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22810;&#20010;&#23884;&#20837;&#24335;&#34920;&#65292;&#27599;&#20010;&#34920;&#22823;&#32422;1-10GB&#30340;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#35774;&#22791;&#19978;&#21464;&#24471;&#19981;&#23454;&#38469;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31169;&#23494;&#20449;&#24687;&#26816;&#32034;&#65288;PIR&#65289;&#20174;&#26381;&#21153;&#22120;&#26377;&#25928;&#19988;&#31169;&#23494;&#22320;&#26816;&#32034;&#23884;&#20837;&#24335;&#34920;&#65292;&#32780;&#19981;&#20849;&#20139;&#20219;&#20309;&#31169;&#23494;&#20449;&#24687;&#12290;&#30001;&#20110;&#29616;&#25104;&#30340;PIR&#31639;&#27861;&#36890;&#24120;&#23545;&#20110;&#24310;&#36831;&#25935;&#24863;&#30340;&#25512;&#26029;&#20219;&#21153;&#26469;&#35828;&#35745;&#31639;&#37327;&#36807;&#22823;&#65292;&#25105;&#20204;1&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;PIR&#21152;&#36895;&#31574;&#30053;&#65292;&#24182;&#19988;2&#65289;&#19982;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20849;&#21516;&#35774;&#35745;PIR&#65292;&#20197;&#33719;&#24471;&#36827;&#19968;&#27493;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;GPU&#21152;&#36895;&#31574;&#30053;&#23558;&#31995;&#32479;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#36229;&#36807;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device machine learning (ML) inference can enable the use of private user data on user devices without revealing them to remote servers. However, a pure on-device solution to private ML inference is impractical for many applications that rely on embedding tables that are too large to be stored on-device. In particular, recommendation models typically use multiple embedding tables each on the order of 1-10 GBs of data, making them impractical to store on-device. To overcome this barrier, we propose the use of private information retrieval (PIR) to efficiently and privately retrieve embeddings from servers without sharing any private information. As off-the-shelf PIR algorithms are usually too computationally intensive to directly use for latency-sensitive inference tasks, we 1) propose novel GPU-based acceleration of PIR, and 2) co-design PIR with the downstream ML application to obtain further speedup. Our GPU acceleration strategy improves system throughput by more than $20 \times$
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#25968;&#25454;&#31934;&#28860;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#65292;&#20197;&#21450;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#24212;&#29992;&#12290;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#26550;&#26500;&#25628;&#32034;&#31561;&#22330;&#26223;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.04272</link><description>&lt;p&gt;
&#25968;&#25454;&#31934;&#28860;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Distillation: A Survey. (arXiv:2301.04272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#25968;&#25454;&#31934;&#28860;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#65292;&#20197;&#21450;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#24212;&#29992;&#12290;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#26550;&#26500;&#25628;&#32034;&#31561;&#22330;&#26223;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#27969;&#34892;&#23548;&#33268;&#20102;&#22823;&#37327;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;&#25972;&#29702;&#12290;&#23613;&#31649;&#22312;&#20010;&#21035;&#20219;&#21153;&#19978;&#34920;&#29616;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#24222;&#22823;&#30340;&#27169;&#22411;&#38754;&#20020;&#22810;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22914;&#65288;a&#65289;&#39640;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#65307;&#65288;b&#65289;&#24930;&#30340;&#30740;&#31350;&#36845;&#20195;&#65307;&#21644;&#65288;c&#65289;&#24046;&#30340;&#29983;&#24577;&#21487;&#25345;&#32493;&#24615;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;&#26088;&#22312;&#21512;&#25104;&#31616;&#27905;&#30340;&#25968;&#25454;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#21487;&#20197;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#26550;&#26500;&#25628;&#32034;&#31561;&#22330;&#26223;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#31934;&#28860;&#30340;&#19968;&#20010;&#24418;&#24335;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#35814;&#32454;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65288;&#25512;&#33616;&#31995;&#32479;&#65289;&#65292;&#21516;&#26102;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.
&lt;/p&gt;</description></item><item><title>VeriX&#26159;&#19968;&#20010;&#21487;&#20197;&#20135;&#29983;&#26368;&#20248;&#20581;&#22766;&#35299;&#37322;&#21644;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;&#26463;&#27714;&#35299;&#25216;&#26415;&#21644;&#22522;&#20110;&#29305;&#24449;&#32423;&#25935;&#24863;&#24615;&#25490;&#21517;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#20027;&#39134;&#26426;&#28369;&#34892;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01051</link><description>&lt;p&gt;
VeriX: &#38754;&#21521;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#39564;&#35777;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VeriX: Towards Verified Explainability of Deep Neural Networks. (arXiv:2212.01051v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01051
&lt;/p&gt;
&lt;p&gt;
VeriX&#26159;&#19968;&#20010;&#21487;&#20197;&#20135;&#29983;&#26368;&#20248;&#20581;&#22766;&#35299;&#37322;&#21644;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;&#26463;&#27714;&#35299;&#25216;&#26415;&#21644;&#22522;&#20110;&#29305;&#24449;&#32423;&#25935;&#24863;&#24615;&#25490;&#21517;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#20027;&#39134;&#26426;&#28369;&#34892;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VeriX&#65288;Verified eXplainability&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#26368;&#20248;&#20581;&#22766;&#35299;&#37322;&#21644;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#27714;&#35299;&#25216;&#26415;&#21644;&#22522;&#20110;&#29305;&#24449;&#32423;&#25935;&#24863;&#24615;&#25490;&#21517;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36845;&#20195;&#22320;&#26500;&#24314;&#36825;&#26679;&#30340;&#35299;&#37322;&#21644;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#21644;&#33258;&#20027;&#39134;&#26426;&#28369;&#34892;&#30340;&#23454;&#38469;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present VeriX (Verified eXplainability), a system for producing optimal robust explanations and generating counterfactuals along decision boundaries of machine learning models. We build such explanations and counterfactuals iteratively using constraint solving techniques and a heuristic based on feature-level sensitivity ranking. We evaluate our method on image recognition benchmarks and a real-world scenario of autonomous aircraft taxiing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#30340;&#26174;&#24335;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.00211</link><description>&lt;p&gt;
&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process. (arXiv:2212.00211v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#32479;&#19968;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#30340;&#26174;&#24335;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23398;&#20064;&#20016;&#23500;&#30340;&#25216;&#33021;&#32780;&#26080;&#38656;&#22806;&#37096;&#22870;&#21169;&#30417;&#30563;&#26159;&#19968;&#20010;&#21069;&#27839;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#21464;&#20998;&#21644;&#25289;&#26222;&#25289;&#26031;&#22522;&#20110;&#25216;&#33021;&#65288;&#21448;&#31216;&#20026;&#36873;&#39033;&#65289;&#21457;&#29616;&#12290;&#21069;&#32773;&#36890;&#36807;&#20114;&#20449;&#24687;&#25439;&#22833;&#26368;&#22823;&#21270;&#21457;&#29616;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#29366;&#24577;&#31354;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#32780;&#21518;&#32773;&#20391;&#37325;&#20110;&#36890;&#36807;&#22686;&#21152;&#25506;&#32034;&#36807;&#31243;&#20013;&#30340;&#36830;&#25509;&#24615;&#26469;&#25552;&#39640;&#36873;&#39033;&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#26032;&#22411;&#20351;&#29992;&#26469;&#37327;&#21270;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#24615;&#65292;&#24182;&#26174;&#24335;&#20248;&#21270;&#26080;&#30417;&#30563;&#36873;&#39033;&#21457;&#29616;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#23450;&#20041;DPP&#26680;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#36712;&#36857;&#20013;&#30340;&#26399;&#26395;&#27169;&#25968;&#20316;&#20026;&#30446;&#26631;&#65292;&#26469;&#25429;&#25417;&#21644;&#22686;&#24378;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rich skills through temporal abstractions without supervision of external rewards is at the frontier of Reinforcement Learning research. Existing works mainly fall into two distinctive categories: variational and Laplacian-based skill (a.k.a., option) discovery. The former maximizes the diversity of the discovered options through a mutual information loss but overlooks coverage of the state space, while the latter focuses on improving the coverage of options by increasing connectivity during exploration, but does not consider diversity. In this paper, we propose a unified framework that quantifies diversity and coverage through a novel use of the Determinantal Point Process (DPP) and enables unsupervised option discovery explicitly optimizing both objectives. Specifically, we define the DPP kernel matrix with the Laplacian spectrum of the state transition graph and use the expected mode number in the trajectories as the objective to capture and enhance both diversity and cover
&lt;/p&gt;</description></item><item><title>Geodesic Sinkhorn&#26159;&#19968;&#31181;&#22522;&#20110;&#28024;&#28070;&#26354;&#38754;&#19978;&#28909;&#26680;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#24555;&#36895;&#20934;&#30830;&#22320;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31232;&#30095;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#28909;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;$O(n\log n)$&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#32500;&#21333;&#32454;&#32990;&#25968;&#25454;&#30340;&#22810;&#20010;&#20998;&#24067;&#30340;&#20960;&#20309;&#20013;&#24515;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2211.00805</link><description>&lt;p&gt;
&#22522;&#20110;&#28024;&#28070;&#26354;&#38754;&#19978;&#28909;&#26680;&#25193;&#25955;&#30340;Geodesic Sinkhorn&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Geodesic Sinkhorn for Fast and Accurate Optimal Transport on Manifolds. (arXiv:2211.00805v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00805
&lt;/p&gt;
&lt;p&gt;
Geodesic Sinkhorn&#26159;&#19968;&#31181;&#22522;&#20110;&#28024;&#28070;&#26354;&#38754;&#19978;&#28909;&#26680;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#24555;&#36895;&#20934;&#30830;&#22320;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31232;&#30095;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#28909;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;$O(n\log n)$&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#32500;&#21333;&#32454;&#32990;&#25968;&#25454;&#30340;&#22810;&#20010;&#20998;&#24067;&#30340;&#20960;&#20309;&#20013;&#24515;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#39640;&#25928;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;Sinkhorn&#26041;&#27861;&#26159;&#36825;&#31181;&#35745;&#31639;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;$O(n^2)$&#30340;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;Sinkhorn&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27969;&#24418;&#32467;&#26500;&#31185;&#23398;&#25968;&#25454;&#30340;&#26222;&#21450;&#65292;&#32771;&#34385;&#28024;&#28070;&#22320;&#38754;&#36317;&#31163;&#24120;&#24120;&#26159;&#38656;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28024;&#28070;&#26354;&#38754;&#19978;&#25193;&#25955;&#28909;&#26680;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Geodesic Sinkhorn&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Geodesic Sinkhorn&#21482;&#38656;&#35201;$O(n\log n)$&#30340;&#35745;&#31639;&#65292;&#22240;&#20026;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#31232;&#30095;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#28909;&#26680;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#35745;&#31639;&#21270;&#30103;&#24739;&#32773;&#26679;&#26412;&#30340;&#39640;&#32500;&#21333;&#32454;&#32990;&#25968;&#25454;&#30340;&#22810;&#20010;&#20998;&#24067;&#30340;&#20960;&#20309;&#20013;&#24515;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#20960;&#20309;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20026;&#36825;&#20004;&#20010;&#20960;&#20309;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient computation of optimal transport distance between distributions is of growing importance in data science. Sinkhorn-based methods are currently the state-of-the-art for such computations, but require $O(n^2)$ computations. In addition, Sinkhorn-based methods commonly use an Euclidean ground distance between datapoints. However, with the prevalence of manifold structured scientific data, it is often desirable to consider geodesic ground distance. Here, we tackle both issues by proposing Geodesic Sinkhorn -- based on diffusing a heat kernel on a manifold graph. Notably, Geodesic Sinkhorn requires only $O(n\log n)$ computation, as we approximate the heat kernel with Chebyshev polynomials based on the sparse graph Laplacian. We apply our method to the computation of barycenters of several distributions of high dimensional single cell data from patient samples undergoing chemotherapy. In particular, we define the barycentric distance as the distance between two such barycenters. Us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12566</link><description>&lt;p&gt;
&#36890;&#36807;Q-learning&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Continuous Control via Q-learning. (arXiv:2210.12566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;actor-critic&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#31616;&#21333;&#30340;critic-only&#26041;&#27861;&#22914;Q-learning&#22312;&#28041;&#21450;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#26102;&#24212;&#29992;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;actor-critic&#26041;&#27861;&#30340;&#25104;&#26412;&#26159;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65306;&#31283;&#23450;&#24615;&#21551;&#21457;&#24335;&#12289;&#35745;&#31639;&#35201;&#27714;&#21644;&#26356;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;critic-only&#26041;&#27861;&#22312;&#20174;&#29305;&#24449;&#25110;&#20687;&#32032;&#23398;&#20064;&#26102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#21512;&#20316;MARL&#30340;&#32463;&#20856;&#36172;&#24466;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#25552;&#20379;&#30452;&#35266;&#24863;&#35273;&#30340;&#65292;&#23637;&#31034;&#20102;&#35299;&#32806;&#30340;critics&#22914;&#20309;&#21033;&#29992;&#29366;&#24577;&#20449;&#24687;&#21327;&#35843;&#32852;&#21512;&#20248;&#21270;&#65292;&#24182;&#34920;&#29616;&#20986;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a var
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#26356;&#26032;&#65292;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09921</link><description>&lt;p&gt;
&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09921
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#26356;&#26032;&#65292;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#20013;&#65292;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26368;&#23454;&#38469;&#30340;&#21333;&#26102;&#38388;&#23610;&#24230;&#24418;&#24335;&#19979;&#65292;&#20854;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#20173;&#28982;&#19981;&#22815;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20998;&#26512;&#24037;&#20316;&#20165;&#38480;&#20110;&#31616;&#21270;&#30340;i.i.d.&#37319;&#26679;&#25110;&#34920;&#26684;&#35774;&#32622;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#23454;&#38469;&#30340;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#35780;&#35770;&#23478;&#37319;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#27599;&#20010;&#28436;&#21592;&#27493;&#39588;&#20013;&#20351;&#29992;&#21333;&#20010;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#12290;&#20808;&#21069;&#30340;&#20998;&#26512;&#26080;&#27861;&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#23454;&#29616;&#25910;&#25947;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#22312;&#32447;&#21333;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#33021;&#22815;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#65292;&#32780;&#22312;i.i.d.&#37319;&#26679;&#19979;&#65292;&#36825;&#20010;&#22797;&#26434;&#24230;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20026;$\mathcal{O}(\epsilon^{-2})$&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24573;&#30053;&#39118;&#26684;&#21464;&#21270;&#65292;&#19987;&#27880;&#20110;&#20869;&#23481;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#29615;&#22659;&#26631;&#31614;&#25552;&#39640;&#20102;&#23545;&#27604;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.03103</link><description>&lt;p&gt;
&#29615;&#22659;&#24863;&#30693;&#24322;&#24120;&#26816;&#27979;&#65306;&#24573;&#30053;&#39118;&#26684;&#21464;&#21270;&#65292;&#19987;&#27880;&#20110;&#20869;&#23481;&#65281;
&lt;/p&gt;
&lt;p&gt;
Env-Aware Anomaly Detection: Ignore Style Changes, Stay True to Content!. (arXiv:2210.03103v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24573;&#30053;&#39118;&#26684;&#21464;&#21270;&#65292;&#19987;&#27880;&#20110;&#20869;&#23481;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#29615;&#22659;&#26631;&#31614;&#25552;&#39640;&#20102;&#23545;&#27604;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#35268;&#33539;&#21644;&#22522;&#20934;&#65292;&#24212;&#29992;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;iWildCam&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#20026;&#35270;&#35273;&#25968;&#25454;&#25552;&#20986;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#24863;&#30693;&#30340;&#26041;&#27861;&#27604;&#22522;&#26412;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#27604;&#26041;&#27861;&#30340;&#27491;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#32771;&#34385;&#20102;&#29615;&#22659;&#26631;&#31614;&#65292;&#23558;ERM&#22522;&#32447;&#24471;&#20998;&#25552;&#39640;&#20102;8.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a formalization and benchmark for the unsupervised anomaly detection task in the distribution-shift scenario. Our work builds upon the iWildCam dataset, and, to the best of our knowledge, we are the first to propose such an approach for visual data. We empirically validate that environment-aware methods perform better in such cases when compared with the basic Empirical Risk Minimization (ERM). We next propose an extension for generating positive samples for contrastive methods that considers the environment labels when training, improving the ERM baseline score by 8.7%.
&lt;/p&gt;</description></item><item><title>&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.01905</link><description>&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#20540;&#20998;&#31867;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01905
&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26497;&#21270;&#32534;&#30721;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20998;&#31867;&#21644;&#25968;&#20540;&#22411;$[0,1]$&#20540;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#24456;&#22909;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#31639;&#27861;&#37197;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#65292;&#38750;&#24120;&#31616;&#21333;&#26131;&#29992;&#24182;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#19982;&#29616;&#26377;&#30340;&#32570;&#22833;&#25351;&#31034;&#26041;&#27861;&#19981;&#21516;&#65292;&#26497;&#21270;&#32534;&#30721;&#19981;&#38656;&#35201;&#25554;&#34917;&#65292;&#30830;&#20445;&#32570;&#22833;&#20540;&#19982;&#38750;&#32570;&#22833;&#20540;&#31561;&#36317;&#31163;&#65292;&#35753;&#20915;&#31574;&#26641;&#31639;&#27861;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#20998;&#21106;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#8220;&#23646;&#24615;&#20013;&#21253;&#21547;&#32570;&#22833;&#24615;&#8221;&#65288;MIA&#65289;&#30340;&#23454;&#38469;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20998;&#31867;&#21644;$[0,1]$&#20540;&#23646;&#24615;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21333;&#19968;&#23646;&#24615;&#31867;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23545;&#24212;&#20110;&#32463;&#20856;&#30340;&#37325;&#24515;&#22352;&#26631;&#27010;&#24565;&#65292;&#36825;&#25552;&#20379;&#20102;&#26497;&#21270;&#32534;&#30721;&#30340;&#27169;&#31946;&#21270;&#24418;&#24335;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose polar encoding, a representation of categorical and numerical $[0,1]$-valued attributes with missing values to be used in a classification context. We argue that this is a good baseline approach, because it can be used with any classification algorithm, preserves missingness information, is very simple to apply and offers good performance. In particular, unlike the existing missing-indicator approach, it does not require imputation, ensures that missing values are equidistant from non-missing values, and lets decision tree algorithms choose how to split missing values, thereby providing a practical realisation of the "missingness incorporated in attributes" (MIA) proposal. Furthermore, we show that categorical and $[0,1]$-valued attributes can be viewed as special cases of a single attribute type, corresponding to the classical concept of barycentric coordinates, and that this offers a natural interpretation of polar encoding as a fuzzified form of one-hot encoding. With an 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#20302;&#39057;&#20449;&#21495;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#22240;&#20026;&#33258;&#28982;&#22270;&#20687;&#30340;&#39057;&#29575;&#20998;&#24067;&#20351;&#22823;&#37096;&#20998;&#33021;&#37327;&#38598;&#20013;&#22312;&#20302;&#21040;&#20013;&#39057;&#12290;</title><link>http://arxiv.org/abs/2210.01257</link><description>&lt;p&gt;
&#20351;&#29992;CNN&#26469;&#27979;&#35797;&#34920;&#31034;&#25104;&#26412;&#29702;&#35770;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing predictions of representation cost theory with CNNs. (arXiv:2210.01257v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01257
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#20302;&#39057;&#20449;&#21495;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#22240;&#20026;&#33258;&#28982;&#22270;&#20687;&#30340;&#39057;&#29575;&#20998;&#24067;&#20351;&#22823;&#37096;&#20998;&#33021;&#37327;&#38598;&#20013;&#22312;&#20302;&#21040;&#20013;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#19981;&#21516;&#39057;&#29575;&#30340;&#20449;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#35768;&#22810;&#23454;&#35777;&#30740;&#31350;&#24050;&#32463;&#35760;&#24405;&#20102;CNNs&#23545;&#20302;&#39057;&#20449;&#21495;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#25935;&#24863;&#24615;&#26159;&#33258;&#28982;&#22270;&#20687;&#39057;&#29575;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#22823;&#37096;&#20998;&#33021;&#37327;&#38598;&#20013;&#22312;&#20302;&#21040;&#20013;&#39057;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20381;&#36182;&#20110;CNN&#30340;&#23618;&#27425;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#65292;&#36825;&#20010;&#24819;&#27861;&#20043;&#21069;&#26366;&#34987;&#29992;&#26469;&#21152;&#36895;&#35745;&#31639;&#21644;&#30740;&#31350;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#39046;&#22495;&#24212;&#29992;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that trained convolutional neural networks (CNNs) have different levels of sensitivity to signals of different frequency. In particular, a number of empirical studies have documented CNNs sensitivity to low-frequency signals. In this work we show with theory and experiments that this observed sensitivity is a consequence of the frequency distribution of natural images, which is known to have most of its power concentrated in low-to-mid frequencies. Our theoretical analysis relies on representations of the layers of a CNN in frequency space, an idea that has previously been used to accelerate computations and study implicit bias of network training algorithms, but to the best of our knowledge has not been applied in the domain of model robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;&#40479;&#30640;&#35270;&#22270;&#30340;&#20449;&#24687;&#36716;&#25442;&#12289;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#33719;&#21462;&#12289;&#29305;&#24449;&#34701;&#21512;&#20197;&#21450;&#25972;&#20307;&#27969;&#31243;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2209.05324</link><description>&lt;p&gt;
&#25506;&#32034;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#20013;&#30340;&#25361;&#25112;&#65306;&#19968;&#39033;&#32508;&#36848;&#12289;&#35780;&#20272;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#40479;&#30640;&#35270;&#35282;&#24863;&#30693;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;&#40479;&#30640;&#35270;&#22270;&#30340;&#20449;&#24687;&#36716;&#25442;&#12289;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#33719;&#21462;&#12289;&#29305;&#24449;&#34701;&#21512;&#20197;&#21450;&#25972;&#20307;&#27969;&#31243;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#40479;&#30640;&#35270;&#35282;&#65288;BEV&#65289;&#30340;&#24378;&#22823;&#34920;&#31034;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#24341;&#36215;&#20102;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#22823;&#22810;&#25968;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#21069;&#26041;&#25110;&#36879;&#35270;&#35270;&#22270;&#20013;&#36827;&#34892;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#36319;&#36394;&#31561;&#25805;&#20316;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#37197;&#32622;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20174;&#19981;&#21516;&#20256;&#24863;&#22120;&#20013;&#38598;&#25104;&#22810;&#28304;&#20449;&#24687;&#24182;&#20197;&#32479;&#19968;&#30340;&#35270;&#22270;&#34920;&#31034;&#29305;&#24449;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;BEV&#24863;&#30693;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#21363;&#20197;BEV&#34920;&#31034;&#21608;&#22260;&#22330;&#26223;&#30452;&#35266;&#19988;&#26131;&#20110;&#34701;&#21512;&#65307;&#20197;BEV&#34920;&#31034;&#29289;&#20307;&#23545;&#20110;&#21518;&#32493;&#30340;&#35268;&#21010;&#21644;/&#25110;&#25511;&#21046;&#27169;&#22359;&#26159;&#26368;&#29702;&#24819;&#30340;&#12290;BEV&#24863;&#30693;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#65306;&#65288;a&#65289;&#22914;&#20309;&#36890;&#36807;&#20174;&#36879;&#35270;&#35270;&#22270;&#21040;BEV&#30340;&#35270;&#35282;&#36716;&#25442;&#37325;&#24314;&#20002;&#22833;&#30340;3D&#20449;&#24687;&#65307;&#65288;b&#65289;&#22914;&#20309;&#22312;BEV&#32593;&#26684;&#20013;&#33719;&#21462;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#65307;&#65288;c&#65289;&#22914;&#20309;&#26500;&#24314;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#35270;&#22270;&#30340;&#29305;&#24449;&#30340;&#27969;&#31243;&#65307;&#20197;&#21450;&#65288;d&#65289;...
&lt;/p&gt;
&lt;p&gt;
Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) 
&lt;/p&gt;</description></item><item><title>E2EG&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#33410;&#28857;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#25299;&#25169;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#33410;&#28857;&#23646;&#24615;&#65292;&#28040;&#38500;&#20102;&#23884;&#20837;&#21644;&#20998;&#31867;&#20004;&#20010;&#38454;&#27573;&#65292;&#24341;&#20837;&#20102;&#20027;&#35201;&#21644;&#36741;&#21161;&#20998;&#31867;&#30446;&#26631;&#30340;&#20018;&#34892;&#21033;&#29992;&#65292;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#20102;&#20351;&#29992;&#30340;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.04609</link><description>&lt;p&gt;
E2EG: &#20351;&#29992;&#22270;&#25299;&#25169;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#31471;&#21040;&#31471;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes. (arXiv:2208.04609v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04609
&lt;/p&gt;
&lt;p&gt;
E2EG&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#33410;&#28857;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#25299;&#25169;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#33410;&#28857;&#23646;&#24615;&#65292;&#28040;&#38500;&#20102;&#23884;&#20837;&#21644;&#20998;&#31867;&#20004;&#20010;&#38454;&#27573;&#65292;&#24341;&#20837;&#20102;&#20027;&#35201;&#21644;&#36741;&#21161;&#20998;&#31867;&#30446;&#26631;&#30340;&#20018;&#34892;&#21033;&#29992;&#65292;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#20102;&#20351;&#29992;&#30340;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#23398;&#26415;&#24341;&#29992;&#22270;&#20013;&#39044;&#27979;&#35770;&#25991;&#20027;&#39064;&#21040;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#20013;&#29992;&#25143;&#29305;&#24449;&#30340;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#33410;&#28857;&#20998;&#31867;&#26694;&#26550;&#65292;&#22914;GIANT&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#27969;&#31243;&#65306;&#39318;&#20808;&#23884;&#20837;&#22270;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#24471;&#21040;&#30340;&#23884;&#20837;&#36755;&#20837;&#33410;&#28857;&#20998;&#31867;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#20004;&#20010;&#38454;&#27573;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;GIANT&#22522;&#30784;&#19978;&#30340;&#31471;&#21040;&#31471;&#33410;&#28857;&#20998;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;End-to-End-GIANT&#65288;E2EG&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20027;&#35201;&#20998;&#31867;&#30446;&#26631;&#21644;&#36741;&#21161;&#20998;&#31867;&#30446;&#26631;&#30340;&#20018;&#34892;&#21033;&#29992;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#20351;&#24471;BERT&#20027;&#24178;&#21487;&#20197;&#34987;&#19968;&#20010;&#21442;&#25968;&#20943;&#23569;25%-40%&#30340;&#33976;&#39311;&#32534;&#30721;&#22120;&#21462;&#20195;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#29305;&#24615;&#22686;&#21152;&#20102;&#20351;&#29992;&#30340;&#20415;&#25463;&#24615;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#20026;&#33410;&#28857;&#20998;&#31867;&#38142;&#25509;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#19982;GIANT&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node classification utilizing text-based node attributes has many real-world applications, ranging from prediction of paper topics in academic citation graphs to classification of user characteristics in social media networks. State-of-the-art node classification frameworks, such as GIANT, use a two-stage pipeline: first embedding the text attributes of graph nodes then feeding the resulting embeddings into a node classification model. In this paper, we eliminate these two stages and develop an end-to-end node classification model that builds upon GIANT, called End-to-End-GIANT (E2EG). The tandem utilization of a main and an auxiliary classification objectives in our approach results in a more robust model, enabling the BERT backbone to be switched out for a distilled encoder with a 25% - 40% reduction in the number of parameters. Moreover, the model's end-to-end nature increases ease of use, as it avoids the need of chaining multiple models for node classification. Compared to a GIANT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.00755</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27493; Q-learning &#32531;&#35299; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#31163;&#31574;&#30053;&#20559;&#24046;&#65306;&#19968;&#31181;&#26032;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#31163;&#31574;&#30053;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#24046;&#22686;&#21152;&#26102;&#65292;&#31163;&#31574;&#30053;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#31163;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#26469;&#34917;&#20607;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#19968;&#31995;&#21015;&#38271;&#36712;&#36857;&#65292;&#24182;&#23548;&#33268;&#39069;&#22806;&#30340;&#38382;&#39064;&#65292;&#22914;&#28040;&#22833;/&#29190;&#28856;&#26799;&#24230;&#25110;&#25243;&#24323;&#35768;&#22810;&#26377;&#29992;&#30340;&#32463;&#39564;&#65292;&#26368;&#32456;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#36830;&#32493;&#21160;&#20316;&#22495;&#25110;&#30001;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#31574;&#30053;&#30340;&#27867;&#21270;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#36830;&#32493;&#25511;&#21046;&#20013;&#36825;&#31181;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#36731; Actor-Critic &#26041;&#27861;&#20013;&#31163;&#25919;&#31574;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#24182;&#22312;Polairs&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2207.00479</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24322;&#27493;&#20998;&#25955;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#24182;&#22312;Polairs&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#20960;&#20998;&#38047;&#21040;&#20960;&#23567;&#26102;&#30340;&#26102;&#38388;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#37319;&#29992;&#35745;&#31639;&#20415;&#23452;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#23398;&#20064;&#21442;&#25968;&#37197;&#32622;&#19982;&#24615;&#33021;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21333;&#20010;&#31649;&#29702;&#22120;-&#22810;&#20010;&#24037;&#20316;&#32773;&#31574;&#30053;&#65292;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23613;&#31649;&#36229;&#21442;&#25968;&#35780;&#20272;&#26102;&#38388;&#30456;&#24403;&#38271;&#65292;&#20294;&#36825;&#31181;&#38598;&#20013;&#24335;&#26041;&#26696;&#30340;&#24320;&#38144;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22823;&#37327;&#24037;&#20316;&#32773;&#19978;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#24037;&#20316;&#32773;&#36816;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#23384;&#20648;&#24322;&#27493;&#36890;&#20449;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;1,920&#20010;&#24182;&#34892;&#24037;&#20316;&#32773;&#65288;Polaris&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#23436;&#25972;&#29983;&#20135;&#38431;&#21015;&#65289;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;EPIC-KITCHENS-100&#22810;&#23454;&#20363;&#26816;&#32034;&#25361;&#25112;2022&#20013;&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#19978;&#19979;&#25991;&#21270;&#30340;&#35270;&#39057;&#29305;&#24449;&#36827;&#34892;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20803;&#25439;&#22833;&#20989;&#25968;&#22312;&#22810;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#34701;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#65292;&#22312;nDCG&#21644;mAP&#26041;&#38754;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2206.14381</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#19978;&#19979;&#25991;&#21270;&#30340;&#35270;&#39057;&#29305;&#24449;&#22312;EPIC-KITCHENS-100&#22810;&#23454;&#20363;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#25361;&#25112;2022&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.14381v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;EPIC-KITCHENS-100&#22810;&#23454;&#20363;&#26816;&#32034;&#25361;&#25112;2022&#20013;&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#19978;&#19979;&#25991;&#21270;&#30340;&#35270;&#39057;&#29305;&#24449;&#36827;&#34892;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20803;&#25439;&#22833;&#20989;&#25968;&#22312;&#22810;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#34701;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#65292;&#22312;nDCG&#21644;mAP&#26041;&#38754;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;EPIC-KITCHENS-100&#22810;&#23454;&#20363;&#26816;&#32034;&#25361;&#25112;2022&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#21477;&#23376;&#35299;&#26512;&#20026;&#19982;&#21160;&#35789;&#21644;&#21517;&#35789;&#30456;&#23545;&#24212;&#30340;&#35821;&#20041;&#35282;&#33394;&#65307;&#28982;&#21518;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22810;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#36890;&#36807;&#19977;&#20803;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#19978;&#19979;&#25991;&#21270;&#30340;&#35270;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;&#65288;nDCG&#65289;&#26041;&#38754;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#65292;&#36825;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#26356;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#22312;nDCG&#25490;&#21517;&#31532;&#19977;&#65292;&#22312;mAP&#25490;&#21517;&#31532;&#22235;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. We first parse sentences into semantic roles corresponding to verbs and nouns; then utilize self-attentions to exploit semantic role contextualized video features along with textual features via triplet losses in multiple embedding spaces. Our method overpasses the strong baseline in normalized Discounted Cumulative Gain (nDCG), which is more valuable for semantic similarity. Our submission is ranked 3rd for nDCG and ranked 4th for mAP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.00395</link><description>&lt;p&gt;
&#20855;&#22791;&#36741;&#21161;&#20449;&#24687;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;$f(x)$&#30340;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;$h(x)$&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20010;&#20844;&#24335;&#28085;&#30422;&#20102;&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#35774;&#32622;&#65292;&#22914;i&#65289;&#22312;SGD&#20013;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;ii&#65289;&#36801;&#31227;&#23398;&#20064;&#65292;iii&#65289;&#32852;&#37030;&#23398;&#20064;&#65292;iv&#65289;&#20351;&#29992;&#21387;&#32553;&#27169;&#22411;/&#20002;&#24323;&#31561;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#36825;&#20010;&#26694;&#26550;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NN2Poly&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#30830;&#22810;&#39033;&#24335;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19988;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#65292;&#33021;&#22815;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2112.11397</link><description>&lt;p&gt;
NN2Poly&#65306;&#29992;&#20110;&#28145;&#24230;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NN2Poly&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#30830;&#22810;&#39033;&#24335;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19988;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#65292;&#33021;&#22815;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#29702;&#35770;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;NN2Poly&#65306;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#19968;&#20010;&#26174;&#24335;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;&#22810;&#23618;&#24863;&#30693;&#22120;&#25110;MLP&#65289;&#30340;&#31934;&#30830;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20808;&#21069;&#24819;&#27861;&#65292;&#35813;&#24819;&#27861;&#20165;&#38480;&#20110;&#21333;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20219;&#24847;&#28145;&#24230;MLP&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#27599;&#23618;&#19978;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340;&#27888;&#21202;&#23637;&#24320;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#20960;&#20010;&#32452;&#21512;&#24615;&#36136;&#26469;&#35745;&#31639;&#25152;&#38656;&#22810;&#39033;&#24335;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#20316;&#32773;&#35752;&#35770;&#20102;&#27492;&#26041;&#27861;&#30340;&#20027;&#35201;&#35745;&#31639;&#25361;&#25112;&#20197;&#21450;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#36924;&#36817;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#23613;&#31649;NN2Poly&#26041;&#27861;&#31616;&#21333;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#20294;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#29273;&#40831;&#25918;&#23556;&#22270;&#20013;&#30340;&#40843;&#40831;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#65292;&#24182;&#21487;&#26681;&#25454;&#20998;&#21106;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#35299;&#37322;&#39044;&#27979;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2112.09694</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#29273;&#40831;&#40843;&#40831;&#22312;&#22810;&#20301;X&#20809;&#29255;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays. (arXiv:2112.09694v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#29273;&#40831;&#25918;&#23556;&#22270;&#20013;&#30340;&#40843;&#40831;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#65292;&#24182;&#21487;&#26681;&#25454;&#20998;&#21106;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#35299;&#37322;&#39044;&#27979;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22312;&#29273;&#31185;&#25918;&#23556;&#22270;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40843;&#40831;&#26816;&#27979;&#20219;&#21153;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#21363;&#20351;&#20351;&#29992;&#24369;&#30340;&#22270;&#20687;&#32423;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20063;&#33021;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#12290;&#20854;&#27425;&#65292;&#23427;&#36866;&#29992;&#20110;&#20174;&#20998;&#21106;&#26631;&#31614;&#20013;&#23398;&#20064;&#20197;&#25351;&#23548;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#20154;&#31867;&#29992;&#25143;&#21487;&#20197;&#24544;&#23454;&#22320;&#35299;&#37322;&#39044;&#27979;&#65292;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#20197;&#20915;&#23450;&#35201;&#20851;&#27880;&#30340;&#21306;&#22495;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#32422;38k&#20010;&#22810;&#20301;X&#20809;&#29255;&#65288;&#32422;316k&#20010;&#29273;&#40831;&#65289;&#30340;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22312;&#19982;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#24403;&#30001;&#22806;&#37096;&#40843;&#40831;&#20998;&#21106;&#27169;&#22411;&#25351;&#23548;&#26102;&#65292;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#23450;&#20301;&#24615;&#33021;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient image classification architecture based on deep multiple instance learning, and apply it to the challenging task of caries detection in dental radiographs. Technically, our approach contributes in two ways: First, it outputs a heatmap of local patch classification probabilities despite being trained with weak image-level labels. Second, it is amenable to learning from segmentation labels to guide training. In contrast to existing methods, the human user can faithfully interpret predictions and interact with the model to decide which regions to attend to. Experiments are conducted on a large clinical dataset of $\sim$38k bitewings ($\sim$316k teeth), where we achieve competitive performance compared to various baselines. When guided by an external caries segmentation model, a significant improvement in classification and localization performance is observed.
&lt;/p&gt;</description></item><item><title>&#36335;&#24452;&#27491;&#21017;&#21270;&#20026;&#24182;&#34892;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#23454;&#29616;&#20102;&#20984;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#22791;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2110.09548</link><description>&lt;p&gt;
&#36335;&#24452;&#27491;&#21017;&#21270;&#65306;&#19968;&#31181;&#23545;&#24182;&#34892;ReLU&#32593;&#32476;&#36827;&#34892;&#20984;&#24615;&#21644;&#31232;&#30095;&#24615;&#24341;&#23548;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.09548
&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#27491;&#21017;&#21270;&#20026;&#24182;&#34892;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#23454;&#29616;&#20102;&#20984;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#22791;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#26469;&#25581;&#31034;&#20248;&#21270;&#26223;&#35266;&#20013;&#38544;&#34255;&#30340;&#20984;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#24182;&#34892;ReLU&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20063;&#21253;&#25324;&#26631;&#20934;&#30340;&#28145;&#24230;&#32593;&#32476;&#21644;ResNet&#20316;&#20026;&#20854;&#29305;&#20363;&#12290;&#28982;&#21518;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#36335;&#24452;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;&#31934;&#30830;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#31561;&#20215;&#30340;&#20984;&#38382;&#39064;&#26159;&#36890;&#36807;&#19968;&#31181;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#30340;&#35268;&#33539;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#36335;&#24452;&#27491;&#21017;&#21270;&#30340;&#24182;&#34892;ReLU&#32593;&#32476;&#21487;&#20197;&#34987;&#35270;&#20026;&#39640;&#32500;&#20013;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#27169;&#22411;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#21407;&#22987;&#30340;&#35757;&#32451;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#26377;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#26657;&#27491;&#36873;&#25321;&#20559;&#24046;&#65292;DeepSDRF&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#25512;&#33616;&#31639;&#27861;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#24211;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;DeepSDRF&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.10453</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Recommendation with Deep Survival Dose Response Function. (arXiv:2108.10453v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.10453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#26657;&#27491;&#36873;&#25321;&#20559;&#24046;&#65292;DeepSDRF&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#25512;&#33616;&#31639;&#27861;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#24211;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;DeepSDRF&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#30340;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#20165;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#65288;&#28151;&#26434;&#22240;&#23376;&#65289;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#21644;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#32467;&#26524;&#37117;&#26377;&#24433;&#21709;&#30340;&#26465;&#20214;&#24179;&#22343;&#21058;&#37327;&#21453;&#24212;&#65288;CADR&#65289;&#20989;&#25968;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#20174;DeepSDRF&#20013;&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20855;&#26377;&#36873;&#25321;&#20559;&#24046;&#26657;&#27491;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20004;&#31181;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#24739;&#32773;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#30456;&#20284;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;eICU&#30740;&#31350;&#26426;&#26500;&#65288;eRI&#65289;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;DeepSDRF&#21644;&#30456;&#24212;&#30340;&#25512;&#33616;&#22120;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#20351;&#29992;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#35266;&#23519;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general formulation for continuous treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which observed factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with the correction for selection bias. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that causal models are used to address the continuous treatment effect with observational data in a medical context.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#24674;&#22797;&#22240;&#26524;DAGs&#65292;&#24182;&#24314;&#31435;&#20102;&#32622;&#20449;&#21306;&#38388;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.02600</link><description>&lt;p&gt;
&#26469;&#33258;&#33258;&#28608;&#21644;&#30456;&#20114;&#28608;&#21457;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery from Self and Mutually Exciting Time Series. (arXiv:2106.02600v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#24674;&#22797;&#22240;&#26524;DAGs&#65292;&#24182;&#24314;&#31435;&#20102;&#32622;&#20449;&#21306;&#38388;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#32447;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24674;&#22797;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#38543;&#26426;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;(VI)&#24418;&#24335;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#33324;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#38750;&#28176;&#36827;&#24615;&#30340;&#24674;&#22797;&#20445;&#35777;&#21644;&#21487;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#23450;&#19968;&#31995;&#21015;&#38750;&#32447;&#24615;&#21333;&#35843;&#36830;&#25509;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#21644;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;DAGs&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;Sepsis&#30456;&#20851;&#32010;&#20081;(SADs)&#26041;&#38754;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#24378;&#22823;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#65288;&#22914;XGBoost&#65289;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26410;&#26469;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a generalized linear structural causal model, coupled with a novel data-adaptive linear regularization, to recover causal directed acyclic graphs (DAGs) from time series. By leveraging a recently developed stochastic monotone Variational Inequality (VI) formulation, we cast the causal discovery problem as a general convex optimization. Furthermore, we develop a non-asymptotic recovery guarantee and quantifiable uncertainty by solving a linear program to establish confidence intervals for a wide range of non-linear monotone link functions. We validate our theoretical results and show the competitive performance of our method via extensive numerical experiments. Most importantly, we demonstrate the effectiveness of our approach in recovering highly interpretable causal DAGs over Sepsis Associated Derangements (SADs) while achieving comparable prediction performance to powerful ``black-box'' models such as XGBoost. Thus, the future adoption of our proposed method to conduct con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2007.10126</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Human-like Energy Management Based on Deep Reinforcement Learning and Historical Driving Experiences. (arXiv:2007.10126v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#30340;&#21457;&#23637;&#20381;&#36182;&#20110;&#20808;&#36827;&#39640;&#25928;&#30340;&#33021;&#37327;&#31649;&#29702;&#31574;&#30053;&#65288;EMS&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#37319;&#38598;&#30340;&#21382;&#21490;&#39550;&#39542;&#25968;&#25454;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#20013;&#30340;&#28151;&#21512;&#21160;&#21147;&#20256;&#21160;&#31995;&#32479;&#37319;&#29992;&#20018;&#32852;-&#24182;&#32852;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#39318;&#20808;&#24314;&#31435;&#20102;&#38754;&#21521;&#25511;&#21046;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#21151;&#29575;&#20998;&#37197;&#25511;&#21046;&#65292;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#33719;&#24471;&#30340;&#20840;&#23616;&#26368;&#20248;&#25511;&#21046;&#36712;&#36857;&#20316;&#20026;&#19987;&#23478;&#30693;&#35782;&#26469;&#35757;&#32451;DDPG&#27169;&#22411;&#12290;&#36825;&#20010;&#25805;&#20316;&#30830;&#20445;&#20102;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#32463;&#39564;&#39550;&#39542;&#21592;&#30340;&#37319;&#38598;&#30340;&#21382;&#21490;&#39550;&#39542;&#25968;&#25454;&#26469;&#26367;&#20195;&#22522;&#20110;DP&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#21319;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of hybrid electric vehicles depends on an advanced and efficient energy management strategy (EMS). With online and real-time requirements in mind, this article presents a human-like energy management framework for hybrid electric vehicles according to deep reinforcement learning methods and collected historical driving data. The hybrid powertrain studied has a series-parallel topology, and its control-oriented modeling is founded first. Then, the distinctive deep reinforcement learning (DRL) algorithm, named deep deterministic policy gradient (DDPG), is introduced. To enhance the derived power split controls in the DRL framework, the global optimal control trajectories obtained from dynamic programming (DP) are regarded as expert knowledge to train the DDPG model. This operation guarantees the optimality of the proposed control architecture. Moreover, the collected historical driving data based on experienced drivers are employed to replace the DP-based controls, and thus c
&lt;/p&gt;</description></item></channel></rss>