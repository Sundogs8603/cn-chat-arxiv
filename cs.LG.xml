<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01116</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22810;&#27169;&#22411;MPC&#30340;&#22522;&#20110;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#30340;&#23618;&#32423;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01116
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#12290;&#35813;&#26550;&#26500;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;1) RAID-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24615;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#21608;&#22260;&#36710;&#36742;&#20043;&#38388;&#22312;MPC&#39044;&#27979;&#33539;&#22260;&#20869;&#30340;&#30456;&#20851;&#20132;&#20114;&#65307;2) &#19968;&#20010;&#31616;&#21270;&#30340;&#38543;&#26426;MPC&#38382;&#39064;&#65292;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#36991;&#30896;&#32422;&#26463;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#20132;&#36890;&#36335;&#21475;&#20013;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;12&#20493;&#36895;&#25552;&#21319;&#12290;&#24744;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#23637;&#31034;&#35813;&#26550;&#26500;&#22312;&#22810;&#20010;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#35270;&#39057;&#65306;https://youtu.be/-TcMeolCLWc
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;</title><link>https://arxiv.org/abs/2404.02113</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#36827;&#34892;&#35843;&#25972;&#65306;&#37325;&#26032;&#23457;&#35270;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32487;&#32493;&#25110;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#29615;&#22659;&#30340;&#35775;&#38382;&#24212;&#35813;&#26159;&#26377;&#38480;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#30340;&#31639;&#27861;&#33021;&#22815;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#24182;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#12289;&#24847;&#24819;&#19981;&#21040;&#30340;&#24773;&#20917;&#65292;&#37027;&#20040;&#25105;&#20204;&#24517;&#39035;&#24895;&#24847;&#22312;&#25972;&#20010;&#20195;&#29702;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#37096;&#32626;&#25105;&#20204;&#30340;&#20195;&#29702;&#32780;&#19981;&#35843;&#25972;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#29978;&#33267;&#32487;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#20855;&#22791;&#23545;&#20195;&#29702;&#30340;&#37096;&#32626;&#29615;&#22659;&#20855;&#26377;&#26080;&#38480;&#21046;&#35775;&#38382;&#26435;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#30334;&#20998;&#20043;&#19968;&#21487;&#20197;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;DQN&#21644;Soft Actor Critic&#22312;&#21508;&#31181;&#25345;&#32493;&#21644;&#38750;&#31283;&#23450;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02113v1 Announce Type: new  Abstract: In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;</title><link>https://arxiv.org/abs/2403.16771</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#35328;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#32593;&#32476;&#20132;&#27969;&#20026;&#22312;&#21333;&#20010;&#35805;&#35821;&#20013;&#28151;&#21512;&#22810;&#31181;&#35821;&#35328;&#65288;&#21448;&#31216;&#28151;&#21512;&#20195;&#30721;&#35821;&#35328;&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#21644;&#22122;&#38899;&#30340;&#23384;&#22312;&#65292;&#36825;&#32473;&#35745;&#31639;&#27169;&#22411;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#32763;&#35793;&#21033;&#29992;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#12290;&#26412;&#25991;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#65288;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21512;&#25104;&#24320;&#21457;&#20102;HINMIX&#19968;&#20010;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;420&#19975;&#20010;&#21477;&#23545;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RCMT&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#20581;&#25200;&#21160;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#21333;&#35789;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#23398;&#20064;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#20013;&#30340;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCMT&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#23545;&#23391;&#21152;&#25289;&#35821;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16771v1 Announce Type: new  Abstract: The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish t
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#25913;&#36827;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#23545;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#65292;&#25670;&#33073;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#65292;&#20294;&#20173;&#28982;&#20381;&#36182;&#20110;&#26631;&#20934;&#22240;&#26524;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.14385</link><description>&lt;p&gt;
&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;--&#19968;&#31181;&#26041;&#27861;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Estimating Causal Effects with Double Machine Learning -- A Method Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14385
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#25913;&#36827;&#20102;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#23545;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#65292;&#25670;&#33073;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#65292;&#20294;&#20173;&#28982;&#20381;&#36182;&#20110;&#26631;&#20934;&#22240;&#26524;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25918;&#23485;&#20256;&#32479;&#20551;&#35774;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;-"&#21452;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;"&#65288;DML&#65289;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#23427;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30456;&#23545;&#20110;&#26356;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;DML&#20013;&#24212;&#29992;&#19968;&#20010;&#36866;&#24403;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25913;&#36827;&#23545;&#21508;&#31181;&#38750;&#32447;&#24615;&#28151;&#28102;&#20851;&#31995;&#30340;&#35843;&#25972;&#12290;&#36825;&#31181;&#20248;&#21183;&#20351;&#24471;&#21487;&#20197;&#25670;&#33073;&#36890;&#24120;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#24517;&#38656;&#30340;&#20256;&#32479;&#20989;&#25968;&#24418;&#24335;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26631;&#20934;&#20551;&#35774;&#26041;&#38754;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14385v1 Announce Type: cross  Abstract: The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - "double/debiased machine learning" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23558;&#35821;&#35328;&#27169;&#22411;&#29305;&#24449;&#19982;&#20256;&#32479;&#29305;&#24449;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25110;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20107;&#20214;&#20005;&#37325;&#24615;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13547</link><description>&lt;p&gt;
&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#37325;&#24615;&#20998;&#31867;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23558;&#35821;&#35328;&#27169;&#22411;&#29305;&#24449;&#19982;&#20256;&#32479;&#29305;&#24449;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25110;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20107;&#20214;&#20005;&#37325;&#24615;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#21319;&#20132;&#36890;&#20107;&#20214;&#31649;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#23427;&#32771;&#23519;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#22312;&#20351;&#29992;&#20107;&#25925;&#25253;&#21578;&#23545;&#20107;&#20214;&#20005;&#37325;&#24615;&#36827;&#34892;&#20998;&#31867;&#26102;&#22914;&#20309;&#25913;&#21892;&#25110;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65289;&#32452;&#21512;&#20043;&#38388;&#36827;&#34892;&#20102;&#22810;&#27425;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#25991;&#26412;&#21644;&#20107;&#20214;&#25253;&#21578;&#20013;&#20256;&#32479;&#29305;&#24449;&#21644;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#24449;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#36827;&#34892;&#20005;&#37325;&#24615;&#20998;&#31867;&#12290;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#19982;&#30452;&#25509;&#20174;&#20107;&#20214;&#25253;&#21578;&#20013;&#33719;&#21462;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#23558;&#20107;&#20214;&#30340;&#20005;&#37325;&#32423;&#21035;&#25351;&#27966;&#32473;&#20107;&#20214;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;Ex&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#25110;&#33267;&#23569;&#21305;&#37197;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13547v1 Announce Type: new  Abstract: This study evaluates the impact of large language models on enhancing machine learning processes for managing traffic incidents. It examines the extent to which features generated by modern language models improve or match the accuracy of predictions when classifying the severity of incidents using accident reports. Multiple comparisons performed between combinations of language models and machine learning algorithms, including Gradient Boosted Decision Trees, Random Forests, and Extreme Gradient Boosting. Our research uses both conventional and language model-derived features from texts and incident reports, and their combinations to perform severity classification. Incorporating features from language models with those directly obtained from incident reports has shown to improve, or at least match, the performance of machine learning techniques in assigning severity levels to incidents, particularly when employing Random Forests and Ex
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#20135;&#29983;&#25163;&#37096;&#22270;&#20687;&#21644;&#20998;&#21106;&#25513;&#27169;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340; ControlNet &#27169;&#22411;&#32472;&#21046;&#29983;&#25104;&#25163;&#37096;&#21608;&#22260;&#30340;&#36523;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.10731</link><description>&lt;p&gt;
&#19968;&#31181;&#20026;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#24110;&#21161;&#30340;&#26041;&#27861;&#65306;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10731
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26465;&#20214;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#20135;&#29983;&#25163;&#37096;&#22270;&#20687;&#21644;&#20998;&#21106;&#25513;&#27169;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340; ControlNet &#27169;&#22411;&#32472;&#21046;&#29983;&#25104;&#25163;&#37096;&#21608;&#22260;&#30340;&#36523;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#27493;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#26041;&#27861;&#22312;&#29983;&#25104;&#19968;&#33268;&#30340;&#25163;&#37096;&#35299;&#21078;&#32467;&#26500;&#26102;&#36935;&#21040;&#25361;&#25112;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#32570;&#20047;&#23545;&#25163;&#37096;&#23039;&#21183;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#23039;&#21183;&#26465;&#20214;&#30340;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#65292;&#23558;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#25163;&#30340;&#29983;&#25104;&#21644;&#38543;&#21518;&#22260;&#32469;&#25163;&#37096;&#36827;&#34892;&#36523;&#20307;&#22806;&#37096;&#32472;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#20219;&#21153;&#35774;&#32622;&#35757;&#32451;&#25163;&#37096;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#36866;&#24212;&#30340; ControlNet &#27169;&#22411;&#26469;&#32472;&#21046;&#21608;&#22260;&#30340;&#36523;&#20307;&#65292;&#29983;&#25104;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#25216;&#26415;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20445;&#30041;&#25163;&#37096;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10731v1 Announce Type: cross  Abstract: Recent years have seen significant progress in human image generation, particularly with the advancements in diffusion models. However, existing diffusion methods encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted ControlNet model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18002</link><description>&lt;p&gt;
&#32771;&#34385;&#23545;&#31216;&#24615;&#30340;&#36719;&#33109;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#26426;&#22120;&#20154;&#35013;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#36719;&#33109;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#20195;&#34920;&#24615;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23500;&#25509;&#35302;PEG-IN-HOLE&#20219;&#21153;&#65292;&#35813;&#36719;&#33109;&#21487;&#20197;&#27604;&#21018;&#24615;&#33109;&#37096;&#26356;&#23433;&#20840;&#22320;&#25805;&#20316;&#24182;&#23481;&#24525;&#36739;&#20302;&#39057;&#29575;&#30340;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#20844;&#24335;&#19981;&#21516;&#65292;&#35813;&#20844;&#24335;&#38656;&#35201;&#22806;&#37096;&#35774;&#32622;&#25110;&#20272;&#35745;&#22120;&#26469;&#33719;&#21462;PEG-TO-HOLE&#23039;&#24577;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#20844;&#24335;&#21644;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#23436;&#20840;&#22522;&#20110;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#20449;&#21495;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#34701;&#21512;&#28508;&#22312;&#39046;&#22495;&#23545;&#31216;&#24615;&#65292;&#22240;&#27492;&#24517;&#39035;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#24182;&#26500;&#24314;&#36741;&#21161;&#25439;&#22833;&#26469;&#24378;&#36843;&#20195;&#29702;&#36981;&#23432;&#23545;&#31216;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#23545;&#31216;PEG&#24418;&#29366;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#21487;&#20197;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18002v1 Announce Type: cross  Abstract: This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to 
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21464;&#21270;&#27599;&#20010;&#32500;&#24230;&#30340;&#27604;&#20363;&#26469;&#35843;&#26597;&#31354;&#38388;&#20449;&#24687;&#23545;&#20110;&#33041;&#30005;&#22270;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#31354;&#38388;&#20449;&#24687;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;</title><link>https://arxiv.org/abs/2402.13523</link><description>&lt;p&gt;
&#24179;&#34913;&#33041;&#30005;&#22270;&#20013;&#30340;&#39057;&#35889;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21464;&#21270;&#27599;&#20010;&#32500;&#24230;&#30340;&#27604;&#20363;&#26469;&#35843;&#26597;&#31354;&#38388;&#20449;&#24687;&#23545;&#20110;&#33041;&#30005;&#22270;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#31354;&#38388;&#20449;&#24687;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#27835;&#30103;&#21069;&#26223;&#38656;&#35201;&#24320;&#21457;&#32463;&#27982;&#26377;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31579;&#26597;&#26041;&#27861;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#26159;&#26368;&#32463;&#27982;&#30340;&#25104;&#20687;&#27169;&#24335;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#33041;&#30005;&#22270;&#20998;&#26512;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22914;&#22270;&#20449;&#21495;&#22788;&#29702;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#30456;&#23545;&#20110;&#39057;&#35889;&#25110;&#26102;&#38388;&#20449;&#24687;&#30340;&#31354;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#32500;&#24230;&#25152;&#21344;&#27604;&#20363;&#26469;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35268;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21508;&#31181;&#32500;&#24230;&#20998;&#36776;&#29575;&#37197;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31354;&#38388;&#20449;&#24687;&#22987;&#32456;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#32771;&#34385;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13523v1 Announce Type: cross  Abstract: The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks. Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we test various dimension resolution configurations on two routine EEG datasets. We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information. These results emphasise the necessity to consider spatial information for EEG-based AD classification. On our second dataset, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.10142</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Changing Probabilities via Dynamic Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#39044;&#27979;&#22120;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#31163;&#25955;&#39033;&#30446;&#12290;&#39044;&#27979;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#27010;&#29575;&#22810;&#31867;&#21035;&#39044;&#27979;&#65292;&#21363;&#36890;&#36807;&#36755;&#20986;&#26377;&#38646;&#20010;&#25110;&#22810;&#20010;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#25509;&#19979;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#39033;&#30446;&#65292;&#28982;&#21518;&#25581;&#31034;&#23454;&#38469;&#39033;&#30446;&#24182;&#20174;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36755;&#20986;&#27010;&#29575;&#65292;&#39044;&#27979;&#22120;&#20250;&#36319;&#36394;&#20854;&#25152;&#35265;&#39033;&#30446;&#30340;&#27604;&#20363;&#12290;&#39044;&#27979;&#22120;&#20855;&#26377;&#24658;&#23450;&#65288;&#26377;&#38480;&#65289;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#23547;&#27714;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#26356;&#26032;&#25216;&#26415;&#65306;&#27969;&#26159;&#26080;&#30028;&#30340;&#65292;&#39033;&#30446;&#30340;&#38598;&#21512;&#23545;&#39044;&#27979;&#22120;&#26159;&#26410;&#30693;&#30340;&#65292;&#23427;&#20204;&#30340;&#24635;&#25968;&#20063;&#21487;&#33021;&#26080;&#38480;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#65306;&#39033;&#30446;&#30340;&#28508;&#22312;&#39057;&#29575;&#21487;&#33021;&#20250;&#19981;&#26102;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#26032;&#39033;&#30446;&#21487;&#33021;&#24320;&#22987;&#20986;&#29616;&#65292;&#19968;&#20123;&#24403;&#21069;&#39057;&#32321;&#20986;&#29616;&#30340;&#39033;&#30446;&#21487;&#33021;&#20877;&#27425;&#20572;&#27490;&#20986;&#29616;&#12290;&#30001;&#20110;&#26377;&#31354;&#38388;&#38480;&#21046;&#65292;&#39044;&#27979;&#22120;&#21482;&#38656;&#35201;&#25552;&#20379;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05067</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#65306;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#21040;&#23567;&#23610;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29616;&#35937;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#20934;&#30830;&#26377;&#25928;&#22320;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#26222;&#36941;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#32806;&#26041;&#27861;&#23545;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#36827;&#34892;&#34920;&#24449;&#30340;&#26032;&#30340;&#27714;&#35299;&#27169;&#24335;&#12290;&#36890;&#36807;&#29420;&#31435;&#22320;&#24314;&#27169;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#35270;&#20026;&#20174;&#23646;&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35889;PINN&#26041;&#27861;&#65292;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#25509;&#36817;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#32500;Kuramot-Sivashinsky (KS)&#26041;&#31243;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;Navier-Stokes (NS)&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#32593;&#26684;&#12289;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#12289;&#24102;&#22122;&#22768;&#30340;&#22823;&#23610;&#24230;&#25968;&#25454;&#21644;&#39640;&#32500;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.03305</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#35821;&#20041;&#26377;&#24847;&#20041;&#21644;&#39640;&#25928;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#19981;&#23547;&#24120;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20363;&#22914;&#23431;&#33322;&#21592;&#39569;&#22312;&#26376;&#29699;&#19978;&#30340;&#39532;&#65292;&#24182;&#19988;&#26377;&#27491;&#30830;&#30340;&#38452;&#24433;&#12290;&#36825;&#20123;&#36755;&#20986;&#34920;&#26126;&#20102;&#27169;&#22411;&#20855;&#26377;&#32452;&#21512;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#30340;&#21602;&#65311;&#25105;&#20204;&#22312;&#26465;&#20214;DDPMs&#19978;&#36827;&#34892;&#20102;&#25511;&#21046;&#23454;&#39564;&#65292;&#23398;&#20064;&#29983;&#25104;&#20197;&#25351;&#23450;&#30340;$x$&#21644;$y$&#20301;&#32622;&#20026;&#20013;&#24515;&#30340;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#23545;&#20110;&#23454;&#29616;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#34920;&#31034;&#38454;&#27573;&#65306;(A&#38454;&#27573;)&#27809;&#26377;&#28508;&#22312;&#32467;&#26500;&#65292;(B&#38454;&#27573;)&#19968;&#20010;&#28151;&#20081;&#29366;&#24577;&#30340;2D&#27969;&#24418;&#65292;&#20197;&#21450;(C&#38454;&#27573;)&#19968;&#20010;&#26377;&#24207;&#30340;2D&#27969;&#24418;&#12290;&#23545;&#24212;&#20110;&#36825;&#20123;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#65306;1&#65289;&#29983;&#25104;&#22810;&#20010;&#20984;&#36215;&#65292;2&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#65292;&#20294;$x$&#21644;$y$&#20301;&#32622;&#19981;&#20934;&#30830;&#65292;3&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#19988;&#20301;&#32622;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16043</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#32447;&#24615;&#20998;&#31867;&#30340;&#25193;&#23637;&#38750;&#23545;&#31216;sigmoid&#21644;&#24863;&#30693;&#26426;(SIGTRON)
&lt;/p&gt;
&lt;p&gt;
An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;&#65292;&#31216;&#20026;SIGTRON&#65292;&#23427;&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#38750;&#23545;&#31216;sigmoid&#20989;&#25968;&#21644;&#24863;&#30693;&#26426;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#23427;&#30340;&#20276;&#38543;&#20984;&#27169;&#22411;SIGTRON-&#19981;&#24179;&#34913;&#20998;&#31867;(SIC)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#34394;&#25311;SIGTRON&#20135;&#29983;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;$\pi$-&#21152;&#26435;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;SIC&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#27809;&#26377;&#22806;&#37096;&#30340;$\pi$-&#26435;&#37325;&#65292;&#32780;&#26159;&#22312;&#34394;&#25311;&#30340;SIGTRON&#20135;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#26377;&#20869;&#37096;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#24403;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#27604;&#22914;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#27604;&#20363;&#19981;&#24179;&#34913;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#36866;&#24212;&#26159;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25311;&#29275;&#39039;&#20248;&#21270;(L-BFGS)&#26694;&#26550;&#30340;&#34394;&#25311;&#20984;&#25439;&#22833;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#20108;&#20998;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2306.06945</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Underwater Acoustic Target Recognition based on Smoothness-inducing Regularization and Spectrogram-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#24402;&#22240;&#20110;&#22797;&#26434;&#30340;&#27700;&#19979;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;&#25968;&#25454;&#19981;&#36275;&#21487;&#33021;&#20250;&#38459;&#30861;&#35782;&#21035;&#31995;&#32479;&#25903;&#25345;&#22797;&#26434;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#21457;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#35832;&#22914;&#25968;&#25454;&#22686;&#24378;&#20043;&#31867;&#30340;&#25216;&#26415;&#26469;&#27169;&#25311;&#27700;&#19979;&#20449;&#21495;&#24182;&#20016;&#23500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#27700;&#19979;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#25311;&#20449;&#21495;&#20559;&#31163;&#30495;&#23454;&#22330;&#26223;&#65292;&#23548;&#33268;&#21463;&#21040;&#38750;&#30495;&#23454;&#25968;&#25454;&#35823;&#23548;&#30340;&#20559;&#35265;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#39318;&#20808;&#65292;&#20316;&#20026;&#26367;&#20195;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#20165;&#32435;&#20837;&#27169;&#25311;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06945v2 Announce Type: replace-cross  Abstract: Underwater acoustic target recognition is a challenging task owing to the intricate underwater environments and limited data availability. Insufficient data can hinder the ability of recognition systems to support complex modeling, thus impeding their advancement. To improve the generalization capacity of recognition models, techniques such as data augmentation have been employed to simulate underwater signals and diversify data distribution. However, the complexity of underwater environments can cause the simulated signals to deviate from real scenarios, resulting in biased models that are misguided by non-true data. In this study, we propose two strategies to enhance the generalization ability of models in the case of limited data while avoiding the risk of performance degradation. First, as an alternative to traditional data augmentation, we utilize smoothness-inducing regularization, which only incorporates simulated signal
&lt;/p&gt;</description></item><item><title>&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;&#65292;&#23558;&#31639;&#27861;&#35270;&#20026;&#24320;&#25918;&#30340;&#21160;&#24577;&#31995;&#32479;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.14029</link><description>&lt;p&gt;
&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Systems Theory of Algorithms. (arXiv:2401.14029v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14029
&lt;/p&gt;
&lt;p&gt;
&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;&#65292;&#23558;&#31639;&#27861;&#35270;&#20026;&#24320;&#25918;&#30340;&#21160;&#24577;&#31995;&#32479;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#25968;&#20540;&#31639;&#27861;&#34987;&#35270;&#20026;&#29420;&#31435;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#23616;&#38480;&#20110;``in silico''&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35266;&#28857;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#20195;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#25511;&#21046;&#12289;&#23398;&#20064;&#25110;&#20248;&#21270;&#20013;&#65292;&#20854;&#20013;``in vivo''&#31639;&#27861;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#20123;``&#24320;&#25918;''&#30340;&#20363;&#23376;&#21253;&#25324;&#21508;&#31181;&#23454;&#26102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#31574;&#30053;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20915;&#31574;&#26550;&#26500;&#12289;&#22312;&#32447;&#20248;&#21270;&#31561;&#31561;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#23398;&#20064;&#25110;&#20248;&#21270;&#20013;&#65292;``&#38381;&#21512;''&#30340;&#31639;&#27861;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25277;&#35937;&#20026;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#27169;&#22359;&#21644;&#31649;&#36947;&#30340;&#22359;&#22270;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#25105;&#20204;&#23545;&#21363;&#23558;&#21457;&#23637;&#30340;``&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;''&#30340;&#24895;&#26223;&#65292;&#24182;&#20027;&#24352;&#23558;&#31639;&#27861;&#35270;&#20026;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#30340;&#24320;&#25918;&#21160;&#24577;&#31995;&#32479;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#31995;&#32479;&#29702;&#35770;&#30340;&#20254;&#19979;&#24320;&#21457;&#30340;&#22810;&#31181;&#24037;&#20855;&#20063;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, numerical algorithms are seen as isolated pieces of code confined to an {\em in silico} existence. However, this perspective is not appropriate for many modern computational approaches in control, learning, or optimization, wherein {\em in vivo} algorithms interact with their environment. Examples of such {\em open} include various real-time optimization-based control strategies, reinforcement learning, decision-making architectures, online optimization, and many more. Further, even {\em closed} algorithms in learning or optimization are increasingly abstracted in block diagrams with interacting dynamic modules and pipelines. In this opinion paper, we state our vision on a to-be-cultivated {\em systems theory of algorithms} and argue in favour of viewing algorithms as open dynamical systems interacting with other algorithms, physical systems, humans, or databases. Remarkably, the manifold tools developed under the umbrella of systems theory also provide valuable insights
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13639</link><description>&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#65306;&#23398;&#20064;&#29992;&#25143;&#21453;&#39304;&#32780;&#26080;&#38656;RL&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Learning: Learning from Human Feedback without RL. (arXiv:2310.13639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13639
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#30340;RLHF&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#40784;&#27169;&#22411;&#12290;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#26159;&#26681;&#25454;&#22870;&#21169;&#20998;&#24067;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23427;&#20204;&#36981;&#24490;&#29992;&#25143;&#26368;&#20339;&#31574;&#30053;&#19979;&#30340;&#36951;&#25022;&#12290;&#22240;&#27492;&#65292;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19981;&#20165;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#38169;&#35823;&#20551;&#35774;&#65292;&#36824;&#23548;&#33268;&#20102;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#25110;RL&#38454;&#27573;&#30340;&#33258;&#21161;&#27861;&#24341;&#36215;&#30340;&#26840;&#25163;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;&#36825;&#20123;&#20248;&#21270;&#25361;&#25112;&#65292;&#24403;&#20195;&#30340;RLHF&#26041;&#27861;&#38480;&#21046;&#33258;&#24049;&#21482;&#33021;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#25110;&#38480;&#21046;&#20102;&#35266;&#27979;&#32500;&#24230;&#65288;&#22914;&#22522;&#20110;&#29366;&#24577;&#30340;&#26426;&#22120;&#20154;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new famil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybridTree&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26641;&#20013;&#30340;&#19968;&#33268;&#20998;&#21106;&#35268;&#21017;&#65292;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11865</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#19978;&#26377;&#25928;&#39640;&#25928;&#30340;&#32852;&#37030;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Effective and Efficient Federated Tree Learning on Hybrid Data. (arXiv:2310.11865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11865
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybridTree&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26641;&#20013;&#30340;&#19968;&#33268;&#20998;&#21106;&#35268;&#21017;&#65292;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#20419;&#36827;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#38598;&#20013;&#22312;&#27700;&#24179;&#25968;&#25454;&#25110;&#22402;&#30452;&#25968;&#25454;&#35774;&#32622;&#19978;&#65292;&#20854;&#20013;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26469;&#33258;&#30456;&#21516;&#30340;&#29305;&#24449;&#25110;&#26679;&#26412;&#31354;&#38388;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#28151;&#21512;&#25968;&#25454;&#35774;&#32622;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#22312;&#29305;&#24449;&#21644;&#26679;&#26412;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HybridTree&#65292;&#19968;&#31181;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26641;&#20013;&#23384;&#22312;&#19968;&#33268;&#30340;&#20998;&#21106;&#35268;&#21017;&#12290;&#20511;&#21161;&#36825;&#20123;&#20998;&#21106;&#35268;&#21017;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#38656;&#35201;&#39057;&#32321;&#30340;&#36890;&#20449;&#27969;&#37327;&#26469;&#35757;&#32451;&#19968;&#26869;&#26641;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.07240</link><description>&lt;p&gt;
CacheGen&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24555;&#36895;&#19978;&#19979;&#25991;&#21152;&#36733;
&lt;/p&gt;
&lt;p&gt;
CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07240
&lt;/p&gt;
&lt;p&gt;
CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25215;&#25285;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#23558;&#25972;&#21512;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#24212;&#23545;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25110;&#29992;&#25143;&#29305;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#21709;&#24212;&#24335;&#30340;LLM&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#25152;&#26377;&#19978;&#19979;&#25991;&#34987;&#33719;&#21462;&#21644;LLM&#22788;&#29702;&#20043;&#21069;&#65292;&#26080;&#27861;&#29983;&#25104;&#20219;&#20309;&#20869;&#23481;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#35745;&#31639;&#24310;&#36831;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#32531;&#23384;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#20013;&#38388;&#38190;&#20540;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#19978;&#19979;&#25991;&#33719;&#21462;&#30340;&#32593;&#32476;&#24310;&#36831;&#26356;&#38271;&#65288;&#20363;&#22914;&#65292;&#38190;&#20540;&#29305;&#24449;&#28040;&#32791;&#30340;&#24102;&#23485;&#27604;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CacheGen&#65292;&#20197;&#26368;&#23567;&#21270;LLM&#19978;&#19979;&#25991;&#33719;&#21462;&#21644;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;CacheGen&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#29305;&#24449;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#27969;&#34920;&#31034;&#65292;&#20943;&#23569;&#20102;&#20256;&#36755;&#25152;&#38656;&#30340;&#24102;&#23485;&#12290;&#32534;&#30721;&#22120;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#37327;&#21270;&#21644;......
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#21307;&#23398;VLP&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.07027</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#32469;&#36807;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images. (arXiv:2310.07027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#21307;&#23398;VLP&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#20174;&#21307;&#23398;&#22270;&#20687;&#21644;&#37197;&#23545;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#32852;&#21512;&#23398;&#20064;&#34920;&#31034;&#12290;&#23427;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#12290;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#20165;&#20351;&#29992;&#20174;&#30495;&#23454;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26469;&#23454;&#29616;VLP&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#22823;&#37327;&#37197;&#23545;&#21644;&#31579;&#36873;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#38656;&#35201;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;VLP&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#26469;&#23457;&#26597;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#30495;&#23454;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#26367;&#25442;&#30495;&#23454;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;VLP&#31639;&#27861;&#19987;&#38376;&#22312;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#36830;&#32493;&#20219;&#21153;&#65292;&#21363;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Vision-Language Pre-training (VLP) learns representations jointly from medical images and paired radiology reports. It typically requires large-scale paired image-text datasets to achieve effective pre-training for both the image encoder and text encoder. The advent of text-guided generative models raises a compelling question: Can VLP be implemented solely with synthetic images generated from genuine radiology reports, thereby mitigating the need for extensively pairing and curating image-text datasets? In this work, we scrutinize this very question by examining the feasibility and effectiveness of employing synthetic images for medical VLP. We replace real medical images with their synthetic equivalents, generated from authentic medical reports. Utilizing three state-of-the-art VLP algorithms, we exclusively train on these synthetic samples. Our empirical evaluation across three subsequent tasks, namely image classification, semantic segmentation and object detection, reveals
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03718</link><description>&lt;p&gt;
&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19987;&#27880;&#20110;&#35757;&#32451;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#19979;&#33021;&#22815;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23433;&#20840;&#32422;&#26463;&#35201;&#27714;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#22810;&#21151;&#33021;&#23433;&#20840;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#20026;&#26410;&#24320;&#21457;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#20010;&#20027;&#35201;&#38656;&#27714;&#65306;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conditioned Constrained Policy Optimization&#65288;CCPO&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#65288;1&#65289;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#65288;VVE&#65289;&#65292;&#29992;&#20110;&#22312;&#26410;&#35265;&#36807;&#30340;&#38408;&#20540;&#26465;&#20214;&#19979;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#65288;2&#65289;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#65292;&#29992;&#20110;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#32534;&#30721;&#20219;&#24847;&#32422;&#26463;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CCPO&#22312;&#23433;&#20840;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#19981;&#21516;&#32422;&#26463;&#30340;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.08313</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#25311;&#21512;&#32622;&#20449;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Heteroskedastic conformal regression. (arXiv:2309.08313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#20197;&#21450;&#29305;&#23450;&#30340;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#19987;&#27880;&#20110;&#36793;&#38469;&#35206;&#30422;&#26102;&#65292;&#21363;&#22312;&#26657;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#39044;&#27979;&#21306;&#38388;&#24179;&#22343;&#21253;&#21547;&#39044;&#23450;&#20041;&#35206;&#30422;&#27700;&#24179;&#30340;&#30495;&#23454;&#20540;&#65292;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#21487;&#20197;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21306;&#38388;&#36890;&#24120;&#19981;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#22238;&#24402;&#38382;&#39064;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#38416;&#26126;&#22914;&#20309;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, and split conformal prediction as a specific implementation, offer a distribution-free approach to estimating prediction intervals with statistical guarantees. Recent work has shown that split conformal prediction can produce state-of-the-art prediction intervals when focusing on marginal coverage, i.e., on a calibration dataset the method produces on average prediction intervals that contain the ground truth with a predefined coverage level. However, such intervals are often not adaptive, which can be problematic for regression problems with heteroskedastic noise. This paper tries to shed new light on how adaptive prediction intervals can be constructed using methods such as normalized and Mondrian conformal prediction. We present theoretical and experimental results in which these methods are investigated in a systematic way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00957</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;(&#20998;&#32452;)&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference with Differentially Private (Clustered) Outcomes. (arXiv:2308.00957v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#23454;&#39564;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#21482;&#26377;&#22312;&#21442;&#19982;&#32773;&#21516;&#24847;&#36879;&#38706;&#20182;&#20204;&#21487;&#33021;&#25935;&#24863;&#30340;&#21709;&#24212;&#26102;&#25165;&#21487;&#34892;&#12290;&#22312;&#30830;&#20445;&#38544;&#31169;&#30340;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#38544;&#31169;&#20445;&#35777;&#24230;&#37327;&#65292;&#21487;&#20197;&#40723;&#21169;&#21442;&#19982;&#32773;&#20998;&#20139;&#21709;&#24212;&#32780;&#19981;&#20250;&#38754;&#20020;&#21435;&#21311;&#21517;&#21270;&#30340;&#39118;&#38505;&#12290;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#20250;&#21521;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#38899;&#26469;&#23454;&#29616;&#36825;&#31181;&#38544;&#31169;&#20445;&#35777;&#65292;&#36825;&#20250;&#22686;&#21152;&#22823;&#22810;&#25968;&#32479;&#35745;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#31934;&#30830;&#27979;&#37327;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#22256;&#38590;&#65306;&#20174;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#23384;&#22312;&#30528;&#22266;&#26377;&#30340;&#38544;&#31169;-&#26041;&#24046;&#26435;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#30340;&#36739;&#20302;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#21033;&#29992;&#25968;&#25454;&#30340;&#20219;&#20309;&#32473;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#21516;&#26102;&#20173;&#28982;&#20801;&#35768;&#23545;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;IQST&#31561;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#22810;&#37325;&#20219;&#21153;&#20248;&#21270;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#39318;&#20010;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13105</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-task Learning for Radar Signal Characterisation. (arXiv:2306.13105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;IQST&#31561;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#22810;&#37325;&#20219;&#21153;&#20248;&#21270;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#39318;&#20010;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#27665;&#29992;&#36824;&#26159;&#20891;&#20107;&#24212;&#29992;&#65292;&#26080;&#32447;&#30005;&#20449;&#21495;&#35782;&#21035;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26410;&#30693;&#20449;&#21495;&#30340;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#35782;&#21035;&#26159;&#39057;&#35889;&#31649;&#29702;&#21644;&#30005;&#23376;&#25112;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#37324;&#65292;&#22823;&#37096;&#20998;&#30340;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#36827;&#34892;&#35843;&#21046;&#20998;&#31867;&#65292;&#32780;&#27809;&#33021;&#20805;&#20998;&#30740;&#31350;&#20449;&#21495;&#29305;&#24449;&#21270;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#21644;&#29305;&#24449;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; IQ Signal Transformer (IQST) &#21644;&#20854;&#20182;&#21442;&#32771;&#26550;&#26500;&#65292;&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#22810;&#37325;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; MTL &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#27425;&#30340;&#38647;&#36798;&#20449;&#21495;&#29305;&#24449;&#21270;&#22522;&#20934;&#27979;&#35797;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio signal recognition is a crucial task in both civilian and military applications, as accurate and timely identification of unknown signals is an essential part of spectrum management and electronic warfare. The majority of research in this field has focused on applying deep learning for modulation classification, leaving the task of signal characterisation as an understudied area. This paper addresses this gap by presenting an approach for tackling radar signal classification and characterisation as a multi-task learning (MTL) problem. We propose the IQ Signal Transformer (IQST) among several reference architectures that allow for simultaneous optimisation of multiple regression and classification tasks. We demonstrate the performance of our proposed MTL model on a synthetic radar dataset, while also providing a first-of-its-kind benchmark for radar signal characterisation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#23450;&#20041;&#20986;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#32500;&#24230;&#20026;&#36755;&#20837;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#26041;&#20415;&#65292;&#21482;&#38656;&#25351;&#23450;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#19988;&#22312;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21487;&#20197;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.06327</link><description>&lt;p&gt;
&#20219;&#24847;&#32500;&#24230;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Any-dimensional equivariant neural networks. (arXiv:2306.06327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#21487;&#20197;&#23450;&#20041;&#20986;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#32500;&#24230;&#20026;&#36755;&#20837;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#26041;&#20415;&#65292;&#21482;&#38656;&#25351;&#23450;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#19988;&#22312;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21487;&#20197;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23558;&#20989;&#25968;&#25311;&#21512;&#21040;&#19968;&#32452;&#20855;&#26377;&#22266;&#23450;&#32500;&#24230;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#26469;&#23398;&#20064;&#26410;&#30693;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#22312;&#30456;&#21516;&#32500;&#24230;&#30340;&#36755;&#20837;&#19978;&#23450;&#20041;&#25311;&#21512;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26410;&#30693;&#26144;&#23556;&#20197;&#20219;&#24847;&#32500;&#24230;&#30340;&#36755;&#20837;&#20316;&#20026;&#36755;&#20837;&#65307;&#20363;&#22914;&#65292;&#23450;&#20041;&#22312;&#20219;&#24847;&#22823;&#23567;&#30340;&#22270;&#24418;&#19978;&#30340;&#22270;&#24418;&#21442;&#25968;&#21644;&#23450;&#20041;&#22312;&#20219;&#24847;&#25968;&#37327;&#31890;&#23376;&#19978;&#30340;&#29289;&#29702;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#20195;&#25968;&#25299;&#25169;&#20013;&#30340;&#26032;&#29616;&#35937;&#8212;&#8212;&#34920;&#31034;&#31283;&#23450;&#24615;&#65292;&#26469;&#23450;&#20041;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20351;&#29992;&#22266;&#23450;&#32500;&#24230;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#25193;&#23637;&#25509;&#21463;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#20351;&#29992;&#65292;&#21482;&#38656;&#35201;&#32593;&#32476;&#26550;&#26500;&#21644;&#31561;&#21464;&#24615;&#30340;&#32452;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#35757;&#32451;&#36807;&#31243;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24320;&#28304;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning aims to learn an unknown mapping by fitting a function to a set of input-output pairs with a fixed dimension. The fitted function is then defined on inputs of the same dimension. However, in many settings, the unknown mapping takes inputs in any dimension; examples include graph parameters defined on graphs of any size and physics quantities defined on an arbitrary number of particles. We leverage a newly-discovered phenomenon in algebraic topology, called representation stability, to define equivariant neural networks that can be trained with data in a fixed dimension and then extended to accept inputs in any dimension. Our approach is user-friendly, requiring only the network architecture and the groups for equivariance, and can be combined with any training procedure. We provide a simple open-source implementation of our methods and offer preliminary numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20197;&#21450;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#21028;&#21035;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.18204</link><description>&lt;p&gt;
&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#30340;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Kernel Mixtures for Probabilistic Deep Learning. (arXiv:2305.18204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20197;&#21450;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#21028;&#21035;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#37327;&#23376;&#26680;&#28151;&#21512;&#65292;&#23427;&#26159;&#20174;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#30340;&#25968;&#23398;&#24418;&#24335;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20174;&#32780;&#33021;&#22815;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#36793;&#38469;&#21644;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#24320;&#21457;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#21487;&#36870;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#23494;&#24230;&#20272;&#35745;&#12289;&#21028;&#21035;&#23398;&#20064;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65306;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#24471;&#30410;&#20110;&#37327;&#23376;&#26680;&#28151;&#21512;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to probabilistic deep learning (PDL), quantum kernel mixtures, derived from the mathematical formalism of quantum density matrices, which provides a simpler yet effective mechanism for representing joint probability distributions of both continuous and discrete random variables. The framework allows for the construction of differentiable models for density estimation, inference, and sampling, enabling integration into end-to-end deep neural models. In doing so, we provide a versatile representation of marginal and joint probability distributions that allows us to develop a differentiable, compositional, and reversible inference procedure that covers a wide range of machine learning tasks, including density estimation, discriminative learning, and generative modeling. We illustrate the broad applicability of the framework with two examples: an image classification model, which can be naturally transformed into a conditional generative model thanks to
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07769</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23436;&#25972;&#24490;&#29615;&#19968;&#33268;&#24615;GAN&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Regularized Complete Cycle Consistent GAN for Anomaly Detection. (arXiv:2304.07769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#35823;&#24046;&#20013;&#24490;&#29615;&#19968;&#33268;&#24615;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23041;&#21147;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#30001;&#20110;&#31867;&#21035;&#38388;&#31934;&#24230;&#39640;&#24230;&#24046;&#24322;&#32780;&#26410;&#34987;&#24212;&#29992;&#20110;&#25152;&#26377;&#31867;&#22411;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;RCALAD&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#23558;&#26032;&#30340;&#37492;&#21035;&#22120;&#24341;&#20837;&#21040;&#32467;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;RCALAD&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#34917;&#20805;&#20998;&#24067;&#65292;&#23558;&#37325;&#24314;&#24341;&#23548;&#21040;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#26679;&#26412;&#19982;&#20854;&#37325;&#24314;&#20998;&#31163;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#23637;&#31034;&#20986;&#20854;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an adversarial method for anomaly detection in real-world applications, leveraging the power of generative adversarial neural networks (GANs) through cycle consistency in reconstruction error. Previous methods suffer from the high variance between class-wise accuracy which leads to not being applicable for all types of anomalies. The proposed method named RCALAD tries to solve this problem by introducing a novel discriminator to the structure, which results in a more efficient training process. Additionally, RCALAD employs a supplementary distribution in the input space to steer reconstructions toward the normal data distribution, effectively separating anomalous samples from their reconstructions and facilitating more accurate anomaly detection. To further enhance the performance of the model, two novel anomaly scores are introduced. The proposed model has been thoroughly evaluated through extensive experiments on six various datasets, yielding results that demonst
&lt;/p&gt;</description></item><item><title>R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22686;&#24378;&#29256;&#30340;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#20989;&#25968;&#21253;&#21547;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#21151;&#33021;&#65292;&#20351;&#29992;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#12290;&#24182;&#21487;&#21019;&#24314;&#21487;&#35270;&#21270;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20854;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06145</link><description>&lt;p&gt;
R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
The growclusters Package for R. (arXiv:2304.06145v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06145
&lt;/p&gt;
&lt;p&gt;
R&#35821;&#35328;&#30340;growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22686;&#24378;&#29256;&#30340;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#20989;&#25968;&#21253;&#21547;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#21151;&#33021;&#65292;&#20351;&#29992;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#12290;&#24182;&#21487;&#21019;&#24314;&#21487;&#35270;&#21270;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20854;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
growclusters&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#19968;&#20010;&#22686;&#24378;&#29256;k-means&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#22810;&#32452;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#32858;&#31867;&#25110;&#20998;&#21306;&#65292;&#27599;&#32452;&#25968;&#25454;&#30340;&#32858;&#31867;&#20013;&#24515;&#37117;&#26469;&#28304;&#20110;&#19968;&#20010;&#20840;&#23616;&#20998;&#21306;&#12290;&#35813;&#36719;&#20214;&#21253;&#21253;&#21547;&#19968;&#20123;&#20272;&#35745;&#22810;&#20803;&#25968;&#25454;&#20998;&#21306;&#32467;&#26500;&#30340;&#20989;&#25968;&#12290;&#20272;&#35745;&#26159;&#22522;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#34920;&#36848;&#25512;&#23548;&#20986;&#30340;&#19968;&#31181;&#24809;&#32602;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;growclusters&#36719;&#20214;&#21253;&#30340;&#19968;&#20123;&#21151;&#33021;&#21644;&#33021;&#21147;&#65292;&#21253;&#25324;&#21019;&#24314;R Shiny&#24212;&#29992;&#31243;&#24207;&#20197;&#21487;&#35270;&#21270;&#23637;&#31034;growclusters&#36719;&#20214;&#21253;&#30340;&#25805;&#20316;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growclusters package for R implements an enhanced version of k-means clustering that allows discovery of local clusterings or partitions for a collection of data sets that each draw their cluster means from a single, global partition. The package contains functions to estimate a partition structure for multivariate data. Estimation is performed under a penalized optimization derived from Bayesian non-parametric formulations. This paper describes some of the functions and capabilities of the growclusters package, including the creation of R Shiny applications designed to visually illustrate the operation and functionality of the growclusters package.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#31639;&#23376;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#65292;&#22312; Korteweg-de Vries &#26041;&#31243;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02243</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#21450;&#38271;&#26102;&#38388;&#31215;&#20998;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks. (arXiv:2303.02243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#31639;&#23376;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#65292;&#22312; Korteweg-de Vries &#26041;&#31243;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20256;&#32479;&#31185;&#23398;&#35745;&#31639;&#26041;&#27861;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#12289;&#21487;&#20197;&#30452;&#25509;&#20174;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#31215;&#20998;&#20013;&#23384;&#22312;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31070;&#32463;&#31639;&#23376;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#25552;&#21319;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20934;&#30830;&#30340;&#31215;&#20998;&#12290;&#36825;&#20010;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#31639;&#23376;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#36882;&#24402;&#32467;&#26500;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#32508;&#21512;&#26694;&#26550;&#23545;&#20110; Korteweg-de Vries &#26041;&#31243;&#25554;&#20540;&#21644;&#22806;&#25512;&#22343;&#31283;&#23450;&#35299;&#20915;&#20102;&#35823;&#24046;&#31215;&#32047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks to construct a novel and effective architecture, resulting in superior accuracy compared to the state-of-the-art. The new hybrid model is based on operator learning while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;$\mathscr{L}_2(\mathbb{R})$&#30340;&#27491;&#20132;&#22522;&#20013;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#65292;&#23454;&#29616;&#20102;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2206.08648</link><description>&lt;p&gt;
&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Orthonormal Expansions for Translation-Invariant Kernels. (arXiv:2206.08648v3 [math.CA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08648
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;$\mathscr{L}_2(\mathbb{R})$&#30340;&#27491;&#20132;&#22522;&#20013;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#65292;&#23454;&#29616;&#20102;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#24179;&#31227;&#19981;&#21464;&#26680;&#20989;&#25968;&#30340;&#27491;&#20132;&#22522;&#23637;&#24320;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;$\mathscr{L}_2(\mathbb{R})$&#19978;&#30340;&#27491;&#20132;&#22522;&#65292;&#24471;&#21040;&#20102;&#23454;&#36724;&#19978;&#25152;&#26377;&#21322;&#25972;&#25968;&#38454;&#39532;&#29305;&#23572;&#26680;&#20989;&#25968;&#12289;&#26607;&#35199;&#26680;&#20989;&#25968;&#20197;&#21450;&#39640;&#26031;&#26680;&#20989;&#25968;&#30340;&#26126;&#30830;&#23637;&#24320;&#34920;&#36798;&#24335;&#65292;&#20998;&#21035;&#30001;&#30456;&#20851;&#30340;&#25289;&#30422;&#23572;&#20989;&#25968;&#12289;&#26377;&#29702;&#20989;&#25968;&#21644;&#21380;&#31859;&#20989;&#25968;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general Fourier analytic technique for constructing orthonormal basis expansions of translation-invariant kernels from orthonormal bases of $\mathscr{L}_2(\mathbb{R})$. This allows us to derive explicit expansions on the real line for (i) Mat\'ern kernels of all half-integer orders in terms of associated Laguerre functions, (ii) the Cauchy kernel in terms of rational functions, and (iii) the Gaussian kernel in terms of Hermite functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;</title><link>http://arxiv.org/abs/1511.05240</link><description>&lt;p&gt;
McDiarmid&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
An extension of McDiarmid's inequality. (arXiv:1511.05240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1511.05240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#25512;&#24191;&#35770;&#35777;&#25512;&#24191;McDiarmid&#19981;&#31561;&#24335;&#65292;&#20351;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#30340;&#20989;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#39640;&#27010;&#29575;&#38598;&#21512;&#12290;&#36825;&#20123;&#20989;&#25968;&#38598;&#20013;&#20110;&#23427;&#20204;&#30340;&#26465;&#20214;&#26399;&#26395;&#21608;&#22260;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize McDiarmid's inequality for functions with bounded differences on a high probability set, using an extension argument. Those functions concentrate around their conditional expectations. We further extend the results to concentration in general metric spaces.
&lt;/p&gt;</description></item></channel></rss>