<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2311.01276</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#36317;&#31163;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20998;&#23376;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GNN&#20027;&#35201;&#25797;&#38271;&#21033;&#29992;&#30701;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;SRI&#65289;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;LRI&#65289;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#30830;&#23450;&#20998;&#23376;&#24615;&#36136;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25152;&#26377;&#21407;&#23376;&#38544;&#24335;&#25237;&#23556;&#20026;&#23569;&#25968;&#31070;&#32463;&#21407;&#23376;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#31070;&#32463;&#21407;&#23376;&#25277;&#35937;&#20986;&#20998;&#23376;&#20869;&#21407;&#23376;&#32452;&#30340;&#38598;&#20307;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#22312;&#31070;&#32463;&#21407;&#23376;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#23558;&#20854;&#37325;&#26032;&#25237;&#23556;&#21040;&#21407;&#23376;&#30340;&#34920;&#31034;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26426;&#21046;&#65292;&#31070;&#32463;&#21407;&#23376;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36890;&#20449;&#36890;&#36947;&#65292;&#26377;&#25928;&#22320;&#23558;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#20943;&#23569;&#21040;&#21333;&#27425;&#36339;&#36291;&#12290;&#20026;&#20102;&#20174;&#29289;&#29702;&#35282;&#24230;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#19982;&#20256;&#32479;&#30340;LRI&#35745;&#31639;&#26041;&#27861;Ewald&#27714;&#21644;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21019;&#26032;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8,800&#20010;MOF&#26448;&#26009;&#30340;&#36229;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#26377;&#21069;&#36884;&#30340;&#21560;&#38468;&#21058;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00341</link><description>&lt;p&gt;
&#12298;2023&#24180;&#24320;&#25918;&#24335;DAC&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#21560;&#38468;&#21058;&#21457;&#29616;&#30340;&#25361;&#25112;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture. (arXiv:2311.00341v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21019;&#26032;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8,800&#20010;MOF&#26448;&#26009;&#30340;&#36229;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#26377;&#21069;&#36884;&#30340;&#21560;&#38468;&#21058;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#38656;&#26032;&#30340;&#20108;&#27687;&#21270;&#30899;&#21435;&#38500;&#26041;&#27861;&#26469;&#24212;&#23545;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#12290;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;(DAC)&#26159;&#19968;&#31181;&#20174;&#29615;&#22659;&#31354;&#27668;&#20013;&#30452;&#25509;&#25429;&#38598;&#20108;&#27687;&#21270;&#30899;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;(MOFs)&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;DAC&#30340;&#28508;&#22312;&#21487;&#23450;&#21046;&#21560;&#38468;&#21058;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#25506;&#32034;&#30340;&#24040;&#22823;&#21270;&#23398;&#31354;&#38388;&#21644;&#38656;&#35201;&#20102;&#35299;&#26448;&#26009;&#38543;&#28287;&#24230;&#21644;&#28201;&#24230;&#21464;&#21270;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#26377;&#21069;&#36884;&#30340;DAC MOF&#21560;&#38468;&#21058;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21019;&#26032;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Open DAC 2023 (ODAC23)&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;8,800&#20010;&#21547;&#26377;&#21560;&#38468;CO2&#21644;/&#25110;H2O&#30340;MOF&#26448;&#26009;&#30340;&#36229;&#36807;3800&#19975;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#35745;&#31639;&#12290; ODAC23&#26159;&#30446;&#21069;&#21487;&#29992;&#31934;&#30830;&#24230;&#30340;DFT&#32423;&#21035;&#20013;&#26368;&#22823;&#30340;MOF&#21560;&#38468;&#35745;&#31639;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25506;&#32034;&#21560;&#38468;&#20998;&#23376;&#30340;&#24615;&#36136;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#36824;&#26159;&#20449;&#24687;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
New methods for carbon dioxide removal are urgently needed to combat global climate change. Direct air capture (DAC) is an emerging technology to capture carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have been widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC is challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18285</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#32676;&#20307;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20419;&#20351;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#22330;&#26223;&#12290;&#20026;&#20102;&#28385;&#36275;FL&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#38656;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#24341;&#20837;&#20102;&#21516;&#26102;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#21516;&#26102;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#32676;&#20307;&#29305;&#23450;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#36873;&#25321;&#27169;&#22359;&#20026;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#20010;&#24615;&#21270;&#30340;&#32676;&#20307;&#25552;&#31034;&#65292;&#20351;&#20840;&#23616;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25645;&#24314;&#20102;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16842</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#23884;&#20837;&#24335;FPGA&#20013;LSTM&#21333;&#20803;&#30340;&#21534;&#21520;&#37327;&#29942;&#39048;&#65292;&#22686;&#24378;&#33021;&#25928;
&lt;/p&gt;
&lt;p&gt;
Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#23884;&#20837;&#24335;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#19968;&#32500;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#36807;&#21435;&#65292;&#32463;&#24120;&#20351;&#29992;CNN&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#29305;&#27530;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#27604;&#22914;FPGA&#26469;&#35828;&#24456;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#22411;LSTM&#21333;&#20803;&#20248;&#21270;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#30340;&#31616;&#21333;LSTM&#27169;&#22411;&#22312;FPGA XC7S15&#65288;&#26469;&#33258;Spartan-7&#31995;&#21015;&#65289;&#19978;&#27599;&#31186;&#21487;&#23454;&#29616;17534&#20010;&#25512;&#26029;&#65292;&#20165;&#28040;&#32791;&#27599;&#20010;&#25512;&#26029;3.8&#24494;&#28966;&#32819;&#30340;&#33021;&#37327;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#30340;&#21534;&#21520;&#37327;&#33267;&#23569;&#25552;&#39640;&#20102;5.4&#20493;&#65292;&#33021;&#25928;&#25552;&#39640;&#20102;1.37&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.15848</link><description>&lt;p&gt;
&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#20844;&#24179;&#24615;&#12289;&#38544;&#31169;&#21644;&#27861;&#35268;&#20934;&#21017;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#36827;&#20837;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#20449;&#24615;&#23384;&#22312;&#20005;&#37325;&#25285;&#24551;&#12290;&#31185;&#23398;&#30028;&#33268;&#21147;&#20110;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20854;&#24320;&#21457;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#34892;&#20026;&#30446;&#26631;&#12290;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#26377;&#21487;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#31639;&#27861;&#30340;&#32570;&#38519;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#31639;&#27861;&#30340;&#21518;&#26399;&#35780;&#20272;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#65292;&#32780;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21333;&#29420;&#32771;&#34385;&#25968;&#25454;&#32452;&#20214;&#20197;&#29702;&#35299;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
&lt;/p&gt;</description></item><item><title>ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.13258</link><description>&lt;p&gt;
ManiCast: &#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13258
&lt;/p&gt;
&lt;p&gt;
ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#23494;&#21512;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20381;&#36182;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#22823;&#35268;&#27169;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#25805;&#20316;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20851;&#38190;&#36716;&#25240;&#28857;&#22788;&#31215;&#32047;&#20102;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#19979;&#28216;&#35268;&#21010;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19982;&#20854;&#39044;&#27979;&#26368;&#26377;&#21487;&#33021;&#30340;&#20154;&#20307;&#36816;&#21160;&#65292;&#20135;&#29983;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#23601;&#36275;&#22815;&#20102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ManiCast&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#35268;&#21010;&#22120;&#20197;&#25191;&#34892;&#21327;&#21516;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26426;&#26800;&#33218;&#22312;&#22914;&#21453;&#24212;&#25605;&#25292;&#12289;&#29289;&#20307;&#20132;&#25509;&#21644;&#21327;&#21516;&#25670;&#26700;&#31561;&#22810;&#20010;&#30495;&#23454;&#20219;&#21153;&#20013;&#30340;&#27969;&#30021;&#12289;&#23454;&#26102;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;&#36816;&#21160;&#39044;&#27979;&#21644;&#31471;&#21040;&#31471;&#30340;&#39044;&#27979;-&#35268;&#21010;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;MUD-EXP3&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#65292;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#12290;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11188</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;&#21450;&#20854;&#22312;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback. (arXiv:2310.11188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;MUD-EXP3&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#65292;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#12290;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#24310;&#36831;&#21453;&#39304;&#32467;&#26524;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#65292;&#24182;&#19988;&#23545;&#20869;&#37096;&#20998;&#24067;&#27809;&#26377;&#38480;&#21046;&#12290;&#24403;&#29609;&#23478;&#36873;&#25321;&#19968;&#20010;&#33218;&#26102;&#65292;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#21453;&#39304;&#21487;&#33021;&#19981;&#20250;&#31435;&#21363;&#25509;&#25910;&#21040;&#65292;&#32780;&#26159;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#26102;&#38388;&#24310;&#36831;&#20043;&#21518;&#25165;&#33021;&#25509;&#25910;&#21040;&#12290;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#65292;&#19981;&#21516;&#29992;&#25143;&#30340;&#21453;&#39304;&#24310;&#36831;&#27809;&#26377;&#28508;&#22312;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MUD-EXP3&#30340;&#25913;&#36827;EXP3&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#30340;&#36951;&#25022;&#20540;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#26356;&#24120;&#35265;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.10541</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#19968;&#22823;&#22411;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#21644;&#21442;&#25968;&#35843;&#25972;&#36807;&#31243;&#21464;&#24471;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#23558;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#35757;&#32451;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#33021;&#26377;&#25928;&#26367;&#20195;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#20165;&#20851;&#27880;&#25913;&#36827;&#23398;&#29983;&#25104;&#32489;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#35748;&#35782;&#21040;&#19987;&#23478;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#37325;&#35201;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#21518;&#32493;&#25968;&#25454;&#38598;&#31934;&#28860;&#20013;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#19987;&#23478;&#36712;&#36857;&#26102;&#65292;&#19987;&#23478;&#30340;&#24179;&#28369;&#24615;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#65292;&#20197;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#20197;&#21069;&#30740;&#31350;&#34920;&#26126;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#20294;&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#25277;&#26679;&#30340;&#21152;&#26435;LBA&#21487;&#20197;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.10092</link><description>&lt;p&gt;
&#36890;&#36807;&#32858;&#21512;&#23454;&#29616;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Label Differential Privacy via Aggregation. (arXiv:2310.10092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10092
&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#30740;&#31350;&#34920;&#26126;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#20294;&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#20855;&#26377;&#38543;&#26426;&#25277;&#26679;&#30340;&#21152;&#26435;LBA&#21487;&#20197;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#38544;&#31169;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20445;&#25252;&#25935;&#24863;&#35757;&#32451;&#26631;&#31614;&#30340;&#38544;&#31169;&#12290;&#22312;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;(LLP)&#26694;&#26550;&#20013;&#65292;&#25968;&#25454;&#38598;&#34987;&#21010;&#20998;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#21253;&#65292;&#21482;&#33021;&#33719;&#24471;&#27599;&#20010;&#21253;&#20013;&#26631;&#31614;&#30340;&#24635;&#21644;&#12290;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#38480;&#21046;&#23398;&#20064;(LBA)&#26159;&#21482;&#33021;&#33719;&#24471;&#21253;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#24635;&#21644;&#65288;&#21487;&#33021;&#26159;&#21152;&#26435;&#30340;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#31181;&#32858;&#21512;&#25216;&#26415;&#26159;&#21542;&#33021;&#22815;&#22312;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;(label-DP)&#30340;&#27010;&#24565;&#19979;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#35813;&#27010;&#24565;&#20043;&#21069;&#22312;[Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22]&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#24456;&#23481;&#26131;&#30475;&#20986;&#65292;&#26420;&#32032;&#30340;LBA&#21644;LLP&#19981;&#33021;&#25552;&#20379;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;$m$&#20010;&#38543;&#26426;&#25277;&#26679;&#30340;&#19981;&#30456;&#20132;$k$-&#22823;&#23567;&#21253;&#30340;&#21152;&#26435;LBA&#23454;&#38469;&#19978;&#26159;$(\varepsilon,
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].  It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, 
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.08049</link><description>&lt;p&gt;
&#25506;&#32034;&#27169;&#22411;&#26550;&#26500;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08049
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26550;&#26500;&#21644;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#26377;&#20160;&#20040;&#20851;&#32852;&#65311;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21313;&#20116;&#31181;&#27169;&#22411;&#26550;&#26500;&#22312;&#19968;&#22871;&#21512;&#25104;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25152;&#36873;&#30340;&#26550;&#26500;&#20195;&#34920;&#20102;&#21508;&#31181;&#33539;&#24335;&#65292;&#21253;&#25324;&#24490;&#29615;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#25442;&#22120;&#20197;&#21450;&#26032;&#20852;&#30340;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26550;&#26500;&#37117;&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;&#24403;&#20195;&#26550;&#26500;&#34920;&#29616;&#26368;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21518;&#32493;&#23454;&#39564;&#25506;&#32034;&#20102;&#19968;&#20123;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26576;&#20123;&#26550;&#26500;&#21576;&#29616;&#20986;&#24179;&#31283;&#12289;&#28176;&#36827;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#32780;...
&lt;/p&gt;
&lt;p&gt;
What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03915</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#24490;&#29615;&#36830;&#25509;&#36827;&#34892;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24320;&#21457;&#33021;&#22815;&#19982;&#21464;&#21270;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#33258;&#20027;&#20195;&#29702;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#31283;&#20581;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#20195;&#29702;&#36890;&#24120;&#26159;&#22312;&#19987;&#23478;&#31034;&#33539;&#20013;&#36827;&#34892;&#31163;&#32447;&#25311;&#21512;&#65292;&#20294;&#22312;&#22312;&#32447;&#37096;&#32626;&#26102;&#24517;&#39035;&#33021;&#22815;&#25512;&#24191;&#21040;&#29615;&#22659;&#20869;&#30340;&#38381;&#29615;&#21453;&#39304;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#36825;&#31867;&#20219;&#21153;&#65292;&#24182;&#20102;&#35299;&#20854;&#24490;&#29615;&#36830;&#25509;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#38381;&#29615;&#35774;&#32622;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24490;&#29615;&#36830;&#25509;&#34920;&#31034;&#20026;&#31209;&#21644;&#31232;&#30095;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#23637;&#31034;&#35843;&#33410;&#36825;&#20004;&#20010;&#21464;&#37327;&#23545;&#32593;&#32476;&#21160;&#24577;&#30340;&#26377;&#30410;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#20026;&#32593;&#32476;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#20808;&#39564;&#65292;&#23545;&#20110;&#19968;&#31867;&#31216;&#20026;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#65288;CfCs&#65289;&#30340;&#27169;&#22411;&#26368;&#20026;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;CfCs&#21487;&#20197;&#36229;&#36807;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#25913;&#36827;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#27604;&#36739;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#23485;&#24230;&#65292;&#21457;&#29616;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#36739;&#23567;&#30340;&#26497;&#38480;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01547</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#30028;&#22343;&#20540;&#30340;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#36817;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the near-optimality of betting confidence sets for bounded means. (arXiv:2310.01547v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#30340;&#25913;&#36827;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#27604;&#36739;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#23485;&#24230;&#65292;&#21457;&#29616;&#25237;&#27880;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#36739;&#23567;&#30340;&#26497;&#38480;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#20013;&#65292;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#35266;&#27979;&#20013;&#26500;&#24314;&#19968;&#20803;&#20998;&#24067;&#30340;&#38750;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#23545;&#20110;&#26377;&#30028;&#35266;&#27979;&#20540;&#65292;&#32463;&#20856;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#36890;&#36807;&#21453;&#36716;&#26631;&#20934;&#27987;&#24230;&#30028;&#38480;&#65288;&#22914;Hoeffding&#25110;Bernstein&#19981;&#31561;&#24335;&#65289;&#26469;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#26367;&#20195;&#30340;&#22522;&#20110;&#25237;&#27880;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#23450;&#20041;CI&#21644;&#20854;&#26102;&#38388;&#19968;&#33268;&#21464;&#20307;&#65292;&#31216;&#20026;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;&#26412;&#25991;&#20026;&#36825;&#31181;&#25237;&#27880;CI&#21644;CS&#30340;&#25913;&#36827;&#32463;&#39564;&#24615;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;CI&#65292;&#20351;&#29992;&#23427;&#20204;&#30340;&#19968;&#38454;&#28176;&#36817;&#23485;&#24230;&#30340;&#20540;&#65288;&#32463;&#36807;$\sqrt{n}$&#32553;&#25918;&#65289;&#65292;&#24182;&#19988;&#34920;&#26126;Waudby-Smith&#21644;Ramdas&#65288;2023&#65289;&#30340;&#25237;&#27880;CI&#27604;&#29616;&#26377;&#30340;&#32463;&#39564;Bernstein&#65288;EB&#65289;CI&#30340;&#26497;&#38480;&#23485;&#24230;&#26356;&#23567;&#12290;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.  Our main contributions are as follows: (i) We first compare CIs using the values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two lower bounds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17338</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#36890;&#36807;&#21435;&#25481;&#33322;&#28857;&#26469;&#25913;&#36827;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#30340;&#22810;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#26412;&#36136;&#32473;&#20934;&#30830;&#24314;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#24517;&#39035;&#26377;&#25928;&#22320;&#20174;&#36807;&#21435;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22534;&#21472;&#27169;&#22411;&#20013;&#30340;&#21333;&#29420;&#32452;&#20214;&#23398;&#20064;&#26102;&#38388;&#36816;&#21160;&#65292;&#20197;&#25429;&#25417;&#26102;&#38388;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Temporal Waypoint Dropping&#65288;TWD&#65289;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#12290;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#23398;&#20064;&#21487;&#20197;&#36843;&#20351;&#27169;&#22411;&#25913;&#21892;&#20854;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#32852;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#35266;&#27979;&#21040;&#30340;&#36712;&#36857;&#33322;&#28857;&#24207;&#21015;&#26159;&#23436;&#25972;&#30340;&#65292;&#24573;&#30053;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.16883</link><description>&lt;p&gt;
&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;Lipschitz-&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22122;&#22768;&#36755;&#20837;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#21322;&#24452;&#26159;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36275;&#22815;&#35748;&#35777;&#21322;&#24452;&#30340;&#39640;&#25928;&#20998;&#31867;&#22120;&#21602;&#65311;&#38543;&#26426;&#24179;&#28369;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#33719;&#24471;&#24179;&#28369;&#19988;&#26356;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;&#21478;&#22806;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#21363;&#20854;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#23545;&#24179;&#28369;&#20998;&#31867;&#22120;&#21644;&#32463;&#39564;&#26041;&#24046;&#30340;&#21452;&#37325;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#65292;&#20197;&#20415;&#36890;&#36807;Bernst&#30340;&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;&#26469;&#21033;&#29992;&#22522;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#30028;&#38480;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26080;&#27861;&#25214;&#21040;&#32039;&#33268;&#30340;&#30028;&#38480;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13658</link><description>&lt;p&gt;
&#26080;&#27861;&#25214;&#21040;&#20986;&#33394;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantastic Generalization Measures are Nowhere to be Found. (arXiv:2309.13658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#30028;&#38480;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26080;&#27861;&#25214;&#21040;&#32039;&#33268;&#30340;&#30028;&#38480;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#27867;&#21270;&#30028;&#38480;&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#37117;&#19981;&#26159;&#32039;&#33268;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#20182;&#20204;&#30340;&#35770;&#25991;&#8220;Fantastic Generalization Measures and Where to Find Them&#8221;&#20013;&#65292;Jiang&#31561;&#20154;&#65288;2020&#65289;&#26816;&#26597;&#20102;&#21313;&#20960;&#20010;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#26377;&#21487;&#33021;&#25214;&#21040;&#32039;&#33268;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#20004;&#31181;&#27867;&#21270;&#30028;&#38480;&#65306;&#65288;1&#65289;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#23398;&#20064;&#31639;&#27861;&#36755;&#20986;&#30340;&#30028;&#38480;&#12290;&#25991;&#29486;&#20013;&#26377;&#22810;&#20010;&#36825;&#31181;&#31867;&#22411;&#30340;&#30028;&#38480;&#65288;&#20363;&#22914;&#22522;&#20110;&#33539;&#25968;&#21644;&#22522;&#20110;&#38388;&#38548;&#30340;&#30028;&#38480;&#65289;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#30028;&#38480;&#33021;&#22815;&#19968;&#33268;&#22320;&#32039;&#33268;&#65307;&#65288;2&#65289;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, none of these bounds are tight. For instance, in their paper ``Fantastic Generalization Measures and Where to Find Them'', Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them imply guarantees that can explain the remarkable performance of neural networks. This raises the question of whether tight generalization bounds are at all possible. We consider two types of generalization bounds common in the literature: (1) bounds that depend on the training set and the output of the learning algorithm. There are multiple bounds of this type in the literature (e.g., norm-based and margin-based bounds), but we prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that depend on the training set and on the 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#23558;&#39034;&#24207;&#21464;&#21270;&#26816;&#27979;&#31616;&#21270;&#20026;&#39034;&#24207;&#20272;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#24207;&#21015;&#26469;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.09111</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#21464;&#21270;&#26816;&#27979;&#31616;&#21270;&#20026;&#39034;&#24207;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reducing sequential change detection to sequential estimation. (arXiv:2309.09111v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#23558;&#39034;&#24207;&#21464;&#21270;&#26816;&#27979;&#31616;&#21270;&#20026;&#39034;&#24207;&#20272;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#24207;&#21015;&#26469;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#39034;&#24207;&#21464;&#21270;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#25968;&#25454;&#27969;&#20998;&#24067;&#20013;&#21442;&#25968;&#25110;&#20989;&#25968;&#120579;&#30340;&#20219;&#20309;&#21464;&#21270;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20855;&#26377;&#36739;&#23567;&#30340;&#26816;&#27979;&#24310;&#36831;&#65292;&#20294;&#22312;&#27809;&#26377;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20445;&#35777;&#20551;&#35686;&#25253;&#30340;&#39057;&#29575;&#21463;&#25511;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32622;&#20449;&#24207;&#21015;&#25551;&#36848;&#20102;&#19968;&#31181;&#20174;&#39034;&#24207;&#21464;&#21270;&#26816;&#27979;&#21040;&#39034;&#24207;&#20272;&#35745;&#30340;&#31616;&#21333;&#32422;&#21270;&#26041;&#27861;&#65306;&#25105;&#20204;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#24320;&#22987;&#19968;&#20010;&#26032;&#30340;$(1-\alpha)$&#32622;&#20449;&#24207;&#21015;&#65292;&#24182;&#22312;&#25152;&#26377;&#27963;&#21160;&#32622;&#20449;&#24207;&#21015;&#30340;&#20132;&#38598;&#20026;&#31354;&#26102;&#23459;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24179;&#22343;&#25345;&#32493;&#26102;&#38388;&#33267;&#23569;&#20026;$1/\alpha$&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20855;&#26377;&#26368;&#23567;&#32467;&#26500;&#20551;&#35774;&#30340;&#21464;&#21270;&#26816;&#27979;&#26041;&#26696;&#65288;&#22240;&#27492;&#20801;&#35768;&#21487;&#33021;&#30456;&#20851;&#30340;&#35266;&#27979;&#21644;&#38750;&#21442;&#25968;&#20998;&#24067;&#31867;&#65289;&#65292;&#20294;&#21364;&#20855;&#26377;&#24378;&#22823;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;Lorden&#65288;1971&#65289;&#30340;&#21464;&#21270;&#26816;&#27979;&#21040;&#39034;&#24207;&#27979;&#35797;&#30340;&#31616;&#21270;&#21644;Shin&#31561;&#20154;&#30340;e-detector&#26377;&#30528;&#26377;&#36259;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequential change detection, where the goal is to design a scheme for detecting any changes in a parameter or functional $\theta$ of the data stream distribution that has small detection delay, but guarantees control on the frequency of false alarms in the absence of changes. In this paper, we describe a simple reduction from sequential change detection to sequential estimation using confidence sequences: we begin a new $(1-\alpha)$-confidence sequence at each time step, and proclaim a change when the intersection of all active confidence sequences becomes empty. We prove that the average run length is at least $1/\alpha$, resulting in a change detection scheme with minimal structural assumptions~(thus allowing for possibly dependent observations, and nonparametric distribution classes), but strong guarantees. Our approach bears an interesting parallel with the reduction from change detection to sequential testing of Lorden (1971) and the e-detector of Shin e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#38024;&#23545;&#35828;&#35805;&#20154;&#32423;&#21035;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#30340;&#22768;&#38899;&#26159;&#21542;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#22768;&#38899;&#26377;&#20851;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#30456;&#21516;&#12290;&#36825;&#23545;&#23454;&#36341;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07983</link><description>&lt;p&gt;
SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems
&lt;/p&gt;
&lt;p&gt;
SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems. (arXiv:2309.07983v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#38024;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#38024;&#23545;&#35828;&#35805;&#20154;&#32423;&#21035;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#30340;&#22768;&#38899;&#26159;&#21542;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#22768;&#38899;&#26377;&#20851;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#30456;&#21516;&#12290;&#36825;&#23545;&#23454;&#36341;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36827;&#34892;&#27492;&#31867;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#30740;&#31350;&#19987;&#27880;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;SR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#22522;&#20110;&#22768;&#38899;&#30340;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLMIA-SR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;SR&#37327;&#36523;&#23450;&#21046;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#31034;&#20363;&#32423;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#29305;&#28857;&#26159;&#35828;&#35805;&#20154;&#32423;&#21035;&#30340;&#25104;&#21592;&#25512;&#26029;&#65292;&#21363;&#30830;&#23450;&#32473;&#23450;&#25512;&#26029;&#22768;&#38899;&#20013;&#26159;&#21542;&#26377;&#20219;&#20309;&#32473;&#23450;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#19982;&#32473;&#23450;&#25512;&#26029;&#22768;&#38899;&#30456;&#21516;&#12290;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#25512;&#26029;&#22768;&#38899;&#36890;&#24120;&#26159;&#19981;&#21516;&#30340;&#65292;&#32780;&#19988;&#32771;&#34385;&#21040;SR&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#20063;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#21363;&#35782;&#21035;&#35828;&#35805;&#20154;&#24448;&#24448;&#27809;&#26377;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#35757;&#32451;&#30446;&#26631;&#65306;&#20869;&#37096;&#25509;&#36817;&#24230;&#21644;&#22806;&#37096;&#36828;&#31163;&#24230;&#65292;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks allow adversaries to determine whether a particular example was contained in the model's training dataset. While previous works have confirmed the feasibility of such attacks in various applications, none has focused on speaker recognition (SR), a promising voice-based biometric recognition technique. In this work, we propose SLMIA-SR, the first membership inference attack tailored to SR. In contrast to conventional example-level attack, our attack features speaker-level membership inference, i.e., determining if any voices of a given speaker, either the same as or different from the given inference voices, have been involved in the training of a model. It is particularly useful and practical since the training and inference voices are usually distinct, and it is also meaningful considering the open-set nature of SR, namely, the recognition speakers were often not present in the training data. We utilize intra-closeness and inter-farness, two training objec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07176</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#20844;&#24179;&#30340;&#40723;&#21169;&#25919;&#31574;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#24378;&#21046;&#20010;&#20307;&#25509;&#21463;&#27835;&#30103;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#27492;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25509;&#21463;&#27835;&#30103;&#30340;&#20010;&#20307;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#65292;&#27835;&#30103;&#25928;&#26524;&#20063;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#34429;&#28982;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#21487;&#20197;&#26368;&#22823;&#21270;&#25972;&#20010;&#20154;&#32676;&#30340;&#22240;&#26524;&#32467;&#26524;&#65292;&#20294;&#22312;&#40723;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#35775;&#38382;&#24179;&#31561;&#38480;&#21046;&#25110;&#20854;&#20182;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#38590;&#39064;&#26159;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#20174;&#20013;&#21463;&#30410;&#30340;&#20154;&#20013;&#37027;&#20123;&#33719;&#30410;&#26381;&#21153;&#30340;&#20351;&#29992;&#24046;&#36317;&#12290;&#24403;&#20915;&#31574;&#32773;&#23545;&#35775;&#38382;&#21644;&#24179;&#22343;&#32467;&#26524;&#37117;&#26377;&#20998;&#37197;&#20559;&#22909;&#26102;&#65292;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#35782;&#21035;&#12289;&#32479;&#35745;&#26041;&#24046;&#20943;&#23569;&#20272;&#35745;&#21644;&#31283;&#20581;&#20272;&#35745;&#30340;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#65292;&#21253;&#25324;&#22312;&#36829;&#21453;&#38451;&#24615;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#25913;&#36827;&#26041;&#27861;TKNN-Shapley&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.15709</link><description>&lt;p&gt;
&#38408;&#20540;KNN-Shapley&#65306;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#21644;&#38544;&#31169;&#21451;&#22909;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation. (arXiv:2308.15709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#25913;&#36827;&#26041;&#27861;TKNN-Shapley&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26159;&#25968;&#25454;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#26088;&#22312;&#37327;&#21270;&#21333;&#20010;&#25968;&#25454;&#28304;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30528;&#24456;&#22810;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#38544;&#31169;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#26368;&#23454;&#29992;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#20043;&#19968;KNN-Shapley&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;KNN-Shapley&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;KNN-Shapley&#25913;&#36827;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;(DP)&#30340;&#26174;&#33879;&#25216;&#26415;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TKNN-Shapley&#65292;KNN-Shapley&#30340;&#19968;&#31181;&#25913;&#36827;&#21464;&#20307;&#65292;&#20855;&#26377;&#38544;&#31169;&#21451;&#22909;&#24615;&#65292;&#21487;&#20197;&#36827;&#34892;&#31616;&#21333;&#30340;&#20462;&#27491;&#20197;&#21253;&#21547;DP&#20445;&#35777;&#65288;DP-TKNN-Shapley&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;DP-TKNN-Shapley&#22312;&#36776;&#21035;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#24182;&#22312;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#26420;&#32032;&#21270;&#30340;KNN-Shapley&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#38750;&#38544;&#31169;&#30340;TKNN-Shapley&#20063;&#33021;&#20197;&#32447;&#24615;&#26102;&#38388;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation, a critical aspect of data-centric ML research, aims to quantify the usefulness of individual data sources in training machine learning (ML) models. However, data valuation faces significant yet frequently overlooked privacy challenges despite its importance. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical difficulties in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. Moreover, even non-private TKNN-Shapley ac
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.14969</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;&#19979;&#30340;&#37325;&#26032;&#32534;&#31243;: &#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14969
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#39044;&#31639;&#24040;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#65292;&#19979;&#28216;&#20219;&#21153;&#24050;&#32463;&#36716;&#21521;&#20102;&#39640;&#25928;&#24555;&#36895;&#36866;&#24212;&#30340;&#21465;&#36848;&#12290;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;; &#21069;&#32773;&#26088;&#22312;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#23398;&#20064;&#32447;&#24615;&#22836;&#37096;&#20998;&#31867;&#22120;&#65292;&#32780;&#21518;&#32773;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#26368;&#21021;&#22312;&#20854;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28304;&#25968;&#25454;&#39046;&#22495;&#12290;&#23613;&#31649;&#24191;&#27867;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;LP&#21644;VP&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#24230;&#36724;&#26469;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;: (a) &#25968;&#25454;&#31232;&#30095;&#24615;: &#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450; (b) &#27169;&#22411;&#31232;&#30095;&#24615;: &#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24425;&#31080;&#24335;&#36716;&#31227;&#19981;&#26159;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#65292;&#21363;&#23545;&#20110;&#26576;&#20123;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#37325;&#26032;&#32534;&#31243;&#24425;&#31080;&#24335;&#36716;&#31227;&#20250;&#20135;&#29983;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08736</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#26159;&#20154;&#20204;&#20102;&#35299;&#36719;&#20214;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#30001;&#20110;&#29616;&#20195;&#36719;&#20214;&#26550;&#26500;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#12290;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#25991;&#26412;&#26085;&#24535;&#25968;&#25454;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#21521;&#37327;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#19988;&#24517;&#19981;&#21487;&#23569;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#23545;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#65292;&#36825;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#36873;&#25321;&#20854;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20013;&#26368;&#20339;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#24182;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20845;&#31181;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#19971;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22235;&#31181;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>DeepTSF&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#26694;&#26550;&#65292;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#38761;&#26032;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00709</link><description>&lt;p&gt;
DeepTSF&#65306;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;
&lt;/p&gt;
&lt;p&gt;
DeepTSF: Codeless machine learning operations for time series forecasting. (arXiv:2308.00709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00709
&lt;/p&gt;
&lt;p&gt;
DeepTSF&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#26694;&#26550;&#65292;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#38761;&#26032;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepTSF&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#65288;MLOps&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#26469;&#38761;&#26032;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;DeepTSF&#33258;&#21160;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#20351;&#20854;&#25104;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;MLops&#24037;&#31243;&#24072;&#21442;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#39044;&#27979;&#30340;&#29702;&#24819;&#24037;&#20855;&#12290;DeepTSF&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#35774;&#35745;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#20379;&#22686;&#24378;&#30340;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20854;&#20182;&#39640;&#32423;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21069;&#31471;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#65292;&#36890;&#36807;&#26377;&#35265;&#22320;&#30340;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#23454;&#29616;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;DeepTSF&#36824;&#36890;&#36807;&#36523;&#20221;&#31649;&#29702;&#21644;&#35775;&#38382;&#25480;&#26435;&#26426;&#21046;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#22312;I-NERGY&#39033;&#30446;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DeepTSF&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DeepTSF, a comprehensive machine learning operations (MLOps) framework aiming to innovate time series forecasting through workflow automation and codeless modeling. DeepTSF automates key aspects of the ML lifecycle, making it an ideal tool for data scientists and MLops engineers engaged in machine learning (ML) and deep learning (DL)-based forecasting. DeepTSF empowers users with a robust and user-friendly solution, while it is designed to seamlessly integrate with existing data analysis workflows, providing enhanced productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists, as well as other higher-level stakeholders, enabling comprehensive understanding through insightful visualizations and evaluation metrics. DeepTSF also prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already proven DeepTSF's ef
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20998;&#25968;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#23454;&#29616;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06431</link><description>&lt;p&gt;
&#33021;&#37327;&#24046;&#24322;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#29420;&#31435;&#20110;&#35780;&#20998;&#30340;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06431
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33021;&#37327;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20998;&#25968;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#23454;&#29616;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27169;&#22411;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#26222;&#21450;&#21463;&#21040;&#20102;&#35757;&#32451;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#33021;&#37327;&#24046;&#24322;&#65288;ED&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20998;&#25968;&#30340;&#35745;&#31639;&#25110;&#26114;&#36149;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;&#26497;&#38480;&#19979;&#65292;ED&#25509;&#36817;&#20110;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#65292;&#26377;&#25928;&#22320;&#22312;&#20004;&#32773;&#20043;&#38388;&#25554;&#20540;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;ED&#20272;&#35745;&#20811;&#26381;&#20102;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#20272;&#35745;&#26041;&#27861;&#20013;&#36935;&#21040;&#30340;&#36817;&#35270;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#20139;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26174;&#24335;&#20998;&#25968;&#21305;&#37197;&#25110;&#23545;&#27604;&#25955;&#24230;&#30456;&#27604;&#65292;ED&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20302;&#32500;&#25968;&#25454;&#20998;&#24067;&#12290;&#23545;&#20110;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27969;&#24418;&#20551;&#35774;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#23545;e&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#33021;&#37327;&#24046;&#24322;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#31867;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#22312;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#26368;&#39640;&#25928;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06255</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;3D&#25195;&#25551;&#20013;&#35782;&#21035;&#20986;&#20154;&#31867;&#20083;&#31361;&#30340;&#29305;&#24449;&#12290;&lt;/br&gt;
&lt;/p&gt;
&lt;p&gt;
Machine learning and Topological data analysis identify unique features of human papillae in 3D scans. (arXiv:2307.06255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#31867;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#22312;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#26368;&#39640;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33292;&#34920;&#38754;&#26377;&#35768;&#22810;&#20083;&#31361;&#65292;&#23545;&#20110;&#21619;&#35273;&#21644;&#21475;&#24863;&#30340;&#26426;&#26800;&#21644;&#21270;&#23398;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20083;&#31361;&#30340;&#21619;&#35273;&#21151;&#33021;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#20083;&#31361;&#22312;&#20010;&#20307;&#20869;&#22806;&#30340;&#29420;&#29305;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;3D&#26174;&#24494;&#25195;&#25551;&#30340;&#20154;&#31867;&#20083;&#31361;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;n = 2092&#65289;&#65292;&#25581;&#31034;&#20102;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#12290;&#22522;&#20110;&#31163;&#25955;&#24494;&#20998;&#20960;&#20309;&#21644;&#35745;&#31639;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#65292;&#35745;&#31639;&#26426;&#27169;&#25311;&#20102;&#20083;&#31361;&#24418;&#29366;&#20013;&#24494;&#23567;&#30340;&#24046;&#24322;&#12290;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#26126;&#65292;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#26159;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#26368;&#26377;&#25928;&#30340;&#12290;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;85%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#20083;&#31361;&#31867;&#22411;&#12290;&#20083;&#31361;&#31867;&#22411;&#20998;&#31867;&#27169;&#22411;&#21487;&#20197;&#26144;&#23556;&#20083;&#19997;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae (n = 2092), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#22806;&#25512;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#38477;&#20302;&#22806;&#25512;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#26086;&#27169;&#22411;&#20869;&#25554;&#35823;&#24046;&#20026;&#38646;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#20110;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09478</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#30340;&#22806;&#25512;&#22833;&#25928;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks. (arXiv:2306.09478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#22806;&#25512;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#38477;&#20302;&#22806;&#25512;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#26086;&#27169;&#22411;&#20869;&#25554;&#35823;&#24046;&#20026;&#38646;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#20110;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#33021;&#21147;&#65292;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26368;&#36817;&#22312;&#31185;&#23398;&#30028;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#36890;&#24120;&#34987;&#23616;&#38480;&#20110;&#20869;&#25554;&#22330;&#26223;&#65292;&#20854;&#20013;&#39044;&#27979;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#38598;&#25903;&#25345;&#20869;&#30340;&#36755;&#20837;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#22806;&#25512;&#65292;&#20294;&#26159;PINNs&#30340;&#22495;&#22806;&#34892;&#20026;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PINNs&#30340;&#22806;&#25512;&#34892;&#20026;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#20960;&#20010;&#20808;&#21069;&#30340;&#20551;&#35774;&#30340;&#35777;&#25454;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#23545;&#22806;&#25512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#19968;&#26086;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#38646;&#20869;&#25554;&#35823;&#24046;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#20123;PDE&#65292;PINNs&#22312;&#22806;&#25512;&#20013;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#20869;&#25554;&#19968;&#26679;&#22909;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;Fourier&#21644;Laplace&#35889;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35266;&#23519;&#21040;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#22806;&#25512;&#35823;&#24046;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#28041;&#21450;&#20462;&#25913;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#20197;&#24378;&#35843;&#22806;&#25512;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Networks (PINNs) have recently gained popularity in the scientific community due to their effective approximation of partial differential equations (PDEs) using deep neural networks. However, their application has been generally limited to interpolation scenarios, where predictions rely on inputs within the support of the training set. In real-world applications, extrapolation is often required, but the out of domain behavior of PINNs is understudied. In this paper, we provide a detailed investigation of PINNs' extrapolation behavior and provide evidence against several previously held assumptions: we study the effects of different model choices on extrapolation and find that once the model can achieve zero interpolation error, further increases in architecture size or in the number of points sampled have no effect on extrapolation behavior. We also show that for some PDEs, PINNs perform nearly as well in extrapolation as in interpolation. By analyzing the Fouri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.07294</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Expressivity Enhancement with Efficient Quadratic Neurons for Convolutional Neural Networks. (arXiv:2306.07294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#20998;&#21106;&#31561;&#39046;&#22495;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;&#26032;&#30340;CNN&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#26469;&#33258;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25226;&#37325;&#28857;&#36716;&#21521;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#21487;&#37096;&#32626;&#24615;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#65292;&#20197;&#20165;&#26377;&#24494;&#23567;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#24320;&#38144;&#26469;&#20445;&#30041;&#38750;&#32447;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#21487;&#20197;&#26368;&#22823;&#21270;&#21033;&#29992;&#20108;&#38454;&#35745;&#31639;&#20449;&#24687;&#26469;&#25913;&#21892;&#32593;&#32476;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been successfully applied in a range of fields such as image classification and object segmentation. To improve their expressivity, various techniques, such as novel CNN architectures, have been explored. However, the performance gain from such techniques tends to diminish. To address this challenge, many researchers have shifted their focus to increasing the non-linearity of neurons, the fundamental building blocks of neural networks, to enhance the network expressivity. Nevertheless, most of these approaches incur a large number of parameters and thus formidable computation cost inevitably, impairing their efficiency to be deployed in practice. In this work, an efficient quadratic neuron structure is proposed to preserve the non-linearity with only negligible parameter and computation cost overhead. The proposed quadratic neuron can maximize the utilization of second-order computation information to improve the network performance. The experi
&lt;/p&gt;</description></item><item><title>DP-HyPO &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05734</link><description>&lt;p&gt;
DP-HyPO: &#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31169;&#26377;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework. (arXiv:2306.05734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05734
&lt;/p&gt;
&lt;p&gt;
DP-HyPO &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#24191;&#27867;&#35748;&#21487;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#31169;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32463;&#24120;&#24573;&#35270;&#19982;&#36229;&#21442;&#25968;&#20248;&#21270;&#30456;&#20851;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#26292;&#38706;&#26377;&#20851;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#30446;&#21069;&#65292;&#21807;&#19968;&#29616;&#26377;&#30340;&#20801;&#35768;&#38544;&#31169;&#20445;&#25252;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#38543;&#26426;&#36873;&#25321;&#19968;&#20123;&#36229;&#21442;&#25968;&#36827;&#34892;&#22810;&#27425;&#36816;&#34892;&#65292;&#24182;&#26368;&#32456;&#25253;&#21578;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#38750;&#31169;&#26377;&#35774;&#32622;&#20013;&#65292;&#20174;&#19994;&#32773;&#36890;&#24120;&#20351;&#29992;&#8220;&#33258;&#36866;&#24212;&#8221;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#21270;&#65292;&#35813;&#20248;&#21270;&#26041;&#27861;&#22522;&#20110;&#20808;&#21069;&#36755;&#20986;&#25910;&#38598;&#30340;&#20449;&#24687;&#36873;&#25321;&#19979;&#19968;&#20010;&#20505;&#36873;&#39033;&#12290;&#36825;&#31181;&#31169;&#26377;&#21644;&#38750;&#31169;&#26377;&#36229;&#21442;&#25968;&#20248;&#21270;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#20984;&#26174;&#20986;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DP-HyPO&#65292;&#19968;&#31181;&#25552;&#20379;&#33258;&#36866;&#24212;&#21644;&#38544;&#31169;&#20445;&#25252;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#21019;&#24615;&#26694;&#26550;&#12290;DP-HyPO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20445;&#35777;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#24378;&#22823;&#38544;&#31169;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#38750;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize "adaptive" hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01460</link><description>&lt;p&gt;
ReLU&#25327;&#25937;&#65306;&#29992;&#27491;&#25968;&#20248;&#21183;&#25913;&#36827;&#24744;&#30340;On-Policy Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26126;&#30830;&#22320;&#25972;&#21512;&#35880;&#24910;&#30340;&#29615;&#22659;&#20132;&#20114;&#26469;&#35299;&#20915;&#24403;&#21069;On-Policy&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#21644;Asynchronous Advantage Actor-Critic&#65289;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#30495;&#23454;&#20215;&#20540;&#20989;&#25968;&#21152;&#19978;&#24120;&#37327;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#20419;&#36827;&#8220;&#20445;&#23432;&#20540;&#20272;&#35745;&#8221;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Thompson&#37319;&#26679;&#26469;&#36827;&#34892;&#35880;&#24910;&#25506;&#32034;&#12290;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#23545;A3C&#31639;&#27861;&#36827;&#34892;&#19977;&#20010;&#24778;&#20154;&#31616;&#21333;&#30340;&#20462;&#25913;&#23454;&#29616;&#65306;&#36890;&#36807;ReLU&#20989;&#25968;&#22788;&#29702;&#20248;&#21183;&#20272;&#35745;&#65292;&#36827;&#34892;&#35889;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#22833;&#27963;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26368;&#22823;&#21270;&#20102;&#19979;&#30028;&#65292;&#36825;&#20063;&#26159;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;Regret Matching Policy Gradients&#65288;RMPG&#65289;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel method for enhancing the effectiveness of on-policy Deep Reinforcement Learning (DRL) algorithms. Current on-policy algorithms, such as Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), do not sufficiently account for cautious interaction with the environment. Our method addresses this gap by explicitly integrating cautious interaction in two critical ways: by maximizing a lower-bound on the true value function plus a constant, thereby promoting a \textit{conservative value estimation}, and by incorporating Thompson sampling for cautious exploration. These features are realized through three surprisingly simple modifications to the A3C algorithm: processing advantage estimates through a ReLU function, spectral normalization, and dropout. We provide theoretical proof that our algorithm maximizes the lower bound, which also grounds Regret Matching Policy Gradients (RMPG), a discrete-action on-policy method for multi-agen
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CALICO&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#65292;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#65292;&#24182;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.00349</link><description>&lt;p&gt;
CALICO&#65306;&#29992;&#20110;BEV&#24863;&#30693;&#30340;&#33258;&#25105;&#30417;&#30563;&#30456;&#26426;-LiDAR&#23545;&#27604;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (arXiv:2306.00349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00349
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CALICO&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#65292;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#65292;&#24182;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#39046;&#22495;&#65292;&#24863;&#30693;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20854;&#20013;&#22522;&#20110;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#30340;&#26550;&#26500;&#26368;&#36817;&#24050;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#21462;&#24615;&#28304;&#20110;&#27880;&#37322;2D&#21644;3D&#25968;&#25454;&#30340;&#26114;&#36149;&#21644;&#36153;&#21147;&#36807;&#31243;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;LiDAR&#21644;&#22522;&#20110;&#30456;&#26426;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#32570;&#23569;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CALICO&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CALICO&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#28857;&#21306;&#22495;&#23545;&#27604;&#65288;PRC&#65289;&#21644;&#21306;&#22495;&#24863;&#30693;&#33976;&#39311;&#65288;RAD&#65289;&#12290;PRC&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#23545;LiDAR&#27169;&#24577;&#30340;&#21306;&#22495;&#21644;&#22330;&#26223;&#32423;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;RAD&#26377;&#25928;&#22320;&#22312;&#25105;&#20204;&#30340;&#33258;&#25105;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#23545;&#27604;&#33976;&#39311;&#12290;CALICO&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data. Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.19575</link><description>&lt;p&gt;
&#20851;&#20110;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;.
&lt;/p&gt;
&lt;p&gt;
On the Linear Convergence of Policy Gradient under Hadamard Parameterization. (arXiv:2305.19575v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hadamard&#21442;&#25968;&#21270;&#19979;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#19979;Hadamard&#21442;&#25968;&#21270;&#19979;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#31639;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#38169;&#35823;&#22312;&#25152;&#26377;&#36845;&#20195;&#20013;&#20197;$O(\frac{1}{k})$&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;$k_0$&#27425;&#36845;&#20195;&#20043;&#21518;&#20855;&#26377;&#26356;&#24555;&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$k_0$&#26159;&#20165;&#20381;&#36182;&#20110;MDP&#38382;&#39064;&#21644;&#27493;&#38271;&#30340;&#24120;&#25968;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#26174;&#31034;&#20102;&#19968;&#20010;&#36739;&#24369;&#24120;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of deterministic policy gradient under the Hadamard parametrization is studied in the tabular setting and the global linear convergence of the algorithm is established. To this end, we first show that the error decreases at an $O(\frac{1}{k})$ rate for all the iterations. Based on this result, we further show that the algorithm has a faster local linear convergence rate after $k_0$ iterations, where $k_0$ is a constant that only depends on the MDP problem and the step size. Overall, the algorithm displays a linear convergence rate for all the iterations with a loose constant than that for the local linear convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#32456;&#31471;&#29992;&#25143;&#26131;&#20110;&#29702;&#35299;&#30340;&#12289;&#29992;&#20110;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;</title><link>http://arxiv.org/abs/2305.18434</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34892;&#22352;&#26631;&#30340;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Parallel Coordinates for Discovery of Interpretable Machine Learning Models. (arXiv:2305.18434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#32456;&#31471;&#29992;&#25143;&#26131;&#20110;&#29702;&#35299;&#30340;&#12289;&#29992;&#20110;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24179;&#34892;&#22352;&#26631;&#36827;&#34892;&#21487;&#35270;&#21270;&#30693;&#35782;&#21457;&#29616;&#65292;&#25552;&#21319;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#24179;&#34892;&#22352;&#26631;&#20013;&#30340;&#22270;&#24418;&#25968;&#25454;&#34920;&#31034;&#20351;&#24471;&#32456;&#31471;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#36229;&#31435;&#26041;&#20307;&#21644;&#36229;&#22359;&#30340;&#27010;&#24565;&#12290;&#25991;&#31456;&#24314;&#35758;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#20998;&#31867;&#22120;&#31639;&#27861;Hyper&#20013;&#21516;&#26102;&#20351;&#29992;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Hyper&#27169;&#22411;&#20855;&#26377;&#20915;&#31574;&#26641;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#31639;&#27861;&#34987;&#29992;&#22312;&#20102;&#22810;&#31181;&#35774;&#32622;&#21644;&#36873;&#39033;&#20013;&#65292;&#20197;&#20132;&#20114;&#26041;&#24335;&#25110;&#33258;&#21160;&#26041;&#24335;&#21457;&#29616;&#37325;&#21472;&#25110;&#38750;&#37325;&#21472;&#30340;&#36229;&#22359;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#28436;&#31034;&#20102;&#20351;&#29992;&#36229;&#22359;&#21644;&#35270;&#35273;&#27169;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;UCI ML&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#35780;&#20272;Hyper&#31639;&#27861;&#12290;&#36890;&#36807;10&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;Hyper&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#28151;&#21512;&#21644;&#32431;&#31929;&#30340;&#36229;&#22359;&#12290;&#36229;&#22359;&#12289;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#24050;&#32463;&#24314;&#31435;&#36215;&#26469;&#12290;&#26368;&#32456;&#29992;&#25143;&#33021;&#22815;&#25214;&#21040;&#21644;&#35266;&#23519;&#36825;&#20123;&#36229;&#22359;&#30340;&#33021;&#21147;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uses visual knowledge discovery in parallel coordinates to advance methods of interpretable machine learning. The graphic data representation in parallel coordinates made the concepts of hypercubes and hyperblocks (HBs) simple to understand for end users. It is suggested to use mixed and pure hyperblocks in the proposed data classifier algorithm Hyper. It is shown that Hyper models generalize decision trees. The algorithm is presented in several settings and options to discover interactively or automatically overlapping or non-overlapping hyperblocks. Additionally, the use of hyperblocks in conjunction with language descriptions of visual patterns is demonstrated. The benchmark data from the UCI ML repository were used to evaluate the Hyper algorithm. It enabled the discovery of mixed and pure HBs evaluated using 10-fold cross validation. Connections among hyperblocks, dimension reduction and visualization have been established. The capability of end users to find and observe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#21363;SAMoSSA&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#21644;&#33258;&#22238;&#24402;&#20998;&#26512;&#65292;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#25104;&#20998;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16491</link><description>&lt;p&gt;
SAMoSSA&#65306;&#24102;&#38543;&#26426;&#33258;&#22238;&#24402;&#22122;&#22768;&#30340;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#21363;SAMoSSA&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#21644;&#33258;&#22238;&#24402;&#20998;&#26512;&#65292;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#25104;&#20998;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24815;&#20363;&#26159;&#20808;&#20272;&#35745;&#30830;&#23450;&#24615;&#12289;&#38750;&#24179;&#31283;&#36235;&#21183;&#21644;&#23395;&#33410;&#25104;&#20998;&#65292;&#28982;&#21518;&#23398;&#20064;&#27531;&#24046;&#38543;&#26426;&#12289;&#24179;&#31283;&#25104;&#20998;&#12290;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#30456;&#20851;&#24179;&#31283;&#25104;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#22810;&#20803;&#22855;&#24322;&#35889;&#20998;&#26512;&#65288;mSSA&#65289;&#20934;&#30830;&#22320;&#23398;&#20064;&#30830;&#23450;&#24615;&#38750;&#24179;&#31283;&#25104;&#20998;&#65307;&#21516;&#26102;&#65292;&#22312;&#27809;&#26377;&#30830;&#23450;&#24615;&#38750;&#24179;&#31283;&#25104;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#24179;&#31283;&#25104;&#20998;&#20063;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#65292;&#20363;&#22914;&#36890;&#36807;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#31181;&#20004;&#20010;&#27493;&#39588;&#30340;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#20851;&#20110;&#21516;&#26102;&#28041;&#21450;&#30830;&#23450;&#24615;&#21644;&#24179;&#31283;&#25104;&#20998;&#30340;&#22810;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#25903;&#25745;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#24314;&#31435;&#29702;&#35770;&#20445;&#35777;&#26469;&#35299;&#20915;&#36825;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#39318;&#20808;&#24212;&#29992;mSSA&#26469;&#20272;&#35745;&#38750;&#24179;&#31283;&#25104;&#20998;&#65292;&#23613;&#31649;&#23384;&#22312;&#30456;&#20851;&#24615;&#24179;&#31283;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correla
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#25216;&#26415;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;AI&#25512;&#29702;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#35299;&#30721;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.15617</link><description>&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;&#27969;&#25216;&#26415;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;&#39640;&#36890;&#37327;AI&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
High-Throughput AI Inference for Medical Image Classification and Segmentation using Intelligent Streaming. (arXiv:2305.15617v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#25216;&#26415;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#30340;AI&#25512;&#29702;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#21644;&#35299;&#30721;&#26102;&#38388;&#65292;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20020;&#24202;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37319;&#29992;&#22686;&#22810;&#65292;&#24102;&#23485;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#27969;&#24335;&#20256;&#36755;&#22270;&#20687;&#25968;&#25454;&#26102;&#20986;&#29616;&#36890;&#20449;&#29942;&#39048;&#65292;&#20174;&#32780;&#24310;&#35823;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#22240;&#27492;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;AI&#20379;&#24212;&#21830;&#23558;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#36153;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26234;&#33021;&#27969;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#30340;&#21152;&#36895;&#12289;&#25104;&#26412;&#25928;&#30410;&#12289;&#24102;&#23485;&#20248;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;AI&#25512;&#29702;&#65292;&#20174;&#32780;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#12290;&#23545;&#20110;&#20998;&#31867;&#65292;&#26234;&#33021;&#27969;&#25216;&#26415;&#23558;&#25968;&#25454;&#20256;&#36755;&#20943;&#23569;&#20102;99.01%&#65292;&#35299;&#30721;&#26102;&#38388;&#20943;&#23569;&#20102;98.58%&#65292;&#21516;&#26102;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;27.43&#20493;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25968;&#25454;&#20256;&#36755;&#20943;&#23569;&#20102;90.32%&#65292;&#35299;&#30721;&#26102;&#38388;&#20943;&#23569;&#20102;90.26%&#65292;&#21516;&#26102;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;4.20&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26234;&#33021;&#27969;&#25216;&#26415;&#21487;&#20197;&#21152;&#24555;&#21608;&#36716;&#26102;&#38388;&#65292;&#38477;&#20302;&#25968;&#25454;&#21644;&#20256;&#36755;&#30340;&#24635;&#20307;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the adoption of AI systems within the clinical setup grows, limitations in bandwidth could create communication bottlenecks when streaming imaging data, leading to delays in patient diagnosis and treatment. As such, healthcare providers and AI vendors will require greater computational infrastructure, therefore dramatically increasing costs. To that end, we developed intelligent streaming, a state-of-the-art framework to enable accelerated, cost-effective, bandwidth-optimized, and computationally efficient AI inference for clinical decision making at scale. For classification, intelligent streaming reduced the data transmission by 99.01% and decoding time by 98.58%, while increasing throughput by 27.43x. For segmentation, our framework reduced data transmission by 90.32%, decoding time by 90.26%, while increasing throughput by 4.20x. Our work demonstrates that intelligent streaming results in faster turnaround times, and reduced overall cost of data and transmission, without negativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DIVA&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#23454;&#29616;&#31751;&#30340;&#21160;&#24577;&#36866;&#24212;&#31227;&#21160;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14067</link><description>&lt;p&gt;
DIVA&#65306;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder. (arXiv:2305.14067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DIVA&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#23454;&#29616;&#31751;&#30340;&#21160;&#24577;&#36866;&#24212;&#31227;&#21160;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#22312;&#20998;&#31867;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#21160;&#24577;&#21644;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#31751;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#31751;&#30340;&#8220;&#20986;&#29983;&#8221;&#21644;&#8220;&#21512;&#24182;&#8221;&#31227;&#21160;&#65292;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20197;&#8220;&#21160;&#24577;&#36866;&#24212;&#8221;&#30340;&#26041;&#24335;&#32858;&#31867;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#25226;&#35813;&#26694;&#26550;&#21629;&#21517;&#20026;DIVA&#65292;&#21363;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20998;&#31867;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#29305;&#24449;&#30340;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative model-based deep clustering frameworks excel in classifying complex data, but are limited in handling dynamic and complex features because they require prior knowledge of the number of clusters. In this paper, we propose a nonparametric deep clustering framework that employs an infinite mixture of Gaussians as a prior. Our framework utilizes a memoized online variational inference method that enables the "birth" and "merge" moves of clusters, allowing our framework to cluster data in a "dynamic-adaptive" manner, without requiring prior knowledge of the number of features. We name the framework as DIVA, a Dirichlet Process-based Incremental deep clustering framework via Variational Auto-Encoder. Our framework, which outperforms state-of-the-art baselines, exhibits superior performance in classifying complex data with dynamically changing features, particularly in the case of incremental features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.11650</link><description>&lt;p&gt;
&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#20026;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;EBMs &#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29992;&#20110;&#21487;&#25193;&#23637; EBM &#35757;&#32451;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#65288;DSM&#65289;&#26041;&#27861;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#21040;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65306;&#65288;&#20266;&#65289;Gibbs&#37319;&#26679;&#19982;&#21160;&#37327;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#32463;&#36807;DSM&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#22024;&#26434;&#8221;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22522;&#30784;&#8220;&#24178;&#20928;&#8221;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30456;&#20851;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;</title><link>http://arxiv.org/abs/2305.07721</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#26368;&#20248;&#34892;&#20026;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Designing Optimal Behavioral Experiments Using Machine Learning. (arXiv:2305.07721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27169;&#22411;&#26159;&#29702;&#35299;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23427;&#20204;&#33021;&#22815;&#28165;&#26224;&#12289;&#31934;&#30830;&#22320;&#34920;&#36798;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#32454;&#24494;&#32780;&#24120;&#24120;&#20986;&#20154;&#24847;&#26009;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20016;&#23500;&#24615;&#21644;&#24778;&#21916;&#30340;&#33021;&#21147;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#31185;&#23398;&#30452;&#35273;&#21644;&#20256;&#32479;&#24037;&#20855;&#19981;&#36866;&#21512;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#38519;&#38449;&#24182;&#23454;&#29616;&#35745;&#31639;&#24314;&#27169;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#23454;&#39564;&#30340;&#24037;&#20855;&#65292;&#20197;&#28165;&#26224;&#22320;&#22238;&#31572;&#27169;&#22411;&#35299;&#37322;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#24335;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20570;&#20986;&#30340;&#36741;&#21161;&#20551;&#35774;&#12290;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;BOED&#65289;&#36890;&#36807;&#30830;&#23450;&#39044;&#26399;&#20135;&#29983;&#20449;&#24687;&#24615;&#25968;&#25454;&#30340;&#23454;&#39564;&#26469;&#27491;&#24335;&#21270;&#25628;&#32034;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models are powerful tools for understanding human cognition and behavior. They let us express our theories clearly and precisely, and offer predictions that can be subtle and often counter-intuitive. However, this same richness and ability to surprise means our scientific intuitions and traditional tools are ill-suited to designing experiments to test and compare these models. To avoid these pitfalls and realize the full potential of computational modeling, we require tools to design experiments that provide clear answers about what models explain human behavior and the auxiliary assumptions those models must make. Bayesian optimal experimental design (BOED) formalizes the search for optimal experimental designs by identifying experiments that are expected to yield informative data. In this work, we provide a tutorial on leveraging recent advances in BOED and machine learning to find optimal experiments for any kind of model that we can simulate data from, and show how by
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;KNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614;KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04258</link><description>&lt;p&gt;
&#20851;&#20110;&#8220;&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#26377;&#25928;&#24615;&#8221;&#30340;&#27880;&#35760;&#65288;arXiv&#65306;2304.04258v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms". (arXiv:2304.04258v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;KNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614;KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#24615;&#26159;&#19968;&#20010;&#30740;&#31350;&#21333;&#20010;&#25968;&#25454;&#28857;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24433;&#21709;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#35770;&#21644;&#32463;&#27982;&#23398;&#65292;&#25968;&#25454; Shapley &#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#26377;&#25928;&#24615;&#35745;&#31639;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#37117;&#30693;&#36947; Shapley &#20540;&#65288;SV&#65289;&#30340;&#35745;&#31639;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;Jia &#31561;&#20154;&#65288;2019&#65289;&#34920;&#26126;&#65292;&#23545;&#20110; K &#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#65292;&#35745;&#31639; Data Shapley &#31455;&#28982;&#38750;&#24120;&#31616;&#21333;&#21644;&#39640;&#25928;&#12290;&#22312;&#26412;&#31508;&#35760;&#20013;&#65292;&#25105;&#20204;&#37325;&#23457;&#20102; Jia &#31561;&#20154;&#65288;2019&#65289;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#21644;&#21487;&#35299;&#37322;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102; KNN &#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#26032;&#25928;&#29992;&#20989;&#25968;&#30340; KNN &#20998;&#31867;&#22120;/&#22238;&#24402;&#22120;&#30340; Data Shapley &#30340;&#30456;&#24212;&#35745;&#31639;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#34987;&#31216;&#20026;&#36719;&#26631;&#31614; KNN-SV&#65292;&#19982;&#21407;&#22987;&#26041;&#27861;&#20855;&#26377;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#30340;&#36719;&#26631;&#31614; KNN-SV &#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a growing research field that studies the influence of individual data points for machine learning (ML) models. Data Shapley, inspired by cooperative game theory and economics, is an effective method for data valuation. However, it is well-known that the Shapley value (SV) can be computationally expensive. Fortunately, Jia et al. (2019) showed that for K-Nearest Neighbors (KNN) models, the computation of Data Shapley is surprisingly simple and efficient.  In this note, we revisit the work of Jia et al. (2019) and propose a more natural and interpretable utility function that better reflects the performance of KNN models. We derive the corresponding calculation procedure for the Data Shapley of KNN classifiers/regressors with the new utility functions. Our new approach, dubbed soft-label KNN-SV, achieves the same time complexity as the original method. We further provide an efficient approximation algorithm for soft-label KNN-SV based on locality sensitive hashing (LSH
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.00553</link><description>&lt;p&gt;
&#20174;&#23396;&#31435;&#30340;&#23707;&#23679;&#21040;&#27867;&#22823;&#38470;&#65306;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#29992;&#20110;&#20154;&#31867;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#29702;&#35299;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#24182;&#19988;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;&#34892;&#20026;&#30340;&#29289;&#29702;&#31354;&#38388;&#21040;&#35821;&#20041;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#20250;&#26681;&#25454;&#29420;&#29305;&#30340;&#36873;&#25321;&#26500;&#24314;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#20041;&#21508;&#31181;&#31867;&#21035;&#24182;&#23558;&#22522;&#20934;&#32447;&#25512;&#21521;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#19981;&#21516;&#30340;&#31867;&#21035;&#31890;&#24230;&#65292;&#23601;&#20687;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#19968;&#26679;&#20114;&#19981;&#20860;&#23481;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;A&#20013;&#30340;&#23478;&#21153;&#21644;&#25968;&#25454;&#38598;B&#20013;&#30340;&#27927;&#30424;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#38598;&#20013;&#31038;&#21306;&#30340;&#21147;&#37327;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#19968;&#36215;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#20197;&#36861;&#27714;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#32473;&#23450;&#21160;&#35789;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#24182;&#28085;&#30422;&#22823;&#37327;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#25105;&#20204;&#30340;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#31614;&#31995;&#32479;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35821;&#20041;&#31354;&#38388;&#21644;&#32479;&#19968;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#23545;&#20914;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#25110;&#37327;&#23376;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20854;&#20998;&#24067;&#24335;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16585</link><description>&lt;p&gt;
&#37327;&#23376;&#28145;&#24230;&#23545;&#20914;
&lt;/p&gt;
&lt;p&gt;
Quantum Deep Hedging. (arXiv:2303.16585v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#23545;&#20914;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#25110;&#37327;&#23376;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20854;&#20998;&#24067;&#24335;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#31561;&#34892;&#19994;&#26377;&#30528;&#28508;&#22312;&#21464;&#38761;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#23545;&#20914;&#38382;&#39064;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#23454;&#29616;&#30495;&#23454;&#24066;&#22330;&#30340;&#26377;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#20110;&#31574;&#30053;&#25628;&#32034;&#21644;&#20998;&#24067;&#24335;Actor-Critic&#31639;&#27861;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21644;&#22797;&#21512;&#23618;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#22788;&#29702;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#20351;&#29992;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#34920;&#26126;&#37327;&#23376;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#24471;&#21040;&#30340;&#24615;&#33021;&#27604;&#26631;&#20934;&#26041;&#27861;&#26356;&#22909;&#65292;&#26080;&#35770;&#26159;&#32463;&#20856;&#30340;&#36824;&#26159;&#37327;&#23376;&#30340;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#25152;&#25552;&#20986;&#27169;&#22411;&#23454;&#29616;&#22312;&#19968;&#20010;&#26368;&#22810;&#26377;16&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#31163;&#23376;&#38519;&#38449;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning has the potential for a transformative impact across industry sectors and in particular in finance. In our work we look at the problem of hedging where deep reinforcement learning offers a powerful framework for real markets. We develop quantum reinforcement learning methods based on policy-search and distributional actor-critic algorithms that use quantum neural network architectures with orthogonal and compound layers for the policy and value functions. We prove that the quantum neural networks we use are trainable, and we perform extensive simulations that show that quantum models can reduce the number of trainable parameters while achieving comparable performance and that the distributional approach obtains better performance than other standard approaches, both classical and quantum. We successfully implement the proposed models on a trapped-ion quantum processor, utilizing circuits with up to $16$ qubits, and observe performance that agrees well with nois
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#36866;&#24212;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#30446;&#26631;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10594</link><description>&lt;p&gt;
AdaptGuard: &#38024;&#23545;&#27169;&#22411;&#36866;&#24212;&#20013;&#30340;&#36890;&#29992;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#36866;&#24212;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#30446;&#26631;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#26088;&#22312;&#35299;&#20915;&#22312;&#20165;&#35775;&#38382;&#24050;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#30340;&#32422;&#26463;&#19979;&#30340;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#20256;&#36755;&#25928;&#29575;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#36825;&#31181;&#33539;&#24335;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#20013;&#30001;&#20110;&#24694;&#24847;&#25552;&#20379;&#26041;&#30340;&#23384;&#22312;&#32780;&#36716;&#31227;&#33258;&#28304;&#22495;&#30340;&#36890;&#29992;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#21644;&#21518;&#38376;&#25915;&#20987;&#20316;&#20026;&#28304;&#20391;&#28431;&#27934;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36866;&#24212;&#21518;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;AdaptGuard&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36991;&#20813;&#30452;&#25509;&#20351;&#29992;&#39118;&#38505;&#28304;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#35843;&#25972;&#21322;&#24452;&#19979;&#30340;&#20266;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;AdaptGuard&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#30340;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation aims at solving the domain transfer problem under the constraint of only accessing the pretrained source models. With the increasing considerations of data privacy and transmission efficiency, this paradigm has been gaining recent popularity. This paper studies the vulnerability to universal attacks transferred from the source domain during model adaptation algorithms due to the existence of the malicious providers. We explore both universal adversarial perturbations and backdoor attacks as loopholes on the source side and discover that they still survive in the target models after adaptation. To address this issue, we propose a model preprocessing framework, named AdaptGuard, to improve the security of model adaptation algorithms. AdaptGuard avoids direct use of the risky source parameters through knowledge distillation and utilizes the pseudo adversarial samples under adjusted radius to enhance the robustness. AdaptGuard is a plug-and-play module that requires neithe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.08112</link><description>&lt;p&gt;
&#29992;&#35843;&#35856;&#36879;&#38236;&#20174;Transformer&#20013;&#33719;&#21462;&#28508;&#22312;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36845;&#20195;&#25512;&#29702;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;transformers&#27169;&#22411;&#65292;&#26088;&#22312;&#20102;&#35299;&#27169;&#22411;&#39044;&#27979;&#26159;&#22914;&#20309;&#36880;&#23618;&#36827;&#34892;&#31934;&#21270;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#22359;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#26159;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21069;&#32773;&#32473;&#20986;&#20102;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20294;&#24120;&#24120;&#26131;&#30862;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20855;&#26377;&#22810;&#36798;20B&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#34920;&#26126;&#20854;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#26174;&#31034;&#65292;&#35843;&#35856;&#36879;&#38236;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/AlignmentResearch/tuned-lens &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01486</link><description>&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding plasticity in neural networks. (arXiv:2303.01486v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#26159;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#26681;&#25454;&#26032;&#20449;&#24687;&#26356;&#25913;&#20854;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21363;&#20351;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#20063;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#21435;&#21487;&#22609;&#24615;&#65292;&#20294;&#39537;&#21160;&#36825;&#31181;&#29616;&#35937;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#28145;&#24230;&#29702;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#20197;&#24341;&#23548;&#26410;&#26469;&#23545;&#26377;&#38024;&#23545;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#30340;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#36890;&#24120;&#21457;&#29983;&#22312;&#26080;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#19968;&#20123;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#35774;&#35745;&#36873;&#25321;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#20445;&#25345;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#24178;&#39044;&#25514;&#26045;&#22312;&#19968;&#31995;&#21015;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25928;&#29992;&#65292;&#35777;&#26126;&#23427;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2301.06732</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;
&lt;/p&gt;
&lt;p&gt;
Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network. (arXiv:2301.06732v4 [astro-ph.SR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#24320;&#22987;&#25506;&#32034;&#22826;&#31354;&#65292;&#22826;&#31354;&#22825;&#27668;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26126;&#26174;&#12290;&#24050;&#32463;&#30830;&#31435;&#20102;&#22826;&#38451;&#20885;&#31354;&#27934;&#36825;&#19968;&#31181;&#22826;&#31354;&#22825;&#27668;&#29616;&#35937;&#20250;&#23545;&#39134;&#26426;&#21644;&#21355;&#26143;&#30340;&#36816;&#20316;&#20135;&#29983;&#24433;&#21709;&#12290;&#22826;&#38451;&#20885;&#31354;&#27934;&#26159;&#22826;&#38451;&#19978;&#30340;&#19968;&#29255;&#21306;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#30913;&#22330;&#32447;&#24320;&#25918;&#32780;&#28201;&#24230;&#30456;&#23545;&#36739;&#20302;&#65292;&#23548;&#33268;&#22826;&#38451;&#39118;&#20197;&#39640;&#20110;&#24179;&#22343;&#36895;&#29575;&#36827;&#34892;&#21457;&#23556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#20934;&#22791;&#22909;&#24212;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#23545;&#22320;&#29699;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#27979;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#22270;&#20687;&#20013;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#21306;&#22495;&#24182;&#35745;&#31639;&#20854;&#22823;&#23567;&#12290;&#25105;&#20204;&#23545;&#22826;&#38451;&#27599;&#20010;&#21306;&#22495;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#36827;&#34892;&#27604;&#36739;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#25968;&#25454;&#30340;&#36235;&#21183;&#65292;&#24182;&#39044;&#27979;&#19981;&#21516;&#22826;&#38451;&#21306;&#22495;&#26410;&#26469;7&#22825;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#22823;&#23567;&#12290;&#36890;&#36807;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36776;&#35782;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#21464;&#21270;&#36235;&#21183;&#21644;&#39044;&#27979;&#20854;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
As humanity has begun to explore space, the significance of space weather has become apparent. It has been established that coronal holes, a type of space weather phenomenon, can impact the operation of aircraft and satellites. The coronal hole is an area on the sun characterized by open magnetic field lines and relatively low temperatures, which result in the emission of the solar wind at higher than average rates. In this study, To prepare for the impact of coronal holes on the Earth, we use computer vision to detect the coronal hole region and calculate its size based on images from the Solar Dynamics Observatory (SDO). We compare the coronal holes for each region of the Sun and analyze the correlation. We then implement deep learning techniques, specifically the Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole area data and predict its size for different sun regions over 7 days. By analyzing time series data on the coronal hole area, this study aims to id
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.14400</link><description>&lt;p&gt;
&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#19978;&#65292;&#20851;&#20110;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#36924;&#36817;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#31354;&#38388;$W^s(L_q(\Omega))$&#21644;Besov&#31354;&#38388;$B^s_r(L_q(\Omega))$&#20013;&#20197;$L_p(\Omega)$&#33539;&#25968;&#24230;&#37327;&#35823;&#24046;&#30340;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20110;&#22312;&#31185;&#23398;&#35745;&#31639;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#36807;&#21435;&#21482;&#26377;&#24403;$p=q=\infty$&#26102;&#25165;&#23436;&#20840;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q\leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#28176;&#36817;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#31232;&#30095;&#21521;&#37327;&#30340;&#26368;&#20339;&#32534;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;$p&gt;q$&#30340;&#38750;&#32447;&#24615;&#21306;&#22495;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#30340;$L_p$&#36924;&#36817;&#19979;&#30028;&#25512;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s &gt; 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p &gt; q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;von Mises-Fisher&#20998;&#24067;&#30340;&#26426;&#21046;&#24212;&#29992;&#26041;&#21521;&#38544;&#31169;&#26469;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#20102;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#24179;&#28369;&#36864;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.04686</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#21521;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Directional Privacy for Deep Learning. (arXiv:2211.04686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;von Mises-Fisher&#20998;&#24067;&#30340;&#26426;&#21046;&#24212;&#29992;&#26041;&#21521;&#38544;&#31169;&#26469;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#20102;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#24179;&#28369;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26799;&#24230;&#21152;&#20837;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#20854;&#25928;&#29992;&#12290;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#20219;&#24847;&#24230;&#37327;&#30340;&#26367;&#20195;&#26426;&#21046;&#65292;&#36825;&#21487;&#33021;&#26356;&#36866;&#21512;&#20110;&#32500;&#25252;&#20854;&#25928;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;von Mises-Fisher&#65288;VMF&#65289;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#37319;&#29992;\textit{&#35282;&#36317;&#31163;} &#25200;&#21160;&#26799;&#24230;&#65292;&#20174;&#32780;&#24191;&#27867;&#20445;&#30041;&#26799;&#24230;&#26041;&#21521;&#65292;&#24212;&#29992;\textit{&#26041;&#21521;&#38544;&#31169;}&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;$\epsilon$-DP&#21644;$\epsilon d$-&#38544;&#31169;&#65292;&#32780;&#19981;&#26159;&#39640;&#26031;&#26426;&#21046;&#30340;$(\epsilon,\delta)$-&#38544;&#31169;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#19981;&#38656;&#35201;$\delta&gt;0$&#39033;&#65292;&#20294;&#20250;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#32780;&#24179;&#28369;&#36864;&#21270;&#12290;&#38543;&#30528;$\epsilon$&#22312;&#20854;&#20013;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26041;&#21521;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#38544;&#31169;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#25506;&#31350;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#26576;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable for preserving utility. In this paper, we apply \textit{directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb gradients in terms of \textit{angular distance} so that gradient direction is broadly preserved. We show that this provides both $\epsilon$-DP and $\epsilon d$-privacy for deep learning training, rather than the $(\epsilon, \delta)$-privacy of the Gaussian mechanism; we observe that the $\epsilon d$-privacy guarantee does not require a $\delta&gt;0$ term but degrades smoothly according to the dissimilarity of the input gradients.  As $\epsilon$s between thes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.06462</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#25351;&#23548;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#26102;&#12290;&#28982;&#32780;&#65292;&#25351;&#23548;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;-&#27880;&#37322;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20854;&#21487;&#29992;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#27880;&#37322;&#38656;&#27714;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#25552;&#21462;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#37322;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#31890;&#24230;&#19978;&#25552;&#20379;&#25351;&#23548;&#20449;&#21495;&#65306;&#20174;&#25972;&#20307;&#22270;&#20687;&#21040;&#29289;&#20307;&#26694;&#65292;&#29978;&#33267;&#21040;&#20998;&#21106;&#33945;&#29256;&#12290;&#25105;&#20204;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#24341;&#36215;&#30340;&#26641;&#26408;&#27515;&#20129;&#26041;&#38754;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#20174;&#26641;&#30382;&#30002;&#34411;&#19982;&#23492;&#20027;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12289;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#19982;&#20197;&#24448;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#26412;&#32508;&#36848;&#21253;&#25324;&#20102;&#25152;&#26377;&#36965;&#24863;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26681;&#25454;&#22810;&#20809;&#35889;&#25110;&#39640;&#20809;&#35889;&#20998;&#26512;&#35299;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#20174;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#21644;&#25915;&#20987;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#25915;&#20987;&#30340;&#26089;&#26399;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#21644;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;/&#31354;&#38388;/&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#20809;&#35889;&#26893;&#34987;&#25351;&#25968;&#65288;SVIs&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#23398;&#20064;&#26041;&#26696;&#12289;&#20219;&#21153;&#31867;&#21035;&#12289;&#27169;&#22411;&#12289;&#31639;&#27861;&#12289;&#31867;&#21035;/&#31751;&#12289;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19982;&#26550;&#26500;&#26041;&#38754;&#25552;&#21462;&#30693;&#35782;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21487;&#35265;&#20809;&#21644;&#28909;&#32418;&#22806;&#31561;&#27874;&#27573;&#19978;&#26816;&#27979;&#24494;&#23567;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species &amp; attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms &amp; sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks &amp; architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, therma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.06817</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A novel physics-informed machine learning strategy to accelerate unsteady heat and mass transfer simulations. (arXiv:2206.06817v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#30340;&#24615;&#33021;&#36805;&#36895;&#25552;&#21319;&#65292;&#20294;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#22312;&#22823;&#22411;&#39046;&#22495;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#21152;&#36895;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#27425;&#35757;&#32451;&#26041;&#27861;&#20013;&#65292;&#38543;&#30528;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#22686;&#21152;&#65292;&#23436;&#20840;&#28040;&#38500;&#38169;&#35823;&#22686;&#38271;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22914;&#26524;&#23450;&#26399;&#36827;&#34892;&#36830;&#32493;&#30340;ML-CFD&#20132;&#21449;&#35745;&#31639;&#65292;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65288;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancements in the performance of central processing units (CPUs), the simulation of unsteady heat and mass transfer is computationally very costly, particularly in large domains. While a big wave of machine learning (ML) has propagated in accelerating computational fluid dynamics (CFD) studies, recent research has revealed that it is unrealistic to completely suppress the error increase as the gap between the training and prediction times increases in single training approach. In this study, we propose a residual-based physics-informed transfer learning (RePIT) strategy to accelerate unsteady heat and mass transfer simulations using ML-CFD cross computation. Our hypothesis is that long-term CFD simulations become feasible if continuous ML-CFD cross computation is periodically carried out to not only reduce increased residuals but also update network parameters with the latest CFD time series data (transfer learning approach). The cross point of ML-CFD is determined 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10952</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21151;&#33021;&#24615;&#31070;&#32463;&#32534;&#30721;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;(DL)&#30340;&#20195;&#29702;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24182;&#34892;/&#39034;&#24207;&#25805;&#20316;&#12290;&#36825;&#20351;&#24471;&#29702;&#35299;DNNs&#30340;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#36866;&#24403;&#30340;&#35786;&#26029;&#12290;&#22312;&#27809;&#26377;&#23545;&#20854;&#20869;&#37096;&#36807;&#31243;&#26377;&#26356;&#22909;&#30340;&#20102;&#35299;&#20043;&#21069;&#65292;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#37096;&#32626;DNNs&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;DNNs/DL&#26469;&#35299;&#20915;&#39640;&#39118;&#38505;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#28145;&#20837;&#20102;&#35299;DNNs&#20915;&#31574;&#32972;&#21518;&#30340;&#20869;&#37096;&#25805;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#19982;DNNs&#20915;&#31574;&#30456;&#20851;&#30340;DL&#27169;&#22411;&#30340;&#20869;&#37096;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38752;&#36817;&#36755;&#20837;&#23618;&#30340;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#38752;&#36817;&#36755;&#20986;&#23618;&#30340;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;DNNs&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#30340;&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#24212;&#29992;&#65292;&#38024;&#23545;&#22312;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2203.09347</link><description>&lt;p&gt;
&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#30340;&#38477;&#32500;&#19982;Wasserstein&#31283;&#23450;&#24615;&#24212;&#29992;&#65292;&#38024;&#23545;&#22312;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#25512;&#23548;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#22238;&#24402;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26420;&#32032;&#30340;&#20004;&#27493;&#27861;&#65292;&#39318;&#20808;&#38477;&#20302;&#36755;&#20837;&#21464;&#37327;&#30340;&#32500;&#25968;&#65292;&#20877;&#20351;&#29992;&#26680;&#22238;&#24402;&#26469;&#39044;&#27979;&#36755;&#20986;&#21464;&#37327;&#12290;&#20026;&#20102;&#20998;&#26512;&#30001;&#27492;&#20135;&#29983;&#30340;&#22238;&#24402;&#35823;&#24046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#38024;&#23545;Wasserstein&#36317;&#31163;&#30340;&#26032;&#30340;&#26680;&#22238;&#24402;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#38480;&#21046;&#24403;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#25311;&#21512;&#22238;&#24402;&#20989;&#25968;&#26102;&#20986;&#29616;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#36890;&#29992;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#24212;&#29992;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26680;&#22238;&#24402;&#25991;&#29486;&#20013;&#30340;&#20272;&#35745;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#27493;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21518;&#32773;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#65292;&#24182;&#19988;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33021;&#24674;&#22797;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#30340;&#30495;&#23454;&#20809;&#28369;&#24230;&#12290;</title><link>http://arxiv.org/abs/2203.05400</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#65292;&#24182;&#19988;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33021;&#24674;&#22797;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#30340;&#30495;&#23454;&#20809;&#28369;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#29992;Matern&#21327;&#26041;&#24046;&#26680;&#23558;&#30830;&#23450;&#24615;&#21709;&#24212;&#20989;&#25968;&#65288;&#22914;&#35745;&#31639;&#26426;&#23454;&#39564;&#30340;&#36755;&#20986;&#65289;&#24314;&#27169;&#20026;&#39640;&#26031;&#36807;&#31243;&#12290;Matern&#26680;&#30340;&#20809;&#28369;&#21442;&#25968;&#20915;&#23450;&#20102;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#26497;&#38480;&#19979;&#30340;&#35768;&#22810;&#37325;&#35201;&#23646;&#24615;&#65292;&#21253;&#25324;&#26465;&#20214;&#22343;&#20540;&#25910;&#25947;&#21040;&#21709;&#24212;&#20989;&#25968;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25968;&#25454;&#22312;&#22266;&#23450;&#26377;&#30028;&#23376;&#38598;$\mathbb{R}^d$&#19978;&#33719;&#24471;&#26102;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22914;&#26524;&#25968;&#25454;&#29983;&#25104;&#30340;&#21709;&#24212;&#20989;&#25968;&#20855;&#26377;Sobolev&#20809;&#28369;&#24230;$\nu_0 &gt; d/2$&#65292;&#37027;&#20040;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#23567;&#20110;$\nu_0$&#12290;&#36825;&#19968;&#19979;&#30028;&#26159;&#31934;&#20934;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#20013;&#33021;&#24674;&#22797;&#30495;&#23454;&#30340;&#20809;&#28369;&#24230;&#12290;&#23545;&#20110;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#28176;&#36817;&#19979;&#30028;$\nu_0-d/2$&#65292;&#20294;&#36825;&#24456;&#19981;&#21487;&#33021;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\'ern covariance kernel. The smoothness parameter of a Mat\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\nu_0 &gt; d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.08693</link><description>&lt;p&gt;
&#23398;&#20064;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24179;&#26041;&#26681;&#36895;&#24230;&#20989;&#25968;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#27604;&#36739;&#26641;&#29366;&#19977;&#32500;&#29289;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#20307;&#24418;&#29366;&#24046;&#24322;&#35745;&#31639;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20998;&#26512;&#23637;&#29616;&#20986;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#21464;&#21270;&#30340;&#35814;&#32454;&#19977;&#32500;&#29983;&#29289;&#29289;&#20307;&#65292;&#20363;&#22914;&#31070;&#32463;&#20803;&#21644;&#26893;&#29289;&#26641;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12289;&#27604;&#36739;&#21644;&#35745;&#31639;&#36825;&#20123;&#26641;&#29366;&#19977;&#32500;&#23545;&#35937;&#30340;&#24418;&#29366;&#24046;&#24322;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#23558;&#19968;&#20010;&#26641;&#29366;&#29289;&#20307;&#21464;&#24418;&#20026;&#21478;&#19968;&#20010;&#29289;&#20307;&#25152;&#38656;&#30340;&#24367;&#26354;&#12289;&#25289;&#20280;&#21644;&#20998;&#25903;&#28369;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
&lt;/p&gt;</description></item></channel></rss>