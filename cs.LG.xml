<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.07899</link><description>&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Regular Expression Inference Challenge. (arXiv:2308.07899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07899
&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#25361;&#25112;&#26159;&#19968;&#20010;&#20197;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#20026;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24212;&#29992;&#24191;&#27867;&#12289;&#38590;&#24230;&#21487;&#35843;&#12289;&#36866;&#29992;&#20110;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#29702;&#65288;REI&#65289;&#20316;&#20026;&#20195;&#30721;/&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#25361;&#25112;&#12290;REI&#26159;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#65292;&#23427;&#25552;&#20986;&#20102;&#20174;&#31034;&#20363;&#20013;&#25214;&#21040;&#26368;&#23567;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#20004;&#20010;&#26377;&#38480;&#23383;&#31526;&#20018;&#38598;&#21512;P&#21644;N&#20197;&#21450;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;cost(&#183;)&#65292;&#20219;&#21153;&#26159;&#29983;&#25104;&#19968;&#20010;&#25509;&#21463;P&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#24182;&#25298;&#32477;N&#20013;&#25152;&#26377;&#23383;&#31526;&#20018;&#30340;&#34920;&#36798;&#24335;r&#65292;&#32780;&#19981;&#23384;&#22312;&#20854;&#20182;&#34920;&#36798;&#24335;r'&#65292;&#20351;&#24471;cost(r')&lt;cost(r)&#12290;REI&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#38382;&#39064;&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;i&#65289;&#27491;&#21017;&#34920;&#36798;&#24335;&#26159;&#20247;&#25152;&#21608;&#30693;&#12289;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#26159;&#20195;&#30721;&#30340;&#33258;&#28982;&#29702;&#24819;&#21270;&#65307;&#65288;ii&#65289;REI&#30340;&#28176;&#36817;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24615;&#24050;&#34987;&#20805;&#20998;&#29702;&#35299;&#65307;&#65288;iii&#65289;REI&#20855;&#26377;&#19968;&#23567;&#37096;&#20998;&#26131;&#20110;&#29702;&#35299;&#30340;&#21442;&#25968;&#65288;&#20363;&#22914;P&#25110;N&#30340;&#22522;&#25968;&#12289;&#31034;&#20363;&#30340;&#23383;&#31526;&#20018;&#38271;&#24230;&#25110;&#25104;&#26412;&#20989;&#25968;&#65289;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;REI&#30340;&#38590;&#24230;&#65307;&#65288;iv&#65289;&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;M&#27169;&#22411;&#32780;&#35328;&#65292;REI&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')&lt;\text{cost}(r)$.  REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based M
&lt;/p&gt;</description></item><item><title>SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07896</link><description>&lt;p&gt;
SciRE-Solver: &#29992;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07896
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26679;&#26412;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;DPMs&#30340;&#23454;&#29616;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;DPMs&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19982;DPMs&#37319;&#26679;&#36807;&#31243;&#23545;&#24212;&#30340;&#25193;&#25955;ODE&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24471;&#20998;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#20026;&#27714;&#35299;&#25193;&#25955;ODE&#30340;&#25968;&#20540;&#31639;&#27861;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;(RDE)&#26041;&#27861;&#26469;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#21644;RDE&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#25910;&#25947;&#39034;&#24207;&#20445;&#35777;&#30340;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;(SciRE-Solver)&#26469;&#35299;&#20915;&#25193;&#25955;ODEs&#12290;SciRE-Solver&#22312;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;DPMs&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#65292;&#24182;&#19988;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#30340;&#24471;&#20998;&#20989;&#25968;&#35780;&#20272;(NFE)&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20272;&#35745;Radon-Nikodym&#23548;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23548;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;&#20272;&#35745;&#31354;&#38388;&#30340;&#23481;&#37327;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07887</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#21270;&#30340;Radon-Nikodym&#23548;&#25968;
&lt;/p&gt;
&lt;p&gt;
On regularized Radon-Nikodym differentiation. (arXiv:2308.07887v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20272;&#35745;Radon-Nikodym&#23548;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23548;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;&#20272;&#35745;&#31354;&#38388;&#30340;&#23481;&#37327;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20272;&#35745;Radon-Nikodym&#23548;&#25968;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#27604;&#22914;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;&#12289;&#20284;&#28982;&#27604;&#26816;&#39564;&#12289;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#26465;&#20214;&#27010;&#29575;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#33324;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#36890;&#36807;&#32771;&#34385;&#23548;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;&#20272;&#35745;&#23427;&#30340;&#31354;&#38388;&#30340;&#23481;&#37327;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#27491;&#21017;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#26159;&#20197;&#19968;&#33324;&#28304;&#26465;&#20214;&#21644;&#27491;&#21017;&#21270;&#30340;Christoffel&#20989;&#25968;&#20026;&#22522;&#30784;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#20219;&#20309;&#29305;&#23450;&#28857;&#19978;&#37325;&#24314;Radon-Nikodym&#23548;&#25968;&#21487;&#20197;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the problem of estimating Radon-Nikodym derivatives. This problem appears in various applications, such as covariate shift adaptation, likelihood-ratio testing, mutual information estimation, and conditional probability estimation. To address the above problem, we employ the general regularization scheme in reproducing kernel Hilbert spaces. The convergence rate of the corresponding regularized algorithm is established by taking into account both the smoothness of the derivative and the capacity of the space in which it is estimated. This is done in terms of general source conditions and the regularized Christoffel functions. We also find that the reconstruction of Radon-Nikodym derivatives at any particular point can be done with high order of accuracy. Our theoretical results are illustrated by numerical simulations.
&lt;/p&gt;</description></item><item><title>&#23545;&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#22238;&#24402;&#22522;&#30784;&#36827;&#34892;&#20102;&#21512;&#29702;&#26816;&#26597;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#32463;&#20856;&#30340;&#26080;&#26102;&#38388;&#39034;&#24207;&#30340;&#34920;&#26684;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.07886</link><description>&lt;p&gt;
&#22238;&#24402;&#22522;&#30784;&#65306;&#23545;&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#21512;&#29702;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms. (arXiv:2308.07886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07886
&lt;/p&gt;
&lt;p&gt;
&#23545;&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#22238;&#24402;&#22522;&#30784;&#36827;&#34892;&#20102;&#21512;&#29702;&#26816;&#26597;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#32463;&#20856;&#30340;&#26080;&#26102;&#38388;&#39034;&#24207;&#30340;&#34920;&#26684;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20174;1NN-DTW&#31639;&#27861;&#21040;ROCKET&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#21069;&#24555;&#36895;&#21457;&#23637;&#26032;&#20998;&#31867;&#22120;&#30340;&#36807;&#31243;&#20013;&#65292;&#36864;&#19968;&#27493;&#36827;&#34892;&#31616;&#21333;&#30340;&#22522;&#20934;&#26816;&#26597;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20123;&#26816;&#26597;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#22240;&#20026;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#24314;&#31435;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24320;&#21457;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#25968;&#25454;&#38598;&#19968;&#24320;&#22987;&#30475;&#36215;&#26469;&#20687;&#26159;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#32463;&#20856;&#30340;&#26080;&#26102;&#38388;&#39034;&#24207;&#30340;&#34920;&#26684;&#26041;&#27861;&#21487;&#33021;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#34920;&#26684;&#26041;&#27861;&#24448;&#24448;&#33021;&#26126;&#26174;&#20248;&#20110;&#26368;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;Ridge&#12289;LDA&#12289;RandomForest&#65289;&#19982;ROCKET&#20998;&#31867;&#22120;&#23478;&#26063;&#65288;&#20363;&#22914;Rocket&#12289;MiniRocket&#12289;MultiRocket&#65289;&#30340;&#25928;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#34920;&#26684;&#27169;&#22411;&#31616;&#21333;&#19988;&#24615;&#33021;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#37325;&#24314;&#20102;&#21160;&#24577;&#26080;&#21628;&#21560;&#32974;&#20799;&#24515;&#33039;MRI&#30340;kt-SENSE style&#37319;&#38598;&#25968;&#25454;&#65292;&#36890;&#36807;&#24674;&#22797;&#27424;&#37319;&#26679;&#25968;&#25454;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#37319;&#38598;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.07885</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#32974;&#20799;&#24515;&#33039;MRI&#37325;&#24314;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning. (arXiv:2308.07885v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#37325;&#24314;&#20102;&#21160;&#24577;&#26080;&#21628;&#21560;&#32974;&#20799;&#24515;&#33039;MRI&#30340;kt-SENSE style&#37319;&#38598;&#25968;&#25454;&#65292;&#36890;&#36807;&#24674;&#22797;&#27424;&#37319;&#26679;&#25968;&#25454;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#37319;&#38598;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26080;&#21628;&#21560;&#32974;&#20799;&#24515;&#33039;MRI&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#27169;&#24577;&#20043;&#19968;&#65292;&#38656;&#35201;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#26469;&#25551;&#36848;&#32974;&#20799;&#24515;&#33039;&#30340;&#24555;&#36895;&#21464;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24674;&#22797;&#27424;&#37319;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#20248;&#21270;kt-SENSE&#37319;&#38598;&#31574;&#30053;&#65292;&#24182;&#25552;&#39640;&#38750;&#38376;&#25511;kt-SENSE&#37325;&#24314;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#37327;&#20307;&#20869;&#25968;&#25454;&#25506;&#32034;&#20102;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;kt-SENSE&#39118;&#26684;&#37319;&#38598;&#25968;&#25454;&#30340;&#37325;&#24314;&#12290;&#36890;&#36807;&#35775;&#38382;&#20840;&#37319;&#26679;&#20302;&#20998;&#36776;&#29575;&#22810;&#32447;&#22280;&#32974;&#20799;&#24515;&#33039;MRI&#65292;&#25105;&#20204;&#30740;&#31350;&#32593;&#32476;&#20174;&#27424;&#37319;&#26679;&#25968;&#25454;&#20013;&#24674;&#22797;&#20840;&#37319;&#26679;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#27169;&#22411;&#26550;&#26500;&#20197;&#21450;&#35757;&#32451;&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#30340;&#30495;&#23454;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#27424;&#37319;&#26679;&#25968;&#25454;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#20462;&#25913;&#26469;&#24418;&#25104;&#21160;&#24577;&#32974;&#20799;&#24515;&#33039;&#37325;&#24314;&#24615;&#33021;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic free-breathing fetal cardiac MRI is one of the most challenging modalities, which requires high temporal and spatial resolution to depict rapid changes in a small fetal heart. The ability of deep learning methods to recover undersampled data could help to optimise the kt-SENSE acquisition strategy and improve non-gated kt-SENSE reconstruction quality. In this work, we explore supervised deep learning networks for reconstruction of kt-SENSE style acquired data using an extensive in vivo dataset. Having access to fully-sampled low-resolution multi-coil fetal cardiac MRI, we study the performance of the networks to recover fully-sampled data from undersampled data. We consider model architectures together with training strategies taking into account their application in the real clinical setup used to collect the dataset to enable networks to recover prospectively undersampled data. We explore a set of modifications to form a baseline performance evaluation for dynamic fetal cardi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#36793;&#32536;&#22238;&#24402;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;TGN&#20316;&#20026;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.07883</link><description>&lt;p&gt;
&#36808;&#21521;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#65306;&#20851;&#20110;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations. (arXiv:2308.07883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#36793;&#32536;&#22238;&#24402;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;TGN&#20316;&#20026;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21160;&#24577;&#22270;&#20219;&#21153;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#22238;&#24402;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24212;&#29992;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;GNNs&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#35774;&#32622;&#19979;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#39044;&#27979;&#22269;&#23478;&#20043;&#38388;&#30340;&#39135;&#21697;&#21644;&#20892;&#19994;&#36152;&#26131;&#20215;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#22269;&#36152;&#26131;&#25968;&#25454;&#38598;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20010;&#38745;&#24577;&#21644;&#19977;&#20010;&#21160;&#24577;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22522;&#32447;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#24322;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;TGN&#20248;&#20110;&#20854;&#20182;GNN&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;TGN&#26159;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35757;&#32451;&#26679;&#26412;&#20013;&#36127;&#36793;&#30340;&#27604;&#20363;&#26174;&#33879;&#24433;&#21709;&#20102;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2308.07871</link><description>&lt;p&gt;
&#24773;&#24863;&#23884;&#20837;&#8212;&#8212;&#20174;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#23450;&#19988;&#22343;&#21248;&#30340;&#25277;&#35937;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets. (arXiv:2308.07871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24773;&#24863;&#36890;&#36807;&#22810;&#31181;&#20132;&#27969;&#26041;&#24335;&#21644;&#23186;&#20307;&#26684;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35745;&#31639;&#30740;&#31350;&#21516;&#26679;&#22810;&#26679;&#21270;&#65292;&#28041;&#21450;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#31561;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#24773;&#24863;&#34987;&#20197;&#19981;&#21516;&#30340;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65288;&#26497;&#24615;&#23610;&#24230;&#12289;&#22522;&#26412;&#24773;&#24863;&#31867;&#21035;&#12289;&#32500;&#24230;&#26041;&#27861;&#12289;&#35780;&#20215;&#29702;&#35770;&#31561;&#65289;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#12289;&#39044;&#27979;&#27169;&#22411;&#21644;&#24773;&#24863;&#20998;&#26512;&#36719;&#20214;&#24037;&#20855;&#30340;&#22810;&#26679;&#21270;&#22686;&#38271;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#22312;&#34920;&#36798;&#21644;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#36843;&#20999;&#38656;&#35201;&#32479;&#19968;&#20197;&#24448;&#23545;&#36234;&#26469;&#36234;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#26631;&#31614;&#31867;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#31181;&#20849;&#20139;&#30340;&#24773;&#24863;&#28508;&#22312;&#34920;&#31034;&#65292;&#21363;&#25152;&#35859;&#24773;&#24863;&#23884;&#20837;&#65292;&#19981;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#34920;&#31034;&#26631;&#31614;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label form
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07870</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#23454;&#29616;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#26412;&#19990;&#32426;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;AI&#39046;&#22495;&#21462;&#24471;&#30340;&#22823;&#37096;&#20998;&#25104;&#26524;&#37117;&#26159;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#21450;&#24212;&#29992;&#24050;&#32463;&#20984;&#26174;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#38590;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#32570;&#20047;&#40065;&#26834;&#24615;&#12289;&#19981;&#21487;&#38752;&#24615;&#21644;&#29983;&#29289;&#23398;&#19978;&#30340;&#19981;&#21512;&#29702;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#38656;&#35201;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#30340;&#21551;&#21457;&#21644;&#25351;&#23548;&#30340;&#26041;&#26696;&#12290;&#20854;&#20013;&#19968;&#31181;&#29702;&#35770;&#31216;&#20026;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#65306;PC&#21487;&#20197;&#27169;&#25311;&#19981;&#21516;&#33041;&#21306;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#35748;&#30693;&#25511;&#21046;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#24182;&#22312;&#21464;&#20998;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#65292;&#36890;&#36807;&#39030;&#28857;&#24230;&#26680;&#21644;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.07867</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#30340;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes. (arXiv:2308.07867v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#65292;&#36890;&#36807;&#39030;&#28857;&#24230;&#26680;&#21644;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#12290;&#35813;&#26680;&#34987;&#21629;&#21517;&#20026;&#39030;&#28857;&#24230;&#26680;&#65288;VDK&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;&#32593;&#32476;&#22270;&#25110;&#25299;&#25169;&#30340;&#30005;&#21387;&#27880;&#20837;&#20851;&#31995;&#30340;&#28508;&#22312;&#20998;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;VDK&#35774;&#35745;&#36991;&#20813;&#20102;&#38656;&#35201;&#35299;&#20915;&#26680;&#25628;&#32034;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#36739;&#23569;&#39033;&#30340;VDK&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#26234;&#33021;&#22320;&#36873;&#25321;&#39034;&#24207;&#35757;&#32451;&#36755;&#20837;&#65292;&#21152;&#36895;VDK&#30340;&#23398;&#20064;&#12290;&#21033;&#29992;VDK&#30340;&#21487;&#21152;&#24615;&#32467;&#26500;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#23545;GP&#30340;&#39044;&#27979;&#26041;&#24046;&#36827;&#34892;&#20102;&#22359;&#19979;&#38477;&#31867;&#22411;&#30340;&#36807;&#31243;&#65292;&#20316;&#20026;&#20449;&#24687;&#22686;&#30410;&#30340;&#20195;&#29702;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;VDK-GP&#19982;&#20013;&#31561;&#35268;&#27169;500&#20010;&#33410;&#28857;&#21644;&#22823;&#35268;&#27169;1354&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;GP&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#20004;&#20493;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.07857</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Impression-Aware Recommender Systems. (arXiv:2308.07857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07857
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#25968;&#25454;&#28304;&#20026;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#36136;&#37327;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#21360;&#35937;&#26159;&#19968;&#31181;&#21253;&#21547;&#36807;&#21435;&#25512;&#33616;&#65288;&#23637;&#31034;&#30340;&#39033;&#30446;&#65289;&#21644;&#20256;&#32479;&#20114;&#21160;&#30340;&#26032;&#22411;&#25968;&#25454;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#21360;&#35937;&#26469;&#20248;&#21270;&#29992;&#25143;&#20559;&#22909;&#24182;&#20811;&#26381;&#24403;&#21069;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#12290;&#21360;&#35937;&#30340;&#30456;&#20851;&#24615;&#21644;&#20852;&#36259;&#24230;&#36880;&#24180;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31867;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20851;&#20110;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#20391;&#37325;&#20110;&#30740;&#31350;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#19977;&#20010;&#20998;&#31867;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#27599;&#31687;&#32508;&#36848;&#35770;&#25991;&#65292;&#25551;&#36848;&#20102;&#20855;&#26377;&#21360;&#35937;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#25991;&#29486;&#20013;&#32570;&#22833;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07843</link><description>&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) &#35813;&#35770;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#65306;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21307;&#30103;&#26088;&#22312;&#36890;&#36807;&#22312;&#20010;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#25552;&#20379;&#24178;&#39044;&#26469;&#25552;&#39640;&#20581;&#24247;&#32467;&#26524;&#12290;&#29031;&#39038;&#20276;&#20387;&#21644;&#31038;&#20250;&#25903;&#25345;&#32593;&#32476;&#30340;&#21442;&#19982;&#32463;&#24120;&#22312;&#24110;&#21161;&#20010;&#20154;&#31649;&#29702;&#32321;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20026;&#31227;&#21160;&#21307;&#30103;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#35774;&#35745;&#38024;&#23545;&#20108;&#20803;&#20851;&#31995;&#8212;&#8212;&#30446;&#26631;&#20154;&#21644;&#20854;&#29031;&#39038;&#20276;&#20387;&#20043;&#38388;&#20851;&#31995;&#8212;&#8212;&#20197;&#25552;&#39640;&#31038;&#20250;&#25903;&#25345;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;Dyadic RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#21450;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#20010;&#24615;&#21270;&#24178;&#39044;&#25514;&#26045;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#22810;&#32452;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30528;&#20108;&#20803;&#20851;&#31995;&#22312;&#22810;&#20010;&#26102;&#38388;&#38388;&#38548;&#20869;&#12290;&#24320;&#21457;&#30340;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#38382;&#39064;&#35774;&#23450;&#65292;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#24182;&#30830;&#23450;&#20102;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37096;&#20998;&#22270;&#25915;&#20987; (PGA) &#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#23427;&#36873;&#21462;&#33030;&#24369;&#33410;&#28857;&#20316;&#20026;&#25915;&#20987;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#20840;&#23616;&#25915;&#20987;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.07834</link><description>&lt;p&gt;
&#31616;&#21333;&#39640;&#25928;&#30340;&#37096;&#20998;&#22270;&#23545;&#25239;&#25915;&#20987;: &#19968;&#20010;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Simple and Efficient Partial Graph Adversarial Attack: A New Perspective. (arXiv:2308.07834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37096;&#20998;&#22270;&#25915;&#20987; (PGA) &#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#23427;&#36873;&#21462;&#33030;&#24369;&#33410;&#28857;&#20316;&#20026;&#25915;&#20987;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#20840;&#23616;&#25915;&#20987;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#28145;&#20837;&#21644;&#20840;&#38754;&#65292;&#23427;&#20204;&#30340;&#31283;&#20581;&#24615;&#21644;&#23433;&#20840;&#24615;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#20840;&#23616;&#25915;&#20987;&#26041;&#27861;&#23558;&#22270;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#35270;&#20026;&#25915;&#20987;&#30446;&#26631;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#24403;&#21069;&#26041;&#27861;&#36807;&#20110;&#20725;&#21270;&#22320;&#36981;&#24490;&#20840;&#23616;&#25915;&#20987;&#30340;&#23450;&#20041;&#12290;&#23427;&#20204;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#19981;&#21516;&#30340;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#31283;&#20581;&#24615;&#65292;&#23545;&#25915;&#20987;&#19981;&#20855;&#26377;&#30456;&#31561;&#30340;&#38887;&#24615;&#12290;&#20174;&#20840;&#23616;&#25915;&#20987;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#24212;&#35813;&#26126;&#26234;&#22320;&#23433;&#25490;&#25915;&#20987;&#39044;&#31639;&#65292;&#32780;&#19981;&#26159;&#28010;&#36153;&#22312;&#20855;&#26377;&#39640;&#31283;&#20581;&#24615;&#30340;&#33410;&#28857;&#19978;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#37096;&#20998;&#22270;&#25915;&#20987;(PGA)&#65292;&#23427;&#36873;&#21462;&#20102;&#33030;&#24369;&#30340;&#33410;&#28857;&#20316;&#20026;&#25915;&#20987;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36873;&#25321;&#33030;&#24369;&#30340;&#33410;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#30446;&#26631;&#36873;&#25321;&#31574;&#30053;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#21482;&#20851;&#27880;&#26131;&#20110;&#25915;&#20987;&#30340;&#33410;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25104;&#26412;&#25928;&#29575;&#30340;&#8230;
&lt;/p&gt;
&lt;p&gt;
As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-e
&lt;/p&gt;</description></item><item><title>REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.07832</link><description>&lt;p&gt;
REFORMS: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07832
&lt;/p&gt;
&lt;p&gt;
REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#37319;&#29992;&#20063;&#20276;&#38543;&#30528;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22833;&#36133;&#12290;&#36825;&#20123;&#22833;&#36133;&#21487;&#33021;&#20250;&#38459;&#30861;&#31185;&#23398;&#36827;&#23637;&#65292;&#23548;&#33268;&#23545;&#26080;&#25928;&#32467;&#35770;&#30340;&#38169;&#35823;&#20849;&#35782;&#65292;&#24182;&#21066;&#24369;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#30340;&#21487;&#20449;&#24230;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#31185;&#20013;&#24120;&#24120;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#24212;&#29992;&#19988;&#22833;&#36133;&#12290;&#20986;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25552;&#20379;&#28165;&#26224;&#30340;&#25253;&#21578;&#26631;&#20934;&#12290;&#22522;&#20110;&#23545;&#36807;&#21435;&#25991;&#29486;&#30340;&#24191;&#27867;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFORMS&#26816;&#26597;&#34920;&#65288;$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience&#65289;&#12290;&#23427;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#37197;&#22871;&#30340;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#12290;REFORMS&#26159;&#22522;&#20110;19&#20301;&#30740;&#31350;&#20154;&#21592;&#30340;&#20849;&#35782;&#24320;&#21457;&#30340;&#65292;&#36825;&#20123;&#20154;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#12290;REFORMS&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#21160;&#24577;&#22320;&#20272;&#35745;&#24403;&#21069;&#23481;&#37327;&#21644;&#39044;&#27979;&#26410;&#26469;&#23481;&#37327;&#12290;&#36825;&#31181;&#27169;&#22411;&#26377;&#21161;&#20110;&#20934;&#30830;&#24555;&#36895;&#20272;&#35745;&#21644;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#32769;&#21270;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.07824</link><description>&lt;p&gt;
Cerberus:&#22522;&#20110;&#25918;&#26494;&#30005;&#21387;&#26354;&#32447;&#30340;&#28145;&#24230;&#23398;&#20064;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#20272;&#35745;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves. (arXiv:2308.07824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#19982;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#21160;&#24577;&#22320;&#20272;&#35745;&#24403;&#21069;&#23481;&#37327;&#21644;&#39044;&#27979;&#26410;&#26469;&#23481;&#37327;&#12290;&#36825;&#31181;&#27169;&#22411;&#26377;&#21161;&#20110;&#20934;&#30830;&#24555;&#36895;&#20272;&#35745;&#21644;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#32769;&#21270;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#36864;&#21270;&#36807;&#31243;&#19982;&#20854;&#20316;&#20026;&#21160;&#21147;&#28304;&#21644;&#20648;&#33021;&#35774;&#22791;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#23494;&#20999;&#30456;&#20851;&#65292;&#28085;&#30422;&#20102;&#24615;&#33021;&#20132;&#20184;&#21644;&#24490;&#29615;&#21033;&#29992;&#31561;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#23545;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#29366;&#24577;&#30340;&#20934;&#30830;&#24555;&#36895;&#20272;&#35745;&#25110;&#39044;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32769;&#21270;&#20272;&#35745;&#25110;&#39044;&#27979;&#19978;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#26041;&#38754;&#30340;&#21160;&#24577;&#34701;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23481;&#37327;&#32769;&#21270;&#20272;&#35745;&#21644;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#20174;&#20805;&#30005;&#21644;&#25918;&#30005;&#25918;&#26494;&#36807;&#31243;&#20013;&#25552;&#21462;&#19982;&#32769;&#21270;&#23494;&#20999;&#30456;&#20851;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#36890;&#36807;&#34701;&#21512;&#21382;&#21490;&#23481;&#37327;&#34928;&#20943;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#21160;&#24577;&#25552;&#20379;&#38146;&#31163;&#23376;&#30005;&#27744;&#24403;&#21069;&#23481;&#37327;&#30340;&#20272;&#35745;&#21644;&#26410;&#26469;&#23481;&#37327;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#19981;&#21516;&#20805;&#25918;&#30005;&#24490;&#29615;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization. Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention. Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets. This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes. By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries. Our approach is validated against a novel dataset involving charge and discharge cycles at varying
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#32508;&#36848;&#21644;&#23637;&#26395;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#28508;&#22312;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#20854;&#22312;&#21270;&#23398;&#24037;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07822</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;: &#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for process design: Review and perspective. (arXiv:2308.07822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07822
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#32508;&#36848;&#21644;&#23637;&#26395;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#28508;&#22312;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#20854;&#22312;&#21270;&#23398;&#24037;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#24037;&#19994;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#21407;&#26009;&#20379;&#24212;&#30340;&#36716;&#22411;&#38656;&#35201;&#26032;&#30340;&#27010;&#24565;&#24615;&#27969;&#31243;&#35774;&#35745;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#20026;&#21152;&#36895;&#36825;&#19968;&#36716;&#21464;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#21644;&#24110;&#21161;&#21487;&#25345;&#32493;&#27969;&#31243;&#35774;&#35745;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#35201;&#32032;&#23545;&#27969;&#31243;&#35774;&#35745;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#65306;&#65288;i&#65289;&#20449;&#24687;&#34920;&#31034;&#12289;&#65288;ii&#65289;&#20195;&#29702;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#29615;&#22659;&#21644;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#25361;&#25112;&#21644;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#24378;&#21270;&#23398;&#20064;&#22312;&#21270;&#23398;&#24037;&#31243;&#30340;&#27969;&#31243;&#35774;&#35745;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07817</link><description>&lt;p&gt;
&#37327;&#21270;&#38431;&#21015;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Cost of Learning in Queueing Systems. (arXiv:2308.07817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38431;&#21015;&#31995;&#32479;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#36890;&#20449;&#32593;&#32476;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#26381;&#21153;&#31995;&#32479;&#31561;&#31561;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#26368;&#20248;&#25511;&#21046;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#31995;&#32479;&#21442;&#25968;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#24456;&#24120;&#35265;&#65292;&#22240;&#27492;&#26368;&#36817;&#19968;&#31995;&#21015;&#20851;&#20110;&#38431;&#21015;&#31995;&#32479;&#30340;&#23398;&#20064;&#30340;&#30740;&#31350;&#20135;&#29983;&#20102;&#12290;&#36825;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#20851;&#27880;&#25152;&#25552;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#28176;&#36817;&#24230;&#37327;&#65292;&#21363;&#30528;&#30524;&#20110;&#21518;&#26399;&#24615;&#33021;&#30340;&#24230;&#37327;&#65292;&#26080;&#27861;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#36890;&#24120;&#20986;&#29616;&#22312;&#26089;&#26399;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#34913;&#37327;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#30340;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#25105;&#20204;&#23545;&#21333;&#38431;&#21015;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#30340;CLQ&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.  In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system,
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.07805</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#21450;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness and Privacy in Federated Learning and Their Implications in Healthcare. (arXiv:2308.07805v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07805
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#24773;&#22659;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#30001;&#20110;&#23433;&#20840;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#32771;&#34385;&#22240;&#32032;&#26159;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#21463;&#21040;HIPAA&#31561;&#25968;&#25454;&#20351;&#29992;&#26465;&#20363;&#30340;&#31649;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26356;&#22823;&#30340;&#26679;&#26412;&#37327;&#21644;&#20849;&#20139;&#25968;&#25454;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20801;&#35768;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#24191;&#65292;&#32771;&#34385;&#21040;&#26356;&#22810;&#30340;&#21464;&#24322;&#24615;&#21644;&#24179;&#34913;&#19981;&#36275;&#30340;&#31867;&#21035;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20801;&#35768;&#25968;&#25454;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#21453;&#36807;&#26469;&#35299;&#20915;&#20102;&#25968;&#25454;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#28431;&#27934;&#32771;&#34385;&#65292;&#22240;&#20026;&#25968;&#25454;&#26412;&#36523;&#24182;&#19981;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#32593;&#32476;&#33410;&#28857;&#20043;&#38388;&#20849;&#20139;&#12290;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#33410;&#28857;&#25968;&#25454;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;(iid)&#65292;&#23458;&#25143;&#26426;&#38656;&#35201;&#22312;&#23545;&#31561;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#39640;&#27700;&#24179;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#21450;&#32593;&#32476;&#20013;&#19981;&#21516;&#23458;&#25143;&#26426;&#30340;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes. Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20272;&#35745;&#21463;&#21040;&#26377;&#33394;&#22122;&#22768;&#24433;&#21709;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22312;&#32447;&#22122;&#22768;&#21327;&#26041;&#24046;&#21644;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.07797</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#26377;&#33394;&#22122;&#22768;&#19979;&#33258;&#36866;&#24212;&#20272;&#35745;&#22122;&#22768;&#21327;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization. (arXiv:2308.07797v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20272;&#35745;&#21463;&#21040;&#26377;&#33394;&#22122;&#22768;&#24433;&#21709;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22312;&#32447;&#22122;&#22768;&#21327;&#26041;&#24046;&#21644;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#20934;&#30830;&#20272;&#35745;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#65288;NCM&#65289;&#23545;&#29366;&#24577;&#20272;&#35745;&#21644;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20854;&#26368;&#20248;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;NCM&#20272;&#35745;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#22122;&#22768;&#26159;&#30333;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22122;&#22768;&#26159;&#26377;&#33394;&#30340;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#23637;&#29616;&#20986;&#26102;&#38388;&#33258;&#30456;&#20851;&#24615;&#65289;&#65292;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31867;&#33041;&#31639;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#33258;&#36866;&#24212;&#22320;&#20272;&#35745;&#21463;&#21040;&#26377;&#33394;&#22122;&#22768;&#24433;&#21709;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;NCM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#25193;&#23637;&#20026;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#33021;&#30446;&#26631;&#21516;&#26102;&#36827;&#34892;&#22312;&#32447;&#22122;&#22768;&#21327;&#26041;&#24046;&#21644;&#29366;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#27169;&#25311;&#23454;&#39564;&#20013;&#25968;&#23398;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;NCM&#20272;&#35745;&#22120;&#25910;&#25947;&#21040;&#35813;&#33258;&#30001;&#33021;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#26368;&#23567;&#22122;&#22768;&#21327;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#20061;&#20010;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate estimation of the noise covariance matrix (NCM) in a dynamic system is critical for state estimation and control, as it has a major influence in their optimality. Although a large number of NCM estimation methods have been developed, most of them assume the noises to be white. However, in many real-world applications, the noises are colored (e.g., they exhibit temporal autocorrelations), resulting in suboptimal solutions. Here, we introduce a novel brain-inspired algorithm that accurately and adaptively estimates the NCM for dynamic systems subjected to colored noise. Particularly, we extend the Dynamic Expectation Maximization algorithm to perform both online noise covariance and state estimation by optimizing the free energy objective. We mathematically prove that our NCM estimator converges to the global optimum of this free energy objective. Using randomized numerical simulations, we show that our estimator outperforms nine baseline methods with minimal noise covarianc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.07791</link><description>&lt;p&gt;
&#20026;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07791
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36234;&#26469;&#36234;&#24378;&#30340;&#33021;&#21147;&#65292;&#24050;&#25104;&#20026;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#25991;&#26412;&#22788;&#29702;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20173;&#28982;&#21463;&#21040;&#20043;&#21069;&#19968;&#20195;&#20165;&#32534;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Informed Named Entity Recognition Decoding&#65288;iNERD&#65289;&#65292;&#23427;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35270;&#20026;&#19968;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#23427;&#20197;&#38754;&#21521;&#26410;&#26469;&#30340;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#25552;&#21462;&#30340;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#20219;&#20309;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#21512;&#24182;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#19978;&#31895;&#35843;&#20248;&#21270;&#20102;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#20843;&#20010;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, espec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#36716;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#35270;&#35273;&#24341;&#23548;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#38899;&#39057;&#20449;&#24687;&#65292;&#20165;&#36890;&#36807;&#36755;&#20837;&#30340;&#35270;&#35273;&#20449;&#24687;&#21363;&#21487;&#20135;&#29983;&#20016;&#23500;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.07787</link><description>&lt;p&gt;
DiffV2S: &#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#36716;&#35821;&#38899;&#21512;&#25104;&#19982;&#35270;&#35273;&#24341;&#23548;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding. (arXiv:2308.07787v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#36716;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#35270;&#35273;&#24341;&#23548;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#38899;&#39057;&#20449;&#24687;&#65292;&#20165;&#36890;&#36807;&#36755;&#20837;&#30340;&#35270;&#35273;&#20449;&#24687;&#21363;&#21487;&#20135;&#29983;&#20016;&#23500;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35270;&#39057;&#36716;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#21363;&#20165;&#36890;&#36807;&#35270;&#35273;&#36755;&#20837;&#37325;&#24314;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#27491;&#30830;&#25512;&#26029;&#20986;&#36866;&#24403;&#22768;&#38899;&#30340;&#20869;&#23481;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#21512;&#25104;&#35821;&#38899;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#37319;&#29992;&#39069;&#22806;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#20316;&#20026;&#26469;&#33258;&#21442;&#32771;&#21548;&#35273;&#20449;&#24687;&#30340;&#35828;&#35805;&#39118;&#26684;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#24448;&#24448;&#26080;&#27861;&#20174;&#30456;&#24212;&#30340;&#35270;&#39057;&#36755;&#20837;&#20013;&#33719;&#24471;&#38899;&#39057;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#30340;&#35270;&#35273;&#24341;&#23548;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#36755;&#20837;&#30340;&#35270;&#35273;&#20449;&#24687;&#20135;&#29983;&#20016;&#23500;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#20449;&#24687;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#38899;&#39057;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#36816;&#21160;&#25511;&#21046;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#26469;&#23454;&#29616;&#22810;&#32423;&#35268;&#21010;&#21644;&#21327;&#35843;&#32930;&#20307;&#36816;&#21160;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.07775</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hierarchical generative modelling for autonomous robots. (arXiv:2308.07775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#36816;&#21160;&#25511;&#21046;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#26469;&#23454;&#29616;&#22810;&#32423;&#35268;&#21010;&#21644;&#21327;&#35843;&#32930;&#20307;&#36816;&#21160;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#19982;&#21608;&#22260;&#29615;&#22659;&#20114;&#21160;&#26102;&#21487;&#20197;&#20135;&#29983;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#65292;&#36890;&#36807;&#35745;&#21010;&#12289;&#25191;&#34892;&#21644;&#32452;&#21512;&#21508;&#20010;&#32930;&#20307;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#36816;&#21160;&#25511;&#21046;&#30340;&#36825;&#19968;&#22522;&#26412;&#26041;&#38754;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#37197;&#22791;&#20102;&#22810;&#32423;&#35268;&#21010;&#65292;&#20197;&#27169;&#20223;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#26102;&#38388;&#28145;&#24230;&#26159;&#25351;&#21069;&#21521;&#25110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36830;&#32493;&#23618;&#27425;&#30340;&#23884;&#22871;&#26102;&#38388;&#23610;&#24230;&#65292;&#20363;&#22914;&#65292;&#20132;&#20184;&#19968;&#20010;&#29289;&#20307;&#38656;&#35201;&#19968;&#20010;&#20840;&#23616;&#35745;&#21010;&#26469;&#19978;&#19979;&#25991;&#21270;&#22810;&#20010;&#32930;&#20307;&#30340;&#24555;&#36895;&#21327;&#35843;&#12290;&#36825;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#20998;&#31163;&#20063;&#25512;&#21160;&#20102;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#21151;&#33021;&#30340;&#24863;&#30693;&#21160;&#20316;&#25511;&#21046;&#65292;&#20197;&#20998;&#23618;&#32467;&#26500;&#21270;&#35268;&#21010;&#21644;&#20302;&#23618;&#32930;&#20307;&#36816;&#21160;&#25511;&#21046;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#21644;&#29289;&#29702;&#27169;&#25311;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07774</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#27744;&#21270;&#25805;&#20316;&#65292;&#23427;&#26088;&#22312;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#27744;&#21270;&#31574;&#30053;&#20381;&#36182;&#20110;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#33719;&#24471;&#30340;&#20998;&#37197;&#30697;&#38453;&#65292;&#35813;&#30697;&#38453;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27744;&#21270;&#36807;&#31243;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;LCPool&#65292;&#23427;&#21033;&#29992;&#23616;&#37096;&#32422;&#26463;&#32447;&#24615;&#32534;&#30721;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#65292;&#36890;&#36807;&#27714;&#35299;&#24102;&#26377;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#30340;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#32858;&#31867;&#20998;&#37197;&#30697;&#38453;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#32422;&#26463;&#65292;LCPool&#34987;&#35774;&#35745;&#25104;&#20813;&#36153;
&lt;/p&gt;
&lt;p&gt;
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
&lt;/p&gt;</description></item><item><title>MOLE&#26159;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#27169;&#22359;&#21270;&#26041;&#24335;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07772</link><description>&lt;p&gt;
MOLE: MOdular Learning FramEwork&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MOLE: MOdular Learning FramEwork via Mutual Information Maximization. (arXiv:2308.07772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07772
&lt;/p&gt;
&lt;p&gt;
MOLE&#26159;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#27169;&#22359;&#21270;&#26041;&#24335;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MOLE&#65288;MOdular Learning Framework&#65289;&#30340;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#26041;&#24335;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22359;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#27599;&#20010;&#27169;&#22359;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#25353;&#39034;&#24207;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#12290;MOLE&#20351;&#24471;&#35757;&#32451;&#21464;&#24471;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#27169;&#22359;&#20043;&#38388;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#26696;&#22312;&#29983;&#29289;&#23398;&#19978;&#26356;&#20855;&#21487;&#34892;&#24615;&#65292;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#21152;&#21512;&#29702;&#12290;&#25105;&#20204;&#22312;&#21521;&#37327;&#12289;&#32593;&#26684;&#21644;&#22270;&#24418;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#35299;&#20915;&#22270;&#24418;&#31867;&#22411;&#25968;&#25454;&#30340;&#22270;&#24418;&#32423;&#21644;&#33410;&#28857;&#32423;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;MOLE&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#21644;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#65292;&#24182;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;</title><link>http://arxiv.org/abs/2308.07748</link><description>&lt;p&gt;
&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Automotive Radar Object Detection Networks. (arXiv:2308.07748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#21644;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#65292;&#24182;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#24863;&#30693;&#29615;&#22659;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#26159;&#36825;&#31867;&#31995;&#32479;&#30340;&#19968;&#20010;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;CNN&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#21367;&#31215;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#23427;&#23558;&#24378;&#22823;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#26816;&#27979;&#19982;&#20302;&#35745;&#31639;&#36164;&#28304;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38647;&#36798;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#65288;SKPP&#65289;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#65288;DVPC&#65289;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SKPP-DPVCN&#26550;&#26500;&#65292;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#12290;&#27492;&#22806;&#65292;SKPP-DPVCN&#36824;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#65288;ASE&#65289;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;
&lt;/p&gt;
&lt;p&gt;
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
&lt;/p&gt;</description></item><item><title>Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07741</link><description>&lt;p&gt;
Real Robot Challenge 2022: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07741
&lt;/p&gt;
&lt;p&gt;
Real Robot Challenge 2022&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#33021;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#23454;&#39564;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#19978;&#35201;&#27714;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#31038;&#21306;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#20351;&#29992;&#27169;&#25311;&#22120;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#27169;&#25311;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#19981;&#19968;&#23450;&#33021;&#22815;&#36716;&#21270;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#28041;&#21450;&#22797;&#26434;&#29615;&#22659;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;Real Robot Challenge 2022&#20316;&#20026;RL&#21644;&#26426;&#22120;&#20154;&#23398;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#35753;&#21442;&#19982;&#32773;&#33021;&#22815;&#20687;&#22312;&#27169;&#25311;&#20013;&#19968;&#26679;&#36731;&#26494;&#22320;&#36828;&#31243;&#23454;&#39564;&#30495;&#23454;&#26426;&#22120;&#20154;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#24050;&#32463;&#25104;&#29087;&#20026;&#19968;&#31181;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20943;&#36731;&#20102;&#23545;&#26114;&#36149;&#22312;&#32447;&#20132;&#20114;&#30340;&#20381;&#36182;.&#22240;&#27492;&#65292;&#25105;&#20204;&#35201;&#27714;&#21442;&#19982;&#32773;&#20174;&#25552;&#20379;&#30340;&#30495;&#23454;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20004;&#20010;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#25512;&#21160;&#12289;&#25235;&#21462;&#21644;&#25163;&#20869;&#23450;&#20301;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#36719;&#20214;&#25991;&#26723;&#21270;&#65292;&#24182;&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#21021;&#27493;&#38454;&#27573;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.  In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07728</link><description>&lt;p&gt;
&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#24050;&#20855;&#22791;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29983;&#30072;&#21464;&#12290;&#22312;&#36866;&#24212;&#26032;&#30446;&#26631;&#39046;&#22495;&#26102;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#23545;&#22836;&#23618;&#36827;&#34892;&#23545;&#40784;&#22788;&#29702;&#21487;&#20197;&#22788;&#29702;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#22788;&#29702;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#12289;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#25913;&#26469;&#26377;&#25928;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#26159;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.07707</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#23454;&#29616;&#24555;&#36895;&#30340;&#26426;&#22120;&#36951;&#24536;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07707
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#26159;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36951;&#24536;&#33021;&#21147;&#65292;&#27491;&#22312;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20415;&#31526;&#21512;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#21024;&#38500;&#26377;&#23475;&#12289;&#31713;&#25913;&#25110;&#36807;&#26102;&#30340;&#20449;&#24687;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#20445;&#25252;&#27169;&#22411;&#22312;&#20854;&#20313;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20445;&#30041;&#25968;&#25454;&#19978;&#36827;&#34892;&#26576;&#31181;&#31243;&#24230;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#20445;&#25252;&#25110;&#24674;&#22797;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#22686;&#21152;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#35201;&#27714;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#21487;&#29992;&#21644;&#21487;&#35775;&#38382;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#26041;&#27861;&#37319;&#29992;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#36807;&#39640;&#65292;&#24182;&#19988;&#24615;&#33021;&#19981;&#21450;&#37325;&#26032;&#35757;&#32451;&#30340;&#23545;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#21518;&#39564;&#12289;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require lon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#29109;&#30340;k-means&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#29109;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.07705</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#29109;&#30340;k-means&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#26041;&#27861;&#22312;&#19981;&#21516;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets. (arXiv:2308.07705v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#29109;&#30340;k-means&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#29109;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k-means&#31639;&#27861;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#24212;&#29992;&#19988;&#31616;&#21333;&#30340;&#32858;&#31867;&#20998;&#26512;&#31639;&#27861;&#12290;&#23427;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#24066;&#22330;&#32454;&#20998;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#25968;&#25454;&#25366;&#25496;&#12289;&#24515;&#29702;&#23398;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;k-means&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#33021;&#20135;&#29983;&#26368;&#20248;&#36136;&#30340;&#32467;&#26524;&#12290;&#23427;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25552;&#20379;&#30340;&#32858;&#31867;&#25968;&#21644;&#21512;&#36866;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#31181;&#23376;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#29109;&#36827;&#34892;&#29109;-based&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19968;&#33324;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#21305;&#37197;&#29109;&#24230;&#37327;&#65292;&#23545;k-means&#22312;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#29109;&#24230;&#37327;&#65292;&#22914;Taneja&#29109;&#12289;Kapur&#29109;&#12289;Aczel Daroczy&#29109;&#12289;Sharma Mittal&#29109;&#12290;&#35266;&#23519;&#21040;&#23545;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#21516;&#30340;&#29109;&#24230;&#37327;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65306;Satellite...
&lt;/p&gt;
&lt;p&gt;
One of the most employed yet simple algorithm for cluster analysis is the k-means algorithm. k-means has successfully witnessed its use in artificial intelligence, market segmentation, fraud detection, data mining, psychology, etc., only to name a few. The k-means algorithm, however, does not always yield the best quality results. Its performance heavily depends upon the number of clusters supplied and the proper initialization of the cluster centroids or seeds. In this paper, we conduct an analysis of the performance of k-means on image data by employing parametric entropies in an entropy based centroid initialization method and propose the best fitting entropy measures for general image datasets. We use several entropies like Taneja entropy, Kapur entropy, Aczel Daroczy entropy, Sharma Mittal entropy. We observe that for different datasets, different entropies provide better results than the conventional methods. We have applied our proposed algorithm on these datasets: Satellite, To
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.07688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#22270;&#20687;&#22686;&#24378;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#32593;&#32476;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#29305;&#24449;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#21487;&#20197;&#32469;&#36807;&#32321;&#37325;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SSL&#39044;&#35757;&#32451;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#65292;&#24182;&#19982;&#38750;&#21307;&#23398;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24182;&#26681;&#25454;&#20197;&#19979;&#26041;&#24335;&#21021;&#22987;&#21270;&#20854;&#26435;&#37325;&#65306;&#65288;i&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;SSL&#39044;&#35757;&#32451;&#65288;DINOv2&#65289;&#12289;&#65288;ii&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;MIMIC-CXR&#25968;&#25454;&#24211;&#20013;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#20845;&#20010;&#20840;&#29699;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;800,000&#22810;&#24352;&#33016;&#37096;X&#23556;&#32447;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35786;&#26029;&#20102;20&#22810;&#31181;&#19981;&#21516;&#30340;&#24433;&#20687;&#25152;&#35265;&#12290;&#25105;&#20204;&#30340;SSL&#39044;&#35757;&#32451;&#22312;&#32463;&#36807;&#31579;&#36873;&#30340;&#22270;&#20687;&#19978;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#65288;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;P&lt;0.001&#65289;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P&lt;0.001 for all datasets) but, in cert
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.07687</link><description>&lt;p&gt;
DiffGuard&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#35821;&#20041;&#24102;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#19982;&#21512;&#27861;&#31867;&#21035;&#20869;&#23481;&#22312;&#35821;&#20041;&#19978;&#30340;&#19981;&#21305;&#37197;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;DiffGuard&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;DiffGuard&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#65292;&#30456;&#36739;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26131;&#20110;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#26465;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;DiffGuard&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#22270;&#20687;&#21644;&#26631;&#31614;&#20316;&#20026;&#26465;&#20214;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#65292;DiffGuard&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35757;&#32451;&#21518;&#37327;&#21270;(GPTQ)&#26041;&#27861;&#20013;&#30340;&#24120;&#35265;&#36873;&#25321;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23545;&#22810;&#20010;&#21464;&#37327;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07662</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35757;&#32451;&#21518;&#37327;&#21270;&#65306;&#25361;&#25112;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Post-Training Quantization: Challenging the Status Quo. (arXiv:2308.07662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35757;&#32451;&#21518;&#37327;&#21270;(GPTQ)&#26041;&#27861;&#20013;&#30340;&#24120;&#35265;&#36873;&#25321;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23545;&#22810;&#20010;&#21464;&#37327;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23545;&#20110;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#28014;&#28857;&#36816;&#31639;&#34987;&#36716;&#21270;&#20026;&#31616;&#21270;&#30340;&#23450;&#28857;&#36816;&#31639;&#12290;&#22312;&#26368;&#31616;&#21333;&#30340;&#24418;&#24335;&#20013;&#65292;&#23427;&#20165;&#20165;&#26159;&#30001;&#32553;&#25918;&#21644;&#33293;&#20837;&#36716;&#25442;&#32452;&#25104;&#65292;&#23548;&#33268;&#21387;&#32553;&#29575;&#26377;&#38480;&#25110;&#32773;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35757;&#32451;&#21518;&#37327;&#21270;(GPTQ)&#26041;&#27861;&#22312;&#31616;&#21333;&#26041;&#27861;&#21644;&#26356;&#24378;&#22823;&#20294;&#26114;&#36149;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451; (QAT)&#26041;&#27861;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#36866;&#30340;&#25240;&#34935;&#65292;&#29305;&#21035;&#26159;&#22312;&#23581;&#35797;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26102;&#65292;&#37327;&#21270;&#36807;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;GPTQ&#20027;&#35201;&#26159;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#26657;&#20934;&#38598;&#21512;&#26469;&#23398;&#20064;&#33293;&#20837;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;GPTQ&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#36873;&#25321;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#36807;&#31243;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23545;&#22810;&#20010;&#21464;&#37327;(&#26435;&#37325;&#36873;&#25321;&#12289;&#29305;&#24449;&#22686;&#24378;&#12289;&#26657;&#20934;&#36873;&#25321;)&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.07661</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#20877;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#19996;&#35199;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Extractor&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65292;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;Extractor&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#27969;&#34892;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24615;&#33021;&#24179;&#34913;&#26469;&#20943;&#23569;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#23545;&#20110;Transformer&#30340;&#25345;&#32493;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21462;&#20195;Transformer&#20013;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#65288;Extractor&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Extractor&#26367;&#25442;&#33258;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Extractor&#20855;&#26377;&#26356;&#30701;&#30340;&#35745;&#31639;&#20851;&#38190;&#36335;&#24452;&#65292;&#22240;&#27492;&#26377;&#28508;&#21147;&#27604;&#33258;&#27880;&#24847;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20351;&#29992;&#21487;&#21464;&#38271;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#38024;&#23545;&#25105;&#20204;&#30340;&#25554;&#20837;&#26367;&#20195;&#22120;&#23545;Transformer&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20174;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#36716;&#21521;&#22522;&#20110;&#21382;&#21490;&#30340;&#25552;&#20132;&#20449;&#24687;&#34917;&#20840;&#30340;&#26032;&#24605;&#36335;&#65292;&#20351;&#29992;&#20808;&#21069;&#30340;&#25552;&#20132;&#21382;&#21490;&#20316;&#20026;&#39069;&#22806;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#21644;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.07655</link><description>&lt;p&gt;
&#20174;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#21040;&#22522;&#20110;&#21382;&#21490;&#30340;&#25552;&#20132;&#20449;&#24687;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
From Commit Message Generation to History-Aware Commit Message Completion. (arXiv:2308.07655v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20174;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#36716;&#21521;&#22522;&#20110;&#21382;&#21490;&#30340;&#25552;&#20132;&#20449;&#24687;&#34917;&#20840;&#30340;&#26032;&#24605;&#36335;&#65292;&#20351;&#29992;&#20808;&#21069;&#30340;&#25552;&#20132;&#21382;&#21490;&#20316;&#20026;&#39069;&#22806;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#21644;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20132;&#20449;&#24687;&#23545;&#36719;&#20214;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36319;&#36394;&#25913;&#21160;&#24182;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#25552;&#20132;&#20449;&#24687;&#32570;&#20047;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#20026;&#32534;&#20889;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#20449;&#24687;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#30446;&#21069;&#20851;&#20110;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#30340;&#30740;&#31350;&#23578;&#26410;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20174;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#36716;&#21521;&#25552;&#20132;&#20449;&#24687;&#34917;&#20840;&#65292;&#24182;&#20351;&#29992;&#20808;&#21069;&#30340;&#25552;&#20132;&#21382;&#21490;&#20316;&#20026;&#39069;&#22806;&#19978;&#19979;&#25991;&#30340;&#26032;&#24605;&#36335;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;&#25552;&#20132;&#20449;&#24687;&#30340;&#36136;&#37327;&#21644;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#20849;&#20139;&#20102;&#19968;&#20010;&#21517;&#20026;CommitChronicle&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;1070&#19975;&#20010;&#25552;&#20132;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#34917;&#20840;&#35774;&#32622;&#21644;&#21382;&#21490;&#19978;&#19979;&#25991;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25552;&#20132;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#21644;GPT-3&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.  In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD) &#30340;&#26032;&#39062;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#36890;&#36807;&#38480;&#21046;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#30697;&#38453;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#23427;&#22312;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07641</link><description>&lt;p&gt;
&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299;&#20316;&#20026;&#32447;&#24615;&#26144;&#23556;&#20013;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping. (arXiv:2308.07641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD) &#30340;&#26032;&#39062;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#36890;&#36807;&#38480;&#21046;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#30697;&#38453;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#23427;&#22312;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD)&#65292;&#20197;&#23454;&#29616;&#21331;&#36234;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#19981;&#21516;&#65292;TSVD&#23558;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;$U$&#21644;$V$&#30697;&#38453;&#38480;&#21046;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#21363;$\{ \pm 1, 0 \}$&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35745;&#31639;$U(\cdot)$&#21644;$V(\cdot)$&#26102;&#65292;TSVD&#21482;&#38656;&#35201;&#21152;&#27861;&#25351;&#20196;&#65292;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20056;&#27861;&#25351;&#20196;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30452;&#25509;&#36716;&#25442;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#28193;&#31639;&#27861;&#65292;&#22914;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30452;&#25509;&#36716;&#25442;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#65292;&#21253;&#25324;&#24403;&#21069;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#22914;ConvNext&#12289;Swim&#12289;BERT&#20197;&#21450;&#31867;&#20284;OPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).  Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.  We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.  In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;backPropagation pAth Search (PAS)&#26041;&#27861;&#26469;&#22686;&#24378;&#23545;&#25239;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#35843;&#25972;&#21367;&#31215;&#27169;&#22359;&#30340;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#21644;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#25628;&#32034;&#31354;&#38388;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#32467;&#26500;&#25915;&#20987;&#32773;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07625</link><description>&lt;p&gt;
&#36816;&#29992;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#25628;&#32034;&#22686;&#24378;&#23545;&#25239;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Backpropagation Path Search On Adversarial Transferability. (arXiv:2308.07625v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;backPropagation pAth Search (PAS)&#26041;&#27861;&#26469;&#22686;&#24378;&#23545;&#25239;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#35843;&#25972;&#21367;&#31215;&#27169;&#22359;&#30340;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#21644;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#25628;&#32034;&#31354;&#38388;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#32467;&#26500;&#25915;&#20987;&#32773;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#22240;&#27492;&#22312;&#37096;&#32626;&#20043;&#21069;&#27979;&#35797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#32773;&#20250;&#38024;&#23545;&#26367;&#20195;&#27169;&#22411;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#40657;&#30418;&#24773;&#20917;&#19979;&#37096;&#32626;&#30340;&#21463;&#23475;&#32773;&#27169;&#22411;&#19978;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#25239;&#36801;&#31227;&#24615;&#65292;&#22522;&#20110;&#32467;&#26500;&#30340;&#25915;&#20987;&#32773;&#35843;&#25972;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#20197;&#36991;&#20813;&#25915;&#20987;&#36807;&#24230;&#25311;&#21512;&#26367;&#20195;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#25915;&#20987;&#32773;&#26410;&#33021;&#25506;&#32034;&#21367;&#31215;&#27169;&#22359;&#65292;&#24182;&#19988;&#21551;&#21457;&#24335;&#22320;&#20462;&#25913;&#21453;&#21521;&#20256;&#25773;&#22270;&#65292;&#23548;&#33268;&#25928;&#26524;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;backPropagation pAth Search (PAS)&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#25552;&#20986;&#20102;SkipConv&#26469;&#35843;&#25972;&#21367;&#31215;&#30340;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#12290;&#20026;&#20102;&#20811;&#26381;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#21453;&#21521;&#20256;&#25773;&#36335;&#24452;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#19968;&#27493;&#36817;&#20284;&#26469;&#23454;&#29616;&#36335;&#24452;&#30340;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model's robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35780;&#20272;&#20013;&#22269;&#20303;&#23429;&#23567;&#21306;&#30340;&#38451;&#20809;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#38454;&#27573;&#39044;&#27979;&#65292;&#36755;&#20986;&#36755;&#20837;&#31435;&#26041;&#20307;&#24418;&#29366;&#24314;&#31569;&#23548;&#33268;&#30340;&#36974;&#25377;&#26102;&#38388;&#38388;&#38548;&#65292;&#20174;&#32780;&#24471;&#21040;&#24314;&#31569;&#30340;&#38451;&#20809;&#23567;&#26102;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#36719;&#20214;&#26356;&#24555;&#36895;&#65292;&#21487;&#20197;&#22312;&#27010;&#24565;&#35774;&#35745;&#38454;&#27573;&#20351;&#29992;&#65292;&#26377;&#21161;&#20110;&#20248;&#21270;&#24314;&#31569;&#38451;&#20809;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24555;&#36895;&#38451;&#20809;&#35780;&#20272;&#27861;&#29992;&#20110;&#20013;&#22269;&#20303;&#23429;&#23567;&#21306;&#30340;&#27010;&#24565;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy. (arXiv:2308.07616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35780;&#20272;&#20013;&#22269;&#20303;&#23429;&#23567;&#21306;&#30340;&#38451;&#20809;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#38454;&#27573;&#39044;&#27979;&#65292;&#36755;&#20986;&#36755;&#20837;&#31435;&#26041;&#20307;&#24418;&#29366;&#24314;&#31569;&#23548;&#33268;&#30340;&#36974;&#25377;&#26102;&#38388;&#38388;&#38548;&#65292;&#20174;&#32780;&#24471;&#21040;&#24314;&#31569;&#30340;&#38451;&#20809;&#23567;&#26102;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#36719;&#20214;&#26356;&#24555;&#36895;&#65292;&#21487;&#20197;&#22312;&#27010;&#24565;&#35774;&#35745;&#38454;&#27573;&#20351;&#29992;&#65292;&#26377;&#21161;&#20110;&#20248;&#21270;&#24314;&#31569;&#38451;&#20809;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#22269;&#30340;&#24314;&#31569;&#35268;&#33539;&#20013;&#65292;&#35201;&#27714;&#20303;&#23429;&#24314;&#31569;&#22312;&#25351;&#23450;&#30340;&#20908;&#23395;&#26085;&#23376;&#19978;&#25509;&#25910;&#21040;&#26368;&#20302;&#23567;&#26102;&#25968;&#30340;&#33258;&#28982;&#30452;&#23556;&#38451;&#20809;&#65292;&#36825;&#20195;&#34920;&#20102;&#19968;&#24180;&#20013;&#26368;&#24046;&#30340;&#38451;&#20809;&#26465;&#20214;&#12290;&#36825;&#19968;&#35201;&#27714;&#26159;&#22312;&#20303;&#23429;&#39033;&#30446;&#30340;&#27010;&#24565;&#35774;&#35745;&#38454;&#27573;&#33719;&#21462;&#24314;&#31569;&#35768;&#21487;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20351;&#29992;&#23448;&#26041;&#35748;&#21487;&#30340;&#36719;&#20214;&#26469;&#35780;&#20272;&#24314;&#31569;&#30340;&#38451;&#20809;&#24615;&#33021;&#12290;&#36825;&#20123;&#36719;&#20214;&#26681;&#25454;&#37325;&#22797;&#30340;&#36974;&#25377;&#35745;&#31639;&#26469;&#39044;&#27979;&#38451;&#20809;&#23567;&#26102;&#25968;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#21363;&#19968;&#38454;&#27573;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#30001;&#36755;&#20837;&#30340;&#38271;&#26041;&#20307;&#24418;&#24335;&#24314;&#31569;&#25152;&#24341;&#36215;&#30340;&#36974;&#25377;&#26102;&#38388;&#38388;&#38548;&#12290;&#36890;&#36807;&#35745;&#31639;&#25152;&#26377;&#24314;&#31569;&#30340;&#38451;&#20809;&#26102;&#38388;&#38388;&#38548;&#30340;&#24182;&#38598;&#65288;&#36974;&#25377;&#26102;&#38388;&#38388;&#38548;&#30340;&#34917;&#38598;&#65289;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#22320;&#22359;&#30340;&#38451;&#20809;&#23567;&#26102;&#25968;&#12290;&#36827;&#34892;&#20102;&#19977;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#21363;&#27700;&#24179;&#27700;&#24179;&#21644;&#22369;&#24230;&#20998;&#26512;&#20197;&#21450;&#22522;&#20110;&#27169;&#25311;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Chinese building codes, it is required that residential buildings receive a minimum number of hours of natural, direct sunlight on a specified winter day, which represents the worst sunlight condition in a year. This requirement is a prerequisite for obtaining a building permit during the conceptual design of a residential project. Thus, officially sanctioned software is usually used to assess the sunlight performance of buildings. These software programs predict sunlight hours based on repeated shading calculations, which is time-consuming. This paper proposed a multilayer perceptron-based method, a one-stage prediction approach, which outputs a shading time interval caused by the inputted cuboid-form building. The sunlight hours of a site can be obtained by calculating the union of the sunlight time intervals (complement of shading time interval) of all the buildings. Three numerical experiments, i.e., horizontal level and slope analysis, and simulation-based optimization are carr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#31995;&#22806;&#34892;&#26143;&#20013;&#23547;&#25214;&#26032;&#30340;&#21270;&#23398;&#29289;&#36136;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#20809;&#35889;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#29992;ROC&#26354;&#32447;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07604</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#31995;&#22806;&#34892;&#26143;&#22823;&#27668;&#20013;&#23547;&#25214;&#26032;&#30340;&#21270;&#23398;&#29289;&#36136;
&lt;/p&gt;
&lt;p&gt;
Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection. (arXiv:2308.07604v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#31995;&#22806;&#34892;&#26143;&#20013;&#23547;&#25214;&#26032;&#30340;&#21270;&#23398;&#29289;&#36136;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#20809;&#35889;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#29992;ROC&#26354;&#32447;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#26395;&#36828;&#38236;&#23558;&#25552;&#20379;&#22823;&#37327;&#39640;&#20998;&#36776;&#29575;&#20809;&#35889;&#25968;&#25454;&#65292;&#29992;&#20110;&#20998;&#26512;&#25968;&#21315;&#20010;&#31995;&#22806;&#34892;&#26143;&#12290;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#21644;&#38656;&#35201;&#20998;&#26512;&#30340;&#34892;&#26143;&#25968;&#37327;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#24320;&#21457;&#26032;&#30340;&#12289;&#24555;&#36895;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26631;&#35760;&#26377;&#36259;&#30340;&#34892;&#26143;&#20197;&#20379;&#37325;&#26032;&#35266;&#27979;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20513;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#31995;&#22806;&#34892;&#26143;&#20940;&#26143;&#20809;&#35889;&#30340;&#24322;&#24120;&#65288;&#26032;&#39062;&#24615;&#65289;&#26816;&#27979;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;&#20855;&#26377;&#24322;&#24120;&#21270;&#23398;&#25104;&#20998;&#29978;&#33267;&#23547;&#25214;&#26410;&#30693;&#30340;&#29983;&#29289;&#26631;&#35760;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23637;&#31034;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#21644;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#22312;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#24211;&#30340;&#21512;&#25104;&#20809;&#35889;&#19978;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#27979;&#35797;&#24773;&#20917;&#65292;&#27599;&#31181;&#24773;&#20917;&#37117;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#20202;&#22120;&#22122;&#22768;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;ROC&#26354;&#32447;&#26469;&#37327;&#21270;&#21644;&#27604;&#36739;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The next generation of telescopes will yield a substantial increase in the availability of high-resolution spectroscopic data for thousands of exoplanets. The sheer volume of data and number of planets to be analyzed greatly motivate the development of new, fast and efficient methods for flagging interesting planets for reobservation and detailed analysis. We advocate the application of machine learning (ML) techniques for anomaly (novelty) detection to exoplanet transit spectra, with the goal of identifying planets with unusual chemical composition and even searching for unknown biosignatures. We successfully demonstrate the feasibility of two popular anomaly detection methods (Local Outlier Factor and One Class Support Vector Machine) on a large public database of synthetic spectra. We consider several test cases, each with different levels of instrumental noise. In each case, we use ROC curves to quantify and compare the performance of the two ML techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;&#28216;&#25103;&#35282;&#33394;&#31574;&#30053;&#65292;&#20197;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#23545;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#36827;&#34892;&#21152;&#26435;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.07598</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#28216;&#25103;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Generating Personas for Games with Multimodal Adversarial Imitation Learning. (arXiv:2308.07598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;&#28216;&#25103;&#35282;&#33394;&#31574;&#30053;&#65292;&#20197;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#23545;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#36827;&#34892;&#21152;&#26435;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#30340;&#29609;&#23478;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#24448;&#24448;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#20026;&#20102;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#65292;&#36229;&#36234;&#24378;&#21270;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#29992;&#22870;&#21169;&#20989;&#25968;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#20010;&#35282;&#33394;&#31574;&#30053;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#22810;&#27169;&#24577;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;MultiGAIL&#65289;&#20351;&#29992;&#36741;&#21161;&#36755;&#20837;&#21442;&#25968;&#65292;&#20351;&#29992;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;MultiGAIL&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#27604;&#36739;&#26234;&#33021;&#20307;&#21644;&#19981;&#21516;&#30340;&#19987;&#23478;&#31574;&#30053;&#26469;&#25512;&#26029;&#29615;&#22659;&#22870;&#21169;&#12290;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#36827;&#34892;&#21152;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#32447;&#21040;&#25209;&#27425;&#36716;&#25442;&#21644;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#26657;&#27491;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#33719;&#24471;&#20960;&#20046;&#26368;&#20248;&#30340;&#39640;&#27010;&#29575;&#39118;&#38505;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.07588</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#39118;&#38505;&#19978;&#30028;&#36890;&#36807;&#39034;&#24207;&#39044;&#27979;&#22120;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
High-Probability Risk Bounds via Sequential Predictors. (arXiv:2308.07588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07588
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#21040;&#25209;&#27425;&#36716;&#25442;&#21644;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#38454;&#26657;&#27491;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#33719;&#24471;&#20960;&#20046;&#26368;&#20248;&#30340;&#39640;&#27010;&#29575;&#39118;&#38505;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#65292;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#39034;&#24207;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#20026;&#32479;&#35745;&#23398;&#20064;&#25552;&#20379;&#26399;&#26395;&#39118;&#38505;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#32447;&#20445;&#35777;&#30456;&#23545;&#20110;&#32479;&#35745;&#20445;&#35777;&#26126;&#26174;&#26377;&#20248;&#21183;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#19978;&#30028;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#32479;&#35745;&#35774;&#32622;&#20013;&#30340;&#32039;&#23494;&#39640;&#27010;&#29575;&#39118;&#38505;&#19978;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24212;&#29992;&#20110;&#19968;&#33324;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#22312;&#32447;&#21040;&#25209;&#27425;&#36716;&#25442;&#21487;&#20197;&#32469;&#36807;&#27492;&#38480;&#21046;&#12290;&#36890;&#36807;&#23545;&#23450;&#20041;&#36951;&#25022;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#19968;&#33324;&#30340;&#20108;&#38454;&#26657;&#27491;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#31181;&#32463;&#20856;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#30340;&#20960;&#20046;&#26368;&#20248;&#39640;&#27010;&#29575;&#39118;&#38505;&#19978;&#30028;&#65292;&#20363;&#22914;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#65292;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#21644;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#35768;&#22810;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#36866;&#24403;&#30340;&#20107;&#23454;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#21463;&#38480;&#20110;&#20351;&#29992;&#32473;&#23450;&#21442;&#32771;&#31867;&#21035;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07575</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#38590;&#28857;&#22312;&#20110;&#19981;&#20165;&#38656;&#35201;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21576;&#29616;&#35270;&#35273;&#32454;&#33410;&#65292;&#36824;&#38656;&#35201;&#23545;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20026;&#27599;&#20010;&#21477;&#23376;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20294;&#22312;&#32473;&#23450;&#27573;&#33853;&#20013;&#32534;&#30721;&#19978;&#19979;&#25991;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#35828;&#26381;&#21147;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#35282;&#33394;&#25110;&#36866;&#24403;&#30340;&#22330;&#26223;&#32972;&#26223;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#65292;&#29992;&#20110;&#21452;&#21521;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#29983;&#25104;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25512;&#29702;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;Pororo-SV&#21644;Flintstones-SV&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;FID&#12289;&#23383;&#31526;...
&lt;/p&gt;
&lt;p&gt;
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.07573</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23545;&#28151;&#21512;&#22270;&#20687;-&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks. (arXiv:2308.07573v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#22270;&#20687;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#21307;&#23398;&#35760;&#24405;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#36827;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;GAN&#65288;&#945;GAN&#65289;&#21644;&#26465;&#20214;&#34920;&#26684;GAN&#65288;CTGAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#28151;&#21512;&#21307;&#23398;&#35760;&#24405;&#65292;&#20854;&#20013;&#21253;&#25324;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65288;CXRs&#65289;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#65288;&#21253;&#25324;&#20154;&#20307;&#27979;&#37327;&#25968;&#25454;&#21644;&#23454;&#39564;&#23460;&#27979;&#35797;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#65288;pDB&#65289;&#19978;&#35757;&#32451;&#19968;&#20010;&#945;GAN&#27169;&#22411;&#65292;&#20197;&#38477;&#20302;CXRs&#30340;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;GAN&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#24211;&#65288;oDB&#65289;&#20013;&#30340;&#22270;&#20687;&#65292;&#20197;&#33719;&#24471;&#28508;&#22312;&#21521;&#37327;&#12290;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#19982;oDB&#20013;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#32852;&#21512;&#25968;&#25454;&#26469;&#35757;&#32451;CTGAN&#27169;&#22411;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#22810;&#26679;&#21270;&#30340;&#28151;&#21512;CXRs&#21644;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#35760;&#24405;&#65292;&#24182;&#20445;&#25345;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this
&lt;/p&gt;</description></item><item><title>Ske2Grid&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20041;&#35268;&#21017;&#21367;&#31215;&#25805;&#20316;&#21644;&#26032;&#39062;&#35774;&#35745;&#30340;&#32593;&#26684;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07571</link><description>&lt;p&gt;
Ske2Grid&#65306;&#39592;&#26550;&#21040;&#32593;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition. (arXiv:2308.07571v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07571
&lt;/p&gt;
&lt;p&gt;
Ske2Grid&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20041;&#35268;&#21017;&#21367;&#31215;&#25805;&#20316;&#21644;&#26032;&#39062;&#35774;&#35745;&#30340;&#32593;&#26684;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ske2Grid&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;Ske2Grid&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35268;&#21017;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#29992;&#20110;&#20154;&#20307;&#39592;&#26550;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#26684;&#34920;&#31034;&#65292;&#23427;&#26159;&#36890;&#36807;&#19977;&#20010;&#26032;&#39062;&#35774;&#35745;&#26500;&#24314;&#21644;&#23398;&#20064;&#30340;&#32039;&#20945;&#30340;&#31867;&#20284;&#22270;&#20687;&#30340;&#32593;&#26684;&#22270;&#20687;&#22359;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#33410;&#28857;&#32034;&#24341;&#21464;&#25442;&#65288;GIT&#65289;&#65292;&#36890;&#36807;&#19968;&#19968;&#23558;&#39592;&#26550;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#37197;&#21040;&#25152;&#38656;&#30340;&#32593;&#26684;&#21333;&#20803;&#65292;&#26500;&#24314;&#19968;&#20010;&#35268;&#21017;&#30340;&#32593;&#26684;&#22270;&#20687;&#22359;&#12290;&#20026;&#20102;&#30830;&#20445;GIT&#26159;&#19968;&#20010;&#21452;&#23556;&#65292;&#24182;&#20016;&#23500;&#32593;&#26684;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#19978;&#37319;&#26679;&#21464;&#25442;&#65288;UPT&#65289;&#65292;&#20197;&#25554;&#20540;&#22635;&#20805;&#32593;&#26684;&#22270;&#20687;&#22359;&#30340;&#39592;&#26550;&#22270;&#33410;&#28857;&#65292;&#20351;&#20854;&#36798;&#21040;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21333;&#27493;UPT&#36807;&#20110;&#28608;&#36827;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#25496;&#38543;&#30528;&#31354;&#38388;&#23610;&#23544;&#22686;&#21152;&#30340;&#32593;&#26684;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#23398;&#20064;&#31574;&#30053;&#65288;PLS&#65289;&#65292;&#23558;UPT&#20998;&#35299;&#20026;&#22810;&#20010;&#27493;&#39588;&#24182;&#23558;&#20854;&#19982;&#22810;&#20010;&#37197;&#23545;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22810;&#37325;&#25554;&#34917;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#38750;&#38543;&#26426;&#32570;&#22833;&#26631;&#31614;&#38382;&#39064;&#30340;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#26356;&#23567;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.07562</link><description>&lt;p&gt;
&#38750;&#38543;&#26426;&#32570;&#22833;&#26631;&#31614;&#30340;&#22810;&#37325;&#25554;&#34917;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels. (arXiv:2308.07562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22810;&#37325;&#25554;&#34917;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#38750;&#38543;&#26426;&#32570;&#22833;&#26631;&#31614;&#38382;&#39064;&#30340;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#26356;&#23567;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#22312;&#35757;&#32451;&#31639;&#27861;&#26102;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#24120;&#35265;&#24212;&#29992;&#65292;&#22240;&#20026;&#33719;&#24471;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35299;&#20915;&#20102;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#38543;&#26426;&#32570;&#22833;&#12289;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#21644;&#38750;&#38543;&#26426;&#32570;&#22833;&#12290;&#38750;&#38543;&#26426;&#32570;&#22833;&#38382;&#39064;&#26159;&#19977;&#32773;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#19981;&#33021;&#23433;&#20840;&#22320;&#20551;&#35774;&#25152;&#26377;&#31867;&#21035;&#20998;&#24067;&#30456;&#31561;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31867;&#24863;&#30693;&#25554;&#34917;&#21644;&#31867;&#20284;&#27010;&#29575;&#65292;&#22823;&#22810;&#24573;&#35270;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#38750;&#38543;&#26426;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22810;&#37325;&#25554;&#34917;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#23567;&#30340;&#20559;&#24046;&#12290;1&#65289;&#25105;&#20204;&#20351;&#29992;&#22810;&#37325;&#25554;&#34917;&#27169;&#22411;&#65292;&#21019;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24212;&#29992;&#38408;&#20540;&#26469;&#24573;&#30053;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#20266;&#26631;&#31614;&#12290;2&#65289;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#21435;&#20559;&#25554;&#34917;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL-DI&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#36807;&#28388;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#24182;&#25214;&#21040;&#19968;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#26469;&#20943;&#23567;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#19981;&#22914;&#35199;&#29677;&#29273;&#25991;&#27169;&#22411;&#22312;&#21516;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.07556</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A User-Centered Evaluation of Spanish Text Simplification. (arXiv:2308.07556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#30340;&#29992;&#25143;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#19981;&#22914;&#35199;&#29677;&#29273;&#25991;&#27169;&#22411;&#22312;&#21516;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#35821;&#26009;&#24211;&#65292;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#21644;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#35789;&#35821;&#30340;&#35782;&#21035;&#65292;&#23545;&#35199;&#29677;&#29273;&#25991;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#29983;&#20135;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#26368;&#27969;&#34892;&#30340;&#35199;&#29677;&#29273;&#29305;&#23450;&#21487;&#35835;&#24615;&#35780;&#20998;&#19982;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21518;&#32773;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#26041;&#38754;&#22987;&#32456;&#26356;&#22909;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#22914;&#20165;&#38480;&#20110;&#35199;&#29677;&#29273;&#25991;&#30340;&#27169;&#22411;&#65292;&#20294;&#25152;&#26377;&#27169;&#22411;&#36807;&#20110;&#39057;&#32321;&#22320;&#20851;&#27880;&#32479;&#35745;&#29305;&#24449;&#65292;&#22914;&#21477;&#23376;&#38271;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#35780;&#20272;&#20013;&#30340;&#35821;&#26009;&#24211;&#21457;&#24067;&#32473;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#65292;&#24076;&#26395;&#25512;&#21160;&#35199;&#29677;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28857;&#23545;&#28857;&#35748;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#33021;&#22815;&#30830;&#20445;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.07553</link><description>&lt;p&gt;
&#21152;&#24378;&#35299;&#33647;&#65306;&#38024;&#23545;&#20013;&#27602;&#25915;&#20987;&#30340;&#25913;&#36827;&#28857;&#23545;&#28857;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (arXiv:2308.07553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28857;&#23545;&#28857;&#35748;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#33021;&#22815;&#30830;&#20445;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#27602;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#23545;&#35757;&#32451;&#35821;&#26009;&#36827;&#34892;&#24494;&#23567;&#25913;&#21160;&#26469;&#23545;&#27169;&#22411;&#34892;&#20026;&#20135;&#29983;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23384;&#22312;&#23545;&#29305;&#23450;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#20445;&#35777;&#65292;&#21487;&#33021;&#34987;&#26032;&#22411;&#25915;&#20987;&#25152;&#23545;&#25239;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#32771;&#23519;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#35748;&#35777;&#38450;&#24481;&#21487;&#20197;&#25552;&#20379;&#38024;&#23545;&#26377;&#38480;&#25968;&#37327;&#35757;&#32451;&#26679;&#26412;&#34987;&#25932;&#23545;&#25915;&#20987;&#20462;&#25913;&#30340;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#31216;&#20026;&#28857;&#23545;&#28857;&#35748;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#21644;&#37319;&#26679;&#39640;&#26031;&#26426;&#21046;&#65292;&#30830;&#20445;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#23545;&#26377;&#38480;&#25968;&#37327;&#20013;&#27602;&#26679;&#26412;&#30340;&#39044;&#27979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20445;&#35777;&#27604;&#20043;&#21069;&#30340;&#35748;&#35777;&#26041;&#27861;&#25552;&#20379;&#30340;&#20445;&#35777;&#26356;&#22823;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#22312;&#21482;&#26377;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07538</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#29109;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30340;&#30495;&#20266;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation via Minimax Entropy for Real/Bogus Classification of Astronomical Alerts. (arXiv:2308.07538v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#22312;&#21482;&#26377;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22495;&#22825;&#25991;&#23398;&#27491;&#26397;&#30528;&#23454;&#26102;&#20998;&#26512;&#22810;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#20419;&#20351;&#22810;&#27969;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;HiTS&#12289;DES&#12289;ATLAS&#21644;ZTF&#65289;&#30740;&#31350;&#20102;&#22825;&#25991;&#35686;&#25253;&#30340;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#25913;&#36827;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#28304;&#30446;&#26631;&#24773;&#26223;&#19979;&#30340;&#24179;&#34913;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#20174;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#21482;&#26377;&#19968;&#20010;&#26631;&#35760;&#30340;&#39033;&#65292;&#32780;&#19988;MME&#27169;&#22411;&#19981;&#20250;&#25439;&#23475;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38543;&#26426;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25237;&#24433;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#21106;&#24179;&#38754;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;&#22312;&#19978;&#23618;&#20026;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;$\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\})$&#20010;&#26597;&#35810;&#20013;&#33719;&#24471;$\epsilon_f$-&#26368;&#20248;&#30340;&#19978;&#23618;&#35299;&#21644;$\epsilon_g$-&#26368;&#20248;&#30340;&#19979;&#23618;&#35299;&#65292;&#36825;&#19968;&#20445;&#35777;&#25913;&#36827;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07536</link><description>&lt;p&gt;
&#26080;&#25237;&#24433;&#26041;&#27861;&#27714;&#35299;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#38543;&#26426;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem. (arXiv:2308.07536v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38543;&#26426;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25237;&#24433;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#21106;&#24179;&#38754;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;&#22312;&#19978;&#23618;&#20026;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;$\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\})$&#20010;&#26597;&#35810;&#20013;&#33719;&#24471;$\epsilon_f$-&#26368;&#20248;&#30340;&#19978;&#23618;&#35299;&#21644;$\epsilon_g$-&#26368;&#20248;&#30340;&#19979;&#23618;&#35299;&#65292;&#36825;&#19968;&#20445;&#35777;&#25913;&#36827;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;&#38543;&#26426;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#65292;&#22312;&#36825;&#31867;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26368;&#23567;&#21270;&#21478;&#19968;&#20010;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#38598;&#19978;&#30340;&#19968;&#20010;&#20809;&#28369;&#38543;&#26426;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#21106;&#24179;&#38754;&#23616;&#37096;&#36817;&#20284;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#24182;&#20351;&#29992;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#26465;&#20214;&#26799;&#24230;&#26356;&#26032;&#20197;&#25511;&#21046;&#30001;&#20110;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#32780;&#24341;&#20837;&#30340;&#35823;&#24046;&#12290;&#24403;&#19978;&#23618;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;$\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\})$&#20010;&#38543;&#26426;&#39044;&#35328;&#26426;&#26597;&#35810;&#25165;&#33021;&#33719;&#24471;&#19978;&#23618;&#20026;$\epsilon_f$&#26368;&#20248;&#65292;&#19979;&#23618;&#20026;$\epsilon_g$&#26368;&#20248;&#30340;&#35299;&#12290;&#36825;&#20010;&#20445;&#35777;&#25913;&#36827;&#20102;&#20808;&#21069;&#26368;&#22909;&#24050;&#30693;&#22797;&#26434;&#24615;$\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19978;&#23618;&#20026;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#35770;&#25991;&#20855;&#26377;&#22810;&#20010;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a class of stochastic bilevel optimization problems, also known as stochastic simple bilevel optimization, where we minimize a smooth stochastic objective function over the optimal solution set of another stochastic convex optimization problem. We introduce novel stochastic bilevel optimization methods that locally approximate the solution set of the lower-level problem via a stochastic cutting plane, and then run a conditional gradient update with variance reduction techniques to control the error induced by using stochastic gradients. For the case that the upper-level function is convex, our method requires $\tilde{\mathcal{O}}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ stochastic oracle queries to obtain a solution that is $\epsilon_f$-optimal for the upper-level and $\epsilon_g$-optimal for the lower-level. This guarantee improves the previous best-known complexity of $\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$. Moreover, for the case that the
&lt;/p&gt;</description></item><item><title>FeatGeNN&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27719;&#38598;&#26041;&#27861;&#25552;&#21462;&#21644;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;&#21367;&#31215;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07527</link><description>&lt;p&gt;
FeatGeNN: &#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25913;&#36827;&#34920;&#26684;&#25968;&#25454;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction. (arXiv:2308.07527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07527
&lt;/p&gt;
&lt;p&gt;
FeatGeNN&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27719;&#38598;&#26041;&#27861;&#25552;&#21462;&#21644;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;&#21367;&#31215;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;AutoFE&#65289;&#24050;&#25104;&#20026;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#20026;&#32479;&#35745;&#20998;&#26512;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#35201;&#20040;&#20381;&#38752;&#25163;&#21160;&#29305;&#24449;&#21019;&#24314;&#65292;&#35201;&#20040;&#20351;&#29992;&#21487;&#33021;&#29983;&#25104;&#22823;&#37327;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36825;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#24182;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#26041;&#27861;&#65292;&#31216;&#20026;FeatGeNN&#65292;&#23427;&#20351;&#29992;&#30456;&#20851;&#24615;&#20316;&#20026;&#27719;&#38598;&#20989;&#25968;&#26469;&#25552;&#21462;&#21644;&#21019;&#24314;&#26032;&#29305;&#24449;&#12290;&#19982;&#20256;&#32479;&#30340;&#26368;&#22823;&#27719;&#38598;&#65288;max-pooling&#65289;&#31561;&#27719;&#38598;&#20989;&#25968;&#19981;&#21516;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27719;&#38598;&#32771;&#34385;&#20102;&#25968;&#25454;&#30697;&#38453;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#20043;&#26356;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;FeatGeNN&#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27719;&#38598;&#21487;&#20197;&#25104;&#20026;&#26368;&#22823;&#27719;&#38598;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Feature Engineering (AutoFE) has become an important task for any machine learning project, as it can help improve model performance and gain more information for statistical analysis. However, most current approaches for AutoFE rely on manual feature creation or use methods that can generate a large number of features, which can be computationally intensive and lead to overfitting. To address these challenges, we propose a novel convolutional method called FeatGeNN that extracts and creates new features using correlation as a pooling function. Unlike traditional pooling functions like max-pooling, correlation-based pooling considers the linear relationship between the features in the data matrix, making it more suitable for tabular data. We evaluate our method on various benchmark datasets and demonstrate that FeatGeNN outperforms existing AutoFE approaches regarding model performance. Our results suggest that correlation-based pooling can be a promising alternative to max-p
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.07523</link><description>&lt;p&gt;
&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System. (arXiv:2308.07523v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07523
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#26680;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#24037;&#31243;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#20013;&#24341;&#20837;&#20102;&#28145;&#23618;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;DeepONet&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#12290;&#38543;&#30528;&#26680;&#33021;&#20316;&#20026;&#19968;&#31181;&#30899;&#20013;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#37319;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#26680;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#36816;&#33829;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;DeepONet&#20855;&#26377;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;DeepONet&#22312;&#35299;&#20915;&#22797;&#26434;&#31890;&#23376;&#20256;&#36755;&#38382;&#39064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#24182;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#25805;&#20316;&#31526;G&#65292;DeepONet&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;DeepONet&#30340;&#24212;&#29992;&#20063;&#25581;&#31034;&#20102;&#19982;&#26368;&#20339;&#20256;&#24863;&#22120;&#25918;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07520</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25214;&#21040;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#12290;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#24050;&#32463;&#34987;&#27979;&#37327;&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#20010;&#21464;&#37327;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#26377;&#26102;&#20027;&#35201;&#20851;&#27880;&#30340;&#21464;&#37327;&#24182;&#38750;&#30452;&#25509;&#21487;&#35266;&#23519;&#65292;&#32780;&#26159;&#36890;&#36807;&#23427;&#20204;&#22312;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#26469;&#25512;&#29702;&#20986;&#26469;&#30340;&#12290;&#36825;&#20123;&#34987;&#31216;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19968;&#20010;&#24191;&#27867;&#34987;&#30693;&#36947;&#30340;&#20363;&#23376;&#26159;&#24515;&#29702;&#26500;&#36896;&#30340;&#26234;&#21830;&#65292;&#22240;&#20026;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#65292;&#25152;&#20197;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#22914;&#26234;&#21830;&#27979;&#35797;&#26469;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#21644;&#28508;&#22312;&#21464;&#37327;&#19982;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#36830;&#25509;&#65292;&#20174;&#32780;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#32479;&#35745;&#19968;&#33268;&#24615;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#24178;&#22122;&#27604;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.07511</link><description>&lt;p&gt;
&#23558;&#36164;&#28304;&#31649;&#29702;&#31639;&#27861;&#30340;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#35757;&#32451;&#36741;&#21161;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach. (arXiv:2308.07511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20449;&#24178;&#22122;&#27604;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#22810;&#29992;&#25143;&#29615;&#22659;&#20013;&#65292;&#26377;&#24456;&#22810;&#26041;&#27861;&#33268;&#21147;&#20110;&#20248;&#21270;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#39640;&#22797;&#26434;&#24615;&#20652;&#29983;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#39640;&#24615;&#33021;&#21644;NN&#26041;&#27861;&#30340;&#20302;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#31639;&#27861;&#33976;&#39311;&#65288;AD&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;NN&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;SINR&#20248;&#21270;&#26041;&#27861;&#20316;&#20026;&#8220;&#32769;&#24072;&#8221;&#26469;&#36741;&#21161;&#35757;&#32451;NNs&#65292;&#21363;&#8220;&#23398;&#29983;&#8221;&#65292;&#20174;&#32780;&#22686;&#24378;&#26080;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#27599;&#31181;&#35757;&#32451;&#33539;&#20363;&#20013;&#36935;&#21040;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#33719;&#24471;&#20934;&#30830;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers" to assist the training of NNs, which are ``students", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2308.07505</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31243;&#24207;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#23494;&#38598;&#22411;&#25163;&#21160;&#24037;&#20855;&#30340;&#21019;&#24314;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DRB-ML&#30340;&#19987;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;DataRaceBench&#65292;&#24182;&#20855;&#26377;&#31934;&#32454;&#30340;&#26631;&#31614;&#65292;&#26174;&#31034;&#20102;&#25968;&#25454;&#31454;&#20105;&#23545;&#21450;&#20854;&#30456;&#20851;&#21464;&#37327;&#12289;&#34892;&#21495;&#21644;&#35835;/&#20889;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;DRB-ML&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;LLMs&#24182;&#24494;&#35843;&#20102;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#38656;&#35201;&#26377;&#20851;&#24341;&#36215;&#25968;&#25454;&#31454;&#20105;&#30340;&#21464;&#37327;&#23545;&#30340;&#35814;&#32454;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
&lt;/p&gt;</description></item><item><title>ST-MLP&#26159;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07496</link><description>&lt;p&gt;
ST-MLP&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#32423;&#32852;&#26102;&#31354;&#32447;&#24615;&#26694;&#26550;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting. (arXiv:2308.07496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07496
&lt;/p&gt;
&lt;p&gt;
ST-MLP&#26159;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#20248;&#21270;&#20132;&#36890;&#27969;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#22240;&#20854;&#36866;&#24212;&#36947;&#36335;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#36190;&#35465;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;STGNNs&#26550;&#26500;&#30340;&#30740;&#31350;&#24120;&#24120;&#20248;&#20808;&#32771;&#34385;&#22797;&#26434;&#30340;&#35774;&#35745;&#65292;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#21152;&#37325;&#65292;&#20165;&#22312;&#31934;&#24230;&#19978;&#26377;&#23569;&#35768;&#25552;&#21319;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ST-MLP&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#27905;&#30340;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;STGNNs&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#40723;&#21169;&#20102;&#26377;&#20851;&#20132;&#36890;&#39044;&#27979;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.07491</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23545;&#21333;&#21018;&#20307;&#35282;&#33394;&#30340;&#33258;&#36866;&#24212;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;DeepMimic&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;&#21518;&#32493;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25193;&#23637;&#27169;&#25311;&#21160;&#20316;&#30340;&#33539;&#30068;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#21033;&#29992;&#36136;&#24515;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CDM&#65289;&#23558;&#20840;&#36523;&#35282;&#33394;&#34920;&#31034;&#20026;&#21333;&#21018;&#20307;&#65288;SRB&#65289;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36319;&#36394;&#21442;&#32771;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#26410;&#35266;&#27979;&#29615;&#22659;&#21464;&#21270;&#21644;&#25511;&#21046;&#22120;&#36716;&#25442;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#30001;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38477;&#32500;&#65292;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#32456;&#30340;&#20840;&#36523;&#21160;&#20316;&#20197;&#29289;&#29702;&#21512;&#29702;&#30340;&#26041;&#24335;&#22522;&#20110;&#27169;&#25311;SRB&#35282;&#33394;&#30340;&#29366;&#24577;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#12290;SRB&#20223;&#30495;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#31574;&#30053;&#36755;&#20986;&#19968;&#20010;&#21160;&#20316;&#65292;&#20801;&#35768;&#35282;&#33394;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;O-1&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#35861;&#20551;&#35774;&#26469;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#24182;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;O-1&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;EMBR&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.07486</link><description>&lt;p&gt;
O-1: &#33258;&#21160;&#26631;&#27880;&#19982;1-best&#20551;&#35774;&#30340;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
O-1: Self-training with Oracle and 1-best Hypothesis. (arXiv:2308.07486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;O-1&#65292;&#36890;&#36807;&#22686;&#24378;&#31070;&#35861;&#20551;&#35774;&#26469;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#24182;&#32479;&#19968;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;O-1&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;EMBR&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;O-1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#33258;&#25105;&#35757;&#32451;&#30446;&#26631;&#65292;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#20559;&#24046;&#65292;&#32479;&#19968;&#35821;&#38899;&#35782;&#21035;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;O-1&#26159;&#26399;&#26395;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;EMBR&#65289;&#30340;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#21464;&#20307;&#65292;&#23427;&#22686;&#24378;&#20102;&#31070;&#35861;&#20551;&#35774;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;SpeechStew&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#20869;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#35782;&#21035;&#25928;&#26524;&#30340;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;Speechstew&#19978;&#65292;&#30456;&#23545;&#20110;&#23558;&#23454;&#38469;&#24615;&#33021;&#19982;&#31070;&#35861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;43%&#30340;EMBR&#65292;O-1&#30446;&#26631;&#36890;&#36807;80%&#30340;&#30456;&#23545;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#22312;SpeechStew&#30340;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;O-1&#30456;&#23545;&#20110;EMBR&#23454;&#29616;&#20102;13%&#21040;25%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22312;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;EMBR&#35757;&#32451;&#30340;&#31070;&#35861;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#32553;&#23567;&#20102;12%&#30340;&#24046;&#36317;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;O-1&#30456;&#23545;&#20110;EMBR&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;9%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#20102;&#35813;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#24182;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07480</link><description>&lt;p&gt;
OCDaf: &#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#19982;&#33258;&#22238;&#24402;&#27969;
&lt;/p&gt;
&lt;p&gt;
OCDaf: Ordered Causal Discovery with Autoregressive Flows. (arXiv:2308.07480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07480
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#24182;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#22312;&#22810;&#21464;&#37327;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#22240;&#26524;&#22270;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#26159;&#23545;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#20801;&#35768;&#38750;&#24120;&#25968;&#22122;&#22768;&#26041;&#24046;&#12290;&#20511;&#37492;&#36825;&#20123;&#27169;&#22411;&#19982;&#20223;&#23556;&#33258;&#22238;&#24402;&#24402;&#19968;&#20114;&#34917;&#35268;&#33539;&#27969;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36830;&#32493;&#25628;&#32034;&#31639;&#27861;&#26469;&#23547;&#25214;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Sachs&#21644;SynTReN&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#32467;&#26500;&#27721;&#26126;&#36317;&#31163;&#65288;SHD&#65289;&#21644;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#65288;SID&#65289;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#22312;&#21508;&#31181;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
&lt;/p&gt;</description></item><item><title>Symphony&#26159;&#19968;&#20010;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#65292;&#21487;&#20197;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26381;&#21153;&#65292;&#22312;&#28385;&#36275;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#21644;&#24310;&#36831;SLO&#30340;&#21516;&#26102;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#12290;&#36890;&#36807;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#21644;&#27169;&#22411;&#20998;&#37197;&#31639;&#27861;&#65292;Symphony&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#21151;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#25552;&#20379;&#39640;&#36798;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.07470</link><description>&lt;p&gt;
Symphony: &#20351;&#29992;&#38598;&#20013;&#24335;&#21327;&#35843;&#26469;&#20248;&#21270;&#27169;&#22411;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Symphony: Optimized Model Serving using Centralized Orchestration. (arXiv:2308.07470v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07470
&lt;/p&gt;
&lt;p&gt;
Symphony&#26159;&#19968;&#20010;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#65292;&#21487;&#20197;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26381;&#21153;&#65292;&#22312;&#28385;&#36275;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#21644;&#24310;&#36831;SLO&#30340;&#21516;&#26102;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#12290;&#36890;&#36807;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#21644;&#27169;&#22411;&#20998;&#37197;&#31639;&#27861;&#65292;Symphony&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#21151;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#25552;&#20379;&#39640;&#36798;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPU&#38598;&#32676;&#19978;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#25512;&#29702;&#30340;&#21327;&#35843;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#22312;&#28385;&#36275;&#25209;&#22788;&#29702;&#23646;&#24615;&#30340;&#27169;&#22411;&#25512;&#29702;&#21516;&#26102;&#23454;&#29616;&#39640;&#21152;&#36895;&#22120;&#25928;&#29575;&#65292;&#24182;&#28385;&#36275;&#24310;&#36831;&#26381;&#21153;&#32423;&#21035;&#30446;&#26631;(SLO)&#65292;&#20197;&#21450;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#30340;&#21464;&#21270;&#65292;&#26080;&#35770;&#26159;&#30701;&#26399;&#27874;&#21160;&#36824;&#26159;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Symphony&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#25193;&#23637;&#21040;&#27599;&#31186;&#25968;&#30334;&#19975;&#20010;&#35831;&#27714;&#24182;&#21327;&#35843;&#25968;&#19975;&#20010;GPU&#30340;&#38598;&#20013;&#24335;&#35843;&#24230;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#38750;&#24037;&#20316;&#20445;&#25345;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25209;&#22788;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20063;&#33021;&#23454;&#29616;&#24378;&#22823;&#30340;&#33258;&#21160;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20998;&#37197;&#23376;&#38598;&#32676;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;Symphony&#30340;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#31995;&#32479;&#65292;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;4.7&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.07469</link><description>&lt;p&gt;
Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#35774;&#35745;&#36866;&#24403;&#30340;&#22870;&#21169;&#26426;&#21046;&#23545;&#20854;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30446;&#26631;&#30340;&#22797;&#26434;&#24615;&#36229;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#33021;&#21147;&#33539;&#22260;&#65292;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;&#22870;&#21169;&#26426;&#22120;&#21644;&#969;-&#27491;&#35268;&#35821;&#35328;&#26159;&#29992;&#20110;&#34920;&#31034;&#23450;&#37327;&#21644;&#23450;&#24615;&#30446;&#26631;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#20004;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#26469;&#35745;&#31639;&#38024;&#23545;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;&#25968;&#23383;&#33402;&#26415;&#21490;&#30340;&#23384;&#22312;&#19982;&#21457;&#23637;&#65292;&#24182;&#25351;&#20986;&#22823;&#35268;&#27169;&#35270;&#35273;&#27169;&#22411;&#23545;&#25968;&#23383;&#33402;&#26415;&#21490;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25171;&#24320;&#20102;&#25552;&#21462;&#21644;&#33258;&#21160;&#21270;&#35270;&#35273;&#36923;&#36753;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07464</link><description>&lt;p&gt;
&#25968;&#23383;&#33402;&#26415;&#21490;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
There Is a Digital Art History. (arXiv:2308.07464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;&#25968;&#23383;&#33402;&#26415;&#21490;&#30340;&#23384;&#22312;&#19982;&#21457;&#23637;&#65292;&#24182;&#25351;&#20986;&#22823;&#35268;&#27169;&#35270;&#35273;&#27169;&#22411;&#23545;&#25968;&#23383;&#33402;&#26415;&#21490;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25171;&#24320;&#20102;&#25552;&#21462;&#21644;&#33258;&#21160;&#21270;&#35270;&#35273;&#36923;&#36753;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32422;&#32752;&#23068;&#183;&#24503;&#40065;&#20811;(Johanna Drucker)&#21313;&#24180;&#21069;&#25552;&#20986;&#30340;&#8220;&#26159;&#21542;&#23384;&#22312;&#25968;&#23383;&#33402;&#26415;&#21490;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#20986;&#29616;&#20043;&#21518;&#30340;&#12290;&#34429;&#28982;&#20256;&#32479;&#22411;&#31070;&#32463;&#32593;&#32476;&#26089;&#24050;&#25104;&#20026;&#25968;&#23383;&#33402;&#26415;&#21490;&#30340;&#19968;&#37096;&#20998;&#65292;&#25968;&#23383;&#20154;&#25991;&#39033;&#30446;&#26368;&#36817;&#24320;&#22987;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#35748;&#35782;&#35770;&#24433;&#21709;&#21644;&#26041;&#27861;&#23398;&#21487;&#36127;&#25285;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#21270;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#37325;&#28857;&#25918;&#22312;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#20849;&#21516;&#34920;&#26126;&#20102;&#21521;&#24503;&#40065;&#20811;&#25152;&#35828;&#30340;&#8220;&#25968;&#23383;&#8221;&#33402;&#26415;&#21490;&#30340;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#36716;&#21464;&#30340;&#36235;&#21183;&#12290;&#19968;&#26041;&#38754;&#65292;&#22823;&#35268;&#27169;&#35270;&#35273;&#27169;&#22411;&#20013;&#26032;&#32534;&#30721;&#30340;&#35270;&#35273;&#25991;&#21270;&#20195;&#34920;&#20316;&#21697;&#23545;&#25968;&#23383;&#33402;&#26415;&#21490;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#21253;&#21547;&#22823;&#37327;&#38750;&#25668;&#24433;&#24433;&#20687;&#20351;&#24471;&#21487;&#20197;&#25552;&#21462;&#21644;&#33258;&#21160;&#21270;&#19981;&#21516;&#24418;&#24335;&#30340;&#35270;&#35273;&#36923;&#36753;&#12290;&#22823;&#35268;&#27169;&#35270;&#35273;&#27169;&#22411;&#8220;&#30475;&#21040;&#20102;&#8221;&#36890;&#36807;&#32593;&#32476;&#20013;&#20171;&#30340;&#35199;&#26041;&#35270;&#35273;&#32463;&#20856;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit Johanna Drucker's question, "Is there a digital art history?" -- posed exactly a decade ago -- in the light of the emergence of large-scale, transformer-based vision models. While more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net vi
&lt;/p&gt;</description></item><item><title>GRU-D-Weibull&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#34928;&#20943;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#20013;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#32456;&#28857;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07452</link><description>&lt;p&gt;
GRU-D-Weibull:&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction. (arXiv:2308.07452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07452
&lt;/p&gt;
&lt;p&gt;
GRU-D-Weibull&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#34928;&#20943;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#20013;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#32456;&#28857;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#30340;&#20010;&#20307;&#32423;&#32456;&#28857;&#21644;&#32456;&#28857;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;GRU-D-Weibull&#65292;&#23427;&#23558;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#19982;&#34928;&#20943;&#25216;&#26415;&#65288;GRU-D&#65289;&#32467;&#21512;&#65292;&#26469;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#21644;&#32676;&#20307;&#32423;&#39118;&#38505;&#31649;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;6,879&#21517;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#65288;CKD4&#65289;&#24739;&#32773;&#30340;&#38431;&#21015;&#65292;&#35780;&#20272;&#20102;GRU-D-Weibull&#22312;&#32456;&#28857;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#25351;&#26631;&#26085;&#26399;&#65292;GRU-D-Weibull&#30340;C-&#25351;&#25968;&#32422;&#20026;0.7&#65292;&#38543;&#30528;4.3&#24180;&#30340;&#38543;&#35775;&#65292;C-&#25351;&#25968;&#25552;&#39640;&#21040;&#32422;0.77&#65292;&#31867;&#20284;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CKD4&#25351;&#26631;&#26085;&#26399;&#30340;&#32477;&#23545;L1&#25439;&#22833;&#32422;&#20026;1.1&#24180;&#65288;&#26631;&#20934;&#24046;0.95&#65289;&#65292;&#38543;&#35775;4&#24180;&#21518;&#30340;&#26368;&#23567;L1&#25439;&#22833;&#32422;&#20026;0.45&#24180;&#65288;&#26631;&#20934;&#24046;0.3&#65289;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GRU-D-Weibull&#22312;&#20107;&#20214;&#21457;&#29983;&#26102;&#30340;&#39044;&#27979;&#29983;&#23384;&#27010;&#29575;&#33539;&#22260;&#26356;&#23567;&#19988;&#26356;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07445</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#25551;&#36848;&#20102;&#22312;&#27979;&#35797;&#26102;&#20986;&#29616;&#26410;&#30693;&#30340;&#20027;&#39064;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#12290;&#23427;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#30340;&#26041;&#27861;&#65292;&#36824;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#25968;&#30334;&#21644;&#25968;&#21315;&#20010;&#20027;&#39064;&#30340;&#22270;&#24211;&#12290;&#23427;&#30001;&#32858;&#31867;&#21644;&#19968;&#32452;&#20108;&#36827;&#21046;&#23398;&#20064;&#31639;&#27861;&#32452;&#25104;&#65292;&#29992;&#20110;&#20272;&#35745;&#26597;&#35810;&#20154;&#33080;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20154;&#33080;&#22270;&#24211;&#65292;&#24182;&#26816;&#32034;&#20854;&#27491;&#30830;&#30340;&#36523;&#20221;&#12290;&#35813;&#26041;&#27861;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#22270;&#24211;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;LFW&#21644;YTF&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#65292;&#20063;&#21487;&#20197;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.07441</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#38477;&#20302;&#32852;&#21512;&#39044;&#27979;&#27694;&#27687;&#21270;&#29289;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides. (arXiv:2308.07441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#20013;&#30340;&#27694;&#27687;&#21270;&#29289;&#65288;NOx&#65289;&#20027;&#35201;&#26469;&#33258;&#29123;&#26009;&#29123;&#28903;&#65292;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#26377;&#26126;&#26174;&#30340;&#24613;&#24615;&#21644;&#24930;&#24615;&#24433;&#21709;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#22312;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#19979;&#39044;&#27979;&#22320;&#38754;&#19978;NOx&#27987;&#24230;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#22823;&#27668;&#27745;&#26579;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#30693;&#35782;&#65292;&#21487;&#33021;&#23384;&#22312;&#39640;&#20272;&#20559;&#24046;&#12290;&#21270;&#23398;&#20256;&#36755;&#27169;&#22411;&#65288;CTMs&#65289;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65307;&#28982;&#32780;&#65292;&#20934;&#30830;&#39044;&#27979;&#22320;&#38754;&#27987;&#24230;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#21518;&#26657;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#23558;ML&#27169;&#22411;&#30340;&#20559;&#24046;&#38477;&#20302;&#20102;21-42&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#21040;&#20102;NO2&#21644;NOx&#30340;&#32454;&#31890;&#24230;&#20256;&#36755;&#65292;&#29983;&#25104;&#20102;&#24378;&#22823;&#30340;&#31354;&#38388;&#22806;&#25512;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#23558;CTM&#30340;&#30693;&#35782;&#39537;&#21160;&#30340;&#29289;&#29702;&#21270;&#23398;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07439</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24863;&#30693;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#23545;&#20110;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20010;&#21035;&#39550;&#39542;&#21592;&#30340;&#20010;&#24615;&#21270;&#39550;&#39542;&#27169;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#27969;&#31243;&#65306;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#23450;&#39550;&#39542;&#25968;&#25454;&#38024;&#23545;&#27599;&#20010;&#39550;&#39542;&#21592;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;&#20154;&#26426;&#21327;&#21516;&#20223;&#30495;&#26469;&#25910;&#38598;&#20010;&#24615;&#21270;&#30340;&#33258;&#28982;&#39550;&#39542;&#36712;&#36857;&#21450;&#20854;&#30456;&#24212;&#30340;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;EEG&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.07436</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26102;&#31354;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#8212;&#8212;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#33041;&#30005;&#22270;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson's Disease Diagnosis Using Resting State EEG Signals. (arXiv:2308.07436v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;EEG&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#20005;&#37325;&#19988;&#36827;&#34892;&#24615;&#30340;&#31070;&#32463;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#12290;&#20026;&#20102;&#23454;&#29616;&#24085;&#37329;&#26862;&#30149;&#30340;&#26377;&#25928;&#27835;&#30103;&#21644;&#31649;&#29702;&#65292;&#20934;&#30830;&#21644;&#26089;&#26399;&#30340;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;EEG&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#38544;&#34255;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#24182;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#23637;&#31034;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;Bi-GRU&#65289;&#21644;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65288;UC San Diego&#25968;&#25454;&#38598;&#12289;PRED-CT&#25968;&#25454;&#38598;&#21644;University of Iowa (UI)&#25968;&#25454;&#38598;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20854;&#20013;&#19968;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#65292;&#21478;&#22806;&#20004;&#20010;&#29992;&#20110;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#39640;&#24615;&#33021;&#22320;&#20934;&#30830;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD), a severe and progressive neurological illness, affects millions of individuals worldwide. For effective treatment and management of PD, an accurate and early diagnosis is crucial. This study presents a deep learning-based model for the diagnosis of PD using resting state electroencephalogram (EEG) signal. The objective of the study is to develop an automated model that can extract complex hidden nonlinear features from EEG and demonstrate its generalizability on unseen data. The model is designed using a hybrid model, consists of convolutional neural network (CNN), bidirectional gated recurrent unit (Bi-GRU), and attention mechanism. The proposed method is evaluated on three public datasets (Uc San Diego Dataset, PRED-CT, and University of Iowa (UI) dataset), with one dataset used for training and the other two for evaluation. The results show that the proposed model can accurately diagnose PD with high performance on both the training and hold-out datasets. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07424</link><description>&lt;p&gt;
&#36890;&#36807;&#25351;&#25968;&#20542;&#26012;&#35299;&#20915;RTB&#24066;&#22330;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Distribution Shift in RTB Markets via Exponential Tilting. (arXiv:2308.07424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#26159;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#20559;&#31227;&#30340;&#29305;&#24615;&#65292;&#20027;&#35201;&#38024;&#23545;&#23454;&#26102;&#31454;&#20215;&#65288;RTB&#65289;&#24066;&#22330;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#20004;&#32773;&#22343;&#26159;&#20998;&#24067;&#20559;&#31227;&#30340;&#24378;&#26377;&#21147;&#35825;&#22240;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#65288;Exponential Tilt Reweighting Alignment&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30001;Marty&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;ExTRA&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#28857;&#26159;&#23427;&#33021;&#22815;&#20351;&#29992;&#26377;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#24615;&#36136;&#65292;&#24182;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper delves into the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We emphasize the challenges posed by class imbalance and sample selection bias, both potent instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method is designed to determine the importance weights on the source data, aiming to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicacy of the proposed model.
&lt;/p&gt;</description></item><item><title>U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07421</link><description>&lt;p&gt;
U-Turn&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07421
&lt;/p&gt;
&lt;p&gt;
U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#39537;&#21160;&#30340;&#21160;&#24577;&#36741;&#21161;&#26102;&#38388;&#26426;&#21046;&#65292;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#33719;&#21462;&#20998;&#25968;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35780;&#20272;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#25928;&#29575;&#30340;&#26631;&#20934;&#65306;&#29983;&#25104;&#36807;&#31243;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#22312;&#21453;&#21521;/&#21435;&#22122;&#38454;&#27573;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;U-Turn Diffusion&#8221;&#30340;&#26041;&#27861;&#12290;U-Turn Diffusion&#25216;&#26415;&#20174;&#26631;&#20934;&#30340;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#24320;&#22987;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20256;&#32479;&#35774;&#32622;&#65292;&#23427;&#30340;&#25345;&#32493;&#26102;&#38388;&#26356;&#30701;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25191;&#34892;&#26631;&#20934;&#30340;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#20197;&#21069;&#21521;&#36807;&#31243;&#30340;&#26368;&#32456;&#37197;&#32622;&#20026;&#21021;&#22987;&#20540;&#12290;&#36825;&#31181;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#30340;U-Turn Diffusion&#36807;&#31243;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#22788;&#29702;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07418</link><description>&lt;p&gt;
&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Locally Adaptive and Differentiable Regression. (arXiv:2308.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#22788;&#29702;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#36229;&#21442;&#25968;&#21270;&#30340;&#26412;&#22320;&#33258;&#36866;&#24212;&#27169;&#22411;&#20013;&#65292;&#24120;&#35265;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30446;&#26631;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23545;&#24212;&#30340;&#26412;&#22320;&#21306;&#22495;&#20013;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#26500;&#24314;&#20840;&#23616;&#36830;&#32493;&#21487;&#24494;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#23494;&#24230;&#25110;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#20013;&#30340;&#20989;&#25968;&#20540;&#23610;&#24230;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#20013;&#28151;&#21512;&#20351;&#29992;&#26680;&#23725;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#39033;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#36830;&#32493;&#25340;&#25509;&#26102;&#65292;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35821;&#38899;&#27169;&#22411;&#20013;&#21033;&#29992;&#25991;&#26412;&#27880;&#20837;&#36827;&#34892;&#38750;ASR&#20219;&#21153;&#30340;&#36741;&#21161;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#25552;&#21319;&#22823;&#20889;&#20934;&#30830;&#29575;&#21644;&#20132;&#26367;&#26816;&#27979;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07395</link><description>&lt;p&gt;
&#35821;&#38899;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#20889;&#21644;&#20132;&#26367;&#39044;&#27979;&#30340;&#25991;&#26412;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Text Injection for Capitalization and Turn-Taking Prediction in Speech Models. (arXiv:2308.07395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35821;&#38899;&#27169;&#22411;&#20013;&#21033;&#29992;&#25991;&#26412;&#27880;&#20837;&#36827;&#34892;&#38750;ASR&#20219;&#21153;&#30340;&#36741;&#21161;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#25552;&#21319;&#22823;&#20889;&#20934;&#30830;&#29575;&#21644;&#20132;&#26367;&#26816;&#27979;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27880;&#20837;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#20854;&#20013;&#38750;&#37197;&#23545;&#30340;&#32431;&#25991;&#26412;&#25968;&#25454;&#29992;&#20110;&#34917;&#20805;&#38899;&#39057;-&#25991;&#26412;&#25968;&#25454;&#65292;&#24050;&#26174;&#31034;&#20986;&#23545;&#35789;&#38169;&#35823;&#29575;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#25991;&#26412;&#27880;&#20837;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#65292;&#22312;E2E&#27169;&#22411;&#20013;&#24120;&#29992;&#20110;&#38750;ASR&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32852;&#21512;&#31471;&#21040;&#31471;&#21644;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65288;JEIT&#65289;&#20316;&#20026;&#25105;&#20204;&#30340;&#25991;&#26412;&#27880;&#20837;&#31639;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;ASR&#27169;&#22411;&#26469;&#23436;&#25104;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#22823;&#20889;&#65292;&#26159;&#19968;&#31181;&#21435;&#26631;&#20934;&#21270;&#20219;&#21153;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20132;&#26367;&#39044;&#27979;&#65292;&#23581;&#35797;&#35782;&#21035;&#29992;&#25143;&#26159;&#21542;&#24050;&#23436;&#25104;&#23545;&#35805;&#20132;&#26367;&#65292;&#22312;&#25968;&#23383;&#21161;&#29702;&#20114;&#21160;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25991;&#26412;&#27880;&#20837;&#26041;&#27861;&#33021;&#25552;&#21319;&#38271;&#23614;&#25968;&#25454;&#30340;&#22823;&#20889;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#20102;&#20132;&#26367;&#26816;&#27979;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
&lt;/p&gt;</description></item><item><title>DISBELIEVE &#26159;&#19968;&#31181;&#26412;&#22320;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#25110;&#26799;&#24230;&#20043;&#38388;&#21019;&#24314;&#24694;&#24847;&#36317;&#31163;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#36317;&#31163;&#36739;&#23567;&#30340;&#24694;&#24847;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#30772;&#22351;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07387</link><description>&lt;p&gt;
DISBELIEVE&#65306;&#23458;&#25143;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#23545;&#20110;&#26377;&#25928;&#30340;&#26412;&#22320;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (arXiv:2308.07387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07387
&lt;/p&gt;
&lt;p&gt;
DISBELIEVE &#26159;&#19968;&#31181;&#26412;&#22320;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#25110;&#26799;&#24230;&#20043;&#38388;&#21019;&#24314;&#24694;&#24847;&#36317;&#31163;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#36317;&#31163;&#36739;&#23567;&#30340;&#24694;&#24847;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#30772;&#22351;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#35299;&#20915;&#20998;&#20139;&#24739;&#32773;&#25935;&#24863;&#25968;&#25454;&#30456;&#20851;&#38544;&#31169;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#32852;&#21512;&#31995;&#32479;&#36890;&#24120;&#20551;&#35774;&#21442;&#19982;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#26159;&#35802;&#23454;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#20123;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#30340;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#21487;&#20197;&#27745;&#26579;&#32852;&#21512;&#35774;&#32622;&#65292;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22362;&#22266;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22362;&#22266;&#32858;&#21512;&#26041;&#27861;&#38750;&#24120;&#20381;&#36182;&#20110;&#24694;&#24847;&#23458;&#25143;&#31471;&#21644;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#21442;&#25968;&#25110;&#26799;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#26412;&#22320;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24403;&#24694;&#24847;&#21644;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#21442;&#25968;&#25110;&#26799;&#24230;&#24456;&#25509;&#36817;&#26102;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DISBELIEVE&#30340;&#26412;&#22320;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#23427;&#21019;&#24314;&#24694;&#24847;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#20351;&#24471;&#23427;&#20204;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36317;&#31163;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clien
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#35760;&#24405;&#21512;&#24182;&#30142;&#30149;&#30340;&#27604;&#20363;&#39033;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;ICD&#30721;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07359</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#31243;&#24230;&#30340;&#21512;&#24182;&#30142;&#30149;&#20449;&#24687;&#25913;&#36827;&#22522;&#20110;ICD&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity. (arXiv:2308.07359v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#35760;&#24405;&#21512;&#24182;&#30142;&#30149;&#30340;&#27604;&#20363;&#39033;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;ICD&#30721;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#20934;&#21307;&#23398;&#20013;&#65292;&#23547;&#25214;&#30456;&#20284;&#30340;&#24739;&#32773;&#26159;&#24120;&#35265;&#30340;&#30446;&#26631;&#65292;&#26377;&#21161;&#20110;&#27835;&#30103;&#32467;&#26524;&#35780;&#20272;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;&#36873;&#25321;&#24191;&#27867;&#21487;&#29992;&#30340;&#24739;&#32773;&#29305;&#24449;&#21644;&#36866;&#24403;&#30340;&#25968;&#23398;&#26041;&#27861;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22269;&#38469;&#30142;&#30149;&#21644;&#30456;&#20851;&#20581;&#24247;&#38382;&#39064;&#32479;&#35745;&#20998;&#31867;&#65288;ICD&#65289;&#30721;&#34987;&#20840;&#29699;&#33539;&#22260;&#20869;&#29992;&#20110;&#32534;&#30721;&#30142;&#30149;&#65292;&#24182;&#19988;&#20960;&#20046;&#36866;&#29992;&#20110;&#25152;&#26377;&#24739;&#32773;&#12290;&#23558;&#20854;&#32858;&#21512;&#20026;&#21253;&#21547;&#20027;&#35201;&#21644;&#27425;&#35201;&#35786;&#26029;&#30340;&#38598;&#21512;&#65292;&#23427;&#20204;&#21487;&#20197;&#26174;&#31034;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#21512;&#24182;&#30142;&#30149;&#24182;&#25581;&#31034;&#21512;&#24182;&#30142;&#30149;&#27169;&#24335;&#12290;&#21487;&#20197;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#31639;&#27861;&#26681;&#25454;&#24739;&#32773;&#30340;ICD&#30721;&#35745;&#31639;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#21333;&#26415;&#35821;&#19987;&#23478;&#35780;&#20998;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24739;&#32773;&#25968;&#25454;&#24448;&#24448;&#26174;&#31034;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#35760;&#24405;&#21512;&#24182;&#30142;&#30149;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#35760;&#24405;&#21512;&#24182;&#30142;&#30149;&#30340;&#27604;&#20363;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.  However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#25351;&#23548;&#33258;&#21160;&#21270;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.07358</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#19987;&#23478;&#24341;&#23548;&#30340;&#32593;&#26684;&#21010;&#20998;&#30340;&#30830;&#35748;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks. (arXiv:2308.07358v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#25351;&#23548;&#33258;&#21160;&#21270;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#22312;&#19981;&#21516;&#30340;&#24037;&#31243;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20934;&#30830;&#30340;&#27169;&#25311;&#20381;&#36182;&#20110;&#20223;&#30495;&#22495;&#30340;&#36866;&#24403;&#32593;&#26684;&#21010;&#20998;&#12290;&#34429;&#28982;&#39640;&#24230;&#31934;&#32454;&#30340;&#32593;&#26684;&#21487;&#20197;&#30830;&#20445;&#31934;&#24230;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#31867;&#20284;&#22320;&#65292;&#33258;&#36866;&#24212;&#37325;&#32593;&#26684;&#25216;&#26415;&#38656;&#35201;&#22810;&#27425;&#20223;&#30495;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36825;&#24847;&#21619;&#30528;&#32593;&#26684;&#21010;&#20998;&#36807;&#31243;&#20381;&#36182;&#20110;&#19987;&#19994;&#30693;&#35782;&#21644;&#22810;&#24180;&#30340;&#32463;&#39564;&#12290;&#33258;&#21160;&#21270;&#32593;&#26684;&#29983;&#25104;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#24182;&#24102;&#26469;&#26356;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#19987;&#23478;&#25351;&#23548;&#26469;&#33258;&#21160;&#29983;&#25104;&#39134;&#26426;&#27169;&#22411;&#30340;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20004;&#20010;&#20808;&#36827;&#27169;&#22411;&#65288;PointNet++&#21644;PointMLP&#65289;&#30340;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#19977;&#32500;&#32593;&#26684;&#20998;&#21106;&#27169;&#22411;&#20013;&#25237;&#23556;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;sMRI&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#24739;&#32773;&#20013;&#30340;&#24418;&#24577;&#36830;&#25509;&#27169;&#24335;&#22312;&#19981;&#21516;&#24180;&#40836;&#32452;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26174;&#33879;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.07356</link><description>&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20013;&#24418;&#24577;&#36830;&#25509;&#27169;&#24335;&#30340;&#24180;&#40836;&#20998;&#23618;&#24046;&#24322;&#65306;&#19968;&#31181;sMRI&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach. (arXiv:2308.07356v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;sMRI&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#24739;&#32773;&#20013;&#30340;&#24418;&#24577;&#36830;&#25509;&#27169;&#24335;&#22312;&#19981;&#21516;&#24180;&#40836;&#32452;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26174;&#33879;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24180;&#40836;&#20559;&#35265;&#24050;&#34987;&#30830;&#23450;&#20026;&#33258;&#38381;&#30151;&#35786;&#26029;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#27604;&#36739;&#19981;&#21516;&#24180;&#40836;&#32452;&#22312;&#20351;&#29992;&#24418;&#24577;&#29305;&#24449;&#65288;MF&#65289;&#21644;&#24418;&#24577;&#36830;&#25509;&#29305;&#24449;&#65288;MCF&#65289;&#36827;&#34892;&#33258;&#38381;&#30151;&#20998;&#31867;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#24211;ABIDE-I&#21644;ABIDE-II&#30340;sMRI&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#20010;&#24180;&#40836;&#32452;&#65306;6&#33267;11&#23681;&#12289;11&#33267;18&#23681;&#21644;6&#33267;18&#23681;&#12290;sMRI&#25968;&#25454;&#32463;&#36807;&#26631;&#20934;&#22788;&#29702;&#27969;&#31243;&#39044;&#22788;&#29702;&#65292;&#28982;&#21518;&#26681;&#25454;Destrieux&#35270;&#22270;&#23558;&#20854;&#21010;&#20998;&#20026;148&#20010;&#19981;&#21516;&#30340;&#21306;&#22495;&#12290;&#28982;&#21518;&#25552;&#21462;&#27599;&#20010;&#21306;&#22495;&#30340;&#38754;&#31215;&#12289;&#21402;&#24230;&#12289;&#20307;&#31215;&#21644;&#24179;&#22343;&#26354;&#29575;&#20449;&#24687;&#65292;&#29992;&#20110;&#21019;&#24314;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#24635;&#20849;592&#20010;MF&#21644;10,878&#20010;MCF&#12290;&#20351;&#29992;&#32479;&#35745;t&#26816;&#39564;&#65288;p&lt;0.05&#65289;&#30830;&#23450;&#20102;&#26174;&#33879;&#29305;&#24449;&#65292;&#24182;&#29992;&#20854;&#36827;&#34892;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p&lt;0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#24320;&#21457;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#39044;&#27979;&#24615;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.07352</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#31243;&#32435;&#31859;&#39063;&#31890;&#22312;&#21463;&#27745;&#26579;&#21547;&#27700;&#23618;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer. (arXiv:2308.07352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#24320;&#21457;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#39044;&#27979;&#24615;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#26377;&#35768;&#22810;&#21463;&#27745;&#26579;&#30340;&#22320;&#19979;&#27700;&#22330;&#22320;&#38656;&#35201;&#31215;&#26497;&#30340;&#20462;&#22797;&#35745;&#21010;&#26469;&#24674;&#22797;&#24403;&#22320;&#29983;&#24577;&#31995;&#32479;&#21644;&#29615;&#22659;&#12290;&#24037;&#31243;&#32435;&#31859;&#39063;&#31890;&#65288;ENPs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22320;&#19979;&#27700;&#20013;&#27745;&#26579;&#29289;&#21407;&#20301;&#38477;&#35299;&#30340;&#26377;&#25928;&#21453;&#24212;&#21058;&#12290;&#34429;&#28982;&#36825;&#20123;ENPs&#22312;&#23454;&#39564;&#23460;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#22320;&#26465;&#20214;&#19979;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;ENPs&#30340;&#22797;&#26434;&#36755;&#36816;&#21644;&#28382;&#30041;&#26426;&#21046;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#39044;&#27979;&#24615;&#24037;&#20855;&#26469;&#29702;&#35299;ENPs&#30340;&#36755;&#36816;&#21644;&#28382;&#30041;&#34892;&#20026;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20855;&#20027;&#35201;&#26159;&#20197;&#25968;&#20540;&#27169;&#25311;&#22120;&#20026;&#20027;&#65292;&#23545;&#31232;&#30095;&#25968;&#25454;&#38598;&#21644;&#21547;&#27700;&#23618;&#24322;&#36136;&#24615;&#30340;&#23384;&#22312;&#20855;&#26377;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;actor-critic&#26694;&#26550;&#20013;&#20351;&#29992;Q&#20989;&#25968;&#26469;&#36873;&#25321;&#28304;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.07351</link><description>&lt;p&gt;
IOB&#65306;&#38598;&#25104;&#20248;&#21270;&#20256;&#36882;&#21644;&#34892;&#20026;&#20256;&#36882;&#29992;&#20110;&#22810;&#31574;&#30053;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse. (arXiv:2308.07351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;actor-critic&#26694;&#26550;&#20013;&#20351;&#29992;Q&#20989;&#25968;&#26469;&#36873;&#25321;&#28304;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#26469;&#24555;&#36895;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#20063;&#21487;&#20197;&#36890;&#36807;&#20174;&#28304;&#31574;&#30053;&#21521;&#30456;&#20851;&#30446;&#26631;&#20219;&#21153;&#20256;&#36882;&#30693;&#35782;&#26469;&#20570;&#21040;&#21516;&#26679;&#30340;&#20107;&#24773;&#12290;&#20256;&#36882;RL&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#31574;&#30053;&#20248;&#21270;&#30446;&#26631;&#65288;&#20248;&#21270;&#20256;&#36882;&#65289;&#25110;&#32773;&#24433;&#21709;&#34892;&#20026;&#31574;&#30053;&#65288;&#34892;&#20026;&#20256;&#36882;&#65289;&#26469;&#21033;&#29992;&#28304;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#26469;&#24341;&#23548;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32452;&#20214;&#65292;&#27604;&#22914;&#23618;&#27425;&#31574;&#30053;&#25110;&#32773;&#28304;&#31574;&#30053;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38750;&#24179;&#31283;&#30340;&#31574;&#30053;&#20248;&#21270;&#25110;&#32773;&#22823;&#37327;&#30340;&#37319;&#26679;&#25104;&#26412;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20256;&#36882;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;RL&#26041;&#27861;&#65292;&#36873;&#25321;&#28304;&#31574;&#30053;&#32780;&#26080;&#38656;&#35757;&#32451;&#39069;&#22806;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;actor-critic&#26694;&#26550;&#20013;&#30340;Q&#20989;&#25968;&#26469;&#25351;&#23548;&#31574;&#30053;&#36873;&#25321;&#65292;&#36873;&#25321;&#28304;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source poli
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30740;&#31350;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#37327;&#21270;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07350</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural PDE-Solvers using Quantization Aware Training. (arXiv:2308.07350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07350
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30740;&#31350;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#37327;&#21270;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20316;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#25104;&#20026;&#36825;&#20010;&#26377;&#30528;&#30334;&#24180;&#21382;&#21490;&#30340;&#25968;&#23398;&#39046;&#22495;&#28508;&#22312;&#33539;&#24335;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#21487;&#34892;&#24615;&#26041;&#38754;&#65292;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;PDE&#23450;&#20041;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#20943;&#36731;&#36825;&#20010;&#25361;&#25112;&#12290;&#23545;&#20110;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20570;&#24471;&#26356;&#22909;&#65306;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36827;&#34892;&#37327;&#21270;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#26631;&#20934;PDE&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#36866;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#21644;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;FLOPs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#23454;&#35777;&#30340;&#26041;&#24335;&#35777;&#26126;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38181;&#19979;&#38477;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#23545;&#20598;&#38382;&#39064;&#20013;&#24471;&#21040;&#20960;&#20309;&#25512;&#23548;&#65292;&#25552;&#20986;&#20102;&#21160;&#37327;&#38181;&#19979;&#38477;&#65288;MOCO&#65289;&#21464;&#20307;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#36895;&#23545;&#20598;&#25910;&#25947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07343</link><description>&lt;p&gt;
Conic Descent Redux for Memory-Efficient Optimization &#65288;&#20869;&#23384;&#39640;&#25928;&#20248;&#21270;&#30340;&#38181;&#19979;&#38477;&#37325;&#36820;&#65289;
&lt;/p&gt;
&lt;p&gt;
Conic Descent Redux for Memory-Efficient Optimization. (arXiv:2308.07343v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38181;&#19979;&#38477;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#23545;&#20598;&#38382;&#39064;&#20013;&#24471;&#21040;&#20960;&#20309;&#25512;&#23548;&#65292;&#25552;&#20986;&#20102;&#21160;&#37327;&#38181;&#19979;&#38477;&#65288;MOCO&#65289;&#21464;&#20307;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#36895;&#23545;&#20598;&#25910;&#25947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38181;&#32534;&#31243;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26377;&#30528;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#37325;&#35775;&#20102;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#19968;&#38454;&#38181;&#19979;&#38477;&#65288;CD&#65289;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#30452;&#35273;&#12289;&#29702;&#35770;&#21644;&#31639;&#27861;&#23454;&#29616;&#26041;&#38754;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#21457;&#29616;CD&#21487;&#20197;&#20174;&#23545;&#20598;&#38382;&#39064;&#20013;&#24471;&#21040;&#19968;&#20010;&#30452;&#35266;&#30340;&#20960;&#20309;&#25512;&#23548;&#65292;&#36825;&#20026;&#26032;&#30340;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;CD&#30340;&#21160;&#37327;&#21464;&#20307;&#8212;&#8212;&#21160;&#37327;&#38181;&#19979;&#38477;&#65288;MOCO&#65289;&#12290;&#23545;CD&#21644;MOCO&#30340;&#23545;&#20598;&#34892;&#20026;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#25581;&#31034;&#20986;&#65306;i&#65289;&#19968;&#20010;&#32463;&#36807;&#20998;&#26512;&#39564;&#35777;&#30340;&#20572;&#27490;&#20934;&#21017;&#65307;ii&#65289;&#35774;&#35745;&#39044;&#22788;&#29702;&#22120;&#21152;&#36895;&#23545;&#20598;&#25910;&#25947;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38024;&#23545;&#20302;&#31209;&#35299;&#23588;&#20854;&#26159;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#36827;&#34892;&#35268;&#27169;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;MOCO&#21464;&#20307;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#21644;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#21450;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#26469;&#26377;&#25928;&#35299;&#20915;&#35013;&#22635;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07335</link><description>&lt;p&gt;
&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#29992;&#20110;&#35013;&#22635;&#22278;&#24418;
&lt;/p&gt;
&lt;p&gt;
An Encoder-Decoder Approach for Packing Circles. (arXiv:2308.07335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#21644;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#21450;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#26469;&#26377;&#25928;&#35299;&#20915;&#35013;&#22635;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20960;&#21313;&#24180;&#20197;&#26469;&#65292;&#23558;&#36739;&#23567;&#30340;&#29289;&#20307;&#23436;&#20840;&#25918;&#32622;&#22312;&#36739;&#22823;&#30340;&#29289;&#20307;&#20869;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38500;&#20102;&#35201;&#27714;&#36739;&#23567;&#30340;&#29289;&#20307;&#24517;&#39035;&#23436;&#20840;&#20301;&#20110;&#36739;&#22823;&#30340;&#29289;&#20307;&#20869;&#65292;&#36824;&#35201;&#27714;&#23427;&#20204;&#19981;&#37325;&#21472;&#25110;&#23613;&#37327;&#26368;&#23567;&#21270;&#37325;&#21472;&#38754;&#31215;&#12290;&#22240;&#27492;&#65292;&#35013;&#22635;&#38382;&#39064;&#25104;&#20026;&#19968;&#20010;&#38750;&#20984;&#38382;&#39064;&#65292;&#20854;&#26368;&#20248;&#35299;&#30340;&#33719;&#21462;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#33719;&#21462;&#20122;&#26368;&#20248;&#35299;&#65292;&#24182;&#38024;&#23545;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#33719;&#24471;&#20102;&#21487;&#35777;&#26126;&#26368;&#20248;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#22359;&#12289;&#25200;&#21160;&#22359;&#21644;&#35299;&#30721;&#22120;&#22359;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#32534;&#30721;&#22120;&#20197;&#35201;&#25918;&#32622;&#30340;&#22278;&#30340;&#32034;&#24341;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#21270;&#23618;&#36755;&#20986;&#20854;&#20013;&#24515;&#65292;&#25200;&#21160;&#23618;&#23545;&#20013;&#24515;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#65292;&#20197;&#30830;&#20445;&#23427;&#19981;&#20250;&#36229;&#20986;&#36793;&#30028;&#12290;&#35299;&#30721;&#22120;&#36890;&#36807;&#21453;&#36716;&#32534;&#30721;&#22120;&#36807;&#31243;&#26469;&#24674;&#22797;&#22278;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#35757;&#32451;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22278;&#24418;&#35013;&#22635;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#29260;&#22534;&#22823;&#23567;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;Blackjack&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#21464;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#30340;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#26377;&#20851;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#35748;&#35782;&#21040;&#29260;&#22534;&#22823;&#23567;&#26159;&#24433;&#21709;Blackjack&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.07329</link><description>&lt;p&gt;
Blackjack&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variations on the Reinforcement Learning performance of Blackjack. (arXiv:2308.07329v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#29260;&#22534;&#22823;&#23567;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;Blackjack&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#21464;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#30340;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#26377;&#20851;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#35748;&#35782;&#21040;&#29260;&#22534;&#22823;&#23567;&#26159;&#24433;&#21709;Blackjack&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blackjack&#25110;&#31216;&#20026;&#8220;21&#28857;&#8221;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25169;&#20811;&#29260;&#30340;&#36816;&#27668;&#21644;&#25216;&#24039;&#28216;&#25103;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#36229;&#36807;21&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#27604;&#24196;&#23478;&#26356;&#39640;&#30340;&#25163;&#29260;&#24635;&#25968;&#12290;&#29702;&#24819;&#30340;&#40657;&#26480;&#20811;&#31574;&#30053;&#23558;&#22312;&#38271;&#26399;&#20869;&#26368;&#22823;&#21270;&#36130;&#21153;&#22238;&#25253;&#65292;&#21516;&#26102;&#36991;&#20813;&#36172;&#24466;&#30340;&#30772;&#20135;&#12290;&#30001;&#20110;&#40657;&#26480;&#20811;&#30340;&#38543;&#26426;&#29615;&#22659;&#21644;&#22266;&#26377;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#36825;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#29609;&#27861;&#30340;Q-learning&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30740;&#31350;&#20102;&#31639;&#27861;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#20801;&#35768;&#20351;&#29992;&#36890;&#29992;&#40657;&#26480;&#20811;&#35268;&#21017;&#30340;&#27169;&#25311;&#22120;&#65292;&#20197;&#23637;&#31034;&#19968;&#20010;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#23436;&#32654;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#65292;&#20197;&#21450;&#29615;&#22659;&#21464;&#21270;&#23545;&#36825;&#20010;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#29260;&#22534;&#22823;&#23567;&#30340;&#24433;&#21709;&#27010;&#24565;&#24615;&#22320;&#29702;&#35299;&#20026;&#40657;&#26480;&#20811;&#20013;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blackjack or "21" is a popular card-based game of chance and skill. The objective of the game is to win by obtaining a hand total higher than the dealer's without exceeding 21. The ideal blackjack strategy will maximize financial return in the long run while avoiding gambler's ruin. The stochastic environment and inherent reward structure of blackjack presents an appealing problem to better understand reinforcement learning agents in the presence of environment variations. Here we consider a q-learning solution for optimal play and investigate the rate of learning convergence of the algorithm as a function of deck size. A blackjack simulator allowing for universal blackjack rules is also implemented to demonstrate the extent to which a card counter perfectly using the basic strategy and hi-lo system can bring the house to bankruptcy and how environment variations impact this outcome. The novelty of our work is to place this conceptual understanding of the impact of deck size in the con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.07326</link><description>&lt;p&gt;
AI&#25991;&#26412;-&#34892;&#20026;&#65306;&#21487;&#25805;&#25511;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI Text-to-Behavior: A Study In Steerability. (arXiv:2308.07326v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;ChatGPT&#36845;&#20195;&#29256;&#26412;&#30340;&#21487;&#25805;&#25511;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;OCEAN&#65288;&#24320;&#25918;&#24615;&#65292;&#36131;&#20219;&#24515;&#65292;&#22806;&#21521;&#24615;&#65292;&#23452;&#20154;&#24615;&#65292;&#31070;&#32463;&#36136;&#65289;&#30340;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#25105;&#20204;&#37327;&#21270;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#23450;&#21046;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#24403;&#35201;&#27714;&#29983;&#25104;&#31867;&#20284;&#20110;&#22806;&#21521;&#20154;&#26684;&#30340;&#25991;&#26412;&#26102;&#65292;OCEAN&#24471;&#20998;&#23545;&#40784;&#21040;&#20102;&#35813;&#34892;&#20026;&#29305;&#36136;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#8220;&#24320;&#25918;&#24615;&#8221;&#21576;&#29616;&#20102;&#35821;&#35328;&#30340;&#27169;&#31946;&#24615;&#65292;&#32780;&#8220;&#36131;&#20219;&#24515;&#8221;&#21644;&#8220;&#31070;&#32463;&#36136;&#8221;&#22312;OCEAN&#26694;&#26550;&#20013;&#26126;&#30830;&#22320;&#34987;&#21796;&#36215;&#65292;&#32780;&#8220;&#22806;&#21521;&#24615;&#8221;&#21644;&#8220;&#23452;&#20154;&#24615;&#8221;&#21017;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#26126;&#26174;&#37325;&#21472;&#20294;&#21448;&#26377;&#26126;&#26174;&#20998;&#31163;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;GPT&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35782;&#21035;&#20197;&#21450;&#36866;&#24212;&#24494;&#22937;&#25351;&#23548;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21382;&#21490;&#20154;&#29289;&#27169;&#25311;&#31361;&#20986;&#20102;LLM&#20869;&#21270;&#21644;&#25237;&#23556;&#21487;&#25351;&#23548;&#20010;&#24615;&#30340;&#33021;&#21147;&#65292;&#31934;&#30830;&#22797;&#21046;&#20102;&#20182;&#20204;&#30340;&#21746;&#23398;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. How
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#35774;&#35745;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#21306;&#20998;&#27010;&#29575;&#26816;&#27979;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;Expected Performance Drop (EPD)&#24230;&#37327;&#65292;&#36890;&#36807;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#23450;&#20041;&#24322;&#24120;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#20020;&#24202;&#24433;&#21709;&#23545;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.07324</link><description>&lt;p&gt;
&#25913;&#36827;3D&#21307;&#23398;&#22270;&#20687;&#30340;&#21306;&#20998;&#27010;&#29575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Redesigning Out-of-Distribution Detection on 3D Medical Images. (arXiv:2308.07324v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#35774;&#35745;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#21306;&#20998;&#27010;&#29575;&#26816;&#27979;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;Expected Performance Drop (EPD)&#24230;&#37327;&#65292;&#36890;&#36807;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#23450;&#20041;&#24322;&#24120;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#20020;&#24202;&#24433;&#21709;&#23545;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21487;&#20449;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21306;&#20998;&#27010;&#29575;&#65288;OOD&#65289;&#26679;&#26412;&#30340;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20851;&#38190;&#38382;&#39064;&#22312;&#20110;&#32570;&#20047;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#26080;&#27861;&#34913;&#37327;&#20020;&#24202;&#24433;&#21709;&#30340;&#20154;&#20026;&#38382;&#39064;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#20307;&#31215;&#21307;&#23398;&#25104;&#20687;&#21644;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#65289;&#30340;&#29305;&#23450;&#24773;&#20917;&#37325;&#26032;&#35774;&#35745;&#20102;OOD&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#20316;&#20026;&#22270;&#20687;&#20043;&#38388;&#24322;&#24120;&#26679;&#26412;&#30340;&#20266;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#24615;&#33021;&#30340;&#24433;&#21709;&#26435;&#34913;&#19981;&#21516;&#30340;&#26679;&#26412;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30340;ID/OOD&#21306;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Expected Performance Drop (EPD) &#20013;&#21152;&#20837;&#20102;&#36825;&#31181;&#26435;&#37325;&#12290; EPD&#26159;&#25105;&#20204;&#23545;&#26032;&#38382;&#39064;&#35774;&#35745;&#30340;&#26680;&#24515;&#36129;&#29486;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#20854;&#20020;&#24202;&#24433;&#21709;&#23545;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;11&#20010;CT&#21644;MRI OOD&#26816;&#27979;&#25361;&#25112;&#20013;&#23637;&#31034;&#20102;EPD&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
&lt;/p&gt;</description></item><item><title>AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07221</link><description>&lt;p&gt;
AudioFormer: &#36890;&#36807;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#38899;&#39057;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07221
&lt;/p&gt;
&lt;p&gt;
AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#26469;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#30340;&#24418;&#24335;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411; (MLM)&#65292;&#20174;&#32780;&#33719;&#24471;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604; (MPC) &#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21516;&#19968;&#38899;&#39057;&#36755;&#20837;&#20013;&#22810;&#20010;&#31163;&#25955;&#22768;&#23398;&#20195;&#30721;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#35270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MPC&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36319;&#36394;&#28436;&#21464;&#27169;&#24335;&#30340;&#26102;&#31354;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;tPARAFAC2&#65292;&#36890;&#36807;&#26102;&#38388;&#27491;&#21017;&#21270;&#22120;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#25552;&#21462;&#36880;&#28176;&#28436;&#21464;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.07126</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#36319;&#36394;&#28436;&#21464;&#27169;&#24335;&#30340;&#26102;&#31354;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Time-aware tensor decomposition for tracking evolving patterns. (arXiv:2308.07126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07126
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36319;&#36394;&#28436;&#21464;&#27169;&#24335;&#30340;&#26102;&#31354;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;tPARAFAC2&#65292;&#36890;&#36807;&#26102;&#38388;&#27491;&#21017;&#21270;&#22120;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#25552;&#21462;&#36880;&#28176;&#28436;&#21464;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#21487;&#20197;&#32452;&#32455;&#25104;&#19968;&#20010;&#39640;&#38454;&#24352;&#37327;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#27169;&#24335;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#34429;&#28982;&#24352;&#37327;&#20998;&#35299;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#25429;&#25417;&#36825;&#31867;&#39640;&#38454;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#20294;&#24448;&#24448;&#24573;&#30053;&#20102;&#26102;&#38388;&#30340;&#22240;&#32032;&#65292;&#20801;&#35768;&#26102;&#38388;&#28857;&#30340;&#37325;&#26032;&#25490;&#24207;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24341;&#20837;&#20102;&#26102;&#38388;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#19981;&#20801;&#35768;&#28508;&#22312;&#27169;&#24335;&#22312;&#26102;&#38388;&#19978;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#22823;&#33041;&#20013;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#20027;&#39064;&#20013;&#30340;&#19978;&#19979;&#25991;&#21464;&#21270;&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PARAFAC2&#30340;&#26102;&#31354;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;tPARAFAC2&#65292;&#36890;&#36807;&#26102;&#38388;&#27491;&#21017;&#21270;&#22120;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#25552;&#21462;&#36880;&#28176;&#28436;&#21464;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;tPARAFAC2&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#28436;&#21464;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#34920;&#29616;&#20248;&#20110;PARAFAC2&#21644;&#24102;&#26377;&#26102;&#38388;&#24179;&#28369;&#27491;&#21017;&#21270;&#30340;&#32806;&#21512;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regulariza
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07074</link><description>&lt;p&gt;
#InsTag:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30417;&#30563;&#24494;&#35843;&#30340;&#25351;&#20196;&#26631;&#27880;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. (arXiv:2308.07074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#34987;&#35748;&#20026;&#26159;&#25104;&#21151;&#30340;SFT&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#20854;&#23450;&#20041;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#32570;&#20047;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#24320;&#25918;&#30340;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#65292;&#26681;&#25454;&#35821;&#20041;&#21644;&#24847;&#22270;&#23545;SFT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#19988;&#36890;&#36807;&#26631;&#31614;&#26469;&#23450;&#20041;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;6.6K&#20010;&#26631;&#31614;&#26469;&#25551;&#36848;&#32508;&#21512;&#29992;&#25143;&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#33021;&#21147;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;InsTag&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#20174;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;6K&#20010;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TagLM&#27169;&#22411;&#22312;MT-Bench&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and compl
&lt;/p&gt;</description></item><item><title>SAILOR&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#23614;&#33410;&#28857;&#34920;&#31034;&#19978;&#30340;&#24615;&#33021;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06801</link><description>&lt;p&gt;
SAILOR: &#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAILOR: Structural Augmentation Based Tail Node Representation Learning. (arXiv:2308.06801v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06801
&lt;/p&gt;
&lt;p&gt;
SAILOR&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#23614;&#33410;&#28857;&#34920;&#31034;&#19978;&#30340;&#24615;&#33021;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#26377;&#25928;&#24615;&#20027;&#35201;&#20381;&#36182;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#65292;&#21363;&#22270;&#20013;&#22823;&#37096;&#20998;&#33410;&#28857;&#37117;&#26159;&#21482;&#26377;&#23569;&#25968;&#36830;&#25509;&#36793;&#30340;&#23614;&#33410;&#28857;&#12290;&#30001;&#20110;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#65292;GNN&#23545;&#23614;&#33410;&#28857;&#20135;&#29983;&#36739;&#24046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#21319;GNN&#23545;&#23614;&#33410;&#28857;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#26500;&#20449;&#24687;&#19981;&#36275;&#22914;&#20309;&#24694;&#21270;&#23614;&#33410;&#28857;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAILOR&#30340;&#36890;&#29992;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on pu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21387;&#32553;&#26679;&#26412;&#21644;&#21015;&#34920;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#36827;&#34892;&#20102;&#23398;&#20064;&#19978;&#38480;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#19981;&#21487;&#30693;&#23398;&#20064;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#30340;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06239</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#65306;&#22522;&#20110;&#26679;&#26412;&#21387;&#32553;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Private Distribution Learning with Public Data: The View from Sample Compression. (arXiv:2308.06239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21387;&#32553;&#26679;&#26412;&#21644;&#21015;&#34920;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#36827;&#34892;&#20102;&#23398;&#20064;&#19978;&#38480;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#19981;&#21487;&#30693;&#23398;&#20064;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21487;&#20197;&#35775;&#38382;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20844;&#31169;&#23398;&#20064;&#65292;&#23398;&#20064;&#22120;&#34987;&#32473;&#20104;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;p&#30340;&#23646;&#20110;&#31867;$\mathcal Q$&#30340;&#20844;&#20849;&#26679;&#26412;&#21644;&#31169;&#26377;&#26679;&#26412;&#65292;&#30446;&#26631;&#26159;&#36755;&#20986;&#19968;&#20010;&#23545;p&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#36981;&#23432;&#19982;&#31169;&#26377;&#26679;&#26412;&#30456;&#20851;&#30340;&#38544;&#31169;&#32422;&#26463;&#65288;&#36825;&#37324;&#26159;&#32431;&#24046;&#20998;&#38544;&#31169;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;$\mathcal Q$&#30340;&#20844;&#31169;&#21487;&#23398;&#20064;&#24615;&#19982;$\mathcal Q$&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#20197;&#21450;&#20013;&#38388;&#27010;&#24565;&#8212;&#8212;&#21015;&#34920;&#23398;&#20064;&#30340;&#23384;&#22312;&#24615;&#26377;&#20851;&#12290;&#21033;&#29992;&#36825;&#20010;&#32852;&#31995;&#65306;&#65288;1&#65289;&#36817;&#20284;&#24674;&#22797;&#20102;&#20851;&#20110;$\mathbb R^d$&#19978;&#39640;&#26031;&#20998;&#24067;&#30340;&#20808;&#21069;&#32467;&#26524;&#65307;&#65288;2&#65289;&#24471;&#20986;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#20219;&#24847;$k$-&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#22312;$\mathbb R^d$&#19978;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#19981;&#21487;&#30693;&#21644;&#20998;&#24067;&#21464;&#21270;&#25269;&#25239;&#23398;&#20064;&#22120;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#20844;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#38381;&#21253;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.  We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02804</link><description>&lt;p&gt;
MiAMix: &#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02804
&lt;/p&gt;
&lt;p&gt;
MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#36807;&#25311;&#21512;&#20381;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#21313;&#20998;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#31574;&#30053;&#65292;&#20294;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#22312;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;MiAMix&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#12290;MiAMix&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#28151;&#21512;&#25513;&#27169;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26174;&#33879;&#24615;&#20449;&#24687;&#65292;&#32780;MiAMix&#30340;&#35774;&#35745;&#20063;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23545;MiAMix&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#22270;&#20687;&#22522;&#20934;&#21644;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
&lt;/p&gt;</description></item><item><title>Tirtha&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#30340;&#22270;&#20687;&#24182;&#21019;&#24314;&#23427;&#20204;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#36825;&#20010;&#24179;&#21488;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#25216;&#26415;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#29305;&#28857;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.01246</link><description>&lt;p&gt;
Tirtha -- &#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#22270;&#20687;&#24182;&#21019;&#24314;&#21382;&#21490;&#36951;&#22336;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01246
&lt;/p&gt;
&lt;p&gt;
Tirtha&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#30340;&#22270;&#20687;&#24182;&#21019;&#24314;&#23427;&#20204;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#36825;&#20010;&#24179;&#21488;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#25216;&#26415;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#29305;&#28857;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#20445;&#23384;&#25991;&#21270;&#36951;&#20135;&#65288;CH&#65289;&#36951;&#22336;&#23545;&#20110;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#33258;&#28982;&#28798;&#23475;&#25110;&#20154;&#31867;&#27963;&#21160;&#30340;&#25439;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#21019;&#24314;&#25991;&#21270;&#36951;&#20135;&#36951;&#22336;&#30340;&#19977;&#32500;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25968;&#23383;&#21270;&#20445;&#23384;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#24471;&#30410;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25668;&#24433;&#27979;&#37327;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#30340;&#35774;&#22791;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#23384;&#22312;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19968;&#20010;&#24320;&#25918;&#30340;&#19977;&#32500;&#27169;&#22411;&#23384;&#20648;&#24211;&#38459;&#30861;&#20102;&#23545;&#36951;&#20135;&#30340;&#30740;&#31350;&#21644;&#20844;&#20247;&#21442;&#19982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tirtha&#65292;&#19968;&#20010;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#22270;&#20687;&#21644;&#19977;&#32500;&#27169;&#22411;&#21019;&#24314;&#30340;&#32593;&#32476;&#24179;&#21488;&#12290;Tirtha&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#65288;MVS&#65289;&#25216;&#26415;&#12290;&#23427;&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#65292;&#21487;&#20197;&#38543;&#30528;&#25668;&#24433;&#27979;&#37327;&#30340;&#36827;&#23637;&#32780;&#24341;&#20837;&#26032;&#25216;&#26415;&#12290;Tirtha&#21487;&#20197;&#36890;&#36807;https://tirtha.niser.ac.in&#20316;&#20026;&#32593;&#39029;&#30028;&#38754;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#27169;&#22411;&#22312;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#21512;&#25104;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#27169;&#25311;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.00924</link><description>&lt;p&gt;
&#19981;&#26029;&#36866;&#24212;&#38470;&#22320;&#24433;&#20687;&#20013;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#27169;&#22411;&#22312;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#21512;&#25104;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#27169;&#25311;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#20943;&#23569;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65288;&#28304;&#22495;&#65289;&#19982;&#20351;&#29992;&#25968;&#25454;&#65288;&#30446;&#26631;&#22495;&#65289;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#24403;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#31354;&#20013;&#24179;&#21488;&#19978;&#26102;&#65292;&#23427;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#35780;&#20272;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#25193;&#22823;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#29616;&#26377;&#30340;&#31354;&#20013;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20174;&#23454;&#38469;&#22270;&#20687;&#20013;&#21512;&#25104;&#20102;&#20004;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#29983;&#25104;&#20102;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#36830;&#32493;&#25110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65306;&#19968;&#20010;&#22522;&#32447;&#26631;&#20934;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#21644;&#20004;&#20010;&#36830;&#32493;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#27599;&#27425;&#21482;&#33021;&#35775;&#38382;&#19968;&#23567;&#37096;&#20998;&#25110;&#19968;&#20010;&#25209;&#27425;&#30340;&#30446;&#26631;&#25968;&#25454;&#65292;&#24182;&#19988;&#36866;&#24212;&#21482;&#21457;&#29983;&#22312;&#19968;&#27425;&#25968;&#25454;&#36845;&#20195;&#20013;&#12290;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#21644;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#32467;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07893</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#25968;&#25454;&#26377;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;(AFP)&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#26679;&#26412;&#65292;&#32780;&#36825;&#20123;&#26679;&#26412;&#24456;&#38590;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20174;&#22522;&#30784;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#27491;&#24120;&#26679;&#26412;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#32420;&#32500;&#23618;&#29255;&#65288;tow&#65289;&#23545;&#32420;&#32500;&#38138;&#35774;&#34920;&#38754;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20998;&#21106;&#25104;&#23567;&#31383;&#21475;&#12290;&#20854;&#20013;&#19981;&#21253;&#21547;&#24322;&#24120;&#30340;&#31383;&#21475;&#23376;&#38598;&#20256;&#36882;&#32473;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#36755;&#20837;&#12290;&#22240;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;&#29992;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#36825;&#20123;&#26679;&#26412;&#65292;&#23427;&#20135;&#29983;&#30340;&#37325;&#26500;&#27604;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#26356;&#31934;&#30830;&#12290;&#22240;&#27492;&#65292;&#37325;&#26500;&#35823;&#24046;&#30340;&#20540;&#34987;&#29992;&#20316;&#19968;&#20010;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#28508;&#22312;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13050</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#65306;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#22686;&#24191;&#21644;&#32454;&#21270;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#35780;&#20272;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36807;&#21435;&#21916;&#22909;&#21644;&#20854;&#20182;&#29992;&#25143;&#30340;&#21487;&#29992;&#20559;&#22909;&#20449;&#24687;&#39044;&#27979;&#20854;&#23545;&#26032;&#29289;&#21697;&#30340;&#35780;&#20998;&#12290;&#23613;&#31649;CF&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#30340;&#31232;&#30095;&#24615;&#30340;&#26497;&#22823;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#26368;&#22823;&#36793;&#38469;&#30697;&#38453;&#20998;&#35299;&#65288;MMMF&#65289;&#30340;&#25968;&#25454;&#22686;&#24191;&#21644;&#32454;&#21270;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#26159;&#24191;&#27867;&#25509;&#21463;&#30340;&#29992;&#20110;&#35780;&#32423;&#39044;&#27979;&#30340;CF&#25216;&#26415;&#65292;&#20043;&#21069;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;CF&#31639;&#27861;&#30340;&#22266;&#26377;&#29305;&#24615;&#26469;&#35780;&#20272;&#21333;&#20010;&#35780;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#35780;&#32423;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#20219;&#20309;CF&#31639;&#27861;&#30340;&#39044;&#27979;&#20302;&#32622;&#20449;&#24230;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#26576;&#20123;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#30340;&#25968;&#25454;&#22686;&#24191;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.10656</link><description>&lt;p&gt;
&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65306;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20154;&#31867;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics. (arXiv:2306.10656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21307;&#30103;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#21892;&#36523;&#20307;&#21644;&#31934;&#31070;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65288;VHGM&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#30340;&#23646;&#24615;&#12290;VHGM&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#30721;&#24314;&#27169;&#35757;&#32451;&#65292;&#22312;&#24050;&#30693;&#23646;&#24615;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#23646;&#24615;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#21033;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#39640;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#25968;&#20540;&#35780;&#20272;&#20102;VHGM&#21450;&#20854;&#35757;&#32451;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;VHGM&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102;&#29992;&#25143;&#24773;&#22659;&#65292;&#20363;&#22914;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental conditions. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose Virtual Human Generative Model (VHGM), a machine learning model for estimating attributes about healthcare, lifestyles, and personalities. VHGM is a deep generative model trained with masked modeling to learn the joint distribution of attributes conditioned on known ones. Using heterogeneous tabular datasets, VHGM learns more than 1,800 attributes efficiently. We numerically evaluate the performance of VHGM and its training techniques. As a proof-of-concept of VHGM, we present several applications demonstrating user scenarios, such as virtual measurements of healthcare attributes and hypothesis verifications of lifestyles.
&lt;/p&gt;</description></item><item><title>GCformer&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#20840;&#23616;&#21367;&#31215;&#21644;&#23616;&#37096;Transformer&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#65292;GCformer&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08325</link><description>&lt;p&gt;
GCformer:&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#25193;&#23637;&#30340;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. (arXiv:2306.08325v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08325
&lt;/p&gt;
&lt;p&gt;
GCformer&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#20840;&#23616;&#21367;&#31215;&#21644;&#23616;&#37096;Transformer&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#65292;GCformer&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#19981;&#22815;&#20934;&#30830;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#26410;&#33021;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38271;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23610;&#23544;&#22823;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCformer&#65292;&#23427;&#23558;&#29992;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#32467;&#26500;&#21270;&#20840;&#23616;&#21367;&#31215;&#20998;&#25903;&#19982;&#29992;&#20110;&#25429;&#25417;&#30701;&#26399;&#36817;&#26399;&#20449;&#21495;&#30340;&#23616;&#37096;Transformer&#20998;&#25903;&#30456;&#32467;&#21512;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#23616;&#21367;&#31215;&#26680;&#30340;&#36830;&#36143;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#20840;&#23616;&#20998;&#25903;&#20013;&#36873;&#25321;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#32463;&#36807;&#29305;&#27530;&#35774;&#35745;&#65292;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#32780;&#22024;&#26434;&#30340;&#36755;&#20837;&#20449;&#21495;&#12290;&#23545;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;GCformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have emerged as promising tools for time series forecasting.  However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity.  To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs. (arXiv:2306.07699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26088;&#22312;&#27169;&#25311;&#22270;&#20687;&#30340;&#20256;&#36882;&#24615;&#36136;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22270;&#20687;&#32467;&#26500;&#24448;&#24448;&#26159;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;TGSL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#969;-UCB&#30340;&#26032;&#19978;&#32622;&#20449;&#21306;&#38388;&#25277;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#23545;&#31216;&#32622;&#20449;&#21306;&#38388;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#32039;&#23494;&#22320;&#20272;&#35745;&#22870;&#21169;&#25104;&#26412;&#27604;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#39044;&#31639;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#31574;&#30053;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.07071</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#23545;&#31216;&#32622;&#20449;&#21306;&#38388;&#30340;&#26377;&#38480;&#39044;&#31639;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals. (arXiv:2306.07071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07071
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#969;-UCB&#30340;&#26032;&#19978;&#32622;&#20449;&#21306;&#38388;&#25277;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#23545;&#31216;&#32622;&#20449;&#21306;&#38388;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#32039;&#23494;&#22320;&#20272;&#35745;&#22870;&#21169;&#25104;&#26412;&#27604;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#39044;&#31639;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#31574;&#30053;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#39044;&#31639;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#36873;&#25321;&#20855;&#26377;&#26410;&#30693;&#26399;&#26395;&#22870;&#21169;&#21644;&#25104;&#26412;&#30340;K&#20010;&#33218;&#12290;&#30446;&#26631;&#26159;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#24635;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#29609;&#23478;&#35797;&#22270;&#23613;&#21487;&#33021;&#32463;&#24120;&#22320;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#22870;&#21169;&#25104;&#26412;&#27604;&#30340;&#33218;&#12290;&#24403;&#21069;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#31574;&#30053;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20104;&#20197;&#35828;&#26126;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#25277;&#26679;&#31574;&#30053;&#65292;&#31216;&#20026;&#969;-UCB&#65292;&#24182;&#20351;&#29992;&#19981;&#23545;&#31216;&#32622;&#20449;&#21306;&#38388;&#12290;&#36825;&#20123;&#21306;&#38388;&#23610;&#24230;&#38543;&#30528;&#26679;&#26412;&#22343;&#20540;&#21644;&#38543;&#26426;&#21464;&#37327;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#32780;&#21464;&#21270;&#65292;&#30456;&#23545;&#20110;&#25105;&#20204;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#32039;&#23494;&#22320;&#20272;&#35745;&#22870;&#21169;&#25104;&#26412;&#27604;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#23545;&#25968;&#21518;&#24724;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from $K$ arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current state-of-the-art policies for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, $\omega$-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has logarithmic regret and consistently outperforms existing policies in synthetic and real settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PRDC&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#38598;&#32422;&#26463;&#26469;&#27491;&#21017;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PRDC&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25351;&#23548;&#31574;&#30053;&#30340;&#26356;&#26032;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06569</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#31574;&#30053;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Regularization with Dataset Constraint for Offline Reinforcement Learning. (arXiv:2306.06569v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PRDC&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#38598;&#32422;&#26463;&#26469;&#27491;&#21017;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PRDC&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25351;&#23548;&#31574;&#30053;&#30340;&#26356;&#26032;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21363;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#31574;&#30053;&#27491;&#21017;&#21270;&#26469;&#32422;&#26463;&#23398;&#20064;&#31574;&#30053;&#30340;&#20998;&#24067;&#25110;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#21644;&#25903;&#25345;&#32422;&#26463;&#36807;&#20110;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#35201;&#27714;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#29366;&#24577;&#19979;&#36873;&#25321;&#19982;&#34892;&#20026;&#31574;&#30053;&#30456;&#20284;&#30340;&#21160;&#20316;&#12290;&#36825;&#23558;&#38480;&#21046;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#34892;&#20026;&#31574;&#30053;&#26159;&#27425;&#20248;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23558;&#31574;&#30053;&#27491;&#21017;&#21270;&#25351;&#21521;&#26368;&#36817;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#21487;&#33021;&#26356;&#21152;&#26377;&#25928;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#31574;&#30053;&#27491;&#21017;&#21270;&#65288;PRDC&#65289;&#12290;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#26356;&#26032;&#31574;&#30053;&#26102;&#65292;PRDC&#20250;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#25509;&#36817;&#30340;&#29366;&#24577;-&#21160;&#20316;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#35813;&#26679;&#26412;&#30340;&#21160;&#20316;&#26469;&#32422;&#26463;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;PRDC&#21487;&#20197;&#25351;&#23548;&#31574;&#30053;&#26681;&#25454;&#26368;&#30456;&#20851;&#30340;&#29366;&#24577;-&#21160;&#20316;&#26469;&#36827;&#34892;&#26356;&#26032;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.05694</link><description>&lt;p&gt;
&#23567;&#37327;&#23376;&#24577;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Representation Learning of Small Quantum States. (arXiv:2306.05694v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#20154;&#31867;&#25351;&#23548;&#25110;&#29305;&#24449;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#36215;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#21040;&#20851;&#20110;&#22788;&#29702;&#20219;&#21153;&#25152;&#38656;&#30340;&#25968;&#25454;&#29305;&#24449;&#20449;&#24687;&#12290;&#22312;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#32972;&#26223;&#19979;&#65292;&#35757;&#32451;&#27169;&#22411;&#20197;&#25551;&#36848;&#37327;&#23376;&#24577;&#21448;&#26410;&#32463;&#20154;&#31867;&#24178;&#39044;&#22320;&#23398;&#20064;&#20449;&#24687;&#26159;&#33719;&#24471;&#28145;&#20837;&#20102;&#35299;&#26426;&#22120;&#22914;&#20309;&#21576;&#29616;&#22797;&#26434;&#37327;&#23376;&#24577;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#23558;&#20026;&#38750;&#24179;&#20961;&#30340;&#37327;&#23376;&#31995;&#32479;&#21450;&#20854;&#26377;&#25928;&#34920;&#31034;&#24102;&#26469;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#38024;&#23545;&#30001;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#29983;&#25104;&#30340;&#20004;&#37327;&#23376;&#27604;&#29305;&#23494;&#24230;&#30697;&#38453;&#35757;&#32451;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35745;&#31639;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#27169;&#22411;&#25152;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20197;&#21450;&#20854;&#20869;&#37096;&#23545;&#25968;&#25454;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#23558;&#37327;&#23376;&#24577;&#19982;&#20854;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#20063;&#33021;&#22815;&#36798;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01991</link><description>&lt;p&gt;
&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65306;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#19982;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Bio-Inspired Chaos Sensor Model Based on the Perceptron Neural Network: Machine Learning Concept and Application for Computational Neuro-Science. (arXiv:2306.01991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#20063;&#33021;&#22815;&#36798;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#38544;&#34255;&#23618;&#26377;50&#20010;&#31070;&#32463;&#20803;&#65292;&#36755;&#20986;&#23618;&#26377;1&#20010;&#31070;&#32463;&#20803;&#65292;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#20915;&#23450;&#31995;&#25968;R2&#32422;&#20026;0.9&#12290;&#20351;&#29992;Hindmarsh-Rose&#33033;&#20914;&#27169;&#22411;&#29983;&#25104;&#33033;&#20914;&#38388;&#38548;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#24863;&#30693;&#22120;&#12290;&#20351;&#29992;K-block&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#36873;&#25321;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#20272;&#35745;&#20256;&#24863;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#35813;&#27169;&#22411;&#20063;&#33021;&#22815;&#20197;&#33391;&#22909;&#30340;&#32467;&#26524;&#36817;&#20284;&#27169;&#31946;&#29109;&#65292;&#24182;&#19988;&#20915;&#23450;&#31995;&#25968;R2&#32422;&#20026;0.5-0.8&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#19988;&#31532;&#19968;&#23618;&#30340;&#26435;&#37325;&#30456;&#31561;&#65292;&#36817;&#20284;&#30340;&#21407;&#29702;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#24179;&#22343;&#20540;&#30340;&#32447;&#24615;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study presents a bio-inspired chaos sensor model based on the perceptron neural network for the estimation of entropy of spike train in neurodynamic systems. After training, the sensor on perceptron, having 50 neurons in the hidden layer and 1 neuron at the output, approximates the fuzzy entropy of a short time series with high accuracy, with a determination coefficient of R2 ~ 0.9. The Hindmarsh-Rose spike model was used to generate time series of spike intervals, and datasets for training and testing the perceptron. The selection of the hyperparameters of the perceptron model and the estimation of the sensor accuracy were performed using the K-block cross-validation method. Even for a hidden layer with one neuron, the model approximates the fuzzy entropy with good results and the metric R2 ~ 0.5-0.8. In a simplified model with one neuron and equal weights in the first layer, the principle of approximation is based on the linear transformation of the average value of the time seri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18378</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#36827;&#34892;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#29420;&#31435;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#32780;&#27169;&#22411;&#24182;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#36825;&#20123;&#22240;&#32032;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#24402;&#32435;&#20559;&#35265;&#22312;&#23454;&#29616;&#35299;&#32544;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26397;&#30528;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#25968;&#25454;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#28508;&#22312;&#32500;&#24230;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#31163;&#25955;&#32534;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#24212;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#26631;&#37327;&#30721;&#20070;&#12290;&#28508;&#22312;&#37327;&#21270;&#36843;&#20351;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#25968;&#25454;&#28857;&#19978;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#20540;&#65292;&#20174;&#32780;&#20351;&#35299;&#30721;&#22120;&#33021;&#22815;&#20026;&#27599;&#20010;&#20540;&#20998;&#37197;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;&#35268;&#33539;&#21270;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#24341;&#21521;&#36825;&#31181;&#31616;&#26126;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#24212;&#29992;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#22312;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#30340;Dice&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05424</link><description>&lt;p&gt;
&#26469;&#33258;&#22122;&#22768;&#30340;&#22238;&#38899;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#29983;&#25104;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation. (arXiv:2305.05424v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#22312;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#30340;Dice&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21644;&#24515;&#33039;&#36229;&#22768;&#35821;&#20041;&#26631;&#31614;&#22320;&#22270;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#21487;&#20197;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#65292;&#22914;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;2D&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#36827;&#34892;&#20998;&#21106;&#12290;&#22312;&#20165;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#24038;&#24515;&#23460;&#20869;&#33180;&#12289;&#24515;&#22806;&#33180;&#21644;&#24038;&#24515;&#25151;&#30340;&#20998;&#21106;&#20998;&#21035;&#20135;&#29983;&#20102;88.5&#177;6.0&#65285;&#12289;92.3&#177;3.9&#65285;&#21644;86.3&#177;10.7&#65285;&#30340;&#24179;&#22343;Dice&#20998;&#25968;&#12290;&#36825;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#12290;&#35813;&#25552;&#35758;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel pipeline for the generation of synthetic images via Denoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasound semantic label maps. We show that these synthetic images can serve as a viable substitute for real data in the training of deep-learning models for medical image analysis tasks such as image segmentation. To demonstrate the effectiveness of this approach, we generated synthetic 2D echocardiography images and trained a neural network for segmentation of the left ventricle and left atrium. The performance of the network trained on exclusively synthetic images was evaluated on an unseen dataset of real images and yielded mean Dice scores of 88.5 $\pm 6.0$ , 92.3 $\pm 3.9$, 86.3 $\pm 10.7$ \% for left ventricular endocardial, epicardial and left atrial segmentation respectively. This represents an increase of $9.09$, $3.7$ and $15.0$ \% in Dice scores compared to the previous state-of-the-art. The proposed pipeline has the potential for applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.13917</link><description>&lt;p&gt;
&#27604;&#20363;&#20195;&#34920;&#24615;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20844;&#24179;&#27010;&#24565;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#32858;&#31867;&#65292;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#30784;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#25105;&#20204;&#35748;&#20026;&#35813;&#27010;&#24565;&#20197;&#19968;&#31181;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;&#26041;&#24335;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#20960;&#20010;&#29616;&#23384;&#27010;&#24565;&#30340;&#29702;&#30001;&#12290;&#20294;&#29616;&#26377;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#19981;&#33021;&#28385;&#36275;&#25105;&#20204;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#28385;&#36275;&#26080;&#32422;&#26463;&#32858;&#31867;&#21644;&#31163;&#25955;&#32858;&#31867;&#38382;&#39064;&#30340;PRF&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07769</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23436;&#25972;&#24490;&#29615;&#19968;&#33268;&#24615;GAN&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Regularized Complete Cycle Consistent GAN for Anomaly Detection. (arXiv:2304.07769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RCALAD&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#21644;&#26032;&#30340;&#37492;&#21035;&#22120;&#22686;&#24378;&#20102;GAN&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#34917;&#20805;&#20998;&#24067;&#24341;&#23548;&#37325;&#24314;&#21644;&#24341;&#20837;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#35823;&#24046;&#20013;&#24490;&#29615;&#19968;&#33268;&#24615;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23041;&#21147;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#30001;&#20110;&#31867;&#21035;&#38388;&#31934;&#24230;&#39640;&#24230;&#24046;&#24322;&#32780;&#26410;&#34987;&#24212;&#29992;&#20110;&#25152;&#26377;&#31867;&#22411;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;RCALAD&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#23558;&#26032;&#30340;&#37492;&#21035;&#22120;&#24341;&#20837;&#21040;&#32467;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;RCALAD&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#34917;&#20805;&#20998;&#24067;&#65292;&#23558;&#37325;&#24314;&#24341;&#23548;&#21040;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#26679;&#26412;&#19982;&#20854;&#37325;&#24314;&#20998;&#31163;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#24322;&#24120;&#35780;&#20998;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#23637;&#31034;&#20986;&#20854;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an adversarial method for anomaly detection in real-world applications, leveraging the power of generative adversarial neural networks (GANs) through cycle consistency in reconstruction error. Previous methods suffer from the high variance between class-wise accuracy which leads to not being applicable for all types of anomalies. The proposed method named RCALAD tries to solve this problem by introducing a novel discriminator to the structure, which results in a more efficient training process. Additionally, RCALAD employs a supplementary distribution in the input space to steer reconstructions toward the normal data distribution, effectively separating anomalous samples from their reconstructions and facilitating more accurate anomaly detection. To further enhance the performance of the model, two novel anomaly scores are introduced. The proposed model has been thoroughly evaluated through extensive experiments on six various datasets, yielding results that demonst
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03646</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness through Aleatoric Uncertainty. (arXiv:2304.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03646
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#36890;&#24120;&#30456;&#20114;&#31454;&#20105;&#30340;&#30446;&#26631;&#12290; &#20844;&#24179;&#24615;&#30830;&#20445;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#24102;&#20559;&#35265;&#22320;&#38024;&#23545;&#20219;&#20309;&#29305;&#23450;&#32676;&#20307;&#65292;&#32780;&#25928;&#29992;&#21017;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#20272;&#31639;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#26080;&#20851;&#12290;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#65292;&#25105;&#20204;&#34920;&#26126;&#20855;&#26377;&#20302;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26679;&#26412;&#27604;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#26679;&#26412;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#22320;&#24314;&#27169;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unique solution to tackle the often-competing goals of fairness and utility in machine learning classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group, utility focuses on maximizing the accuracy of the model's predictions. Our aim is to investigate the relationship between uncertainty and fairness. Our approach leverages this concept by employing Bayesian learning to estimate the uncertainty in sample predictions where the estimation is independent of confounding effects related to the protected attribute. Through empirical evidence, we show that samples with low classification uncertainty are modeled more accurately and fairly than those with high uncertainty, which may have biased representations and higher prediction errors. To address the challenge of balancing fairness and utility, we propose a novel fairness-utility objective that is defined based on uncertainty quantification. The w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02229</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#28151;&#21512;&#22238;&#24402;&#65288;Mixed Regression via Approximate Message Passing&#65289;
&lt;/p&gt;
&lt;p&gt;
Mixed Regression via Approximate Message Passing. (arXiv:2304.02229v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#20013;&#20855;&#26377;&#22810;&#20010;&#20449;&#21495;&#21644;&#28508;&#21464;&#37327;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#34987;&#31216;&#20026;&#30697;&#38453;GLM&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#31639;&#27861;&#26469;&#20272;&#35745;&#30697;&#38453;GLM&#20013;&#30340;&#20449;&#21495;&#21644;&#28508;&#21464;&#37327;&#65292;&#24182;&#22312;&#39640;&#32500;&#26497;&#38480;&#20013;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#34920;&#24449;&#12290;&#35813;&#34920;&#24449;&#26159;&#36890;&#36807;&#29366;&#24577;&#28436;&#21270;&#36882;&#24402;&#26469;&#35745;&#31639;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#28176;&#36817;&#24615;&#33021;&#24230;&#37327;&#65292;&#20363;&#22914;&#20449;&#22122;&#27604;&#19979;&#38477;&#38408;&#20540;&#65288;threshold&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables. This model, which we call a matrix GLM, covers many widely studied problems in statistical learning, including mixed linear regression, max-affine regression, and mixture-of-experts. In mixed linear regression, each observation comes from one of $L$ signal vectors (regressors), but we do not know which one; in max-affine regression, each observation comes from the maximum of $L$ affine functions, each defined via a different signal vector. The goal in all these problems is to estimate the signals, and possibly some of the latent variables, from the observations. We propose a novel approximate message passing (AMP) algorithm for estimation in a matrix GLM and rigorously characterize its performance in the high-dimensional limit. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.12398</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;Transformers&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20854;&#26680;&#24515;&#26159;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24402;&#32435;&#20559;&#35265;&#65292;&#36890;&#36807;&#21152;&#26435;&#22522;&#30784;&#23558;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;token&#19982;&#27599;&#20010;&#20854;&#20182;token&#30456;&#20851;&#32852;&#12290;&#26631;&#20934;&#30340;SA&#26426;&#21046;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38590;&#20197;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;Multiscale Wavelet Attention&#65288;MWA&#65289;&#65292;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;CIFAR&#21644;ImageNet&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MWA&#27604;ViT&#21644;AFNO&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11582</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36866;&#24212;&#24615;&#23454;&#39564;&#65306;&#28789;&#27963;&#25209;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#20551;&#23450;&#25345;&#32493;&#37325;&#26032;&#20998;&#37197;&#27979;&#37327;&#24037;&#20316;&#65292;&#36825;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#23384;&#22312;&#24310;&#36831;&#21453;&#39304;&#21644;&#22522;&#30784;&#35774;&#26045;/&#32452;&#32455;&#38590;&#39064;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20165;&#26377;&#23569;&#25968;&#37325;&#26032;&#20998;&#37197;&#38454;&#27573;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#20854;&#20013;&#27979;&#37327;&#32467;&#26524;&#26159;&#20197;&#25209;&#22788;&#29702;&#24418;&#24335;&#27979;&#37327;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#27491;&#24577;&#36817;&#20284;&#20063;&#21487;&#20197;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#12290;&#36890;&#36807;&#25512;&#23548;&#28176;&#36827;&#39034;&#24207;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#65292;&#21487;&#20197;&#21033;&#29992;&#24179;&#22343;&#22238;&#25253;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#21160;&#24577;&#35268;&#21010;&#30340;&#29366;&#24577;&#36716;&#31227;&#30456;&#23545;&#20110;&#37319;&#26679;&#20998;&#37197;&#26159;&#21487;&#24494;&#30340;&#65292;&#20801;&#35768;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#35268;&#21010;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#35268;&#21010;&#26041;&#27861;&#65292;&#21363;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#35268;&#21010;&#30446;&#26631;&#26469;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
&lt;/p&gt;</description></item><item><title>FUSQA&#26159;&#19968;&#20010;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#36807;&#31243;&#23450;&#20026;&#33258;&#21160;&#21270;&#20998;&#31867;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23381;&#40836;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.04418</link><description>&lt;p&gt;
FUSQA: &#32974;&#20799;&#36229;&#22768;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FUSQA: Fetal Ultrasound Segmentation Quality Assessment. (arXiv:2303.04418v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04418
&lt;/p&gt;
&lt;p&gt;
FUSQA&#26159;&#19968;&#20010;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#36807;&#31243;&#23450;&#20026;&#33258;&#21160;&#21270;&#20998;&#31867;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23381;&#40836;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#32974;&#20799;&#36229;&#22768;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#21521;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#36716;&#25442;&#38656;&#35201;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36136;&#37327;&#20445;&#35777;&#27969;&#31243;&#26469;&#39564;&#35777;&#36716;&#25442;&#21518;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#65292;&#20854;&#20013;&#38382;&#39064;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;dice&#20998;&#25968;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#32974;&#20799;&#36229;&#22768;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#65288;FUSQA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24403;&#19981;&#23384;&#22312;&#29992;&#20110;&#27604;&#36739;&#30340;&#25513;&#30721;&#26102;&#30340;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#20998;&#21106;&#36136;&#37327;&#35780;&#20272;&#36807;&#31243;&#23450;&#20026;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20248;&#36136;&#21644;&#20302;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23381;&#40836;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#20174;&#20004;&#20010;&#21307;&#38498;&#25910;&#38598;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been effective for various fetal ultrasound segmentation tasks. However, generalization to new unseen data has raised questions about their effectiveness for clinical adoption. Normally, a transition to new unseen data requires time-consuming and costly quality assurance processes to validate the segmentation performance post-transition. Segmentation quality assessment efforts have focused on natural images, where the problem has been typically formulated as a dice score regression task. In this paper, we propose a simplified Fetal Ultrasound Segmentation Quality Assessment (FUSQA) model to tackle the segmentation quality assessment when no masks exist to compare with. We formulate the segmentation quality assessment process as an automated classification task to distinguish between good and poor-quality segmentation masks for more accurate gestational age estimation. We validate the performance of our proposed approach on two datasets we collect from two hosp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#19982;&#19968;&#20010;&#26368;&#20339;&#30340;&#24050;&#30693;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#36739;&#22823;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30456;&#31454;&#20105;&#65292;&#20174;&#32780;&#20351;&#22522;&#20934;&#31639;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#20540;&#25968;&#25454;&#38598;&#19978;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#23454;&#29616;&#23454;&#20363;&#20248;&#21270;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#23545;&#22343;&#20540;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20272;&#35745;&#19968;&#31867;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#26102;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01262</link><description>&lt;p&gt;
&#38544;&#31169;&#20272;&#35745;&#20013;&#22522;&#20110;&#23376;&#38598;&#30340;&#23454;&#20363;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subset-Based Instance Optimality in Private Estimation. (arXiv:2303.01262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#26469;&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#19982;&#19968;&#20010;&#26368;&#20339;&#30340;&#24050;&#30693;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#36739;&#22823;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30456;&#31454;&#20105;&#65292;&#20174;&#32780;&#20351;&#22522;&#20934;&#31639;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#20540;&#25968;&#25454;&#38598;&#19978;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#23454;&#29616;&#23454;&#20363;&#20248;&#21270;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#23545;&#22343;&#20540;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20272;&#35745;&#19968;&#31867;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#26102;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#31639;&#27861;&#30340;&#23454;&#20363;&#20248;&#21270;&#30340;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#35201;&#27714;&#26368;&#20248;&#31639;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;$D$&#19978;&#37117;&#19982;&#26368;&#20339;&#30340;&#24050;&#30693;$D$&#24182;&#20197;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#23545;$D$&#30340;&#22823;&#23376;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#38544;&#31169;&#22522;&#20934;&#31639;&#27861;&#21516;&#26102;&#31454;&#20105;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22522;&#20934;&#31639;&#27861;&#22312;&#28508;&#22312;&#30340;&#26497;&#31471;&#28857;&#34987;&#28155;&#21152;&#21040;$D$&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#22909;&#65307;&#23427;&#21482;&#38656;&#35201;&#22788;&#29702;&#21024;&#38500;&#24050;&#32463;&#23384;&#22312;&#30340;&#19968;&#23567;&#37096;&#20998;&#30495;&#23454;&#25968;&#25454;&#28857;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#22522;&#20934;&#31639;&#27861;&#27604;&#20043;&#21069;&#25552;&#20986;&#30340;&#22522;&#20934;&#31639;&#27861;&#26174;&#33879;&#26356;&#24378;&#22823;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#20173;&#28982;&#23637;&#31034;&#20102;&#23545;&#20110;&#23454;&#20540;&#25968;&#25454;&#38598;&#65292;&#22914;&#20309;&#26500;&#36896;&#33021;&#22815;&#23454;&#29616;&#25105;&#20204;&#30340;&#23454;&#20363;&#20248;&#21270;&#27010;&#24565;&#30340;&#38544;&#31169;&#31639;&#27861;&#65292;&#20197;&#20272;&#35745;&#21253;&#25324;&#22343;&#20540;&#12289;&#20998;&#20301;&#25968;&#21644;$\ell_p$-&#33539;&#25968;&#26368;&#23567;&#21270;&#22120;&#22312;&#20869;&#30340;&#24191;&#27867;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#23646;&#24615;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#22343;&#20540;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#21305;&#37197;&#25110;&#36229;&#36807;&#20102;&#28176;&#36817;&#30340;p
&lt;/p&gt;
&lt;p&gt;
We propose a new definition of instance optimality for differentially private estimation algorithms. Our definition requires an optimal algorithm to compete, simultaneously for every dataset $D$, with the best private benchmark algorithm that (a) knows $D$ in advance and (b) is evaluated by its worst-case performance on large subsets of $D$. That is, the benchmark algorithm need not perform well when potentially extreme points are added to $D$; it only has to handle the removal of a small number of real data points that already exist. This makes our benchmark significantly stronger than those proposed in prior work. We nevertheless show, for real-valued datasets, how to construct private algorithms that achieve our notion of instance optimality when estimating a broad class of dataset properties, including means, quantiles, and $\ell_p$-norm minimizers. For means in particular, we provide a detailed analysis and show that our algorithm simultaneously matches or exceeds the asymptotic p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#33410;&#28857;&#26469;&#24212;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;&#26679;&#26412;&#19981;&#36275;&#21644;&#20559;&#20506;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14061</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#35821;&#20041;&#24863;&#30693;&#33410;&#28857;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Node Synthesis for Imbalanced Heterogeneous Information Networks. (arXiv:2302.14061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14061
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#33410;&#28857;&#26469;&#24212;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;&#26679;&#26412;&#19981;&#36275;&#21644;&#20559;&#20506;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#22312;&#24314;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HINs&#65289;&#20013;&#30340;&#22797;&#26434;&#24322;&#36136;&#24615;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#25928;&#33021;&#12290;HGNNs&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#36890;&#36807;&#25552;&#21462;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;HINs&#20013;&#19981;&#21516;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;HINs&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#20026;&#29616;&#26377;&#30340;HGNNs&#21019;&#24314;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#38500;&#20102;&#33410;&#28857;&#25968;&#37327;&#30340;&#19981;&#24179;&#34913;&#22806;&#65292;HINs&#20013;&#26356;&#20851;&#38190;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#26159;&#35821;&#20041;&#19981;&#24179;&#34913;&#12290;HINs&#20013;&#30340;&#23569;&#25968;&#31867;&#21035;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#21270;&#21644;&#36275;&#22815;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#23548;&#33268;&#20559;&#20506;&#21644;&#19981;&#23436;&#25972;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#35821;&#20041;&#19981;&#24179;&#34913;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#20934;&#30830;&#20998;&#31867;&#23569;&#25968;&#33410;&#28857;&#30340;&#22256;&#38590;&#65292;&#23548;&#33268;HGNNs&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#23569;&#25968;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#21644;&#34917;&#20805;&#20854;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (HGNNs) have exhibited exceptional efficacy in modeling the complex heterogeneity in heterogeneous information networks (HINs). The critical advantage of HGNNs is their ability to handle diverse node and edge types in HINs by extracting and utilizing the abundant semantic information for effective representation learning. However, as a widespread phenomenon in many real-world scenarios, the class-imbalance distribution in HINs creates a performance bottleneck for existing HGNNs. Apart from the quantity imbalance of nodes, another more crucial and distinctive challenge in HINs is semantic imbalance. Minority classes in HINs often lack diverse and sufficient neighbor nodes, resulting in biased and incomplete semantic information. This semantic imbalance further compounds the difficulty of accurately classifying minority nodes, leading to the performance degradation of HGNNs. To tackle the imbalance of minority classes and supplement their inadequate se
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Ritz&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.13163</link><description>&lt;p&gt;
&#21033;&#29992;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#23454;&#29616;PINNs&#30340;&#39640;&#20934;&#30830;&#24230;
&lt;/p&gt;
&lt;p&gt;
Achieving High Accuracy with PINNs via Energy Natural Gradients. (arXiv:2302.13163v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13163
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Ritz&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Hessian&#35825;&#23548;&#30340;&#40654;&#26364;&#24230;&#37327;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#65292;&#20316;&#20026;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Ritz&#26041;&#27861;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20316;&#20026;&#20027;&#35201;&#21160;&#26426;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#30340;&#26356;&#26032;&#26041;&#21521;&#22312;&#20989;&#25968;&#31354;&#38388;&#30340;&#32467;&#26524;&#19982;&#29275;&#39039;&#26041;&#21521;&#27169;&#38500;&#27169;&#22411;&#20999;&#31354;&#38388;&#30340;&#27491;&#20132;&#25237;&#24433;&#30456;&#23545;&#24212;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#21363;&#20351;&#20801;&#35768;&#26631;&#20934;&#20248;&#21270;&#22120;&#22914;&#26799;&#24230;&#19979;&#38477;&#25110;Adam&#20855;&#26377;&#26356;&#38271;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#33021;&#37327;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#20173;&#28982;&#33021;&#20135;&#29983;&#27604;&#20182;&#20204;&#30340;&#35823;&#24046;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#39640;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose energy natural gradient descent, a natural gradient method with respect to a Hessian-induced Riemannian metric as an optimization algorithm for physics-informed neural networks (PINNs) and the deep Ritz method. As a main motivation we show that the update direction in function space resulting from the energy natural gradient corresponds to the Newton direction modulo an orthogonal projection onto the model's tangent space. We demonstrate experimentally that energy natural gradient descent yields highly accurate solutions with errors several orders of magnitude smaller than what is obtained when training PINNs with standard optimizers like gradient descent or Adam, even when those are allowed significantly more computation time.
&lt;/p&gt;</description></item><item><title>SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12449</link><description>&lt;p&gt;
SGL-PT: &#19968;&#31181;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12449
&lt;/p&gt;
&lt;p&gt;
SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#35774;&#35745;&#22270;&#24418;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21069;&#25991;&#20219;&#21153;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#36317;&#65292;&#36825;&#19981;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#23548;&#33268;&#36127;&#20256;&#36882;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#23545;&#40784;&#65292;&#25552;&#31034;&#35843;&#20248;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#22270;&#39046;&#22495;&#20013;&#21508;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#32570;&#20047;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35774;&#35745;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#26082;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20063;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGL-PT&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36981;&#24490;&#23398;&#20064;&#31574;&#30053;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#21644;&#39044;&#27979;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26399;&#26395;&#25913;&#36827;&#65288;REI&#65289;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#24615;&#26041;&#27861;&#24341;&#20837;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22312;&#24191;&#38420;&#30340;&#21560;&#24341;&#22495;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08612</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#26399;&#26395;&#25913;&#36827;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust expected improvement for Bayesian optimization. (arXiv:2302.08612v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26399;&#26395;&#25913;&#36827;&#65288;REI&#65289;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#24615;&#26041;&#27861;&#24341;&#20837;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22312;&#24191;&#38420;&#30340;&#21560;&#24341;&#22495;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#23558;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#25311;&#21512;&#19982;&#39034;&#24207;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#20363;&#22914;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22914;&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#65292;&#22312;&#20005;&#26684;&#30340;&#35780;&#20272;&#39044;&#31639;&#19979;&#24179;&#34913;&#21208;&#25506;&#21644;&#24320;&#21457;&#65292;&#20197;&#25552;&#20379;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#35299;&#20915;&#40065;&#26834;&#26368;&#20248;&#35299;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#65292;&#24847;&#21619;&#30528;&#26356;&#24191;&#38420;&#30340;&#21560;&#24341;&#22495;&#20013;&#20248;&#20808;&#32771;&#34385;&#35299;&#20915;&#26041;&#26696;&#12290;&#24403;&#36755;&#20837;&#19981;&#31934;&#30830;&#25351;&#23450;&#25110;&#38656;&#35201;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#24456;&#26377;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#35265;&#30340;&#25968;&#23398;&#35268;&#21010;&#25216;&#26415;&#28041;&#21450;&#23545;&#25239;&#24615;&#30446;&#26631;&#65292;&#23558;&#23616;&#37096;&#27714;&#35299;&#22120;&#20174;&#8220;&#23574;&#38160;&#8221;&#30340;&#20302;&#35895;&#20559;&#31163;&#12290;&#22312;&#25551;&#36848;&#26041;&#27861;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#40065;&#26834;&#26399;&#26395;&#25913;&#36827;&#65288;REI&#65289;&#30340;&#25311;&#21512;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#23545;&#25239;&#26041;&#27861;&#24341;&#20837;&#20102;BO / GP&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#24182;&#23545;&#27604;&#20102;&#20960;&#20010;&#31454;&#20105;&#23545;&#25163;&#22312;&#22522;&#20934;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) links Gaussian Process (GP) surrogates with sequential design toward optimizing expensive-to-evaluate black-box functions. Example design heuristics, or so-called acquisition functions, like expected improvement (EI), balance exploration and exploitation to furnish global solutions under stringent evaluation budgets. However, they fall short when solving for robust optima, meaning a preference for solutions in a wider domain of attraction. Robust solutions are useful when inputs are imprecisely specified, or where a series of solutions is desired. A common mathematical programming technique in such settings involves an adversarial objective, biasing a local solver away from ``sharp'' troughs. Here we propose a surrogate modeling and active learning technique called robust expected improvement (REI) that ports adversarial methodology into the BO/GP framework. After describing the methods, we illustrate and draw comparisons to several competitors on benchmark s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.06433</link><description>&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Label-efficient Time Series Representation Learning: A Review. (arXiv:2302.06433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#36801;&#31227;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#20013;&#33719;&#21462;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#26469;&#26681;&#25454;&#23427;&#20204;&#23545;&#22806;&#37096;&#25968;&#25454;&#28304;&#30340;&#20381;&#36182;&#65292;&#23545;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#26356;&#22909;&#36827;&#23637;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of labeled data is one of the main challenges of applying deep learning models on time series data in the real world. Therefore, several approaches, e.g., transfer learning, self-supervised learning, and semi-supervised learning, have been recently developed to promote the learning capability of deep learning models from the limited time series labels. In this survey, for the first time, we provide a novel taxonomy to categorize existing approaches that address the scarcity of labeled data problem in time series data based on their dependency on external data sources. Moreover, we present a review of the recent advances in each approach and conclude the limitations of the current works and provide future directions that could yield better progress in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26679;&#26412;&#19981;&#24179;&#34913;&#21450;&#26631;&#35760;&#19981;&#31934;&#30830;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#20165;&#29992;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#21363;&#21487;&#35757;&#32451;&#20855;&#31454;&#20105;&#21147;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.03224</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#21892;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#30340;&#19979;&#37319;&#26679;&#21644;&#32047;&#31215;&#31867;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia. (arXiv:2302.03224v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26679;&#26412;&#19981;&#24179;&#34913;&#21450;&#26631;&#35760;&#19981;&#31934;&#30830;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#20165;&#29992;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#21363;&#21487;&#35757;&#32451;&#20855;&#31454;&#20105;&#21147;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#28608;&#21160;&#26159;&#30196;&#21574;&#30151;&#24739;&#32773;&#26368;&#24120;&#35265;&#30340;&#30151;&#29366;&#20043;&#19968;&#65292;&#21487;&#33021;&#20250;&#23545;&#20182;&#20204;&#21644;&#30475;&#25252;&#32773;&#30340;&#23433;&#20840;&#36896;&#25104;&#23041;&#32961;&#12290;&#24320;&#21457;&#23458;&#35266;&#30340;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#26041;&#27861;&#23545;&#25903;&#25345;&#22312;&#23621;&#20303;&#29615;&#22659;&#20013;&#29983;&#27963;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#30340;&#20581;&#24247;&#21644;&#23433;&#20840;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#19981;&#21516;&#30340;&#19979;&#37319;&#26679;&#26041;&#27861;&#28040;&#38500;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#21152;&#26435;&#19979;&#37319;&#26679;&#26041;&#27861;&#26469;&#35780;&#20272;&#25163;&#21160;&#26631;&#35760;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25913;&#21892;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#36275;&#20197;&#35757;&#32451;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agitation is one of the most prevalent symptoms in people with dementia (PwD) that can place themselves and the caregiver's safety at risk. Developing objective agitation detection approaches is important to support health and safety of PwD living in a residential setting. In a previous study, we collected multimodal wearable sensor data from 17 participants for 600 days and developed machine learning models for predicting agitation in one-minute windows. However, there are significant limitations in the dataset, such as imbalance problem and potential imprecise labels as the occurrence of agitation is much rarer in comparison to the normal behaviours. In this paper, we first implement different undersampling methods to eliminate the imbalance problem, and come to the conclusion that only 20\% of normal behaviour data are adequate to train a competitive agitation detection model. Then, we design a weighted undersampling method to evaluate the manual labeling mechanism given the ambiguo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#65292;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#24182;&#25193;&#23637;&#20026;DCEM-P&#20197;&#28385;&#36275;&#26356;&#22810;&#26041;&#31243;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.01538</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#30340;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;&#24212;&#29992;&#20110;&#22266;&#20307;&#21147;&#23398;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A deep complementary energy method for solid mechanics using minimum complementary energy principle. (arXiv:2302.01538v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#65292;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#24182;&#25193;&#23637;&#20026;DCEM-P&#20197;&#28385;&#36275;&#26356;&#22810;&#26041;&#31243;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#65292;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#24322;&#36924;&#36817;&#33021;&#21147;&#12290;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;(PINNs)&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;(DEM)&#22312;&#35299;&#20915;PDE&#26041;&#38754;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#23567;&#21183;&#33021;&#21407;&#29702;&#21644;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#26159;&#22266;&#20307;&#21147;&#23398;&#20013;&#20004;&#20010;&#37325;&#35201;&#30340;&#21464;&#20998;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;DEM&#26159;&#22522;&#20110;&#26368;&#23567;&#21183;&#33021;&#21407;&#29702;&#65292;&#20294;&#23427;&#32570;&#20047;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#30340;&#37325;&#35201;&#24418;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#30340;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#12290;DCEM&#30340;&#36755;&#20986;&#20989;&#25968;&#26159;&#24212;&#21147;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;DCEM&#25193;&#23637;&#21040;DCEM-Plus (DCEM-P)&#65292;&#28155;&#21152;&#28385;&#36275;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#31639;&#23376;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of deep learning has significantly impacted various fields, particularly in solving partial differential equations (PDEs) in solid mechanics, benefiting greatly from the remarkable approximation capabilities of neural networks. In solving PDEs, Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) have garnered substantial attention. The principle of minimum potential energy and complementary energy are two important variational principles in solid mechanics. However,DEM is based on the principle of minimum potential energy, but it lacks the important form of minimum complementary energy. To bridge this gap, we propose the deep complementary energy method (DCEM) based on the principle of minimum complementary energy. The output function of DCEM is the stress function. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfy partial differential equations. Furthermore, we propose a deep complementary energy operator metho
&lt;/p&gt;</description></item><item><title>InfiniCity&#26159;&#19968;&#20010;&#26080;&#38480;&#35268;&#27169;&#30340;&#22478;&#24066;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2301.09637</link><description>&lt;p&gt;
InfiniCity: &#26080;&#38480;&#35268;&#27169;&#22478;&#24066;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
InfiniCity: Infinite-Scale City Synthesis. (arXiv:2301.09637v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09637
&lt;/p&gt;
&lt;p&gt;
InfiniCity&#26159;&#19968;&#20010;&#26080;&#38480;&#35268;&#27169;&#30340;&#22478;&#24066;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26080;&#38480;&#35268;&#27169;&#30340;3D&#22478;&#24066;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;InfiniCity&#65292;&#23427;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#12290;InfiniCity&#23558;&#30475;&#20284;&#19981;&#21487;&#34892;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#19977;&#20010;&#21487;&#34892;&#27169;&#22359;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;2D&#21644;3D&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#26080;&#38480;&#20687;&#32032;&#22270;&#20687;&#21512;&#25104;&#27169;&#22359;&#20174;&#40479;&#30640;&#22270;&#29983;&#25104;&#20219;&#24847;&#23610;&#24230;&#30340;2D&#22320;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#22522;&#20110;&#20843;&#21449;&#26641;&#30340;&#20307;&#32032;&#23436;&#25104;&#27169;&#22359;&#23558;&#29983;&#25104;&#30340;2D&#22320;&#22270;&#36716;&#25442;&#20026;3D&#20843;&#21449;&#26641;&#12290;&#26368;&#21518;&#65292;&#19968;&#20010;&#22522;&#20110;&#20307;&#32032;&#30340;&#31070;&#32463;&#28210;&#26579;&#27169;&#22359;&#32473;&#20307;&#32032;&#36148;&#19978;&#32441;&#29702;&#24182;&#28210;&#26579;2D&#22270;&#20687;&#12290;InfiniCity&#33021;&#22815;&#21512;&#25104;&#20219;&#24847;&#23610;&#24230;&#21644;&#21487;&#36941;&#21382;&#30340;3D&#22478;&#24066;&#29615;&#22659;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36827;&#34892;&#28789;&#27963;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04900</link><description>&lt;p&gt;
&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#36817;&#20284;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32570;&#20047;&#26126;&#30830;&#21407;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#19968;&#31867;&#30001;&#32593;&#32476;&#37051;&#25509;&#30697;&#38453;&#32806;&#21512;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#31038;&#20132;&#21644;&#31070;&#32463;&#31995;&#32479;&#65292;&#23646;&#20110;&#36825;&#31867;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#36825;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#24378;&#35843;&#19982;&#38745;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#25552;&#20513;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#32463;&#20856;&#20551;&#35774;&#20043;&#22806;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#25512;&#26029;&#26102;&#20272;&#35745;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#29992;&#30340;&#31354;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#21508;&#31181;&#22797;&#26434;&#32593;&#32476;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21457;&#29616;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#65292;&#22312;&#25209;&#24402;&#19968;&#21270;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.02982</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25209;&#24402;&#19968;&#21270;&#20250;&#25439;&#23475;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Batch Normalization Damage Federated Learning on Non-IID Data?. (arXiv:2301.02982v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21457;&#29616;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#65292;&#22312;&#25209;&#24402;&#19968;&#21270;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22312;&#32593;&#32476;&#36793;&#32536;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#36793;&#32536;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#20026;&#20102;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;DNN&#27169;&#22411;&#65292;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#36895;&#35757;&#32451;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;BN&#20250;&#26174;&#33879;&#25439;&#23475;FL&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;FL&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#38598;&#20013;&#24335;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#27809;&#26377;&#25552;&#20379;&#20851;&#20110;BN&#22914;&#20309;&#25439;&#23475;FL&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#65292;BN&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a promising distributed learning paradigm, federated learning (FL) involves training deep neural network (DNN) models at the network edge while protecting the privacy of the edge clients. To train a large-scale DNN model, batch normalization (BN) has been regarded as a simple and effective means to accelerate the training and improve the generalization capability. However, recent findings indicate that BN can significantly impair the performance of FL in the presence of non-i.i.d. data. While several FL algorithms have been proposed to address this issue, their performance still falls significantly when compared to the centralized scheme. Furthermore, none of them have provided a theoretical explanation of how the BN damages the FL convergence. In this paper, we present the first convergence analysis to show that under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models, which, 
&lt;/p&gt;</description></item><item><title>FedICT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00389</link><description>&lt;p&gt;
FedICT:&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
FedICT: Federated Multi-task Distillation for Multi-access Edge Computing. (arXiv:2301.00389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00389
&lt;/p&gt;
&lt;p&gt;
FedICT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31227;&#21160;&#35774;&#22791;&#26234;&#33021;&#26381;&#21153;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22810;&#26679;&#30340;&#29992;&#25143;&#34892;&#20026;&#35201;&#27714;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#20351;&#29992;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#26469;&#20026;&#19981;&#21516;&#35774;&#22791;&#35757;&#32451;&#30456;&#20851;&#20294;&#20010;&#24615;&#21270;&#30340;ML&#27169;&#22411;&#65292;&#28982;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36807;&#22810;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#24573;&#35270;&#20102;MEC&#20013;&#35774;&#22791;&#20043;&#38388;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;&#23558;&#30693;&#35782;&#33976;&#39311;&#24341;&#20837;FMTL&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#23454;&#38469;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#65288;FedICT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Federated MultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2212.13925</link><description>&lt;p&gt;
&#36136;&#37327;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32771;&#34385;&#21040;&#25512;&#29702;&#36136;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20005;&#33499;&#30340;&#29615;&#22659;&#19979;&#65292;&#35201;&#27714;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#25351;&#26631;&#30340;&#35201;&#27714;&#12290;&#24573;&#35270;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#26041;&#38754;&#37117;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21644;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65292;&#21253;&#25324;&#20154;&#21592;&#20260;&#20129;&#21644;&#36130;&#20135;&#25439;&#22833;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#20840;&#38754;&#32771;&#34385;&#65292;&#36890;&#24120;&#22312;&#29702;&#24819;&#25110;&#23485;&#26494;&#26465;&#20214;&#19979;&#36827;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#19981;&#23436;&#25972;&#25110;&#19981;&#30452;&#35266;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#30340;&#27874;&#21160;&#65292;&#36827;&#19968;&#27493;&#32473;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#20998;&#24067;&#23614;&#37096;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25512;&#26029;&#21151;&#33021;&#65292;&#24182;&#22312;&#39046;&#20808;&#30340;&#35745;&#31639;&#35774;&#26045;&#20013;&#36827;&#34892;&#20102;&#37096;&#32626;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.11317</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#23646;&#24615;&#30340;&#31471;&#21040;&#31471;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
End-to-end AI framework for interpretable prediction of molecular and crystal properties. (arXiv:2212.11317v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11317
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25512;&#26029;&#21151;&#33021;&#65292;&#24182;&#22312;&#39046;&#20808;&#30340;&#35745;&#31639;&#35774;&#26045;&#20013;&#36827;&#34892;&#20102;&#37096;&#32626;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;DeepHyper&#24211;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;AI&#25512;&#26029;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#65292;&#21253;&#25324;CGCNN&#12289;PhysNet&#12289;SchNet&#12289;MPNN&#12289;MPNN-transformer&#21644;TorchMD-NET&#12290;&#25105;&#20204;&#21033;&#29992;QM9&#12289;hMOF&#21644;MD17&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22312;&#29616;&#20195;&#35745;&#31639;&#29615;&#22659;&#20013;&#39044;&#27979;&#29992;&#25143;&#25351;&#23450;&#30340;&#26448;&#26009;&#23646;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#32479;&#19968;&#12289;&#29420;&#31435;&#26694;&#26550;&#22312;&#23567;&#20998;&#23376;&#12289;&#26080;&#26426;&#26230;&#20307;&#21644;&#32435;&#31859;&#22810;&#23380;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#24314;&#27169;&#20013;&#30340;&#21487;&#36801;&#31227;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;Argonne&#39046;&#23548;&#35745;&#31639;&#35774;&#26045;&#30340;ThetaGPU&#36229;&#32423;&#35745;&#31639;&#26426;&#21644;&#22269;&#23478;&#36229;&#32423;&#35745;&#31639;&#24212;&#29992;&#20013;&#24515;&#30340;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#21644;&#27979;&#35797;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#21152;&#36895;&#30340;AI&#39537;&#21160;&#21457;&#29616;&#30340;&#29616;&#20195;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an end-to-end computational framework that allows for hyperparameter optimization using the DeepHyper library, accelerated model training, and interpretable AI inference. The framework is based on state-of-the-art AI models including CGCNN, PhysNet, SchNet, MPNN, MPNN-transformer, and TorchMD-NET. We employ these AI models along with the benchmark QM9, hMOF, and MD17 datasets to showcase how the models can predict user-specified material properties within modern computing environments. We demonstrate transferable applications in the modeling of small molecules, inorganic crystals and nanoporous metal organic frameworks with a unified, standalone framework. We have deployed and tested this framework in the ThetaGPU supercomputer at the Argonne Leadership Computing Facility, and in the Delta supercomputer at the National Center for Supercomputing Applications to provide researchers with modern tools to conduct accelerated AI-driven discovery in leadership-class computing env
&lt;/p&gt;</description></item><item><title>FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01197</link><description>&lt;p&gt;
FedALA: &#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01197
&lt;/p&gt;
&lt;p&gt;
FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#36825;&#20250;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated learning with Adaptive Local Aggregation&#65288;FedALA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#25429;&#25417;&#20840;&#23616;&#27169;&#22411;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;FedALA&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#23616;&#37096;&#30446;&#26631;&#33258;&#36866;&#24212;&#32858;&#21512;&#19979;&#36733;&#30340;&#20840;&#23616;&#27169;&#22411;&#21644;&#26412;&#22320;&#27169;&#22411;&#20197;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21021;&#22987;&#21270;&#26412;&#22320;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;FedALA&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20351;&#29992;&#20102;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;FedALA&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#27604;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22810;3.27%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;ALA&#27169;&#22359;&#24212;&#29992;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#22810;24.19%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MELD-FAIR&#26469;&#35299;&#20915;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#37325;&#26032;&#23545;&#40784;&#20102;MELD&#35270;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#33719;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;</title><link>http://arxiv.org/abs/2211.15377</link><description>&lt;p&gt;
&#35841;&#30340;&#24773;&#32490;&#26356;&#37325;&#35201;&#65311;&#27809;&#26377;&#20808;&#21069;&#30693;&#35782;&#30340;&#35828;&#35805;&#27963;&#21160;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MELD-FAIR&#26469;&#35299;&#20915;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#37325;&#26032;&#23545;&#40784;&#20102;MELD&#35270;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#33719;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#30340;&#20219;&#21153;&#21463;&#30410;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#65292;&#20363;&#22914;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#32447;&#25968;&#25454;&#38598;&#65288;MELD&#65289;&#20013;&#25552;&#20379;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#26041;&#27861;&#20351;&#29992;&#20102;MELD&#35270;&#39057;&#20013;&#30340;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#36825;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;MELD&#20013;&#30340;&#26631;&#31614;&#21040;&#35270;&#39057;&#30340;&#23545;&#40784;&#26159;&#26377;&#22122;&#22768;&#30340;&#65292;&#36825;&#20351;&#24471;&#37027;&#20123;&#35270;&#39057;&#25104;&#20026;&#20102;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#30340;&#19981;&#21487;&#38752;&#26469;&#28304;&#12290;&#20854;&#27425;&#65292;&#20250;&#35805;&#21487;&#20197;&#28041;&#21450;&#21040;&#21516;&#19968;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#20154;&#65292;&#36825;&#38656;&#35201;&#23450;&#20301;&#35805;&#35821;&#26469;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#36817;&#30340;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#22266;&#23450;&#38899;&#39057;&#35270;&#35273;&#20449;&#24687;&#30340;MELD-FAIR&#65292;&#33021;&#22815;&#37325;&#26032;&#23545;&#40784;MELD&#35270;&#39057;&#24182;&#25429;&#33719;96.92&#65285;&#30340;MELD&#20013;&#25552;&#20379;&#30340;&#35805;&#35821;&#30340;&#35762;&#35805;&#32773;&#38754;&#37096;&#34920;&#24773;&#12290;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#22768;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37325;&#26032;&#23545;&#40784;&#30340;MELD-FAIR&#35270;&#39057;&#26356;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl
&lt;/p&gt;</description></item><item><title>uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12872</link><description>&lt;p&gt;
{\mu}Split: &#26174;&#24494;&#38236;&#25968;&#25454;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
{\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12872
&lt;/p&gt;
&lt;p&gt;
uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; uSplit&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#35757;&#32451;&#22270;&#20687;&#20998;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24120;&#35268;&#30340;&#28145;&#24230;&#32467;&#26500;&#20307;&#31995;&#32467;&#26500;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#22823;&#22270;&#20687;&#22359;&#20250;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#20351;&#20869;&#23384;&#28040;&#32791;&#25104;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65288;LC&#65289;&#65292;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#24378;&#22823;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;LC&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#22987;&#32456;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;LC&#19982;U-Nets&#12289;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;VAEs&#38598;&#25104;&#65292;&#20026;&#27492;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;ELBO loss&#12290;&#27492;&#22806;&#65292;LC&#20351;&#24471;&#35757;&#32451;&#27604;&#21407;&#26412;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20943;&#23569;&#20351;&#29992;&#20998;&#21106;VAE&#39044;&#27979;&#26102;&#19981;&#21487;&#36991;&#20813;&#30340;&#24179;&#38138;&#20266;&#24433;&#12290;&#25105;&#20204;&#23558;uSplit&#24212;&#29992;&#20110;&#20116;&#20010;&#20998;&#35299;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21478;&#22806;&#22235;&#20010;&#26469;&#33258;&#23454;&#38469;&#26174;&#24494;&#38236;&#25968;&#25454;&#12290;LC&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#65288;&#24179;&#22343;im&#65289;
&lt;/p&gt;
&lt;p&gt;
We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.08413</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#26694;&#26550;&#12289;&#36235;&#21183;&#21644;&#25361;&#25112; (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#33258;&#38382;&#19990;&#20197;&#26469;&#65292;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#24515;&#21270;&#23454;&#20307;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20013;&#24515;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#29942;&#39048;&#22686;&#21152;&#12289;&#31995;&#32479;&#25925;&#38556;&#39118;&#38505;&#22686;&#39640;&#65292;&#24433;&#21709;&#36127;&#36131;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#30340;&#23454;&#20307;&#30340;&#21487;&#20449;&#24230;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#25512;&#24191;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#24182;&#26368;&#23567;&#21270;&#23545;&#20013;&#24515;&#21270;&#26550;&#26500;&#30340;&#20381;&#36182;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#22312;DFL&#26041;&#38754;&#26377;&#25152;&#21162;&#21147;&#65292;&#25991;&#29486;&#36824;&#27809;&#26377;&#30740;&#31350;(i)DFL&#21644;CFL&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;;(ii)&#20998;&#26512;DFL&#26694;&#26550;&#20197;&#21019;&#24314;&#21644;&#35780;&#20272;&#26032;&#35299;&#20915;&#26041;&#26696;;(iii)&#22238;&#39038;&#20351;&#29992;DFL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#32852;&#37030;&#26550;&#26500;&#12289;&#23433;&#20840;&#24615;&#12289;&#36890;&#20449;&#31561;&#26041;&#38754;&#35782;&#21035;&#24182;&#20998;&#26512;&#20102;DFL&#30340;&#20027;&#35201;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#20869;&#26680;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#23567;&#35268;&#27169;&#33267;&#20013;&#31561;&#35268;&#27169;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36127;&#25285;&#36716;&#31227;&#21040;&#32463;&#20856;&#20248;&#21270;&#22120;&#32452;&#20214;&#19978;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#26597;&#35810;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#20195;&#29702;&#27169;&#22411;&#30456;&#27604;&#20256;&#32479;&#32463;&#20856;&#20869;&#26680;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.01134</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#20869;&#26680;&#30340;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster variational quantum algorithms with quantum kernel-based surrogate models. (arXiv:2211.01134v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01134
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#20869;&#26680;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#23567;&#35268;&#27169;&#33267;&#20013;&#31561;&#35268;&#27169;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36127;&#25285;&#36716;&#31227;&#21040;&#32463;&#20856;&#20248;&#21270;&#22120;&#32452;&#20214;&#19978;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#26597;&#35810;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#20195;&#29702;&#27169;&#22411;&#30456;&#27604;&#20256;&#32479;&#32463;&#20856;&#20869;&#26680;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#36827;&#34892;&#23567;&#35268;&#27169;&#33267;&#20013;&#31561;&#35268;&#27169;&#30340;&#21464;&#20998;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#37197;&#22791;&#32463;&#20856;&#35780;&#20272;&#30340;&#37327;&#23376;&#20869;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#21464;&#20998;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20294;&#22312;&#24403;&#21069;&#30340;&#22122;&#22768;&#35774;&#22791;&#19978;&#23454;&#26045;&#36215;&#26469;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#23558;&#36825;&#20010;&#35745;&#31639;&#36127;&#25285;&#36716;&#31227;&#21040;&#36825;&#20123;&#28151;&#21512;&#31639;&#27861;&#30340;&#32463;&#20856;&#20248;&#21270;&#22120;&#32452;&#20214;&#19978;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#26597;&#35810;&#27425;&#25968;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21464;&#20998;&#37327;&#23376;&#29305;&#24449;&#20540;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#31639;&#27861;&#65292;&#24182;&#32463;&#36807;&#25968;&#20540;&#39564;&#35777;&#34920;&#26126;&#65292;&#36825;&#31181;&#20195;&#29702;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#35813;&#31639;&#27861;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#21040;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#30340;VQE&#27169;&#25311;&#20013;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#26368;&#32456;&#20934;&#30830;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#32463;&#20856;&#20869;&#26680;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new optimization method for small-to-intermediate scale variational algorithms on noisy near-term quantum processors which uses a Gaussian process surrogate model equipped with a classically-evaluated quantum kernel. Variational algorithms are typically optimized using gradient-based approaches however these are difficult to implement on current noisy devices, requiring large numbers of objective function evaluations. Our scheme shifts this computational burden onto the classical optimizer component of these hybrid algorithms, greatly reducing the number of queries to the quantum processor. We focus on the variational quantum eigensolver (VQE) algorithm and demonstrate numerically that such surrogate models are particularly well suited to the algorithm's objective function. Next, we apply these models to both noiseless and noisy VQE simulations and show that they exhibit better performance than widely-used classical kernels in terms of final accuracy and convergence speed.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;AG-OG&#65292;&#29992;&#20110;&#21487;&#20998;&#31163;&#30340;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#32454;&#22320;&#21033;&#29992;&#38382;&#39064;&#32467;&#26500;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#24378;&#20984;-&#24378;&#20985;&#12289;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#21644;&#21452;&#32447;&#24615;&#21338;&#24328;&#12290;&#35813;&#31639;&#27861;&#36824;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#37117;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#21333;&#27425;&#35843;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.17550</link><description>&lt;p&gt;
Nesterov&#36935;&#35265;&#20048;&#35266;&#20027;&#20041;&#65306;&#36895;&#29575;&#26368;&#20248;&#30340;&#21487;&#20998;&#31163;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization. (arXiv:2210.17550v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17550
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;AG-OG&#65292;&#29992;&#20110;&#21487;&#20998;&#31163;&#30340;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#32454;&#22320;&#21033;&#29992;&#38382;&#39064;&#32467;&#26500;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#24378;&#20984;-&#24378;&#20985;&#12289;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#21644;&#21452;&#32447;&#24615;&#21338;&#24328;&#12290;&#35813;&#31639;&#27861;&#36824;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20013;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#37117;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#21333;&#27425;&#35843;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861; - &#21152;&#36895;&#26799;&#24230;-&#20048;&#35266;&#26799;&#24230;&#65288;AG-OG&#65289;&#19979;&#38477;&#19978;&#21319;&#27861;&#65292;&#29992;&#20110;&#21487;&#20998;&#31163;&#30340;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#31934;&#32454;&#22320;&#21033;&#29992;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22312;&#20010;&#20307;&#32452;&#20214;&#19978;&#36827;&#34892;Nesterov&#21152;&#36895;&#65292;&#24182;&#22312;&#32806;&#21512;&#32452;&#20214;&#19978;&#36827;&#34892;&#20048;&#35266;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AG-OG&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65288;&#21253;&#25324;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#24378;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#21644;&#21452;&#32447;&#24615;&#21338;&#24328;&#65289;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65288;&#24120;&#25968;&#22240;&#23376;&#20043;&#20869;&#65289;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#38543;&#26426;&#35774;&#32622;&#65292;&#24182;&#22312;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#24378;&#20984;-&#24378;&#20985;&#21644;&#20984;-&#24378;&#20985;&#35774;&#32622;&#19979;&#36798;&#21040;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;AG-OG&#26159;&#31532;&#19968;&#20010;&#22312;&#21452;&#32447;&#24615;&#32806;&#21512;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#37117;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#21333;&#27425;&#35843;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new first-order optimization algorithm -AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent -- for separable convex-concave minimax optimization. The main idea of our algorithm is to carefully leverage the structure of the minimax problem, performing Nesterov acceleration on the individual component and optimistic gradient on the coupling component. Equipped with proper restarting, we show that AG-OG achieves the optimal convergence rate (up to a constant) for a variety of settings, including bilinearly coupled strongly convex-strongly concave minimax optimization (bi-SC-SC), bilinearly coupled convex-strongly concave minimax optimization (bi-C-SC), and bilinear games. We also extend our algorithm to the stochastic setting and achieve the optimal convergence rate in both bi-SC-SC and bi-C-SC settings. AG-OG is the first single-call algorithm with optimal convergence rates in both deterministic and stochastic settings for bilinearly coupled minimax optimization 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#21452;&#26354;&#22810;&#32500;&#26631;&#24230;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#34920;&#31034;&#20302;&#32500;&#22270;&#24418;&#26469;&#22788;&#29702;&#39640;&#32500;&#30456;&#20851;&#25968;&#25454;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.15081</link><description>&lt;p&gt;
Bayesian&#21452;&#26354;&#22810;&#32500;&#26631;&#24230;
&lt;/p&gt;
&lt;p&gt;
Bayesian Hyperbolic Multidimensional Scaling. (arXiv:2210.15081v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#21452;&#26354;&#22810;&#32500;&#26631;&#24230;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#34920;&#31034;&#20302;&#32500;&#22270;&#24418;&#26469;&#22788;&#29702;&#39640;&#32500;&#30456;&#20851;&#25968;&#25454;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#26631;&#24230;&#65288;MDS&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#31034;&#39640;&#32500;&#12289;&#30456;&#20851;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;MDS&#36890;&#36807;&#20026;&#27599;&#20010;&#35266;&#27979;&#20998;&#37197;&#19968;&#20010;&#20302;&#32500;&#20960;&#20309;&#22270;&#24418;&#19978;&#30340;&#20301;&#32622;&#26469;&#24037;&#20316;&#65292;&#22270;&#24418;&#19978;&#30340;&#36317;&#31163;&#34920;&#31034;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22788;&#29702;&#20302;&#32500;&#22270;&#24418;&#20026;&#21452;&#26354;&#32447;&#30340;&#22810;&#32500;&#26631;&#24230;&#12290;&#20351;&#29992;&#21452;&#26354;&#31354;&#38388;&#26377;&#21161;&#20110;&#34920;&#31034;&#35768;&#22810;&#22330;&#26223;&#20013;&#24120;&#35265;&#30340;&#26641;&#29366;&#32467;&#26500;&#65288;&#20363;&#22914;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25991;&#26412;&#25110;&#36951;&#20256;&#25968;&#25454;&#65289;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#35266;&#27979;&#25968;&#25454;&#20013;&#27979;&#37327;&#35823;&#24046;&#30340;&#26368;&#23567;&#21270;&#24433;&#21709;&#21644;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#20917;&#23545;&#29031;&#20284;&#28982;&#20272;&#35745;&#36817;&#20284;&#20540;&#65292;&#20801;&#35768;&#22312;&#26356;&#22823;&#30340;&#25968;&#25454;&#35774;&#32622;&#20013;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#39640;&#25928;&#37319;&#26679;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24615;&#20174;&#36817;&#20284;$O(n^2)$&#38477;&#20302;&#21040;$O(n)$&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#12289;&#32463;&#20856;&#21442;&#32771;&#25968;&#25454;&#38598;&#12289;&#21360;&#24230;vil&#36827;&#34892;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multidimensional scaling (MDS) is a widely used approach to representing high-dimensional, dependent data. MDS works by assigning each observation a location on a low-dimensional geometric manifold, with distance on the manifold representing similarity. We propose a Bayesian approach to multidimensional scaling when the low-dimensional manifold is hyperbolic. Using hyperbolic space facilitates representing tree-like structures common in many settings (e.g. text or genetic data with hierarchical structure). A Bayesian approach provides regularization that minimizes the impact of measurement error in the observed data and assesses uncertainty. We also propose a case-control likelihood approximation that allows for efficient sampling from the posterior distribution in larger data settings, reducing computational complexity from approximately $O(n^2)$ to $O(n)$. We evaluate the proposed method against state-of-the-art alternatives using simulations, canonical reference datasets, Indian vil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#26469;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#21644;&#24050;&#22788;&#29702;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;&#12290;&#22312;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#65292;&#30456;&#23545;&#20110;DeltaCNN&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;&#39640;&#36798;90%&#12290;</title><link>http://arxiv.org/abs/2210.09887</link><description>&lt;p&gt;
MotionDeltaCNN&#65306;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#31232;&#30095;CNN&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos. (arXiv:2210.09887v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#26469;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#21644;&#24050;&#22788;&#29702;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;&#12290;&#22312;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#65292;&#30456;&#23545;&#20110;DeltaCNN&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;&#39640;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#36755;&#20837;&#19978;&#36827;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#20869;&#23384;&#24102;&#23485;&#12290;&#26368;&#36817;&#65292;DeltaCNN&#36890;&#36807;&#20165;&#22788;&#29702;&#19982;&#19978;&#19968;&#24103;&#30456;&#27604;&#26377;&#26174;&#33879;&#26356;&#26032;&#30340;&#20687;&#32032;&#26469;&#20943;&#23569;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;DeltaCNN&#20381;&#36182;&#20110;&#38745;&#24577;&#25668;&#20687;&#26426;&#36755;&#20837;&#12290;&#31227;&#21160;&#25668;&#20687;&#26426;&#32473;&#22914;&#20309;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#19982;&#24050;&#22788;&#29702;&#21306;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26368;&#23567;&#21270;&#26356;&#26032;&#29575; - &#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#24320;&#38144;&#19988;&#26080;&#38656;&#30693;&#36947;&#26410;&#26469;&#24103;&#30340;&#25668;&#20687;&#26426;&#22806;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#65292;&#20197;&#20415;&#26080;&#32541;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#21306;&#22495;&#21644;&#20197;&#21069;&#22788;&#29702;&#30340;&#21306;&#22495; - &#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#31227;&#21160;&#25668;&#20687;&#22836;&#35270;&#39057;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#36229;&#36807;DeltaCNN&#22810;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network inference on video input is computationally expensive and requires high memory bandwidth. Recently, DeltaCNN managed to reduce the cost by only processing pixels with significant updates over the previous frame. However, DeltaCNN relies on static camera input. Moving cameras add new challenges in how to fuse newly unveiled image regions with already processed regions efficiently to minimize the update rate - without increasing memory overhead and without knowing the camera extrinsics of future frames. In this work, we propose MotionDeltaCNN, a sparse CNN inference framework that supports moving cameras. We introduce spherical buffers and padded convolutions to enable seamless fusion of newly unveiled regions and previously processed regions -- without increasing memory footprint. Our evaluation shows that we outperform DeltaCNN by up to 90% for moving camera videos.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35777;&#26126;&#30340;&#38381;&#24335;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#65292;&#20026;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06591</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rigorous dynamical mean field theory for stochastic gradient descent methods. (arXiv:2210.06591v2 [math-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35777;&#26126;&#30340;&#38381;&#24335;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#65292;&#20026;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#28176;&#36827;&#24615;&#33021;&#38381;&#24335;&#26041;&#31243;&#65292;&#35813;&#26041;&#27861;&#20174;&#39640;&#26031;&#25968;&#25454;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#23398;&#20064;&#20272;&#35745;&#22120;&#65288;&#20363;&#22914;M-&#20272;&#35745;&#22120;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;...&#65289;&#12290;&#36825;&#21253;&#25324;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;Nesterov&#21152;&#36895;&#12290;&#24471;&#21040;&#30340;&#26041;&#31243;&#19982;&#23558;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#26041;&#31243;&#31163;&#25955;&#21270;&#21518;&#24212;&#29992;&#20110;&#26799;&#24230;&#27969;&#26102;&#20135;&#29983;&#30340;&#26041;&#31243;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26126;&#30830;&#25551;&#36848;&#35760;&#24518;&#26680;&#22312;&#26377;&#25928;&#21160;&#21147;&#23398;&#20013;&#22914;&#20309;&#26500;&#24314;&#65292;&#24182;&#19988;&#21253;&#25324;&#38750;&#21487;&#20998;&#31163;&#30340;&#26356;&#26032;&#20989;&#25968;&#65292;&#20801;&#35768;&#20855;&#26377;&#38750;&#21333;&#20301;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#36890;&#29992;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;SGD&#26041;&#31243;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove closed-form equations for the exact high-dimensional asymptotics of a family of first order gradient-based methods, learning an estimator (e.g. M-estimator, shallow neural network, ...) from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory (DMFT) equations from statistical physics when applied to gradient flow. Our proof method allows us to give an explicit description of how memory kernels build up in the effective dynamics, and to include non-separable update functions, allowing datasets with non-identity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch-size and with constant learning rates.
&lt;/p&gt;</description></item><item><title>R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14770</link><description>&lt;p&gt;
R2C-GAN: &#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and COVID-19 Classification. (arXiv:2209.14770v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14770
&lt;/p&gt;
&lt;p&gt;
R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#38024;&#23545;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#30340;&#32852;&#21512;&#27169;&#22411;&#65306;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(R2C-GANs)&#12290;&#35813;&#27169;&#22411;&#22312;&#20445;&#25345;&#30142;&#30149;&#23436;&#25972;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#20174;&#32780;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#30340;&#36136;&#37327;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;&#23558;&#24674;&#22797;&#20219;&#21153;&#23450;&#20041;&#20026;&#20174;&#36136;&#37327;&#36739;&#24046;&#21253;&#21547;&#26377;&#22122;&#22768;&#12289;&#27169;&#31946;&#25110;&#36807;/&#27424;&#26333;&#22270;&#29255;&#21040;&#39640;&#36136;&#37327;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#12290;R2C-GAN&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restoration of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific restoration problems such as image deblurring, denoising, and exposure correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the restoration. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the restoration task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#24615;&#20462;&#25913;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2206.08242</link><description>&lt;p&gt;
&#38750;&#40065;&#26834;&#24615;&#29305;&#24449;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Catastrophic overfitting can be induced with discriminative non-robust features. (arXiv:2206.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#24615;&#20462;&#25913;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#26500;&#24314;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#24555;&#36895;&#21333;&#27493;&#25915;&#20987;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#25511;&#21046;&#24615;&#20462;&#25913;&#65292;&#30740;&#31350;&#20102;&#21333;&#27493;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#27604;&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;&#36825;&#20123;&#29305;&#24449;&#26377;&#21161;&#20110;&#38750;&#40065;&#26834;&#24615;&#20998;&#31867;&#65292;&#20294;&#19981;&#33021;&#21333;&#29420;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#19968;&#26032;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#30340;&#26426;&#21046;&#36824;&#19981;&#22815;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of thes
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01206</link><description>&lt;p&gt;
&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20174;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#32463;&#20856;&#30340;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#30340;&#20219;&#21153;&#26159;&#20165;&#36890;&#36807;&#19968;&#20123;&#26631;&#35760;&#20026;&#27491;&#26679;&#26412;&#21644;&#65288;&#36890;&#24120;&#65289;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#20197;&#26159;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#65289;&#26469;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;infoNCE&#23545;&#27604;&#25439;&#22833;&#30340;&#23478;&#26063;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;PU&#35774;&#32622;&#65307;&#24182;&#19988;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#20026;&#26410;&#26631;&#35760;&#26679;&#26412;&#26500;&#24314;&#20266;&#26631;&#31614;&#65307;&#36825;&#20123;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#26368;&#32456;&#30340;&#65288;&#27491;&#26679;&#26412; vs. &#36127;&#26679;&#26412;&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#26631;&#20934;PU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;PU&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#31867;&#21035;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
&lt;/p&gt;</description></item><item><title>FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.16331</link><description>&lt;p&gt;
FlexFringe:&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16331
&lt;/p&gt;
&lt;p&gt;
FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FlexFringe&#20013;&#21487;&#29992;&#30340;&#27010;&#29575;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#36825;&#20123;&#23454;&#29616;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#65292;&#21253;&#25324;&#20960;&#31181;&#20462;&#25913;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40664;&#35748;&#23454;&#29616;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;FlexFringe&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36739;&#38590;&#35299;&#37322;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#26356;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#22914;&#20309;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36328;&#20219;&#21153;&#20849;&#20139;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36857;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#36138;&#23146;&#31574;&#30053;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#26080;&#38656;&#30693;&#36947;&#28508;&#22312;&#30697;&#38453;&#30340;&#31209;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#30456;&#27604;&#22522;&#32447;&#22312;&#22810;&#20219;&#21153;&#36951;&#25022;&#19978;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.10066</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#19982;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Multi-task Representation Learning with Stochastic Linear Bandits. (arXiv:2202.10066v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36328;&#20219;&#21153;&#20849;&#20139;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36857;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#36138;&#23146;&#31574;&#30053;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#26080;&#38656;&#30693;&#36947;&#28508;&#22312;&#30697;&#38453;&#30340;&#31209;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#30456;&#27604;&#22522;&#32447;&#22312;&#22810;&#20219;&#21153;&#36951;&#25022;&#19978;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20219;&#21153;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#36328;&#20219;&#21153;&#20849;&#20139;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#65292;&#24182;&#30740;&#31350;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#30340;&#30410;&#22788;&#12290;&#26681;&#25454;&#26368;&#26032;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31574;&#30053;&#35774;&#35745;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36857;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#36138;&#23146;&#31574;&#30053;&#12290;&#23427;&#36890;&#36807;&#40723;&#21169;&#20219;&#21153;&#22238;&#24402;&#21521;&#37327;&#24418;&#25104;&#30340;&#30697;&#38453;&#20855;&#26377;&#20302;&#31209;&#26469;&#38544;&#24335;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#12290;&#19982;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19981;&#38656;&#35201;&#30693;&#36947;&#28508;&#22312;&#30697;&#38453;&#30340;&#31209;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#25105;&#20204;&#31574;&#30053;&#30340;&#22810;&#20219;&#21153;&#36951;&#25022;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#26159;$O(\sqrt{NdT(T+d)r})$&#65292;&#20854;&#20013;$T$&#26159;&#20219;&#21153;&#25968;&#65292;$r$&#26159;&#31209;&#65292;$d$&#26159;&#21464;&#37327;&#25968;&#65292;$N$&#26159;&#27599;&#20010;&#20219;&#21153;&#30340;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#22522;&#32447;$Td\sqrt{N}$&#30456;&#27604;&#65292;&#25105;&#20204;&#31574;&#30053;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of transfer-learning in the setting of stochastic linear bandit tasks. We consider that a low dimensional linear representation is shared across the tasks, and study the benefit of learning this representation in the multi-task learning setting. Following recent results to design stochastic bandit policies, we propose an efficient greedy policy based on trace norm regularization. It implicitly learns a low dimensional representation by encouraging the matrix formed by the task regression vectors to be of low rank. Unlike previous work in the literature, our policy does not need to know the rank of the underlying matrix. We derive an upper bound on the multi-task regret of our policy, which is, up to logarithmic factors, of order $\sqrt{NdT(T+d)r}$, where $T$ is the number of tasks, $r$ the rank, $d$ the number of variables and $N$ the number of rounds per task. We show the benefit of our strategy compared to the baseline $Td\sqrt{N}$ obtained by solving each task i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#23545;&#32963;&#30284;&#36827;&#34892;&#20116;&#31181;&#19981;&#21516;&#30340;&#30149;&#29702;&#23398;&#23376;&#20998;&#31867;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#35270;&#35273;Transformer&#32593;&#32476;&#36827;&#34892;&#32963;&#30284;&#35786;&#26029;&#65292;&#22312;&#22810;&#20013;&#24515;&#38431;&#21015;&#19978;&#23637;&#31034;&#20102;&#21487;&#38752;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.08510</link><description>&lt;p&gt;
&#23398;&#20064;&#32963;&#32452;&#32455;&#30340;&#22810;&#23610;&#24230;&#28151;&#21512;&#35270;&#35273;Transformer&#65306;&#32963;&#30284;&#27835;&#30103;&#30340;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Hybrid Vision Transformer for Learning Gastric Histology: AI-Based Decision Support System for Gastric Cancer Treatment. (arXiv:2202.08510v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#23545;&#32963;&#30284;&#36827;&#34892;&#20116;&#31181;&#19981;&#21516;&#30340;&#30149;&#29702;&#23398;&#23376;&#20998;&#31867;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#35270;&#35273;Transformer&#32593;&#32476;&#36827;&#34892;&#32963;&#30284;&#35786;&#26029;&#65292;&#22312;&#22810;&#20013;&#24515;&#38431;&#21015;&#19978;&#23637;&#31034;&#20102;&#21487;&#38752;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32963;&#20869;&#31397;&#38236;&#31579;&#26597;&#26159;&#26089;&#26399;&#30830;&#23450;&#36866;&#24403;&#30340;&#32963;&#30284;&#27835;&#30103;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#26377;&#25928;&#38477;&#20302;&#32963;&#30284;&#30456;&#20851;&#27515;&#20129;&#29575;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20026;&#30149;&#29702;&#23398;&#23478;&#31579;&#26597;&#25968;&#23383;&#21270;&#20840;&#20999;&#29255;&#22270;&#20687;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#30340;AI&#31995;&#32479;&#22312;&#32454;&#31890;&#24230;&#30284;&#30151;&#20998;&#31867;&#21644;&#30284;&#30151;&#27835;&#30103;&#35268;&#21010;&#26041;&#38754;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#20116;&#31181;&#32963;&#30284;&#30149;&#29702;&#23398;&#23376;&#20998;&#31867;&#65292;&#24182;&#30452;&#25509;&#21305;&#37197;&#21040;&#24120;&#35268;&#32963;&#30284;&#27835;&#30103;&#25351;&#21335;&#12290;&#35813;AI&#31995;&#32479;&#21033;&#29992;&#20004;&#38454;&#27573;&#28151;&#21512;Vision Transformer&#65288;ViT&#65289;&#32593;&#32476;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#29702;&#35299;&#32452;&#32455;&#23398;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#33258;&#27880;&#24847;&#26426;&#21046;&#26377;&#25928;&#21306;&#20998;&#22810;&#31867;&#32963;&#30284;&#12290;&#35813;AI&#31995;&#32479;&#22312;&#26469;&#33258;&#22810;&#20013;&#24515;&#38431;&#21015;&#30340;1,212&#24352;&#20999;&#29255;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;0.85&#30340;&#24179;&#22343;&#25935;&#24863;&#24615;&#65292;&#23637;&#31034;&#20102;&#21487;&#38752;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gastric endoscopic screening is an effective way to decide appropriate gastric cancer (GC) treatment at an early stage, reducing GC-associated mortality rate. Although artificial intelligence (AI) has brought a great promise to assist pathologist to screen digitalized whole slide images, existing AI systems are limited in fine-grained cancer subclassifications and have little usability in planning cancer treatment. We propose a practical AI system that enables five subclassifications of GC pathology, which can be directly matched to general GC treatment guidance. The AI system is designed to efficiently differentiate multi-classes of GC through multi-scale self-attention mechanism using 2-stage hybrid Vision Transformer (ViT) networks, by mimicking the way how human pathologists understand histology. The AI system demonstrates reliable diagnostic performance by achieving class-average sensitivity of above 0.85 on a total of 1,212 slides from multicentric cohort. Furthermore, AI-assiste
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21464;&#20998;&#21513;&#24067;&#26031;&#25512;&#26029;&#65288;VGI&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#19981;&#23436;&#20840;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#27169;&#22411;&#20272;&#35745;&#26102;&#30340;&#25361;&#25112;&#12290;&#19982;&#26631;&#20934;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19981;&#21516;&#65292;VGI&#33021;&#22815;&#22788;&#29702;&#20272;&#35745;&#25351;&#25968;&#22810;&#20010;&#32570;&#22833;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20174;&#32780;&#20026;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2111.13180</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#25968;&#25454;&#32479;&#35745;&#27169;&#22411;&#20272;&#35745;&#30340;&#21464;&#20998;&#21513;&#24067;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data. (arXiv:2111.13180v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21464;&#20998;&#21513;&#24067;&#26031;&#25512;&#26029;&#65288;VGI&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#19981;&#23436;&#20840;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#27169;&#22411;&#20272;&#35745;&#26102;&#30340;&#25361;&#25112;&#12290;&#19982;&#26631;&#20934;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19981;&#21516;&#65292;VGI&#33021;&#22815;&#22788;&#29702;&#20272;&#35745;&#25351;&#25968;&#22810;&#20010;&#32570;&#22833;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20174;&#32780;&#20026;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#30001;&#33258;&#30001;&#21442;&#25968;&#25511;&#21046;&#65292;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#25110;&#20854;&#36817;&#20284;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#20250;&#36935;&#21040;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#20197;&#23436;&#20840;&#35266;&#27979;&#30340;&#25968;&#25454;&#20026;&#22522;&#30784;&#30340;&#65292;&#32780;&#23454;&#38469;&#19978;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#32570;&#22833;&#25968;&#25454;&#12290;&#20174;&#19981;&#23436;&#20840;&#25968;&#25454;&#20013;&#36827;&#34892;&#32479;&#35745;&#27169;&#22411;&#20272;&#35745;&#30340;&#29702;&#35770;&#22312;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;&#23384;&#22312;&#35832;&#22914;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20043;&#31867;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#19982;&#26631;&#20934;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19981;&#21516;&#65292;&#20351;&#29992;&#19981;&#23436;&#20840;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#36890;&#24120;&#38656;&#35201;&#20272;&#35745;&#25351;&#25968;&#22810;&#20010;&#32570;&#22833;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#20351;&#24471;&#26631;&#20934;&#30340;VI&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#21513;&#24067;&#26031;&#25512;&#26029;&#65288;VGI&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#36807;&#28193;&#32858;&#31867;&#24182;&#24212;&#29992;&#23436;&#22791;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#22240;&#26524;&#22270;&#20013;&#30340;&#21464;&#37327;&#20851;&#31995;&#65292;&#24182;&#20445;&#30041;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#36776;&#35782;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.04513</link><description>&lt;p&gt;
&#32858;&#31867;&#21644;&#22240;&#26524;&#22270;&#20013;&#30340;&#32467;&#26500;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clustering and Structural Robustness in Causal Diagrams. (arXiv:2111.04513v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#36807;&#28193;&#32858;&#31867;&#24182;&#24212;&#29992;&#23436;&#22791;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#22240;&#26524;&#22270;&#20013;&#30340;&#21464;&#37327;&#20851;&#31995;&#65292;&#24182;&#20445;&#30041;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#36776;&#35782;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#24120;&#29992;&#20110;&#34920;&#31034;&#21644;&#21487;&#35270;&#21270;&#22240;&#26524;&#20851;&#31995;&#12290;&#23545;&#20110;&#23569;&#37327;&#21464;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#27905;&#28165;&#26224;&#30340;&#22330;&#26223;&#35270;&#22270;&#12290;&#20294;&#26159;&#38543;&#30528;&#30740;&#31350;&#21464;&#37327;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#22270;&#34920;&#26041;&#27861;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#24182;&#19988;&#34920;&#31034;&#30340;&#28165;&#26224;&#24230;&#20007;&#22833;&#12290;&#21464;&#37327;&#32858;&#31867;&#26159;&#20943;&#23567;&#22240;&#26524;&#22270;&#22823;&#23567;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#20294;&#22914;&#26524;&#38543;&#24847;&#23454;&#26045;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#25913;&#21464;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#32858;&#31867;&#65292;&#31216;&#20026;&#36807;&#28193;&#32858;&#31867;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#20445;&#35777;&#20445;&#30041;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#36776;&#35782;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#22791;&#30340;&#31639;&#27861;&#26469;&#23547;&#25214;&#32473;&#23450;&#22270;&#34920;&#20013;&#30340;&#25152;&#26377;&#36807;&#28193;&#32858;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#32858;&#31867;&#22914;&#20309;&#31616;&#21270;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21453;&#21521;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#20174;&#19968;&#20010;&#32858;&#31867;&#22270;&#34920;&#24320;&#22987;&#65292;&#23547;&#25214;&#28385;&#36275;&#22240;&#26524;&#25928;&#24212;&#21487;&#36776;&#35782;&#24615;&#23646;&#24615;&#30340;&#25193;&#23637;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are commonly used to represent and visualize causal relations. For a small number of variables, this approach provides a succinct and clear view of the scenario at hand. As the number of variables under study increases, the graphical approach may become impractical, and the clarity of the representation is lost. Clustering of variables is a natural way to reduce the size of the causal diagram, but it may erroneously change the essential properties of the causal relations if implemented arbitrarily. We define a specific type of cluster, called transit cluster, that is guaranteed to preserve the identifiability properties of causal effects under certain conditions. We provide a sound and complete algorithm for finding all transit clusters in a given graph and demonstrate how clustering can simplify the identification of causal effects. We also study the inverse problem, where one starts with a clustered graph and looks for extended graphs where the identifiability properties of ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#35760;&#24518;&#30340;&#38750;&#24179;&#31283;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21160;&#24577;&#31574;&#30053;&#36951;&#25022;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20999;&#25442;&#25104;&#26412;&#24863;&#30693;&#22312;&#32447;&#21512;&#22863;&#26041;&#27861;&#35299;&#20915;&#20102;&#20999;&#25442;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2102.03758</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518;&#19982;&#38750;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#35760;&#24518;&#30340;&#38750;&#24179;&#31283;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21160;&#24577;&#31574;&#30053;&#36951;&#25022;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20999;&#25442;&#25104;&#26412;&#24863;&#30693;&#22312;&#32447;&#21512;&#22863;&#26041;&#27861;&#35299;&#20915;&#20102;&#20999;&#25442;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#35760;&#24518;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65288;OCO&#65289;&#65292;&#20854;&#20013;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#23398;&#20064;&#38382;&#39064;&#30340;&#26102;&#38388;&#25928;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#31574;&#30053;&#36951;&#25022;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#65292;&#20197;&#35774;&#35745;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#40065;&#26834;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#31639;&#27861;&#30340;&#20915;&#31574;&#19982;&#19968;&#31995;&#21015;&#21464;&#21270;&#30340;&#27604;&#36739;&#22120;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;OCO&#35760;&#24518;&#31639;&#27861;&#65292;&#23427;&#22312;&#26102;&#38388;&#36328;&#24230;&#12289;&#38750;&#24179;&#31283;&#24230;&#37327;&#21644;&#35760;&#24518;&#38271;&#24230;&#26041;&#38754;&#20445;&#35777;&#20102;&#26368;&#20339;&#30340;&#21160;&#24577;&#31574;&#30053;&#36951;&#25022;&#12290;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#26159;&#22914;&#20309;&#25511;&#21046;&#20999;&#25442;&#25104;&#26412;&#65292;&#21363;&#21442;&#19982;&#32773;&#20915;&#31574;&#30340;&#32047;&#31215;&#31227;&#21160;&#37327;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20999;&#25442;&#25104;&#26412;&#24863;&#30693;&#22312;&#32447;&#21512;&#22863;&#26041;&#27861;&#24471;&#21040;&#20102;&#24039;&#22937;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#21160;&#24577;&#31574;&#30053;&#36951;&#25022;&#30340;&#26032;&#30340;&#20803;&#22522;&#20998;&#35299;&#21644;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20803;&#23398;&#20064;&#22120;&#21644;&#22522;&#23398;&#20064;&#22120;&#65292;&#20197;&#26174;&#24335;&#22320;&#35268;&#33539;&#21270;&#20999;&#25442;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of Online Convex Optimization (OCO) with memory, which allows loss functions to depend on past decisions and thus captures temporal effects of learning problems. In this paper, we introduce dynamic policy regret as the performance measure to design algorithms robust to non-stationary environments, which competes algorithms' decisions with a sequence of changing comparators. We propose a novel algorithm for OCO with memory that provably enjoys an optimal dynamic policy regret in terms of time horizon, non-stationarity measure, and memory length. The key technical challenge is how to control the switching cost, the cumulative movements of player's decisions, which is neatly addressed by a novel switching-cost-aware online ensemble approach equipped with a new meta-base decomposition of dynamic policy regret and a careful design of meta-learner and base-learner that explicitly regularizes the switching cost. The results are further applied to tackle non-stationarity i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22686;&#24378;&#31639;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#25351;&#25968;&#26063;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#25311;&#21512;&#65292;&#24182;&#30830;&#20445;&#26368;&#23567;&#30340;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.00188</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#25351;&#25968;&#26063;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#26469;&#23454;&#29616;&#20844;&#24179;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair Densities via Boosting the Sufficient Statistics of Exponential Families. (arXiv:2012.00188v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22686;&#24378;&#31639;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#25351;&#25968;&#26063;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#25311;&#21512;&#65292;&#24182;&#30830;&#20445;&#26368;&#23567;&#30340;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22686;&#24378;&#31639;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#20844;&#24179;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#20174;&#19968;&#20010;&#21021;&#22987;&#30340;&#20844;&#24179;&#20294;&#19981;&#20934;&#30830;&#30340;&#20998;&#24067;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30830;&#20445;&#26368;&#23567;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#21516;&#26102;&#26397;&#30528;&#26356;&#22909;&#30340;&#25968;&#25454;&#25311;&#21512;&#26041;&#21521;&#36827;&#34892;&#36716;&#31227;&#12290;&#20026;&#27492;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#20855;&#26377;&#22686;&#24378;&#25910;&#25947;&#24615;&#30340;&#25351;&#25968;&#26063;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#23398;&#20064;&#30340;&#20998;&#24067;&#23558;&#20855;&#26377;&#34920;&#31034;&#29575;&#21644;&#32479;&#35745;&#29575;&#30340;&#25968;&#25454;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#19982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#36830;&#32493;&#22495;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24403;&#24369;&#23398;&#20064;&#32773;&#34987;&#25351;&#23450;&#20026;&#20915;&#31574;&#26641;&#26102;&#65292;&#21487;&#20197;&#26816;&#26597;&#23398;&#20064;&#20998;&#24067;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20197;&#25552;&#20379;&#65288;&#19981;&#65289;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#32447;&#32034;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a boosting algorithm to pre-process data for fairness. Starting from an initial fair but inaccurate distribution, our approach shifts towards better data fitting while still ensuring a minimal fairness guarantee. To do so, it learns the sufficient statistics of an exponential family with boosting-compliant convergence. Importantly, we are able to theoretically prove that the learned distribution will have a representation rate and statistical rate data fairness guarantee. Unlike recent optimization based pre-processing methods, our approach can be easily adapted for continuous domain features. Furthermore, when the weak learners are specified to be decision trees, the sufficient statistics of the learned distribution can be examined to provide clues on sources of (un)fairness. Empirical results are present to display the quality of result on real-world data.
&lt;/p&gt;</description></item></channel></rss>