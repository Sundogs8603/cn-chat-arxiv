<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tune without Validation (Twin)&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#30340;&#35843;&#25972;&#26469;&#39044;&#27979;&#27867;&#21270;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#26435;&#37325;&#33539;&#25968;&#19982;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05532</link><description>&lt;p&gt;
&#22312;&#35757;&#32451;&#38598;&#19978;&#25628;&#32034;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#65306;&#26080;&#38656;&#39564;&#35777;&#30340;&#35843;&#21442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tune without Validation (Twin)&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#30340;&#35843;&#25972;&#26469;&#39044;&#27979;&#27867;&#21270;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#26435;&#37325;&#33539;&#25968;&#19982;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;Tune without Validation (Twin)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20851;&#20110;&#20551;&#35774;&#31354;&#38388;&#20013;&#23398;&#20064;&#38454;&#27573;&#30340;&#26368;&#26032;&#29702;&#35770;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21738;&#20123;&#36229;&#21442;&#25968;&#32452;&#21512;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;Twin&#26681;&#25454;&#19968;&#20010;&#26089;&#20572;/&#38750;&#26089;&#20572;&#30340;&#35843;&#24230;&#31243;&#24207;&#23545;&#35797;&#39564;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#65292;&#28982;&#21518;&#20998;&#21106;&#20986;&#22312;&#35757;&#32451;&#25439;&#22833;&#26041;&#38754;&#25552;&#20379;&#26368;&#20339;&#32467;&#26524;&#30340;&#21306;&#22495;&#12290;&#22312;&#36825;&#20123;&#35797;&#39564;&#20013;&#65292;&#26435;&#37325;&#33539;&#25968;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#39044;&#27979;&#24378;&#30456;&#20851;&#12290;&#20026;&#20102;&#35780;&#20272;Twin&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;20&#20010;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#31995;&#21015;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21253;&#25324;&#21367;&#31215;&#12289;Transformer&#21644;&#21069;&#39304;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#24494;&#35843;&#26102;&#27491;&#30830;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#23567;&#26679;&#26412;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05532v1 Announce Type: new  Abstract: We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets. We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization. Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss. Among these trials, the weight norm strongly correlates with predicting generalization. To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including convolutional, transformer, and feed-forward models. We demonstrate proper HP selection when training from scratch and fine-tuning, emphasizing small-sample scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.05529</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Learning Gaussian Single-Index Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#26159;&#20855;&#26377;&#26893;&#20837;&#32467;&#26500;&#30340;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#20381;&#36182;&#20110;&#36890;&#36807;&#36890;&#29992;&#12289;&#38750;&#32447;&#24615;&#21644;&#28508;&#22312;&#38750;&#30830;&#23450;&#24615;&#36716;&#25442;&#30340;&#36755;&#20837;&#30340;&#26410;&#30693;&#19968;&#32500;&#25237;&#24433;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32479;&#35745;&#25512;&#26029;&#20219;&#21153;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#30740;&#31350;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25240;&#34935;&#12290;&#23613;&#31649;&#24674;&#22797;&#38544;&#34255;&#26041;&#21521;&#30340;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;$d$&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#26694;&#26550;&#21644;&#20302;&#38454;&#22810;&#39033;&#24335;&#65288;LDP&#65289;&#26694;&#26550;&#20869;&#65292;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#24517;&#39035;&#38656;&#35201;$\Omega(d^{k^\star/2})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$k^\star$&#26159;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#30340;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#8220;&#29983;&#25104;&#8221;&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#20351;&#29992;&#37096;&#20998;&#36857;&#30340;&#21305;&#37197;&#19978;&#30028;&#26469;&#35777;&#26126;&#36825;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.05490</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Poly-View Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#20250;&#21305;&#37197;&#19968;&#32452;&#19981;&#30456;&#20851;&#30340;&#36127;&#35270;&#22270;&#20013;&#30456;&#20851;&#35270;&#22270;&#30340;&#37197;&#23545;&#12290;&#35270;&#22270;&#21487;&#20197;&#26159;&#29983;&#25104;&#30340;&#65288;&#20363;&#22914;&#36890;&#36807;&#22686;&#24378;&#65289;&#25110;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#22810;&#20110;&#20004;&#20010;&#30456;&#20851;&#35270;&#22270;&#26102;&#30340;&#21305;&#37197;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#35270;&#22270;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20449;&#24687;&#26368;&#22823;&#21270;&#21644;&#20805;&#20998;&#32479;&#35745;&#23548;&#20986;&#20102;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#26080;&#38480;&#26102;&#65292;&#24212;&#26368;&#22823;&#21270;&#30456;&#20851;&#35270;&#22270;&#30340;&#25968;&#37327;&#65307;&#32780;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#29420;&#29305;&#26679;&#26412;&#30340;&#25968;&#37327;&#21516;&#26102;&#22686;&#21152;&#36825;&#20123;&#26679;&#26412;&#30340;&#35270;&#22270;&#25968;&#37327;&#26159;&#26377;&#30410;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#20197;256&#30340;&#25209;&#22823;&#23567;&#35757;&#32451;128&#36718;&#30340;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;ImageNet1k&#19978;&#34920;&#29616;&#20248;&#20110;&#22312;&#25209;&#22823;&#23567;&#20026;4096&#19988;&#36827;&#34892;1024&#36718;&#35757;&#32451;&#30340;SimCLR&#27169;&#22411;&#65292;&#25361;&#25112;&#20102;&#23545;&#27604;&#27169;&#22411;&#38656;&#35201;&#22823;&#25209;&#22823;&#23567;&#21644;&#22810;&#27425;&#35757;&#32451;&#36718;&#25968;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05465</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#30340;&#31639;&#27861;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;DNN&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05465
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#26041;&#27861;&#20351;&#29992;&#25972;&#25968;&#12289;&#23450;&#28857;&#25110;&#28014;&#28857;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#22312;&#20302;&#31934;&#24230;&#19979;&#25429;&#25417;&#19981;&#21516;&#30340;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30789;&#24320;&#38144;&#21644;&#23494;&#38598;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#65288;LP&#65289;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#27491;&#23450;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#12289;&#30828;&#20214;&#21451;&#22909;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;LP&#20301;&#22495;&#21160;&#24577;&#36866;&#24212;DNN&#26435;&#37325;/&#28608;&#27963;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;LP&#37327;&#21270;&#65288;LPQ&#65289;&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20248;&#30340;&#36880;&#23618;LP&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;-&#23616;&#37096;&#23545;&#27604;&#30446;&#26631;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#23558;LP&#32435;&#20837;&#35745;&#31639;&#25968;&#25454;&#36890;&#36335;&#20013;&#30340;&#22788;&#29702;&#21333;&#20803;&#65288;PEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05452</link><description>&lt;p&gt;
&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#24555;&#36895;&#31934;&#23494;&#25104;&#20687;&#30340;R2D2&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#38656;&#35201;&#35299;&#20915;&#26469;&#33258;&#22823;&#25968;&#25454;&#37327;&#30340;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#36870;&#38382;&#39064;&#12290;&#26368;&#36817;&#22522;&#20110;&#20248;&#21270;&#29702;&#35770;&#30340;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#25104;&#20687;&#31934;&#24230;&#33021;&#21147;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;CLEAN&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#31639;&#23376;&#25512;&#21160;&#30340;&#20808;&#36827;&#36817;&#31471;&#31639;&#27861;&#65292;&#22914;SARA&#31995;&#21015;&#65292;&#20197;&#21450;&#30001;&#23398;&#20064;&#27491;&#21017;&#21270;&#21435;&#22122;&#22120;&#25512;&#21160;&#30340;&#28151;&#21512;&#25554;&#25300;&#65288;PnP&#65289;&#31639;&#27861;&#65292;&#22914;AIRI&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#21644;PnP&#32467;&#26500;&#39640;&#24230;&#36845;&#20195;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22788;&#29702;&#26410;&#26469;&#20202;&#22120;&#39044;&#26399;&#30340;&#26497;&#31471;&#25968;&#25454;&#22823;&#23567;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#23545;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#32780;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05452v1 Announce Type: cross  Abstract: Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Netw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36807;&#21435;&#26679;&#26412;&#25968;&#37327;&#36873;&#25321;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.05446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28418;&#31227;&#31163;&#25955;&#20998;&#24067;&#30340;&#25913;&#36827;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Algorithm for Learning Drifting Discrete Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05446
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36807;&#21435;&#26679;&#26412;&#25968;&#37327;&#36873;&#25321;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#26469;&#33258;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#31163;&#25955;&#20998;&#24067;&#30340;&#29420;&#31435;&#26679;&#26412;&#24207;&#21015;&#65292;&#30446;&#26631;&#26159;&#20272;&#35745;&#24403;&#21069;&#20998;&#24067;&#12290;&#30001;&#20110;&#25105;&#20204;&#27599;&#20010;&#26102;&#38388;&#27493;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#26679;&#26412;&#65292;&#19968;&#20010;&#33391;&#22909;&#30340;&#20272;&#35745;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#36807;&#21435;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#26412;&#65292;&#25105;&#20204;&#24517;&#39035;&#35785;&#35832;&#26356;&#26089;&#30340;&#26679;&#26412;&#65292;&#36825;&#20250;&#22240;&#20026;&#20998;&#24067;&#21464;&#21270;&#24341;&#20837;&#30340;&#20559;&#24046;&#32780;&#20135;&#29983;&#28418;&#31227;&#35823;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#36739;&#23569;&#30340;&#36807;&#21435;&#26679;&#26412;&#65292;&#20272;&#35745;&#20250;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#65292;&#23548;&#33268;&#36739;&#22823;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#28418;&#31227;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#36866;&#24212;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05446v1 Announce Type: new  Abstract: We present a new adaptive algorithm for learning discrete distributions under distribution drift. In this setting, we observe a sequence of independent samples from a discrete distribution that is changing over time, and the goal is to estimate the current distribution. Since we have access to only a single sample for each time step, a good estimation requires a careful choice of the number of past samples to use. To use more samples, we must resort to samples further in the past, and we incur a drift error due to the bias introduced by the change in distribution. On the other hand, if we use a small number of past samples, we incur a large statistical error as the estimation has a high variance. We present a novel adaptive algorithm that can solve this trade-off without any prior knowledge of the drift. Unlike previous adaptive results, our algorithm characterizes the statistical error using data-dependent bounds. This technicality enab
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20026;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;2022&#24180;&#30340;&#30005;&#21147;&#20215;&#26684;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.05441</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#27010;&#29575;&#30340;&#26085;&#20869;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20026;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;2022&#24180;&#30340;&#30005;&#21147;&#20215;&#26684;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#20805;&#20998;&#32771;&#34385;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21464;&#37327;&#26159;IDFull&#20215;&#26684;&#25351;&#25968;&#65292;&#39044;&#27979;&#20197;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;2022&#24180;&#26497;&#24230;&#27874;&#21160;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#39564;&#35777;&#65292;&#22312;&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#25104;&#20026;&#39044;&#27979;&#30740;&#31350;&#23545;&#35937;&#12290;&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#27979;&#21019;&#24314;&#26102;&#30340;&#25152;&#26377;&#21487;&#29992;&#26085;&#20869;&#20132;&#26131;&#26469;&#35745;&#31639;IDFull&#30340;&#24403;&#21069;&#20540;&#12290;&#26681;&#25454;&#24369;&#24335;&#26377;&#25928;&#20551;&#35774;&#65292;&#20174;&#26368;&#21518;&#20215;&#26684;&#20449;&#24687;&#24314;&#31435;&#30340;&#22522;&#20934;&#26080;&#27861;&#26174;&#33879;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#28857;&#24230;&#37327;&#21644;&#27010;&#29575;&#35780;&#20998;&#26041;&#38754;&#23384;&#22312;&#30528;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#20351;&#29992;LASSO&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#23459;&#24067;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05441v1 Announce Type: cross  Abstract: We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecastin
&lt;/p&gt;</description></item><item><title>&#20313;&#24358;&#30456;&#20284;&#24230;&#21487;&#20197;&#20135;&#29983;&#20219;&#24847;&#21644;&#26080;&#24847;&#20041;&#30340;&#8220;&#30456;&#20284;&#24615;&#8221;&#65292;&#21463;&#27491;&#21017;&#21270;&#25511;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#28145;&#23618;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05440</link><description>&lt;p&gt;
&#23884;&#20837;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#30495;&#30340;&#21482;&#26159;&#20851;&#20110;&#30456;&#20284;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Cosine-Similarity of Embeddings Really About Similarity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05440
&lt;/p&gt;
&lt;p&gt;
&#20313;&#24358;&#30456;&#20284;&#24230;&#21487;&#20197;&#20135;&#29983;&#20219;&#24847;&#21644;&#26080;&#24847;&#20041;&#30340;&#8220;&#30456;&#20284;&#24615;&#8221;&#65292;&#21463;&#27491;&#21017;&#21270;&#25511;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#28145;&#23618;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20313;&#24358;&#30456;&#20284;&#24230;&#26159;&#20004;&#20010;&#21521;&#37327;&#20043;&#38388;&#22841;&#35282;&#30340;&#20313;&#24358;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#35828;&#26159;&#23427;&#20204;&#24402;&#19968;&#21270;&#21518;&#30340;&#28857;&#31215;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#24212;&#29992;&#26159;&#36890;&#36807;&#23558;&#20313;&#24358;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20302;&#32500;&#29305;&#24449;&#23884;&#20837;&#26469;&#37327;&#21270;&#39640;&#32500;&#23545;&#35937;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26102;&#27604;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#26410;&#24402;&#19968;&#21270;&#28857;&#31215;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#26377;&#26102;&#20063;&#26356;&#24046;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#19968;&#32463;&#39564;&#35266;&#23519;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#27491;&#21017;&#21270;&#32447;&#24615;&#27169;&#22411;&#23548;&#20986;&#30340;&#23884;&#20837;&#65292;&#20854;&#20013;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#21161;&#20110;&#20998;&#26512;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#19978;&#25512;&#23548;&#20986;&#20313;&#24358;&#30456;&#20284;&#24615;&#22914;&#20309;&#20135;&#29983;&#20219;&#24847;&#19988;&#22240;&#27492;&#26080;&#24847;&#20041;&#30340;&#8220;&#30456;&#20284;&#24615;&#8221;&#12290;&#23545;&#20110;&#19968;&#20123;&#32447;&#24615;&#27169;&#22411;&#65292;&#30456;&#20284;&#24615;&#29978;&#33267;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#65292;&#23427;&#20204;&#21463;&#21040;&#27491;&#21017;&#21270;&#30340;&#38544;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32447;&#24615;&#27169;&#22411;&#20043;&#22806;&#30340;&#24433;&#21709;&#65306;&#22312;&#23398;&#20064;&#28145;&#23618;&#27169;&#22411;&#26102;&#65292;&#20250;&#37319;&#29992;&#19981;&#21516;&#27491;&#21017;&#21270;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05440v1 Announce Type: cross  Abstract: Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HTV-Trans&#30340;Hierarchical Time series Variational Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#21644;Transformer&#65292;&#33021;&#22815;&#26377;&#25928;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22238;&#22797;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05406</link><description>&lt;p&gt;
&#32771;&#34385;&#38750;&#24179;&#31283;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23618;&#27425;&#21464;&#20998;Transformer
&lt;/p&gt;
&lt;p&gt;
Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05406
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HTV-Trans&#30340;Hierarchical Time series Variational Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#21644;Transformer&#65292;&#33021;&#22815;&#26377;&#25928;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22238;&#22797;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#30340;&#39044;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#36328;&#36234;&#38271;&#26102;&#38388;&#27493;&#30340;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#24179;&#31283;&#21270;&#26041;&#27861;&#26469;&#20943;&#24369;&#21407;&#22987;&#31995;&#21015;&#30340;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24635;&#26159;&#37319;&#29992;&#24179;&#31283;&#21270;&#30340;&#31995;&#21015;&#65292;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#38543;&#26426;&#24615;&#65292;&#24456;&#38590;&#23545;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;MTS&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#65292;&#32771;&#34385;&#20102;MTS&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;Transformer&#32467;&#21512;&#65292;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;Hierarchical Time series Variational Transformer&#65288;HTV-Trans&#65289;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#21464;&#20998;&#29983;&#25104;&#21160;&#24577;&#27169;&#22411;&#65292;&#23558;&#20869;&#22312;&#30340;&#38750;&#24179;&#31283;&#20449;&#24687;&#24674;&#22797;&#21040;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05406v1 Announce Type: cross  Abstract: The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability. However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling MTS with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastic characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being a po
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#21453;&#38382;&#39064;&#20013;&#30340;&#24674;&#22797;&#20445;&#35777;&#19982;&#20351;&#29992;&#26799;&#24230;&#27969;&#26102;&#30340;&#20445;&#35777;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.05395</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#21453;&#38382;&#39064;&#20013;&#30340;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#21453;&#38382;&#39064;&#20013;&#30340;&#24674;&#22797;&#20445;&#35777;&#19982;&#20351;&#29992;&#26799;&#24230;&#27969;&#26102;&#30340;&#20445;&#35777;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26631;&#20934;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#20173;&#28982;&#31232;&#32570;&#19988;&#38590;&#20197;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#36866;&#24403;&#21021;&#22987;&#21270;&#20351;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;Deep Image Prior&#65292;DIP&#65289;&#20165;&#22312;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#33719;&#24471;&#20102;&#25910;&#25947;&#21644;&#24674;&#22797;&#20445;&#35777;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#65292;&#24403;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;/&#23398;&#20064;&#29575;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#36825;&#20123;&#20445;&#35777;&#20173;&#28982;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#31163;&#25955;&#21270;&#20165;&#20197;&#24120;&#25968;&#24433;&#21709;&#20004;&#23618;DIP&#32593;&#32476;&#30340;&#36807;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#22240;&#27492;&#26799;&#24230;&#19979;&#38477;&#25152;&#25214;&#21040;&#30340;&#19981;&#21516;&#20445;&#35777;&#20063;&#36866;&#29992;&#20110;&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05395v1 Announce Type: new  Abstract: Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years. However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve. Only recently did unsupervised methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization. In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate. We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.05385</link><description>&lt;p&gt;
&#22312;&#25209;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20999;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#38477;&#20302;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Switching the Loss Reduces the Cost in Batch Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#65288;FQI-LOG&#65289;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FQI-LOG&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#23545;&#20110;&#37027;&#20123;&#36890;&#36807;&#26368;&#20248;&#34892;&#20026;&#23454;&#29616;&#30446;&#26631;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#20026;&#38646;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25209;RL&#20013;&#35777;&#26126;&#20855;&#26377;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#65292;FQI-LOG&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#26679;&#26412;&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#26412;&#24341;&#29702;&#30340;&#25512;&#24191;&#21644;&#26680;&#22238;&#24402;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#25193;&#23637;&#21644;&#21464;&#25442;&#65292;&#24471;&#21040;&#20102;&#31995;&#32479;&#36712;&#36857;&#30340;&#26032;&#26680;&#34920;&#31034;&#26041;&#27861;&#65292;&#23637;&#31034;&#20986;&#20102;&#19982;&#29305;&#23450;&#26680;&#22238;&#24402;&#38382;&#39064;&#30340;&#31561;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#28508;&#22312;&#26680;&#30340;&#32467;&#26500;&#21450;&#20854;&#23545;&#24212;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.05368</link><description>&lt;p&gt;
&#25506;&#35752;&#22522;&#26412;&#24341;&#29702;&#19982;&#26680;&#22238;&#24402;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Links between the Fundamental Lemma and Kernel Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#26412;&#24341;&#29702;&#30340;&#25512;&#24191;&#21644;&#26680;&#22238;&#24402;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#25193;&#23637;&#21644;&#21464;&#25442;&#65292;&#24471;&#21040;&#20102;&#31995;&#32479;&#36712;&#36857;&#30340;&#26032;&#26680;&#34920;&#31034;&#26041;&#27861;&#65292;&#23637;&#31034;&#20986;&#20102;&#19982;&#29305;&#23450;&#26680;&#22238;&#24402;&#38382;&#39064;&#30340;&#31561;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#28508;&#22312;&#26680;&#30340;&#32467;&#26500;&#21450;&#20854;&#23545;&#24212;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Willems&#31561;&#20154;&#20851;&#20110;&#22522;&#26412;&#24341;&#29702;&#30340;&#25512;&#24191;&#21644;&#21464;&#31181;&#26159;&#26368;&#36817;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#24182;&#24418;&#24335;&#21270;&#20102;&#26680;&#22238;&#24402;&#19982;&#24050;&#30693;&#38750;&#32447;&#24615;&#22522;&#26412;&#24341;&#29702;&#25193;&#23637;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;Hankel&#30697;&#38453;&#20013;&#30340;&#20256;&#32479;&#32447;&#24615;&#26041;&#31243;&#36827;&#34892;&#21464;&#25442;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31995;&#32479;&#36712;&#36857;&#30340;&#21478;&#19968;&#31181;&#38544;&#24335;&#26680;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#28608;&#21169;&#25345;&#20037;&#24615;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#34920;&#31034;&#31561;&#21516;&#20110;&#29305;&#23450;&#26680;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#28508;&#22312;&#26680;&#30340;&#21487;&#33021;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#23545;&#24212;&#30340;&#31995;&#32479;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05368v1 Announce Type: cross  Abstract: Generalizations and variations of the fundamental lemma by Willems et al. are an active topic of recent research. In this note, we explore and formalize the links between kernel regression and known nonlinear extensions of the fundamental lemma. Applying a transformation to the usual linear equation in Hankel matrices, we arrive at an alternative implicit kernel representation of the system trajectories while keeping the requirements on persistency of excitation. We show that this representation is equivalent to the solution of a specific kernel regression problem. We explore the possible structures of the underlying kernel as well as the system classes to which they correspond.
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;Transformer-based&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#12290;</title><link>https://arxiv.org/abs/2403.05365</link><description>&lt;p&gt;
&#37327;&#21270;&#23545;&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Quantization on the Robustness of Transformer-based Text Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05365
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;Transformer-based&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#21270;&#23545;Transformer-based&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#37327;&#21270;&#36890;&#24120;&#28041;&#21450;&#23558;&#39640;&#31934;&#24230;&#23454;&#25968;&#26144;&#23556;&#21040;&#36739;&#20302;&#31934;&#24230;&#30340;&#20540;&#65292;&#26088;&#22312;&#20943;&#23569;&#25152;&#28041;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;BERT&#21644;DistilBERT&#27169;&#22411;&#24212;&#29992;&#37327;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;SST-2&#12289;Emotion&#21644;MR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;TextFooler&#12289;PWWS&#21644;PSO&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#37327;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#25552;&#21319;&#20102;18.68%&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05365v1 Announce Type: new  Abstract: Transformer-based models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks. In this paper, we explore the effect of quantization on the robustness of Transformer-based models. Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models. In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks. Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models. Furthermore, we compare the effe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05358</link><description>&lt;p&gt;
&#24847;&#35265;&#21160;&#24577;&#27169;&#22411;&#20013;&#21442;&#25968;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference of Parameters in Opinion Dynamics Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20195;&#29702;&#20154;&#30340;&#27169;&#22411;&#65288;ABMs&#65289;&#22312;&#30740;&#31350;&#31038;&#20250;&#29616;&#35937;&#20013;&#34987;&#39057;&#32321;&#20351;&#29992;&#65292;&#20294;&#21442;&#25968;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#30340;&#21442;&#25968;&#65292;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05358v1 Announce Type: cross  Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.   Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.   We validate our method on a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#26088;&#22312;&#25913;&#21892;&#20174;MRI&#25195;&#25551;&#20013;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05353</link><description>&lt;p&gt;
&#28151;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#20197;&#25913;&#21892;&#20174;MRI&#25195;&#25551;&#20013;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer's Disease Diagnosis from MRI Scans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#26088;&#22312;&#25913;&#21892;&#20174;MRI&#25195;&#25551;&#20013;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30456;&#20851;&#30142;&#30149;&#27604;&#20854;&#20182;&#30142;&#30149;&#26356;&#25935;&#24863;&#65292;&#22240;&#25163;&#26415;&#31243;&#24207;&#22797;&#26434;&#12289;&#25104;&#26412;&#39640;&#20197;&#21450;&#20854;&#20182;&#25361;&#25112;&#31561;&#22240;&#32032;&#12290;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22823;&#33041;&#30142;&#30149;&#65292;&#23548;&#33268;&#35760;&#24518;&#20007;&#22833;&#21644;&#33041;&#32454;&#32990;&#33806;&#32553;&#12290;&#26089;&#26399;&#26816;&#27979;&#23545;&#20026;&#24739;&#32773;&#25552;&#20379;&#36866;&#24403;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;CT&#25110;MRI&#25195;&#25551;&#25163;&#21160;&#25195;&#25551;&#22312;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#31995;&#32479;&#65292;&#36825;&#21253;&#25324;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#20102;CNN&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;LSTM&#27169;&#22411;&#30340;&#26816;&#27979;&#33021;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#24212;&#29992;&#20102;&#31216;&#20026;VGG16&#30340;&#36801;&#31227;&#23398;&#20064;&#26469;&#20174;MRI&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;LSTM&#29992;&#20110;&#26816;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05353v1 Announce Type: cross  Abstract: Brain-related diseases are more sensitive than other diseases due to several factors, including the complexity of surgical procedures, high costs, and other challenges. Alzheimer's disease is a common brain disorder that causes memory loss and the shrinking of brain cells. Early detection is critical for providing proper treatment to patients. However, identifying Alzheimer's at an early stage using manual scanning of CT or MRI scans is challenging. Therefore, researchers have delved into the exploration of computer-aided systems, employing Machine Learning and Deep Learning methodologies, which entail the training of datasets to detect Alzheimer's disease. This study aims to present a hybrid model that combines a CNN model's feature extraction capabilities with an LSTM model's detection capabilities. This study has applied the transfer learning called VGG16 in the hybrid model to extract features from MRI images. The LSTM detects feat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#21307;&#23398;&#20013;&#23884;&#20837;&#35821;&#20041;&#20998;&#21106;&#30340;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#37096;&#32626;&#26550;&#26500;&#65292;&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#36739;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#35777;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05340</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#20013;&#23884;&#20837;&#35821;&#20041;&#20998;&#21106;&#30340;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Embedded Deployment of Semantic Segmentation in Medicine through Low-Resolution Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05340
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#21307;&#23398;&#20013;&#23884;&#20837;&#35821;&#20041;&#20998;&#21106;&#30340;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#37096;&#32626;&#26550;&#26500;&#65292;&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#36739;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#35777;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#22823;&#23567;&#21644;&#35745;&#31639;&#24320;&#38144;&#24120;&#24120;&#26159;&#38480;&#21046;&#22240;&#32032;&#12290;&#22312;&#35832;&#22914;&#23884;&#20837;&#24335;&#21307;&#30103;&#35774;&#22791;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#24448;&#24448;&#26080;&#27861;&#36127;&#25285;&#22823;&#22411;&#26114;&#36149;&#30340;&#30828;&#20214;&#65292;&#39044;&#31639;&#20063;&#24120;&#24120;&#24456;&#32039;&#24352;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#20805;&#20998;&#21033;&#29992;&#20102;&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#24448;&#24448;&#20250;&#36991;&#20813;&#20351;&#29992;&#26368;&#39640;&#21487;&#29992;&#30340;&#36755;&#20837;&#20998;&#36776;&#29575;&#20197;&#30830;&#20445;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#36825;&#19968;&#20107;&#23454;&#12290;&#23613;&#31649;&#20351;&#29992;&#36739;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#20250;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#26174;&#33879;&#20943;&#23569;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#20173;&#21487;&#20197;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#36825;&#19968;&#20107;&#23454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05340v1 Announce Type: cross  Abstract: When deploying neural networks in real-life situations, the size and computational effort are often the limiting factors. This is especially true in environments where big, expensive hardware is not affordable, like in embedded medical devices, where budgets are often tight. State-of-the-art proposed multiple different lightweight solutions for such use cases, mostly by changing the base model architecture, not taking the input and output resolution into consideration. In this paper, we propose our architecture that takes advantage of the fact that in hardware-limited environments, we often refrain from using the highest available input resolutions to guarantee a higher throughput. Although using lower-resolution input leads to a significant reduction in computing and memory requirements, it may also incur reduced prediction quality. Our architecture addresses this problem by exploiting the fact that we can still utilize high-resolutio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#25913;&#21892;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#21512;&#27861;&#24615;&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.05318</link><description>&lt;p&gt;
&#23637;&#26395;&#36991;&#20813;&#36831;&#21040;&#65306;&#35299;&#20915;&#30828;&#32422;&#26463;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#25913;&#21892;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#21512;&#27861;&#24615;&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#23450;&#24335;&#20026;&#20855;&#26377;&#32422;&#26463;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#24120;&#22797;&#26434;&#32780;&#19988;&#25968;&#37327;&#20247;&#22810;&#65292;&#20351;&#24471;&#35299;&#20915;TSP&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#22797;&#26434;&#32422;&#26463;&#30340;&#25968;&#37327;&#22686;&#38271;&#26102;&#65292;&#20256;&#32479;&#21551;&#21457;&#24335;&#31639;&#27861;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#20197;&#36991;&#20813;&#19981;&#21512;&#27861;&#32467;&#26524;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36719;&#26041;&#24335;&#26469;&#35299;&#20915;TSP&#38382;&#39064;&#65292;&#21516;&#26102;&#25903;&#25345;GPU&#21152;&#36895;&#20197;&#24555;&#36895;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36719;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#30828;&#32422;&#26463;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#21512;&#27861;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#20248;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#24182;&#23545;&#25239;&#30828;&#32422;&#26463;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21521;&#21069;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#26469;&#25913;&#36827;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#65288;TSPTW&#65289;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#27861;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05318v1 Announce Type: new  Abstract: Many real-world problems can be formulated as a constrained Traveling Salesman Problem (TSP). However, the constraints are always complex and numerous, making the TSPs challenging to solve. When the number of complicated constraints grows, it is time-consuming for traditional heuristic algorithms to avoid illegitimate outcomes. Learning-based methods provide an alternative to solve TSPs in a soft manner, which also supports GPU acceleration to generate solutions quickly. Nevertheless, the soft manner inevitably results in difficulty solving hard-constrained problems with learning algorithms, and the conflicts between legality and optimality may substantially affect the optimality of the solution. To overcome this problem and to have an effective solution against hard constraints, we proposed a novel learning-based method that uses looking-ahead information as the feature to improve the legality of TSP with Time Windows (TSPTW) solutions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#25214;&#21040;&#22266;&#26377;&#37327; $\lambda$&#65292;&#23545;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.05293</link><description>&lt;p&gt;
&#21033;&#29992;&#36830;&#32493;&#26102;&#38388;&#29702;&#35299;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05293
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#25214;&#21040;&#22266;&#26377;&#37327; $\lambda$&#65292;&#23545;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#20998;&#26512;&#24102;&#26377;&#27493;&#38271; $\gamma$ &#21644;&#21160;&#37327;&#21442;&#25968; $\beta$ &#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#25214;&#21040;&#19968;&#20010;&#22266;&#26377;&#37327; $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$&#65292;&#36825;&#19968;&#37327;&#21807;&#19968;&#23450;&#20041;&#20102;&#20248;&#21270;&#36335;&#24452;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21152;&#36895;&#35268;&#21017;&#12290;&#22312;&#36229;&#21442;&#25968;&#21270;&#22238;&#24402;&#35774;&#32622;&#20013;&#35757;&#32451; $2$ &#23618;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#19968;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#34920;&#24449;&#24674;&#22797;&#30340;&#35299;&#12290;&#28982;&#21518;&#35777;&#26126;&#23567;&#30340; $\lambda$ &#20540;&#26377;&#21161;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#38543;&#26426;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#31867;&#20284;&#20294;&#36739;&#24369;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#25903;&#25345;&#25105;&#20204;&#35770;&#26029;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05293v1 Announce Type: new  Abstract: In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\gamma$ and momentum parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#21644;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#37051;&#22495;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#28041;&#21450;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#36924;&#36817;&#31354;&#38388;&#30340;&#29305;&#23450;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.05290</link><description>&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#36924;&#36817;&#31354;&#38388;&#30340;&#22522;&#30784;&#21629;&#39064;
&lt;/p&gt;
&lt;p&gt;
Foundational propositions of hesitant fuzzy soft $\beta$-covering approximation spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#21644;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#37051;&#22495;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#28041;&#21450;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#36924;&#36817;&#31354;&#38388;&#30340;&#29305;&#23450;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft set theory&#29992;&#20316;&#22788;&#29702;&#19981;&#30830;&#23450;&#20449;&#24687;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#32780;&#29369;&#35947;&#27169;&#31946;&#38598;&#22312;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#21644;&#29369;&#35947;&#30340;&#24773;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#29369;&#35947;&#27169;&#31946;&#38598;&#34920;&#29616;&#20986;&#22810;&#26679;&#30340;&#38582;&#23646;&#24230;&#65292;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#20135;&#29983;&#21508;&#31181;&#24418;&#24335;&#30340;&#21253;&#21547;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#21644;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#37051;&#22495;&#30340;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#26159;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#20043;&#38388;&#19981;&#21516;&#24418;&#24335;&#30340;&#21253;&#21547;&#20851;&#31995;&#32780;&#21046;&#23450;&#30340;&#12290;&#38543;&#21518;&#65292;&#30740;&#31350;&#20102;&#20960;&#20010;&#30456;&#20851;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#29369;&#35947;&#27169;&#31946;&#31895;&#38598;&#24341;&#20837;&#20102;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#30340;&#20855;&#20307;&#21464;&#20307;&#65292;&#25509;&#30528;&#25506;&#35752;&#20102;&#28041;&#21450;&#29369;&#35947;&#27169;&#31946;&#36719;$\beta$-&#35206;&#30422;&#36924;&#36817;&#31354;&#38388;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05290v1 Announce Type: new  Abstract: Soft set theory serves as a mathematical framework for handling uncertain information, and hesitant fuzzy sets find extensive application in scenarios involving uncertainty and hesitation. Hesitant fuzzy sets exhibit diverse membership degrees, giving rise to various forms of inclusion relationships among them. This article introduces the notions of hesitant fuzzy soft $\beta$-coverings and hesitant fuzzy soft $\beta$-neighborhoods, which are formulated based on distinct forms of inclusion relationships among hesitancy fuzzy sets. Subsequently, several associated properties are investigated. Additionally, specific variations of hesitant fuzzy soft $\beta$-coverings are introduced by incorporating hesitant fuzzy rough sets, followed by an exploration of properties pertaining to hesitant fuzzy soft $\beta$-covering approximation spaces.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;</title><link>https://arxiv.org/abs/2403.05268</link><description>&lt;p&gt;
&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#36785;&#39554;&#35821;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Prompt Multi-task Network for Abuse Language Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28389;&#29992;&#35821;&#35328;&#30340;&#26816;&#27979;&#20173;&#28982;&#26159;&#31038;&#20132;&#32593;&#32476;&#24191;&#27867;&#20351;&#29992;&#20013;&#23384;&#22312;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20219;&#21153;&#23384;&#22312;&#30528;&#20934;&#30830;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#28608;&#21457;PLMs&#30340;&#19968;&#33324;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#65288;DPMN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DPMN&#39318;&#20808;&#23581;&#35797;&#20026;PLMs&#35774;&#35745;&#20004;&#31181;&#24418;&#24335;&#30340;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#38271;&#24230;&#12289;&#35843;&#25972;&#31574;&#30053;&#21644;&#25552;&#31034;&#21021;&#22987;&#21270;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#28389;&#29992;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Bi-LSTM&#21644;FFN&#30340;&#20219;&#21153;&#22836;&#65292;&#21487;&#29992;&#20316;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#26368;&#32456;&#65292;DPMN&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05268v1 Announce Type: cross  Abstract: The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics 
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DuDoUniNeXt&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#22495;MRI&#37325;&#24314;&#32593;&#32476;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#36136;&#37327;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#23569;&#25110;&#20302;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.05256</link><description>&lt;p&gt;
DuDoUniNeXt&#65306;&#29992;&#20110;&#21333;&#21644;&#22810;&#23545;&#27604;&#24230;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#30340;&#21452;&#22495;&#32479;&#19968;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DuDoUniNeXt&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#22495;MRI&#37325;&#24314;&#32593;&#32476;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#36136;&#37327;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#22312;&#32570;&#23569;&#25110;&#20302;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#27604;&#24230;&#65288;MC&#65289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#37325;&#24314;&#26088;&#22312;&#23558;&#36741;&#21161;&#27169;&#24577;&#30340;&#21442;&#32771;&#22270;&#20687;&#32435;&#20837;&#30446;&#26631;&#27169;&#24577;&#30340;&#37325;&#24314;&#36807;&#31243;&#20013;&#12290;&#24050;&#30693;&#30340;MC&#37325;&#24314;&#26041;&#27861;&#22312;&#26377;&#23436;&#20840;&#37319;&#26679;&#30340;&#21442;&#32771;&#22270;&#20687;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#21442;&#32771;&#22270;&#20687;&#32570;&#22833;&#25110;&#36136;&#37327;&#20302;&#19979;&#26102;&#65292;&#19982;&#21333;&#23545;&#27604;&#24230;&#65288;SC&#65289;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DuDoUniNeXt&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#22495;MRI&#37325;&#24314;&#32593;&#32476;&#65292;&#21487;&#36866;&#24212;&#28041;&#21450;&#32570;&#22833;&#12289;&#36136;&#37327;&#20302;&#21644;&#36136;&#37327;&#39640;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#21508;&#31181;&#22330;&#26223;&#12290;DuDoUniNeXt&#37319;&#29992;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;CNN&#21644;ViT&#30340;&#28151;&#21512;&#20027;&#24178;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20687;&#22495;&#21644;k-&#31354;&#38388;&#37325;&#24314;&#30340;&#29305;&#23450;&#35843;&#25972;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31895;&#21040;&#32454;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;AdaC2F&#65289;&#65292;&#21160;&#24577;&#22788;&#29702;&#19981;&#21516;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#37096;&#20998;&#20849;&#20139;&#30340;&#27973;&#23618;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05256v1 Announce Type: cross  Abstract: Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to incorporate a reference image of auxiliary modality to guide the reconstruction process of the target modality. Known MC reconstruction methods perform well with a fully sampled reference image, but usually exhibit inferior performance, compared to single-contrast (SC) methods, when the reference image is missing or of low quality. To address this issue, we propose DuDoUniNeXt, a unified dual-domain MRI reconstruction network that can accommodate to scenarios involving absent, low-quality, and high-quality reference images. DuDoUniNeXt adopts a hybrid backbone that combines CNN and ViT, enabling specific adjustment of image domain and k-space reconstruction. Specifically, an adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to dynamically process the information from reference images of varying qualities. Besides, a partially shared shallow feat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31526;&#21495;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#27874;&#20989;&#25968;&#36817;&#20284;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05249</link><description>&lt;p&gt;
&#29992;&#20855;&#26377;&#31526;&#21495;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30005;&#23376;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
On Representing Electronic Wave Functions with Sign Equivariant Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05249
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31526;&#21495;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#27874;&#20989;&#25968;&#36817;&#20284;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23545;&#30005;&#23376;&#22522;&#24577;&#27874;&#20989;&#25968;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#20934;&#30830;&#36924;&#36817;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#30001;&#19968;&#20010;&#32622;&#25442;&#31561;&#21464;&#24615;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#32622;&#25442;&#21453;&#23545;&#31216;&#25805;&#20316;&#32452;&#25104;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#30005;&#23376;&#20132;&#25442;&#23545;&#31216;&#24615;&#12290;&#34429;&#28982;&#31934;&#30830;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#32763;&#36716;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#30005;&#23376;&#22352;&#26631;&#35745;&#31639;&#21453;&#23545;&#31216;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20855;&#26377;&#31526;&#21495;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#20445;&#30041;&#21453;&#23545;&#31216;&#24615;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25215;&#35834;&#30001;&#20110;&#36739;&#20302;&#32500;&#34920;&#31034;&#32780;&#21152;&#36895;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#21270;&#31616;&#20026;Jastrow&#22240;&#23376;&#65292;&#21363;&#27874;&#20989;&#25968;&#20013;&#24120;&#29992;&#30340;&#32622;&#25442;&#19981;&#21464;&#20056;&#27861;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#19968;&#27493;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;&#21457;&#29616;&#19982;&#22522;&#32447;&#30456;&#27604;&#20960;&#20046;&#27809;&#26377;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#26082;&#26080;&#29702;&#35770;&#20248;&#21183;&#20063;&#26080;&#32463;&#39564;&#20248;&#21183;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05249v1 Announce Type: cross  Abstract: Recent neural networks demonstrated impressively accurate approximations of electronic ground-state wave functions. Such neural networks typically consist of a permutation-equivariant neural network followed by a permutation-antisymmetric operation to enforce the electronic exchange symmetry. While accurate, such neural networks are computationally expensive. In this work, we explore the flipped approach, where we first compute antisymmetric quantities based on the electronic coordinates and then apply sign equivariant neural networks to preserve the antisymmetry. While this approach promises acceleration thanks to the lower-dimensional representation, we demonstrate that it reduces to a Jastrow factor, a commonly used permutation-invariant multiplicative factor in the wave function. Our empirical results support this further, finding little to no improvements over baselines. We conclude with neither theoretical nor empirical advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#34701;&#20837;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#24378;&#21270;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;</title><link>https://arxiv.org/abs/2403.05239</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#20013;&#20154;&#24615;&#20808;&#39564;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#34701;&#20837;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#24378;&#21270;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vanilla&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#23548;&#33268;&#19981;&#23436;&#32654;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#23039;&#21183;&#25110;&#32930;&#20307;&#19981;&#25104;&#27604;&#20363;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#39069;&#22806;&#22270;&#20687;&#25110;&#28155;&#21152;&#39069;&#22806;&#25511;&#21046;&#65288;&#22914;&#23039;&#21183;&#25110;&#28145;&#24230;&#22270;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#25511;&#21046;&#20027;&#35201;&#22312;&#22270;&#20687;&#29983;&#25104;&#38454;&#27573;&#24341;&#20837;&#20154;&#24615;&#20808;&#39564;&#12290;&#26412;&#25991;&#25506;&#35752;&#23558;&#36825;&#20123;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#38598;&#25104;&#21040;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#38454;&#27573;&#28040;&#38500;&#39069;&#22806;&#26465;&#20214;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20154;&#24615;&#23545;&#40784;&#25439;&#22833;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65292;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#24378;&#21270;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24341;&#20837;&#20855;&#26377;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#30340;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#32454;&#33410;&#20016;&#23500;&#24615;&#21644;&#20154;&#20307;&#32467;&#26500;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05239v1 Announce Type: cross  Abstract: Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross
&lt;/p&gt;</description></item><item><title>FAIM&#26159;&#19968;&#20010;&#29992;&#20110;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#30028;&#38754;&#35782;&#21035;&#26368;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.05235</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#21487;&#35299;&#37322;&#24314;&#27169;&#65288;FAIM&#65289;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05235
&lt;/p&gt;
&lt;p&gt;
FAIM&#26159;&#19968;&#20010;&#29992;&#20110;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#30028;&#38754;&#35782;&#21035;&#26368;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#26426;&#22120;&#23398;&#20064;&#19981;&#26029;&#34701;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#25552;&#20986;&#20102;&#37325;&#35201;&#20851;&#20999;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#26694;&#26550; - &#20844;&#24179;&#24863;&#30693;&#21487;&#35299;&#37322;&#24314;&#27169;&#65288;FAIM&#65289;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#20854;&#29305;&#28857;&#26159;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#39640;&#24615;&#33021;&#27169;&#22411;&#20013;&#35782;&#21035;&#20986;&#19968;&#20010;&#8220;&#26356;&#20844;&#24179;&#8221;&#30340;&#27169;&#22411;&#65292;&#24182;&#20419;&#36827;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;&#24773;&#22659;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24211;MIMIC-IV-ED&#21644;SGH-ED&#19978;&#39044;&#27979;&#21307;&#38498;&#20837;&#38498;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;FAIM&#22312;&#20943;&#23569;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;FAIM&#27169;&#22411;&#19981;&#20165;&#23637;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#27495;&#35270;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#24050;&#24314;&#31435;&#30340;&#20844;&#24179;&#24615;&#24230;&#37327;&#26126;&#26174;&#20943;&#36731;&#20102;&#20559;&#35265;&#65292;&#20248;&#20110;&#24120;&#29992;&#30340;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05235v1 Announce Type: cross  Abstract: The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a "fairer" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving f
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#29983;&#25104;&#30340;&#37197;&#23545;&#20449;&#24687;&#26174;&#33879;&#25913;&#21892;&#20102;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#35823;&#24046;&#20943;&#23567;&#20998;&#21035;&#36798;&#21040;4.4&#20493;&#21644;5.6&#20493;</title><link>https://arxiv.org/abs/2403.05220</link><description>&lt;p&gt;
&#21512;&#25104;&#29305;&#26435;&#20449;&#24687;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Privileged Information Enhances Medical Image Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05220
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#25104;&#30340;&#37197;&#23545;&#20449;&#24687;&#26174;&#33879;&#25913;&#21892;&#20102;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#35823;&#24046;&#20943;&#23567;&#20998;&#21035;&#36798;&#21040;4.4&#20493;&#21644;5.6&#20493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#34987;&#35777;&#26126;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#29983;&#29289;&#23398;&#30456;&#20851;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#19981;&#23384;&#22312;&#37197;&#23545;&#25968;&#25454;&#25110;&#21482;&#26377;&#23569;&#37327;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#24456;&#22909;&#22320;&#24037;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#25214;&#21040;&#26410;&#37197;&#23545;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#29983;&#25104;&#26377;&#25928;&#26080;&#38480;&#37327;&#30340;&#37197;&#23545;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#37197;&#23545;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#65292;&#19982;&#21333;&#27169;&#24577;&#35757;&#32451;&#65288;&#35823;&#24046;&#20943;&#23567;&#36798;&#21040;4.4&#20493;&#65289;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65288;&#35823;&#24046;&#20943;&#23567;&#36798;&#21040;5.6&#20493;&#65289;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05220v1 Announce Type: cross  Abstract: Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProUD&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05209</link><description>&lt;p&gt;
&#20811;&#26381;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProUD&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#20010;&#26469;&#28304;&#21644;&#20154;&#32676;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#36825;&#31181;&#19981;&#24179;&#31561;&#22312;&#20026;&#25968;&#25454;&#26377;&#38480;&#30340;&#20154;&#24314;&#27169;&#26102;&#24102;&#26469;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#28145;&#21051;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#34920;&#24615;&#26696;&#20363;&#65292;&#21363;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#65288;SSDG&#65289;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#39046;&#22495;&#34987;&#26631;&#35760;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#27809;&#26377;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;ProUD&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ProUD&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#21253;&#25324;&#21333;&#19968;&#39046;&#22495;&#27867;&#21270;&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05209v1 Announce Type: cross  Abstract: While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations. This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns. In this paper, we address a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains. Our experiments on three different benchmark datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalizati
&lt;/p&gt;</description></item><item><title>DARL&#20351;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#36827;&#34892;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#21644;&#29305;&#23450;&#22122;&#22768;&#35745;&#21010;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05196</link><description>&lt;p&gt;
&#38477;&#22122;&#33258;&#22238;&#24402;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Denoising Autoregressive Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05196
&lt;/p&gt;
&lt;p&gt;
DARL&#20351;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#36827;&#34892;&#33258;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#21644;&#29305;&#23450;&#22122;&#22768;&#35745;&#21010;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DARL&#37319;&#29992;&#20102;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#26469;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#22270;&#20687;&#34917;&#19969;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#36827;&#34892;&#35757;&#32451;&#20250;&#24471;&#21040;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#34917;&#19969;&#35299;&#30721;&#22120;&#23558;MSE&#25439;&#22833;&#26367;&#25442;&#20026;&#25193;&#25955;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;&#22122;&#22768;&#35745;&#21010;&#21644;&#26356;&#22823;&#27169;&#22411;&#30340;&#26356;&#38271;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#20339;&#35745;&#21010;&#19982;&#26631;&#20934;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#35745;&#21010;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#23613;&#31649;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;DARL&#22312;&#24494;&#35843;&#21327;&#35758;&#19979;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#33945;&#29256;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26631;&#24535;&#30528;&#21521;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#24863;&#30693;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;&#27169;&#22411;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05196v1 Announce Type: new  Abstract: In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21452;&#22612;&#27169;&#22411;&#21644;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#24212;&#23545;Spotify&#22312;&#24341;&#20837;&#26377;&#22768;&#35835;&#29289;&#21518;&#38754;&#20020;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05185</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;Spotify&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#26377;&#22768;&#35835;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05185
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21452;&#22612;&#27169;&#22411;&#21644;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#24212;&#23545;Spotify&#22312;&#24341;&#20837;&#26377;&#22768;&#35835;&#29289;&#21518;&#38754;&#20020;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#23383;&#38899;&#39057;&#39046;&#22495;&#20013;&#65292;&#20197;&#20854;&#38899;&#20048;&#21644;&#35848;&#35805;&#20869;&#23481;&#32780;&#38395;&#21517;&#30340;Spotify&#26368;&#36817;&#21521;&#20854;&#24222;&#22823;&#29992;&#25143;&#32676;&#24341;&#20837;&#20102;&#26377;&#22768;&#35835;&#29289;&#12290;&#23613;&#31649;&#21069;&#26223;&#30475;&#22909;&#65292;&#20294;&#36825;&#19968;&#20030;&#25514;&#20026;&#20010;&#24615;&#21270;&#25512;&#33616;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#19982;&#38899;&#20048;&#21644;&#25773;&#23458;&#19981;&#21516;&#65292;&#26368;&#21021;&#38656;&#35201;&#20184;&#36153;&#30340;&#26377;&#22768;&#35835;&#29289;&#22312;&#36141;&#20080;&#21069;&#26080;&#27861;&#36731;&#26494;&#30053;&#35835;&#65292;&#36825;&#22686;&#21152;&#20102;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23558;&#26032;&#20869;&#23481;&#31867;&#22411;&#24341;&#20837;&#29616;&#26377;&#24179;&#21488;&#23548;&#33268;&#25968;&#25454;&#26497;&#24230;&#31232;&#30095;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29992;&#25143;&#23545;&#36825;&#31181;&#26032;&#20869;&#23481;&#31867;&#22411;&#19981;&#29087;&#24713;&#12290;&#26368;&#21518;&#65292;&#21521;&#25968;&#30334;&#19975;&#29992;&#25143;&#25512;&#33616;&#20869;&#23481;&#35201;&#27714;&#27169;&#22411;&#21453;&#24212;&#36805;&#36895;&#19988;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#25773;&#23458;&#21644;&#38899;&#20048;&#29992;&#25143;&#21916;&#22909;&#65292;&#24341;&#20837;&#20102;2T-HGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#21644;&#21452;&#22612;&#65288;2T&#65289;&#27169;&#22411;&#32452;&#25104;&#30340;&#21487;&#25193;&#23637;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#19968;&#26032;&#39062;&#26041;&#27861;&#25581;&#31034;&#20102;&#39033;&#30446;&#20043;&#38388;&#24494;&#22937;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05185v1 Announce Type: cross  Abstract: In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized recommendations. Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations. Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2403.05181</link><description>&lt;p&gt;
Adversarial Sparse Teacher: &#23545;&#25239;&#25932;&#23545;&#31034;&#20363;&#65292;&#38450;&#24481;&#29992;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#30340;&#22522;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20419;&#36827;&#20102;&#23558;&#39640;&#32423;&#25945;&#24072;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#36716;&#31227;&#21040;&#26356;&#31616;&#21333;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#30830;&#20445;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#23427;&#20063;&#34987;&#29992;&#20110;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20351;&#29992;KD&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#21533;&#21868;&#25945;&#24072;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#31232;&#30095;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#30693;&#35782;&#20135;&#26435;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#20445;&#25252;&#20854;logits&#65292;&#21463;&#8220;&#24694;&#27602;&#25945;&#24072;&#8221;&#29702;&#24565;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#21152;&#24378;&#25945;&#24072;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#20943;&#23569;&#20102;&#30456;&#23545;&#30340;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05181v1 Announce Type: new  Abstract: Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation. Our approach carefully reduces the relative e
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05175</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Continual Learning and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05175
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#31456;&#33410;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#21363;&#20174;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#25345;&#32493;&#23398;&#20064;&#26159;&#20154;&#33041;&#30340;&#19968;&#31181;&#33258;&#28982;&#25216;&#33021;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22312;&#23398;&#20064;&#26032;&#30693;&#35782;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#24448;&#24448;&#20250;&#36805;&#36895;&#32780;&#24443;&#24213;&#22320;&#24536;&#35760;&#20197;&#21069;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25345;&#32493;&#23398;&#20064;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#26412;&#20070;&#31456;&#33410;&#22238;&#39038;&#20102;&#36825;&#19968;&#39046;&#22495;&#20135;&#29983;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05175v1 Announce Type: cross  Abstract: This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#25968;&#25454;&#20013;&#24515;&#21487;&#20449;AI&#65288;DCTAI&#65289;&#26694;&#26550; VTruST&#65292;&#20801;&#35768;&#29992;&#25143;&#25511;&#21046;&#19981;&#21516;&#21487;&#20449;&#24230;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;&#22312;&#32447;&#20215;&#20540;&#20989;&#25968;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#31232;&#30095;&#36924;&#36817;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20272;&#20540;&#21644;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#65288;OMP&#65289;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;VTruST&#22312;&#31038;&#20132;&#12289;&#22270;&#20687;&#21644;&#31185;&#23398;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2403.05174</link><description>&lt;p&gt;
VTruST&#65306;&#22522;&#20110;&#21487;&#25511;&#20215;&#20540;&#20989;&#25968;&#30340;&#25968;&#25454;&#20013;&#24515;&#20449;&#36182;AI&#30340;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05174
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#25968;&#25454;&#20013;&#24515;&#21487;&#20449;AI&#65288;DCTAI&#65289;&#26694;&#26550; VTruST&#65292;&#20801;&#35768;&#29992;&#25143;&#25511;&#21046;&#19981;&#21516;&#21487;&#20449;&#24230;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;&#22312;&#32447;&#20215;&#20540;&#20989;&#25968;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#31232;&#30095;&#36924;&#36817;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20272;&#20540;&#21644;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#65288;OMP&#65289;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;VTruST&#22312;&#31038;&#20132;&#12289;&#22270;&#20687;&#21644;&#31185;&#23398;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;AI&#23545;&#20110;AI&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19968;&#20123;&#20851;&#38190;&#30340;&#21487;&#20449;&#24230;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#25968;&#25454;&#20013;&#24515;&#21487;&#20449;AI&#65288;DCTAI&#65289;&#26694;&#26550; - VTruST&#65292;&#20801;&#35768;&#29992;&#25143;&#25511;&#21046;&#25152;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#21487;&#20449;&#24230;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#26045;&#39640;&#25928;&#30340;DCTAI&#26694;&#26550;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#35774;&#35745;&#22522;&#20110;&#22312;&#32447;&#20215;&#20540;&#20989;&#25968;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#25968;&#25454;&#20272;&#20540;&#21644;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#22312;&#32447;&#31232;&#30095;&#36924;&#36817;&#24418;&#24335;&#25552;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22312;&#32447;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#65288;OMP&#65289;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VTruST&#22312;&#31038;&#20132;&#12289;&#22270;&#20687;&#21644;&#31185;&#23398;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;VTruST&#29983;&#25104;&#30340;&#25968;&#25454;&#20540;&#21487;&#20197;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05174v1 Announce Type: new  Abstract: Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with fairness, robustness, and accuracy being some of the key trustworthiness metrics. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation. We propose a novel online version of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem. Experimental results show that VTruST outperforms the state-of-the-art baselines on social, image, and scientific datasets. We also show that the data values generated by VTruST ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05171</link><description>&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#20811;&#26381;&#20102;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;AdvPO&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25913;&#36827;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;Anthropic HH&#21644;TL;DR&#25688;&#35201;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdvPO&#22312;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05171v1 Announce Type: cross  Abstract: We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31867;&#20284;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05164</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#31995;&#32479;&#36776;&#35782;&#65306;&#21033;&#29992;&#31867;&#20284;&#31995;&#32479;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation for system identification: leveraging knowledge transfer from similar systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31867;&#20284;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#36807;&#25311;&#21512;&#30340;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#21516;&#19968;&#31867;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#20803;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35813;&#20803;&#27169;&#22411;&#25551;&#36848;&#20102;&#20551;&#23450;&#25152;&#20851;&#27880;&#30340;&#31995;&#32479;&#25152;&#23646;&#30340;&#19968;&#31867;&#31995;&#32479;&#12290;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#39318;&#20808;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#20803;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#29992;&#20197;&#36776;&#21035;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20854;&#34892;&#20026;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#36755;&#20837;&#24207;&#21015;&#30340;&#21512;&#25104;&#36755;&#20986;&#24207;&#21015;&#65307;&#20854;&#27425;&#65292;&#19982;&#21512;&#25104;&#25968;&#25454;&#19968;&#36215;&#65292;&#29992;&#20110;&#23450;&#20041;&#29992;&#20110;&#27169;&#22411;&#20272;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#39564;&#35777;&#25968;&#25454;&#38598;&#29992;&#20110;&#35843;&#25972;&#19968;&#20010;&#26631;&#37327;&#36229;&#21442;&#25968;&#65292;&#24179;&#34913;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05164v1 Announce Type: cross  Abstract: This paper addresses the challenge of overfitting in the learning of dynamical systems by introducing a novel approach for the generation of synthetic data, aimed at enhancing model generalization and robustness in scenarios characterized by data scarcity. Central to the proposed methodology is the concept of knowledge transfer from systems within the same class. Specifically, synthetic data is generated through a pre-trained meta-model that describes a broad class of systems to which the system of interest is assumed to belong. Training data serves a dual purpose: firstly, as input to the pre-trained meta model to discern the system's dynamics, enabling the prediction of its behavior and thereby generating synthetic output sequences for new input sequences; secondly, in conjunction with synthetic data, to define the loss function used for model estimation. A validation dataset is used to tune a scalar hyper-parameter balancing the rel
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05158</link><description>&lt;p&gt;
&#33021;&#37327;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Learning over Energy-Constrained Wireless Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05158
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35774;&#22791;&#19982;&#26381;&#21153;&#22120;&#21512;&#20316;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#22522;&#20110;&#30456;&#21516;&#30340;&#22266;&#23450;&#20998;&#35010;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#21644;&#20449;&#36947;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#26041;&#24335;&#22312;&#35757;&#32451;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#65292;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#28385;&#36275;&#38271;&#26399;&#33021;&#37327;&#28040;&#32791;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#22312;&#20110;&#32570;&#20047;&#26410;&#26469;&#20449;&#24687;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#29702;&#35770;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21517;&#20026;OPEN&#65292;&#23427;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#20010;&#20855;&#26377;&#24403;&#21069;&#30340;&#26032;MIP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05158v1 Announce Type: cross  Abstract: Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the curren
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#26041;&#24335;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05138</link><description>&lt;p&gt;
&#22522;&#20110;&#36138;&#23146;&#26041;&#27861;&#30340;&#20998;&#31867;&#22120;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;: &#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Greedy feature selection: Classifier-dependent feature selection via greedy methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#26041;&#24335;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25490;&#21517;&#26041;&#27861;&#65292;&#21363;&#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#19982;&#24212;&#29992;&#20110;&#21033;&#29992;&#20943;&#23569;&#25968;&#37327;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#30340;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#12290;&#30456;&#21453;&#65292;&#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#26681;&#25454;&#36873;&#23450;&#30340;&#20998;&#31867;&#22120;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#22914;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#24230;&#25110;&#26680;&#23545;&#40784;&#31561;&#27169;&#22411;&#33021;&#21147;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#30340;&#22320;&#36136;&#25928;&#24212;&#34920;&#29616;&#38382;&#39064;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05138v1 Announce Type: cross  Abstract: The purpose of this study is to introduce a new approach to feature ranking for classification tasks, called in what follows greedy feature selection. In statistical learning, feature selection is usually realized by means of methods that are independent of the classifier applied to perform the prediction using that reduced number of features. Instead, greedy feature selection identifies the most important feature at each step and according to the selected classifier. In the paper, the benefits of such scheme are investigated theoretically in terms of model capacity indicators, such as the Vapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested numerically by considering its application to the problem of predicting geo-effective manifestations of the active Sun.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#65292;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;Follow-the-Perturbed-Leader&#31574;&#30053;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#23614;&#37096;&#26368;&#20248;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05134</link><description>&lt;p&gt;
&#24102;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#30340;&#25200;&#21160;&#39046;&#23548;&#32773;&#36861;&#36394;&#65306;&#22312;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#21644;&#26368;&#20339;&#36873;&#25321;&#20013;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Follow-the-Perturbed-Leader with Fr\'{e}chet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05134
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#65292;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;Follow-the-Perturbed-Leader&#31574;&#30053;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#23614;&#37096;&#26368;&#20248;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;$K$&#33218;&#32769;&#34382;&#26426;&#20013;Follow-the-Perturbed-Leader&#65288;FTPL&#65289;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#12290;&#23613;&#31649;Follow-the-Regularized-Leader&#65288;FTRL&#65289;&#26694;&#26550;&#22312;&#21508;&#31181;&#27491;&#21017;&#21270;&#36873;&#25321;&#19979;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20381;&#36182;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;FTPL&#26694;&#26550;&#21364;&#40092;&#26377;&#20851;&#27880;&#65292;&#23613;&#31649;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#12290;&#22312;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#20013;&#65292;FTPL&#33509;&#25200;&#21160;&#36981;&#24490;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#23614;&#37096;&#30340;&#20998;&#24067;&#65292;&#26377;&#20154;&#29468;&#24819;&#21487;&#33021;&#21487;&#20197;&#23454;&#29616;$\mathcal{O}(\sqrt{KT})$&#30340;&#21518;&#24724;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#24418;&#29366;$\alpha=2$&#30340;Fr\'{e}chet&#20998;&#24067;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;FTPL&#30830;&#23454;&#36798;&#21040;&#20102;&#36825;&#19968;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#23545;&#25968;&#21518;&#24724;&#65292;&#24847;&#21619;&#30528;FTPL&#20855;&#22791;&#20102;&#26368;&#20339;&#36873;&#25321;&#65288;BOBW&#65289;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#32467;&#26524;&#21482;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#19978;&#36848;&#29468;&#24819;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;Fr\'{e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05134v1 Announce Type: cross  Abstract: This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL) policy in both adversarial and stochastic $K$-armed bandits. Despite the widespread use of the Follow-the-Regularized-Leader (FTRL) framework with various choices of regularization, the FTPL framework, which relies on random perturbations, has not received much attention, despite its inherent simplicity. In adversarial bandits, there has been conjecture that FTPL could potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a distribution with a Fr\'{e}chet-type tail. Recent work by Honda et al. (2023) showed that FTPL with Fr\'{e}chet distribution with shape $\alpha=2$ indeed attains this bound and, notably logarithmic regret in stochastic bandits, meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result only partly resolves the above conjecture because their analysis heavily relies on the specific form of the Fr\'{e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#35299;&#20915;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#20013;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25299;&#25169;&#25511;&#21046;&#38382;&#39064;&#65292;&#20197;&#20811;&#26381;&#26377;&#38480;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#23545;&#26234;&#33021;&#24863;&#30693;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05133</link><description>&lt;p&gt;
&#22522;&#20110;RIS&#30340;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25299;&#25169;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RIS-empowered Topology Control for Distributed Learning in Urban Air Mobility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#35299;&#20915;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#20013;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25299;&#25169;&#25511;&#21046;&#38382;&#39064;&#65292;&#20197;&#20811;&#26381;&#26377;&#38480;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#23545;&#26234;&#33021;&#24863;&#30693;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#23558;&#36710;&#36742;&#20174;&#22320;&#38754;&#25193;&#23637;&#21040;&#36817;&#22320;&#31354;&#38388;&#65292;&#34987;&#26500;&#24819;&#20026;&#20132;&#36890;&#31995;&#32479;&#30340;&#38761;&#21629;&#12290;&#20840;&#38754;&#30340;&#22330;&#26223;&#24863;&#30693;&#26159;&#26080;&#20154;&#26426;&#39550;&#39542;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;UAM&#38754;&#20020;&#26234;&#33021;&#24863;&#30693;&#25361;&#25112;&#65306;&#39640;&#24863;&#30693;&#23398;&#20064;&#38656;&#27714;&#19982;&#39134;&#34892;&#27773;&#36710;&#30340;&#26377;&#38480;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#33455;&#29255;&#30456;&#20914;&#31361;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31561;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#33021;&#22815;&#20849;&#21516;&#36827;&#34892;&#26426;&#36733;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#12290;&#20294;&#20687;FL&#36825;&#26679;&#30340;&#20256;&#32479;&#21327;&#20316;&#23398;&#20064;&#20381;&#36182;&#20110;&#29992;&#20110;DL&#27169;&#22411;&#32858;&#21512;&#30340;&#20013;&#22830;&#38598;&#25104;&#22120;&#65292;&#36825;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24456;&#38590;&#37096;&#32626;&#12290;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#26041;&#26696;&#21487;&#33021;&#26159;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25910;&#25947;&#26080;&#27861;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05133v1 Announce Type: cross  Abstract: Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground space, envisioned as a revolution for transportation systems. Comprehensive scene perception is the foundation for autonomous aerial driving. However, UAM encounters the intelligent perception challenge: high perception learning requirements conflict with the limited sensors and computing chips of flying cars. To overcome the challenge, federated learning (FL) and other collaborative learning have been proposed to enable resource-limited devices to conduct onboard deep learning (DL) collaboratively. But traditional collaborative learning like FL relies on a central integrator for DL model aggregation, which is difficult to deploy in dynamic environments. The fully decentralized learning schemes may be the intuitive solution while the convergence of distributed learning cannot be guaranteed. Accordingly, this paper explores reconfigurable intelligent surfaces (
&lt;/p&gt;</description></item><item><title>ECToNAS&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#36827;&#21270;&#36328;&#25299;&#25169;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#20803;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#33258;&#20027;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#36328;&#25299;&#25169;&#20248;&#21270;&#65292;&#21160;&#24577;&#28155;&#21152;&#21644;&#31227;&#38500;&#21367;&#31215;&#21333;&#20803;&#65292;&#20026;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21033;&#29992;&#36866;&#24403;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05123</link><description>&lt;p&gt;
ECToNAS: &#36827;&#21270;&#36328;&#25299;&#25169;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ECToNAS: Evolutionary Cross-Topology Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05123
&lt;/p&gt;
&lt;p&gt;
ECToNAS&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#36827;&#21270;&#36328;&#25299;&#25169;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#20803;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#33258;&#20027;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#36328;&#25299;&#25169;&#20248;&#21270;&#65292;&#21160;&#24577;&#28155;&#21152;&#21644;&#31227;&#38500;&#21367;&#31215;&#21333;&#20803;&#65292;&#20026;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21033;&#29992;&#36866;&#24403;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ECToNAS&#65292;&#36825;&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#36827;&#21270;&#36328;&#25299;&#25169;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20803;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#29420;&#31435;&#22320;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#36873;&#25321;&#21512;&#36866;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#25191;&#34892;&#36328;&#25299;&#25169;&#20248;&#21270;&#12290;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#21644;&#25299;&#25169;&#20248;&#21270;&#21512;&#24182;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#36164;&#28304;&#21451;&#22909;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#20845;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#12289;EuroSAT&#12289;Fashion MNIST&#12289;MNIST&#12289;SVHN&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19981;&#20165;&#33021;&#20248;&#21270;&#26550;&#26500;&#31867;&#22411;&#20869;&#30340;&#25299;&#25169;&#65292;&#36824;&#33021;&#26681;&#25454;&#38656;&#35201;&#21160;&#24577;&#28155;&#21152;&#21644;&#31227;&#38500;&#21367;&#31215;&#21333;&#20803;&#65292;&#20174;&#32780;&#36328;&#36234;&#19981;&#21516;&#32593;&#32476;&#31867;&#22411;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#36825;&#20351;&#24471;&#37027;&#20123;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#32972;&#26223;&#30340;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#21033;&#29992;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05123v1 Announce Type: new  Abstract: We present ECToNAS, a cost-efficient evolutionary cross-topology neural architecture search algorithm that does not require any pre-trained meta controllers. Our framework is able to select suitable network architectures for different tasks and hyperparameter settings, independently performing cross-topology optimisation where required. It is a hybrid approach that fuses training and topology optimisation together into one lightweight, resource-friendly process. We demonstrate the validity and power of this approach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion MNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise the topology within an architectural type, but also to dynamically add and remove convolutional cells when and where required, thus crossing boundaries between different network types. This enables researchers without a background in machine learning to make use of appropriate model t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05122</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#22810;&#22612;&#22810;&#20852;&#36259;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Tower Multi-Interest Recommendation with User Representation Repel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#36807;&#36733;&#30340;&#26102;&#20195;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#28145;&#21051;&#35748;&#35782;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#20215;&#20540;&#12290;&#29305;&#21035;&#26159;&#22810;&#20852;&#36259;&#24207;&#21015;&#25512;&#33616;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#36890;&#36807;&#29983;&#25104;&#22810;&#29992;&#25143;&#34920;&#31034;&#65292;&#22810;&#20852;&#36259;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#37117;&#27604;&#21333;&#29992;&#25143;&#34920;&#31034;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#22256;&#25200;&#30528;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#21487;&#37319;&#29992;&#24615;&#65292;&#21363;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#30001;&#20110;&#20854;&#21333;&#22612;&#26550;&#26500;&#32780;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#36328;&#22810;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05122v1 Announce Type: cross  Abstract: In the era of information overload, the value of recommender systems has been profoundly recognized in academia and industry alike. Multi-interest sequential recommendation, in particular, is a subfield that has been receiving increasing attention in recent years. By generating multiple-user representations, multi-interest learning models demonstrate superior expressiveness than single-user representation models, both theoretically and empirically. Despite major advancements in the field, three major issues continue to plague the performance and adoptability of multi-interest learning methods, the difference between training and deployment objectives, the inability to access item information, and the difficulty of industrial adoption due to its single-tower architecture. We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel. Experimental results across multiple large-scale 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26448;&#26009;&#23646;&#24615;&#24555;&#36895;&#20272;&#31639;&#30005;&#23376;&#24102;&#38553;&#33021;&#37327;&#24182;&#39044;&#27979;&#24102;&#38553;&#31867;&#21035;&#30340;&#27169;&#22411;&#65292;&#19981;&#20165;&#26080;&#38656;&#20219;&#20309;&#21021;&#27493;&#30340;DFT&#35745;&#31639;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.05119</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26448;&#26009;&#23646;&#24615;&#20013;&#20272;&#31639;&#30005;&#23376;&#24102;&#38553;&#33021;&#37327;
&lt;/p&gt;
&lt;p&gt;
Estimation of Electronic Band Gap Energy From Material Properties Using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26448;&#26009;&#23646;&#24615;&#24555;&#36895;&#20272;&#31639;&#30005;&#23376;&#24102;&#38553;&#33021;&#37327;&#24182;&#39044;&#27979;&#24102;&#38553;&#31867;&#21035;&#30340;&#27169;&#22411;&#65292;&#19981;&#20165;&#26080;&#38656;&#20219;&#20309;&#21021;&#27493;&#30340;DFT&#35745;&#31639;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20272;&#31639;&#30005;&#23376;&#24102;&#38553;&#33021;&#37327;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#21487;&#37327;&#21270;&#30340;&#23646;&#24615;&#26469;&#39044;&#27979;&#26448;&#26009;&#30340;&#24102;&#38553;&#31867;&#21035;&#12290;&#24102;&#38553;&#33021;&#37327;&#30340;&#30830;&#23450;&#23545;&#20110;&#21306;&#20998;&#26448;&#26009;&#30340;&#37329;&#23646;&#24615;&#36136;&#20197;&#21450;&#22312;&#30005;&#23376;&#21644;&#20809;&#30005;&#35774;&#22791;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#29992;&#20110;&#35745;&#31639;&#24102;&#38553;&#33021;&#37327;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#28041;&#21450;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#26448;&#26009;&#24102;&#38553;&#33021;&#37327;&#65292;&#21033;&#29992;&#23481;&#26131;&#33719;&#21462;&#30340;&#23454;&#39564;&#24615;&#33021;&#21442;&#25968;&#65292;&#23558;&#20026;&#20256;&#32479;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#26041;&#27861;&#25552;&#20379;&#19968;&#31181;&#20248;&#36234;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#21021;&#27493;&#30340;&#22522;&#20110;DFT&#30340;&#35745;&#31639;&#25110;&#32773;&#23545;&#26448;&#26009;&#32467;&#26500;&#30340;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05119v1 Announce Type: cross  Abstract: Machine learning techniques are utilized to estimate the electronic band gap energy and forecast the band gap category of materials based on experimentally quantifiable properties. The determination of band gap energy is critical for discerning various material properties, such as its metallic nature, and potential applications in electronic and optoelectronic devices. While numerical methods exist for computing band gap energy, they often entail high computational costs and have limitations in accuracy and scalability. A machine learning-driven model capable of swiftly predicting material band gap energy using easily obtainable experimental properties would offer a superior alternative to conventional density functional theory (DFT) methods. Our model does not require any preliminary DFT-based calculation or knowledge of the structure of the material. We present a scheme for improving the performance of simple regression and classific
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05110</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#36890;&#36807;&#32452;&#21512;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Collection for Robotic Manipulation via Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05110
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#22320;&#25910;&#38598;&#25968;&#25454;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#32570;&#20047;&#24456;&#22810;&#29702;&#35299;&#12290;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#25910;&#38598;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#21464;&#21270;&#20102;&#35768;&#22810;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#29289;&#20307;&#31867;&#22411;&#21644;&#26700;&#38754;&#32441;&#29702;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#35797;&#22270;&#28085;&#30422;&#21508;&#31181;&#21508;&#26679;&#30340;&#22330;&#26223;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#21040;&#22522;&#20110;&#25968;&#25454;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#30340;&#22797;&#21512;&#33021;&#21147;&#12290;&#22914;&#26524;&#26426;&#22120;&#20154;&#31574;&#30053;&#33021;&#22815;&#20174;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32452;&#21512;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#65288;&#20363;&#22914;&#29289;&#20307;&#31867;&#22411;&#12289;&#26700;&#38754;&#39640;&#24230;&#65289;&#20197;&#22312;&#36935;&#21040;&#30475;&#19981;&#35265;&#30340;&#22240;&#32032;&#32452;&#21512;&#26102;&#25104;&#21151;&#65292;&#37027;&#20040;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#36991;&#20813;&#20026;&#22797;&#21512;&#22788;&#29702;&#30340;&#24773;&#20917;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05110v1 Announce Type: cross  Abstract: Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary a wide range of environmental factors during data collection, such as object types and table textures. While these works attempt to cover a diverse variety of scenarios, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies are able to compose different environmental factors of variation (e.g., object types, table heights) from their training data to succeed when encountering unseen factor combinations, then we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#36129;&#29486;&#20110;TinyML&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.05106</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#30005;&#27744;&#20379;&#30005;TinyML&#31995;&#32479;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#36129;&#29486;&#20110;TinyML&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#26234;&#33021;&#34892;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#24314;&#65292;&#21253;&#25324;&#26234;&#33021;&#20892;&#19994;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26234;&#33021;&#22478;&#24066;&#12290;&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#20026;TinyML&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#21033;&#29992;&#27169;&#25311;&#24314;&#27169;&#65292;&#23545;RL&#31639;&#27861;&#23545;&#30005;&#27744;&#23551;&#21629;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05106v1 Announce Type: new  Abstract: Advances in Tiny Machine Learning (TinyML) have bolstered the creation of smart industry solutions, including smart agriculture, healthcare and smart cities. Whilst related research contributes to enabling TinyML solutions on constrained hardware, there is a need to amplify real-world applications by optimising energy consumption in battery-powered systems. The work presented extends and contributes to TinyML research by optimising battery-powered image-based anomaly detection Internet of Things (IoT) systems. Whilst previous work in this area has yielded the capabilities of on-device inferencing and training, there has yet to be an investigation into optimising the management of such capabilities using machine learning approaches, such as Reinforcement Learning (RL), to improve the deployment battery life of such systems. Using modelled simulations, the battery life effects of an RL algorithm are benchmarked against static and dynamic o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20998;&#23376;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22810;&#26679;&#21270;&#20998;&#23376;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05075</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#23376;&#39044;&#27979;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models for Molecule Prediction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20998;&#23376;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22810;&#26679;&#21270;&#20998;&#23376;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#21069;&#27839;&#12290;&#23613;&#31649;LLMs&#22312;NLP&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20854;&#35774;&#35745;&#21644;&#23454;&#26045;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#20316;&#32773;&#22312;&#26412;&#25991;&#20013;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#26377;&#25928;&#22788;&#29702;&#20998;&#23376;&#39044;&#27979;&#20219;&#21153;&#65311;&#19982;&#36861;&#27714;&#26368;&#39640;&#27700;&#24179;&#24615;&#33021;&#19981;&#21516;&#65292;&#20316;&#32773;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#30340;&#20998;&#23376;&#20219;&#21153;&#20013;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05075v1 Announce Type: new  Abstract: Large Language Models (LLMs) stand at the forefront of a number of Natural Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in NLP, much of their potential in broader fields remains largely unexplored, and significant limitations persist in their design and implementation. Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry. In this paper, we explore a fundamental question: Can LLMs effectively handle molecule prediction tasks? Rather than pursuing top-tier performance, our goal is to assess how LLMs can contribute to diverse molecule tasks. We identify several classification and regression prediction tasks across six standard molecule datasets. Subsequently, we carefully design a set of prompts to query LLMs on these tasks and compare their performance with existing Machi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#25216;&#26415;&#25913;&#36827;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#21435;&#22122;&#22120;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#20102;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36136;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05069</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion-Based Generative Models via Approximated Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05069
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#25216;&#26415;&#25913;&#36827;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#21435;&#22122;&#22120;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#20102;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36136;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#65288;AOT&#65289;&#25216;&#26415;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23558;&#26368;&#20248;&#36755;&#36816;&#36817;&#20284;&#24182;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#21435;&#22122;&#22120;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#23548;&#33268;&#25193;&#25955;&#27169;&#22411;&#30340;ODE&#36712;&#36857;&#20855;&#26377;&#36739;&#20302;&#26354;&#29575;&#21644;&#20943;&#23569;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#37319;&#29992;AOT&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#20943;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#20013;&#20165;&#20351;&#29992;27&#20010;NFEs&#21644;29&#20010;NFEs&#20998;&#21035;&#23454;&#29616;&#20102;1.88&#21644;1.73&#30340;FID&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#24403;&#23558;AOT&#24212;&#29992;&#20110;&#20026;&#24341;&#23548;&#35757;&#32451;&#37492;&#21035;&#22120;&#26102;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#26641;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#65292;&#27599;&#20010;&#20998;&#21035;&#20026;1.68&#21644;1.58&#65292;&#27599;&#20010;&#37117;&#20351;&#29992;&#20102;29&#20010;NFEs&#12290;&#36825;&#19968;&#32467;&#26524;&#23637;&#31034;&#20102;AOT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05069v1 Announce Type: cross  Abstract: We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for diffusion-based generative models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of diffusion models to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of A
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05066</link><description>&lt;p&gt;
&#22797;&#20301;&#21644;&#25552;&#28860;&#65306;&#20811;&#26381;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05066
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#21457;&#23637;&#26377;&#25928;&#30340;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24403;&#38656;&#35201;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#21457;&#29983;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#38382;&#39064;&#22312;CRL&#20013;&#32463;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#26368;&#36817;&#19968;&#20123;&#26088;&#22312;&#20943;&#36731;RL&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#24037;&#20316;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;CRL&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;R&amp;D&#32467;&#21512;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#37325;&#32622;&#20195;&#29702;&#30340;&#22312;&#32447;&#28436;&#21592;&#21644;&#35780;&#35770;&#32593;&#32476;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20197;&#21450;&#31163;&#32447;&#23398;&#20064;&#27493;&#39588;&#65292;&#29992;&#20110;&#25552;&#28860;&#22312;&#32447;&#28436;&#21592;&#21644;&#20197;&#21069;&#19987;&#23478;&#21160;&#20316;&#27010;&#29575;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;Meta-World&#20219;&#21153;&#30340;&#38271;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#32447;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05066v1 Announce Type: cross  Abstract: We argue that one of the main obstacles for developing effective Continual Reinforcement Learning (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates acr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;</title><link>https://arxiv.org/abs/2403.05064</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#19982;&#35299;&#32806;&#33258;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05064
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#30417;&#30563;&#26631;&#31614;&#65292;&#26080;&#27861;&#22788;&#29702;&#30417;&#30563;&#19981;&#21487;&#29992;&#30340;&#26222;&#36941;&#24773;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#25506;&#32034;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#21457;&#29616;&#39537;&#21160;&#22270;&#25968;&#25454;&#24418;&#25104;&#20197;&#21450;&#28508;&#22312;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#22240;&#32032;&#19982;&#26368;&#20248;&#31070;&#32463;&#26550;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30001;&#20110;&#22270;&#30340;&#24615;&#36136;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05064v1 Announce Type: cross  Abstract: The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent graph factors in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21463;&#32422;&#26463;&#20248;&#21270;&#36755;&#36816;&#30340;Sinkhorn&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29109;&#27491;&#21017;&#21270;&#20844;&#24335;&#24182;&#22312;&#23545;&#20598;&#31354;&#38388;&#20013;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20122;&#32447;&#24615;&#19968;&#38454;&#25910;&#25947;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05054</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21463;&#32422;&#26463;&#20248;&#21270;&#36755;&#36816;&#30340;Sinkhorn&#31867;&#22411;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sinkhorn-type Algorithm for Constrained Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21463;&#32422;&#26463;&#20248;&#21270;&#36755;&#36816;&#30340;Sinkhorn&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29109;&#27491;&#21017;&#21270;&#20844;&#24335;&#24182;&#22312;&#23545;&#20598;&#31354;&#38388;&#20013;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20122;&#32447;&#24615;&#19968;&#38454;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27491;&#21017;&#30340;&#26368;&#20248;&#36755;&#36816;(OT)&#21644;Sinkhorn&#31639;&#27861;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#23454;&#29616;&#35745;&#31639;&#32479;&#35745;&#20998;&#24067;&#20043;&#38388;&#36755;&#36816;&#36317;&#31163;&#30340;&#22522;&#26412;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#24037;&#20316;&#20851;&#27880;&#20110;&#22312;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#32452;&#21512;&#19979;&#30340;&#19968;&#33324;&#31867;OT&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#30456;&#24212;&#30340;&#29109;&#27491;&#21017;&#21270;&#20844;&#24335;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27492;&#31867;&#21463;&#32422;&#26463;OT&#38382;&#39064;&#30340;Sinkhorn&#31867;&#22411;&#31639;&#27861;&#65292;&#25903;&#25345;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#36890;&#36807;&#29109;&#27491;&#21017;&#21270;&#35299;&#20915;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#38480;&#21046;&#20102;&#36817;&#20284;&#35823;&#24046;&#65292;&#35813;&#35823;&#24046;&#38543;&#30528;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;Lyapunov&#20989;&#25968;&#34920;&#24449;&#20248;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Sinkhorn&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#20598;&#31354;&#38388;&#20013;&#20855;&#26377;&#20122;&#32447;&#24615;&#30340;&#19968;&#38454;&#25910;&#25947;&#29575;&#12290;&#20026;&#20102;&#22312;&#24369;&#29109;&#27491;&#21017;&#21270;&#19979;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#38454;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05054v1 Announce Type: cross  Abstract: Entropic optimal transport (OT) and the Sinkhorn algorithm have made it practical for machine learning practitioners to perform the fundamental task of calculating transport distance between statistical distributions. In this work, we focus on a general class of OT problems under a combination of equality and inequality constraints. We derive the corresponding entropy regularization formulation and introduce a Sinkhorn-type algorithm for such constrained OT problems supported by theoretical guarantees. We first bound the approximation error when solving the problem through entropic regularization, which reduces exponentially with the increase of the regularization parameter. Furthermore, we prove a sublinear first-order convergence rate of the proposed Sinkhorn-type algorithm in the dual space by characterizing the optimization procedure with a Lyapunov function. To achieve fast and higher-order convergence under weak entropy regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.05045</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#35805;&#26159;&#21542;&#29305;&#27530;&#65311;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Are Human Conversations Special? A Large Language Model Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#20154;&#31867;&#20043;&#38388;&#30340;&#33258;&#28982;&#23545;&#35805;&#65288;&#20154;-&#20154;&#65289;&#26102;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19977;&#31181;&#29992;&#20363;&#65306;&#19982;&#32593;&#32476;&#20869;&#23481;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#25991;&#26412;&#30340;&#20114;&#21160;&#12290;&#36890;&#36807;&#20998;&#26512;&#36328;&#36825;&#20123;&#39046;&#22495;&#30340;&#27880;&#24847;&#36317;&#31163;&#12289;&#20998;&#25955;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#35805;&#25968;&#25454;&#25152;&#25552;&#20986;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#35805;&#38656;&#35201;&#32454;&#33268;&#22788;&#29702;&#38271;&#26399;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#30340;&#27880;&#24847;&#27169;&#24335;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#22312;&#19987;&#38376;&#21270;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#27880;&#24847;&#29109;&#20998;&#26512;&#21644;t-SNE&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05045v1 Announce Type: cross  Abstract: This study analyzes changes in the attention mechanisms of large language models (LLMs) when used to understand natural conversations between humans (human-human). We analyze three use cases of LLMs: interactions over web content, code, and mathematical texts. By analyzing attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed by conversational data. Notably, conversations require nuanced handling of long-term contextual relationships and exhibit higher complexity through their attention patterns. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with a diverse array of high-quality conversational data to enhance understanding and generation of
&lt;/p&gt;</description></item><item><title>CRM&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#21069;&#39304;&#21333;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#19977;&#24179;&#38754;&#30340;&#20960;&#20309;&#20808;&#39564;&#25972;&#21512;&#21040;&#32593;&#32476;&#35774;&#35745;&#20013;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#20845;&#20010;&#27491;&#20132;&#35270;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#21033;&#29992;&#21367;&#31215;U-Net&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05034</link><description>&lt;p&gt;
CRM&#65306;&#20351;&#29992;&#21367;&#31215;&#37325;&#24314;&#27169;&#22411;&#23558;&#21333;&#24352;&#22270;&#20687;&#36716;&#25442;&#20026;3D&#32441;&#29702;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05034
&lt;/p&gt;
&lt;p&gt;
CRM&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#21069;&#39304;&#21333;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#19977;&#24179;&#38754;&#30340;&#20960;&#20309;&#20808;&#39564;&#25972;&#21512;&#21040;&#32593;&#32476;&#35774;&#35745;&#20013;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#20845;&#20010;&#27491;&#20132;&#35270;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#21033;&#29992;&#21367;&#31215;U-Net&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05034v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#20687;&#22823;&#22411;&#37325;&#24314;&#27169;&#22411;&#65288;LRM&#65289;&#36825;&#26679;&#30340;&#21069;&#39304;3D&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#26497;&#39640;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#20854;&#26550;&#26500;&#20013;&#19977;&#24179;&#38754;&#32452;&#20214;&#30340;&#20960;&#20309;&#20808;&#39564;&#65292;&#24120;&#24120;&#23548;&#33268;&#30001;&#20110;3D&#25968;&#25454;&#37327;&#26377;&#38480;&#21644;&#35757;&#32451;&#36895;&#24230;&#24930;&#32780;&#36798;&#19981;&#21040;&#26368;&#20339;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21367;&#31215;&#37325;&#24314;&#27169;&#22411;&#65288;CRM&#65289;&#65292;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#21069;&#39304;&#21333;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#12290;&#35748;&#35782;&#21040;&#31232;&#30095;3D&#25968;&#25454;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;&#20960;&#20309;&#20808;&#39564;&#25972;&#21512;&#21040;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;CRM&#24314;&#31435;&#22312;&#20197;&#19979;&#20851;&#38190;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19977;&#24179;&#38754;&#30340;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;&#20845;&#20010;&#27491;&#20132;&#22270;&#20687;&#30340;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#23427;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#20845;&#20010;&#27491;&#20132;&#35270;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#22270;&#20687;&#39304;&#20837;&#21367;&#31215;U-Net&#65292;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#20687;&#32032;&#32423;&#23545;&#40784;&#33021;&#21147;&#21644;&#26174;&#33879;&#30340;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05034v1 Announce Type: cross  Abstract: Feed-forward 3D generative models like the Large Reconstruction Model (LRM) have demonstrated exceptional generation speed. However, the transformer-based methods do not leverage the geometric priors of the triplane component in their architecture, often leading to sub-optimal quality given the limited size of 3D data and slow training. In this work, we present the Convolutional Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D generative model. Recognizing the limitations posed by sparse 3D data, we highlight the necessity of integrating geometric priors into network design. CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images. First, it generates six orthographic view images from a single input image, then feeds these images into a convolutional U-Net, leveraging its strong pixel-level alignment capabilities and significant bandwidth 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#24230;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05033</link><description>&lt;p&gt;
&#37327;&#21270;&#27969;&#24418;:&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#30340;&#27969;&#24418;&#26159;&#21542;&#25910;&#25947;&#20110;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#24230;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;ML&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;GAN&#27169;&#22411;&#65289;&#23398;&#20064;&#30340;&#27969;&#24418;&#38543;&#30528;&#35757;&#32451;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#27599;&#20010;&#26102;&#26399;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#19982;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#30340;&#30495;&#23454;&#27969;&#24418;&#12290;&#20026;&#20102;&#37327;&#21270;&#19968;&#20010;&#27969;&#24418;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#36825;&#20123;&#24230;&#37327;&#38543;&#30528;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#27169;&#22411;&#32780;&#22914;&#20309;&#21464;&#21270;&#65292;&#20197;&#21450;&#36825;&#20123;&#24230;&#37327;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05033v1 Announce Type: cross  Abstract: This paper presents our experiments to quantify the manifolds learned by ML models (in our experiment, we use a GAN model) as they train. We compare the manifolds learned at each epoch to the real manifolds representing the real data. To quantify a manifold, we study the intrinsic dimensions and topological features of the manifold learned by the ML model, how these metrics change as we continue to train the model, and whether these metrics convergence over the course of training to the metrics of the real data manifold.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#30340;&#35889;&#22495;&#20869;&#30740;&#31350;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#35889;&#22495;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05026</link><description>&lt;p&gt;
&#20998;&#24067;&#28418;&#31227;&#19979;&#21160;&#24577;&#22270;&#30340;&#35889;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#30340;&#35889;&#22495;&#20869;&#30740;&#31350;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#35889;&#22495;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DyGNNs&#65289;&#30446;&#21069;&#22312;&#22788;&#29702;&#21160;&#24577;&#22270;&#22266;&#26377;&#30340;&#20998;&#24067;&#28418;&#31227;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#29616;&#26377;&#38024;&#23545;DyGNNs&#22312;&#20998;&#24067;&#28418;&#31227;&#35774;&#32622;&#20013;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#26102;&#38388;&#22495;&#65292;&#26410;&#33021;&#22788;&#29702;&#28041;&#21450;&#35889;&#22495;&#20013;&#20998;&#24067;&#28418;&#31227;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#21457;&#29616;&#23384;&#22312;&#19968;&#20123;&#24773;&#20917;&#65292;&#26102;&#38388;&#22495;&#20013;&#35266;&#23519;&#19981;&#21040;&#30340;&#20998;&#24067;&#28418;&#31227;&#21364;&#22312;&#35889;&#22495;&#20013;&#21487;&#20197;&#35266;&#23519;&#21040;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#30740;&#31350;&#21160;&#24577;&#22270;&#35889;&#22495;&#20869;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;i&#65289;&#25429;&#25417;&#35889;&#22495;&#20013;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#19981;&#21516;&#39057;&#29575;&#20998;&#37327;&#39537;&#21160;&#30340;&#19981;&#21516;&#22270;&#27169;&#24335;&#24182;&#38750;&#26131;&#20107;&#65307;ii&#65289;&#22914;&#20309;&#22788;&#29702;&#19982;&#21457;&#29616;&#30340;&#35889;&#27169;&#24335;&#30456;&#20851;&#30340;&#20998;&#24067;&#28418;&#31227;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#19979;&#21160;&#24577;&#22270;&#30340;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05026v1 Announce Type: cross  Abstract: Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.05024</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Hadamard U-Net for MRI Bias Field Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#22330;&#19981;&#22343;&#21248;&#24615;&#26657;&#27491;&#22312;MRI&#20998;&#26512;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#26159;&#20026;&#33041;MRI&#35774;&#35745;&#30340;&#65292;&#20551;&#35774;&#30456;&#21516;&#32452;&#32455;&#20013;&#30340;&#22270;&#20687;&#24378;&#24230;&#36981;&#24490;&#22343;&#21248;&#20998;&#24067;&#12290;&#36825;&#31181;&#20551;&#35774;&#19981;&#26131;&#36866;&#29992;&#20110;&#20854;&#20182;&#22120;&#23448;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20307;&#31215;&#23567;&#65292;&#36136;&#22320;&#19981;&#22343;&#21248;&#65288;&#24378;&#24230;&#21464;&#21270;&#22823;&#65289;&#30340;&#22120;&#23448;&#65292;&#27604;&#22914;&#21069;&#21015;&#33146;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65288;PHU-Net&#65289;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#20197;&#25552;&#21462;&#20302;&#39057;&#26631;&#37327;&#22330;&#65292;&#23558;&#20854;&#20056;&#20197;&#21407;&#22987;&#36755;&#20837;&#20197;&#33719;&#24471;&#21407;&#22411;&#26657;&#27491;&#22270;&#20687;&#12290;HU-Net&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#12290;&#22312;&#39057;&#22495;&#20013;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#65288;&#32553;&#25918;&#23618;&#65289;&#12289;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05024v1 Announce Type: cross  Abstract: Magnetic field inhomogeneity correction remains a challenging task in MRI analysis. Most established techniques are designed for brain MRI by supposing that image intensities in the identical tissue follow a uniform distribution. Such an assumption cannot be easily applied to other organs, especially those that are small in size and heterogeneous in texture (large variations in intensity), such as the prostate. To address this problem, this paper proposes a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the low-frequency scalar field, multiplied by the original input to obtain the prototypical corrected image. HU-Net converts the input image from the time domain into the frequency domain via Hadamard transform. In the frequency domain, high-frequency components are eliminated using the trainable filter (scaling layer), hard-thresholding laye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#21644;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22810;&#22270;&#21367;&#31215;&#30340;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.05014</link><description>&lt;p&gt;
&#31616;&#21333;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Simple Multigraph Convolution Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#21644;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22810;&#22270;&#21367;&#31215;&#30340;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#22270;&#21367;&#31215;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#22810;&#20010;&#22270;&#20043;&#38388;&#30340;&#20132;&#21449;&#35270;&#22270;&#20132;&#20114;&#65292;&#35201;&#20040;&#30001;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#35270;&#22270;&#22810;&#39033;&#24335;&#36816;&#31639;&#31526;&#32780;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#23427;&#39318;&#20808;&#20174;&#22810;&#22270;&#20013;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#65292;&#21253;&#25324;&#36793;&#32423;&#21644;&#23376;&#22270;&#32423;&#25299;&#25169;&#65292;&#28982;&#21518;&#22522;&#20110;&#21407;&#22987;&#22810;&#22270;&#21644;&#19968;&#33268;&#30340;&#25299;&#25169;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;SMGCN&#21033;&#29992;&#19968;&#33268;&#30340;&#25299;&#25169;&#36827;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#20132;&#21449;&#35270;&#22270;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#20174;&#32780;&#25191;&#34892;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#65292;&#36981;&#24490;&#35889;&#21367;&#31215;&#33539;&#24335;&#65292;&#24182;&#26377;&#25928;&#38477;&#20302;&#26631;&#20934;&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SMGCN&#22312;ACM&#21644;DBLP&#22810;&#22270;&#22522;&#20934;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05014v1 Announce Type: cross  Abstract: Existing multigraph convolution methods either ignore the cross-view interaction among multiple graphs, or induce extremely high computational cost due to standard cross-view polynomial operators. To alleviate this problem, this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which first extracts consistent cross-view topology from multigraphs including edge-level and subgraph-level topology, then performs polynomial expansion based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes the consistent topologies in polynomial expansion rather than standard cross-view polynomial expansion, which performs credible cross-view spatial message-passing, follows the spectral convolution paradigm, and effectively reduces the complexity of standard polynomial expansion. In the simulations, experimental results demonstrate that SMGCN achieves state-of-the-art performance on ACM and DBLP multigraph benchmark datas
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05006</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20803;&#20154;&#31867;&#21453;&#39304;&#30340;&#21487;&#35777;&#26126;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Party Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#26126;&#30830;&#24314;&#27169;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#22810;&#26041;RLHF&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#22914;&#20309;&#22833;&#36133;&#65292;&#22240;&#20026;&#23398;&#20064;&#21333;&#19968;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#25429;&#25417;&#21644;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#32467;&#21512;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#20010;&#20559;&#22909;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#26469;&#25972;&#21512;&#22810;&#26041;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20026;&#20248;&#21270;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65288;&#22914;Nash&#12289;Utilitarian&#21644;Leximin&#31119;&#21033;&#65289;&#24314;&#31435;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05006v1 Announce Type: cross  Abstract: Reinforcement learning with human feedback (RLHF) is an emerging paradigm to align models with human preferences. Typically, RLHF aggregates preferences from multiple individuals who have diverse viewpoints that may conflict with each other. Our work \textit{initiates} the theoretical study of multi-party RLHF that explicitly models the diverse preferences of multiple individuals. We show how traditional RLHF approaches can fail since learning a single reward function cannot capture and balance the preferences of multiple individuals. To overcome such limitations, we incorporate meta-learning to learn multiple preferences and adopt different social welfare functions to aggregate the preferences across multiple parties. We focus on the offline learning setting and establish sample complexity bounds, along with efficiency and fairness guarantees, for optimizing diverse social welfare functions such as Nash, Utilitarian, and Leximin welfa
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05004</link><description>&lt;p&gt;
&#26080;&#27861;&#35760;&#20303;&#38271;&#25991;&#26723;&#20013;&#30340;&#32454;&#33410;&#65311;&#24744;&#38656;&#35201;&#19968;&#20123;R&amp;R
&lt;/p&gt;
&lt;p&gt;
Can't Remember Details in Long Documents? You Need Some R&amp;R
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05004
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35832;&#22914;&#38271;&#31687;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#38169;&#36807;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#38388;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;$\textit{R&amp;R}$&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26032;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$\textit{reprompting}$&#21644;$\textit{in-context retrieval}$&#65288;ICR&#65289;&#65292;&#20197;&#20943;&#36731;&#25991;&#26723;&#22411;QA&#20013;&#30340;&#36825;&#31181;&#24433;&#21709;&#12290;&#22312;$\textit{reprompting}$&#20013;&#65292;&#25105;&#20204;&#21608;&#26399;&#24615;&#22320;&#22312;&#25972;&#20010;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#37325;&#22797;&#25552;&#31034;&#35828;&#26126;&#65292;&#20197;&#25552;&#37266;LLM&#20854;&#21407;&#22987;&#20219;&#21153;&#12290;&#22312;ICR&#20013;&#65292;&#25105;&#20204;&#24182;&#19981;&#25351;&#31034;LLM&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#26159;&#25351;&#31034;&#23427;&#26816;&#32034;&#19982;&#32473;&#23450;&#38382;&#39064;&#26368;&#30456;&#20851;&#30340;&#21069;$k$&#20010;&#27573;&#33853;&#32534;&#21495;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#31532;&#20108;&#20010;QA&#25552;&#31034;&#20013;&#30340;&#32553;&#30053;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4 Turbo&#21644;Claude-2.1&#22312;&#38271;&#24230;&#36798;&#21040;80k&#26631;&#35760;&#30340;&#25991;&#26723;&#19978;&#27979;&#35797;&#20102;R&amp;R&#65292;&#24182;&#24179;&#22343;&#35266;&#23519;&#21040;QA&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;16&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&amp;R}$ -- a combination of two novel prompt-based methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&amp;R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an
&lt;/p&gt;</description></item><item><title>QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.04990</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21943;&#27880;&#21028;&#21035;
&lt;/p&gt;
&lt;p&gt;
Jet Discrimination with Quantum Complete Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04990
&lt;/p&gt;
&lt;p&gt;
QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#24050;&#25193;&#23637;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#65292;&#21363;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;QCGNN&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#23436;&#20840;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;QCGNN&#30001;&#20110;&#37327;&#23376;&#24182;&#34892;&#24615;&#30340;&#29305;&#24615;&#65292;&#22312;&#36895;&#24230;&#19978;&#23545;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#20855;&#26377;&#22810;&#39033;&#24335;&#21152;&#36895;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;QCGNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21943;&#27880;&#21028;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21943;&#27880;&#29992;&#23436;&#20840;&#22270;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#19982;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04990v1 Announce Type: cross  Abstract: Machine learning, particularly deep neural networks, has been widely utilized in high energy physics and has shown remarkable results in various applications. Moreover, the concept of machine learning has been extended to quantum computers, giving rise to a new research area known as quantum machine learning. In this paper, we propose a novel variational quantum circuit model, Quantum Complete Graph Neural Network (QCGNN), designed for learning complete graphs. We argue that QCGNN has a polynomial speedup against its classical counterpart, due to the property of quantum parallelism. In this paper, we study the application of QCGNN through the challenging jet discrimination, where the jets are represented with complete graphs. Subsequently, we conduct a comparative analysis with classical graph neural networks to establish a benchmark.
&lt;/p&gt;</description></item><item><title>Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.04978</link><description>&lt;p&gt;
Stacking&#20316;&#20026;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stacking as Accelerated Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04978
&lt;/p&gt;
&lt;p&gt;
Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stacking&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#23618;&#25968;&#24182;&#36890;&#36807;&#20174;&#26087;&#23618;&#22797;&#21046;&#21442;&#25968;&#26469;&#21021;&#22987;&#21270;&#26032;&#23618;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;Stacking&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#65306;&#21363;&#65292;Stacking&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#35813;&#29702;&#35770;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#25552;&#21319;&#26041;&#27861;&#20013;&#26500;&#24314;&#30340;&#21152;&#27861;&#38598;&#25104;&#31561;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#27599;&#19968;&#36718;&#25552;&#21319;&#36807;&#31243;&#20013;&#21021;&#22987;&#21270;&#26032;&#20998;&#31867;&#22120;&#30340;&#31867;&#20284;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#28508;&#33021;&#20989;&#25968;&#20998;&#26512;&#65292;Stacking&#30830;&#23454;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#26032;&#20013;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our
&lt;/p&gt;</description></item><item><title>C2P-GCN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#24418;&#25104;&#34917;&#19969;&#32423;&#22270;&#65292;&#31532;&#20108;&#38454;&#27573;&#24418;&#25104;&#22270;&#20687;&#32423;&#22270;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.04962</link><description>&lt;p&gt;
C2P-GCN&#65306;&#29992;&#20110;&#32467;&#30452;&#32928;&#30284;&#20998;&#32423;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04962
&lt;/p&gt;
&lt;p&gt;
C2P-GCN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#24418;&#25104;&#34917;&#19969;&#32423;&#22270;&#65292;&#31532;&#20108;&#38454;&#27573;&#24418;&#25104;&#22270;&#20687;&#32423;&#22270;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#32534;&#30721;&#32452;&#32455;/&#22120;&#23448;&#32467;&#26500;&#20449;&#24687;&#65292;&#36880;&#28176;&#25104;&#20026;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#23398;&#22270;&#20687;&#20998;&#32423;&#30340;&#39318;&#36873;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#28041;&#21450;&#23558;&#25972;&#24352;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20998;&#25104;&#26356;&#23567;&#25110;&#20013;&#31561;&#22823;&#23567;&#30340;&#34917;&#19969;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#34917;&#19969;&#19978;&#26500;&#24314;&#22270;&#20197;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#25972;&#20010;WSI&#20013;&#23384;&#22312;&#30340;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20381;&#36182;&#20110;&#26469;&#33258;&#22823;&#37327;&#22270;&#20687;&#34917;&#19969;&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;C2P-GCN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23427;&#22522;&#20110;WSI&#19978;&#27599;&#20010;&#34917;&#19969;&#19978;&#30340;&#32454;&#32990;&#32452;&#32455;&#24418;&#25104;&#19968;&#20010;&#34917;&#19969;&#32423;&#22270;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#23427;&#22522;&#20110;WSI&#20013;&#27599;&#20010;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#37327;&#24418;&#25104;&#19968;&#20010;&#22270;&#20687;&#32423;&#22270;&#65292;&#23558;&#27599;&#20010;&#34917;&#19969;&#35270;&#20026;&#22270;&#30340;&#33410;&#28857;&#12290;&#36825;&#31181;&#22270;&#34920;&#31034;&#26159;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04962v1 Announce Type: cross  Abstract: Graph-based learning approaches, due to their ability to encode tissue/organ structure information, are increasingly favored for grading colorectal cancer histology images. Recent graph-based techniques involve dividing whole slide images (WSIs) into smaller or medium-sized patches, and then building graphs on each patch for direct use in training. This method, however, fails to capture the tissue structure information present in an entire WSI and relies on training from a significantly large dataset of image patches. In this paper, we propose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a two-stage graph formation-based approach. In the first stage, it forms a patch-level graph based on the cell organization on each patch of a WSI. In the second stage, it forms an image-level graph based on a similarity measure between patches of a WSI considering each patch as a node of a graph. This graph representation is t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04960</link><description>&lt;p&gt;
SecGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SecGPT: An Execution Isolation Architecture for LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25193;&#23637;&#20026;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24320;&#22987;&#25903;&#25345;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;LLMs&#30340;&#20107;&#23454;&#19978;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#33258;&#21160;&#25191;&#34892;&#33539;&#24335;&#65306;&#21363;&#65292;&#24212;&#29992;&#31243;&#24207;&#21450;&#20854;&#20132;&#20114;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#30340;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#34987;&#20801;&#35768;&#33258;&#30001;&#22320;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#19982;&#31995;&#32479;&#20114;&#21160;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#29983;&#24577;&#31995;&#32479;&#31867;&#20284;&#20110;&#26089;&#26399;&#35745;&#31639;&#24179;&#21488;&#30340;&#35774;&#32622;&#65292;&#22312;&#37027;&#37324;&#24212;&#29992;&#31243;&#24207;&#21644;&#31995;&#32479;&#20043;&#38388;&#32570;&#20047;&#36275;&#22815;&#30340;&#38548;&#31163;&#12290;&#30001;&#20110;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#24182;&#19988;&#21463;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#19981;&#31934;&#30830;&#24615;&#21152;&#21095;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#20250;&#20026;&#29992;&#25143;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SecGPT&#65292;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#32531;&#35299;&#30001;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#24341;&#36215;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;SecGPT&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#38548;&#31163;&#24212;&#29992;&#31243;&#24207;&#30340;&#25191;&#34892;&#21644;&#26356;&#22810;&#30340;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Spatiotemporal Style Transfer (STST)&#31639;&#27861;&#65292;&#22522;&#20110;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20801;&#35768;&#29983;&#25104;&#24378;&#22823;&#30340;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.04940</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#29983;&#25104;&#30340;&#26102;&#31354;&#39118;&#26684;&#36716;&#31227;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A spatiotemporal style transfer algorithm for dynamic visual stimulus generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Spatiotemporal Style Transfer (STST)&#31639;&#27861;&#65292;&#22522;&#20110;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20801;&#35768;&#29983;&#25104;&#24378;&#22823;&#30340;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#35270;&#35273;&#20449;&#24687;&#22914;&#20309;&#22312;&#29983;&#29289;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#32534;&#30721;&#36890;&#24120;&#38656;&#35201;&#35270;&#35273;&#31185;&#23398;&#23478;&#29983;&#25104;&#36866;&#24403;&#30340;&#21050;&#28608;&#26469;&#27979;&#35797;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#38761;&#21629;&#65292;&#20363;&#22914;&#22270;&#20687;&#39118;&#26684;&#36716;&#31227;&#65292;&#20294;&#35270;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#21364;&#24456;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#31354;&#39118;&#26684;&#36716;&#31227;&#65288;STST&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#35270;&#35273;&#21050;&#28608;&#29983;&#25104;&#26694;&#26550;&#65292;&#20801;&#35768;&#24378;&#22823;&#22320;&#25805;&#20316;&#21644;&#21512;&#25104;&#35270;&#39057;&#21050;&#28608;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20998;&#35299;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#20197;&#29983;&#25104;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#20854;&#27169;&#22411;&#23618;&#28608;&#27963;&#19982;&#36755;&#20837;&#35270;&#39057;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04940v1 Announce Type: cross  Abstract: Understanding how visual information is encoded in biological and artificial systems often requires vision scientists to generate appropriate stimuli to test specific hypotheses. Although deep neural network models have revolutionized the field of image generation with methods such as image style transfer, available methods for video generation are scarce. Here, we introduce the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus generation framework that allows powerful manipulation and synthesis of video stimuli for vision research. It is based on a two-stream deep neural network model that factorizes spatial and temporal features to generate dynamic visual stimuli whose model layer activations are matched to those of input videos. As an example, we show that our algorithm enables the generation of model metamers, dynamic stimuli whose layer activations within our two-stream model are matched to those of natural
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.04937</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradient-free neural topology optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26799;&#24230;&#20248;&#21270;&#22120;&#21487;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#26080;&#35770;&#20854;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#25110;&#21487;&#24494;&#24615;&#22914;&#20309;&#65292;&#20294;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#30340;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#25299;&#25169;&#20248;&#21270;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#38382;&#39064;&#30340;&#32500;&#24230;&#20063;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#24403;&#22312;&#28508;&#22312;&#31354;&#38388;&#20248;&#21270;&#35774;&#35745;&#26102;&#65292;&#36845;&#20195;&#27425;&#25968;&#33267;&#23569;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#20351;&#29992;&#28508;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24191;&#27867;&#30340;&#35745;&#31639;&#23454;&#39564;&#65292;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#22522;&#20110;&#26799;&#24230;&#30340;&#25299;&#25169;&#20248;&#21270;&#23545;&#20110;&#21487;&#24494;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#32467;&#26500;&#30340;&#21512;&#35268;&#24615;&#20248;&#21270;&#65292;&#20173;&#28982;&#26356;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#37027;&#20123;&#38656;&#35201;&#26080;&#26799;&#24230;&#26041;&#27861;&#30340;&#38382;&#39064;&#24320;&#36767;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04937v1 Announce Type: new  Abstract: Gradient-free optimizers allow for tackling problems regardless of the smoothness or differentiability of their objective function, but they require many more iterations to converge when compared to gradient-based algorithms. This has made them unviable for topology optimization due to the high computational cost per iteration and high dimensionality of these problems. We propose a pre-trained neural reparameterization strategy that leads to at least one order of magnitude decrease in iteration count when optimizing the designs in latent space, as opposed to the conventional approach without latent reparameterization. We demonstrate this via extensive computational experiments in- and out-of-distribution with the training data. Although gradient-based topology optimization is still more efficient for differentiable problems, such as compliance optimization of structures, we believe this work will open up a new path for problems where gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ForgetNet&#21644;G-ForgetNet&#65292;&#36890;&#36807;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#21644;&#24341;&#20837;&#38376;&#25511;&#26426;&#21046;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30683;&#30462;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.04929</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65306;&#20998;&#26512;&#19982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ForgetNet&#21644;G-ForgetNet&#65292;&#36890;&#36807;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#21644;&#24341;&#20837;&#38376;&#25511;&#26426;&#21046;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30683;&#30462;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#27169;&#20223;&#31639;&#27861;&#25191;&#34892;&#36880;&#27493;&#36827;&#34892;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#33539;&#24335;&#28041;&#21450;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#26469;&#39044;&#27979;&#26410;&#26469;&#25191;&#34892;&#27493;&#39588;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#30340;&#35266;&#23519;&#26159;&#65292;&#36825;&#31181;&#21382;&#21490;&#20381;&#36182;&#26412;&#36136;&#19978;&#19982;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30456;&#30683;&#30462;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;ForgetNet&#65292;&#23427;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#65292;&#22240;&#27492;&#19982;&#20219;&#21153;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;ForgetNet&#22312;&#26089;&#26399;&#38454;&#27573;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;G-ForgetNet&#65292;&#23427;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#20801;&#35768;&#26377;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#21382;&#21490;&#23884;&#20837;&#12290;&#36825;&#31181;&#22686;&#24378;&#30340;&#33021;&#21147;&#22312;&#27169;&#22411;&#30340;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#22522;&#20110;CLRS-30&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04929v1 Announce Type: cross  Abstract: Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic reasoning tasks. Based on this motivation, we present our ForgetNet, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training ForgetNet at early stages, we further introduce G-ForgetNet, which uses a gating mechanism to allow for the selective integration of historical embeddings. Such an enhanced capability provides valuable computational pathways during the model's early training phase. Our extensive experiments, based on the CLRS-30 algorithmic reasoning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#30340;&#25511;&#21046;&#23646;&#24615;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04923</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25511;&#21046;&#20026;&#22522;&#30784;&#30340;&#22270;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Control-based Graph Embeddings with Data Augmentation for Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#30340;&#25511;&#21046;&#23646;&#24615;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#21160;&#24577;&#32593;&#32476;&#22312;&#22270;&#19978;&#30340;&#25511;&#21046;&#23646;&#24615;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12290;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#20174;&#36755;&#20837;&#22270;&#21019;&#24314;&#8220;&#22686;&#24378;&#8221;&#22270;&#12290;&#34429;&#28982;&#19982;&#21407;&#22987;&#22270;&#19981;&#21516;&#65292;&#36825;&#20123;&#22686;&#24378;&#22270;&#20445;&#30041;&#20102;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#25511;&#21046;&#23646;&#24615;&#29983;&#25104;&#36825;&#20123;&#22686;&#24378;&#22270;&#12290;&#26680;&#24515;&#27010;&#24565;&#22260;&#32469;&#30528;&#23545;&#21407;&#22987;&#22270;&#36827;&#34892;&#25200;&#21160;&#20197;&#21019;&#24314;&#19968;&#20010;&#26032;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#29305;&#23450;&#20110;&#32593;&#32476;&#21644;&#22270;&#30340;&#21487;&#25511;&#29305;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;&#20102;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04923v1 Announce Type: new  Abstract: In this paper, we study the problem of unsupervised graph representation learning by harnessing the control properties of dynamical networks defined on graphs. Our approach introduces a novel framework for contrastive learning, a widely prevalent technique for unsupervised representation learning. A crucial step in contrastive learning is the creation of 'augmented' graphs from the input graphs. Though different from the original graphs, these augmented graphs retain the original graph's structural characteristics. Here, we propose a unique method for generating these augmented graphs by leveraging the control properties of networks. The core concept revolves around perturbing the original graph to create a new one while preserving the controllability properties specific to networks and graphs. Compared to the existing methods, we demonstrate that this innovative approach enhances the effectiveness of contrastive learning frameworks, lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04919</link><description>&lt;p&gt;
&#37492;&#21035;&#21151;&#33021;&#20381;&#36182;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifying Causal Effects Under Functional Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#65292;&#21463;&#20004;&#20010;&#25913;&#36827;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#22240;&#26524;&#22270;&#20013;&#26576;&#20123;&#21464;&#37327;&#26159;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#31532;&#19968;&#65292;&#24403;&#26576;&#20123;&#21464;&#37327;&#26159;&#21151;&#33021;&#30340;&#26102;&#65292;&#19968;&#20010;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21487;&#33021;&#21464;&#24471;&#21487;&#35782;&#21035;&#12290;&#31532;&#20108;&#65292;&#21487;&#20197;&#25490;&#38500;&#35266;&#27979;&#26576;&#20123;&#21151;&#33021;&#21464;&#37327;&#32780;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22522;&#20110;&#19968;&#20010;&#25490;&#38500;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20174;&#22240;&#26524;&#22270;&#20013;&#21024;&#38500;&#21151;&#33021;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#32467;&#26524;&#22240;&#26524;&#22270;&#20013;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04919v1 Announce Type: new  Abstract: We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04884</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04884
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#26893;&#20837;&#30340;&#35270;&#32593;&#33180;&#20551;&#20307;&#20026;&#36890;&#36807;&#32469;&#36807;&#35270;&#32593;&#33180;&#20013;&#25439;&#22351;&#30340;&#20809;&#24863;&#21463;&#32454;&#32990;&#24182;&#30452;&#25509;&#21050;&#28608;&#21097;&#20313;&#21151;&#33021;&#24615;&#35270;&#32593;&#33180;&#32454;&#32990;&#26469;&#24674;&#22797;&#37096;&#20998;&#35270;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#22836;&#21644;&#35270;&#32593;&#33180;&#32454;&#32990;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36755;&#36890;&#24120;&#21463;&#38480;&#20110;&#30005;&#26497;&#38453;&#21015;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#23545;&#19981;&#21516;&#33410;&#32454;&#32990;&#31867;&#22411;&#30340;&#29305;&#24322;&#24615;&#19981;&#36275;&#65292;&#23548;&#33268;&#21050;&#28608;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#21487;&#36870;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#29992;&#20316;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#26367;&#20195;&#65292;&#24182;&#23558;&#36755;&#20837;&#25668;&#20687;&#22836;&#20449;&#21495;&#32534;&#30721;&#20026;&#30005;&#26497;&#38453;&#21015;&#19978;&#20248;&#21270;&#30340;&#30005;&#21050;&#28608;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#22914;&#31616;&#21333;&#30340;&#38477;&#37319;&#26679;&#12289;&#32447;&#24615;&#27169;&#22411;&#21644;&#21069;&#39304;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04884v1 Announce Type: cross  Abstract: Implantable retinal prostheses offer a promising solution to restore partial vision by circumventing damaged photoreceptor cells in the retina and directly stimulating the remaining functional retinal cells. However, the information transmission between the camera and retinal cells is often limited by the low resolution of the electrode array and the lack of specificity for different ganglion cell types, resulting in suboptimal stimulations. In this work, we propose to utilize normalizing flow-based conditional invertible neural networks to optimize retinal implant stimulation in an unsupervised manner. The invertibility of these networks allows us to use them as a surrogate for the computational model of the visual system, while also encoding input camera signals into optimized electrical stimuli on the electrode array. Compared to other methods, such as trivial downsampling, linear models, and feed-forward convolutional neural networ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; SGNN&#65292;&#29992;&#20110;&#23398;&#20064;&#36328;&#36234;&#19981;&#21516;&#23478;&#26063;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#34892;&#27874;&#23396;&#27874;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#22312;&#22823;&#22411;&#35745;&#31639;&#22495;&#20013;&#30340;&#20256;&#25773;&#22833;&#36133;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04883</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34892;&#27874;&#23396;&#27874;
&lt;/p&gt;
&lt;p&gt;
Learning Traveling Solitary Waves Using Separable Gaussian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; SGNN&#65292;&#29992;&#20110;&#23398;&#20064;&#36328;&#36234;&#19981;&#21516;&#23478;&#26063;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#34892;&#27874;&#23396;&#27874;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#22312;&#22823;&#22411;&#35745;&#31639;&#22495;&#20013;&#30340;&#20256;&#25773;&#22833;&#36133;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#36328;&#36234;&#21508;&#31181;&#20559;&#24494;&#20998;&#26041;&#31243;&#23478;&#26063;&#30340;&#34892;&#27874;&#23396;&#27874;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#65292;&#31216;&#20026;&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#65292;&#38598;&#25104;&#21040;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26694;&#26550;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;PINNs&#23558;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#35270;&#20026;&#29420;&#31435;&#36755;&#20837;&#19981;&#21516;&#65292;&#26412;&#26041;&#27861;&#21033;&#29992;&#27874;&#29305;&#24615;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#25152;&#35859;&#30340;&#20849;&#34892;&#27874;&#26694;&#26550;&#12290;&#36825;&#31181;&#36866;&#24212;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;PINNs&#24212;&#29992;&#20110;&#22823;&#22411;&#35745;&#31639;&#22495;&#26102;&#20986;&#29616;&#30340;&#20256;&#25773;&#22833;&#36133;&#38382;&#39064;&#12290;&#22312;&#27492;&#65292;SGNN&#26550;&#26500;&#23637;&#31034;&#20102;&#23545;&#65288;1+1&#65289;&#32500;&#12289;$b$-&#23478;&#26063;&#30340;PDE&#20013;&#30340;&#21333;&#23792;&#12289;&#22810;&#23792;&#21644;&#23450;&#24120;&#35299;&#30340;&#24378;&#22823;&#36924;&#36817;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#24182;&#19981;&#20165;&#25506;&#32034;&#20102;$ab$-fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04883v1 Announce Type: cross  Abstract: In this paper, we apply a machine-learning approach to learn traveling solitary waves across various families of partial differential equations (PDEs). Our approach integrates a novel interpretable neural network (NN) architecture, called Separable Gaussian Neural Networks (SGNN) into the framework of Physics-Informed Neural Networks (PINNs). Unlike the traditional PINNs that treat spatial and temporal data as independent inputs, the present method leverages wave characteristics to transform data into the so-called co-traveling wave frame. This adaptation effectively addresses the issue of propagation failure in PINNs when applied to large computational domains. Here, the SGNN architecture demonstrates robust approximation capabilities for single-peakon, multi-peakon, and stationary solutions within the (1+1)-dimensional, $b$-family of PDEs. In addition, we expand our investigations, and explore not only peakon solutions in the $ab$-fa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Kronecker&#20998;&#35299;&#30340;&#27880;&#24847;&#21147;&#65292; Hierarchical attention-based Kronecker-decomposition for high-resolution time series classification.</title><link>https://arxiv.org/abs/2403.04882</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147; Kronecker &#20998;&#35299;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04882
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Kronecker&#20998;&#35299;&#30340;&#27880;&#24847;&#21147;&#65292; Hierarchical attention-based Kronecker-decomposition for high-resolution time series classification.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#30001;&#20110;&#21508;&#20010;&#39046;&#22495;&#20013;&#35814;&#32454;&#26102;&#38388;&#25968;&#25454;&#19981;&#26029;&#22686;&#21152;&#32780;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20851;&#38190;&#22312;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#20998;&#36776;&#29575;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22788;&#29702;&#36880;&#28176;&#22686;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;&#22312;&#22788;&#29702;&#36825;&#31867;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#20132;&#20114;&#33539;&#22260;&#23558;&#38271;&#26102;&#38388;&#24207;&#21015;&#20998;&#23618;&#32534;&#30721;&#20026;&#22810;&#20010;&#23618;&#32423;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#23618;&#32423;&#25429;&#33719;&#20851;&#31995;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#21152;&#40065;&#26834;&#12289;&#34920;&#36798;&#20016;&#23500;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#27874;&#21160;&#21644;&#38271;&#26399;&#36235;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#21464;&#25442;&#22120;&#39592;&#24178;&#65288;KronTime&#65289;&#65292;&#36890;&#36807;&#24341;&#20837; Kronecker &#20998;&#35299;&#30340;&#27880;&#24847;&#21147;&#26469;&#22788;&#29702;&#36825;&#31181;&#22810;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04882v1 Announce Type: new  Abstract: The high-resolution time series classification problem is essential due to the increasing availability of detailed temporal data in various domains. To tackle this challenge effectively, it is imperative that the state-of-the-art attention model is scalable to accommodate the growing sequence lengths typically encountered in high-resolution time series data, while also demonstrating robustness in handling the inherent noise prevalent in such datasets. To address this, we propose to hierarchically encode the long time series into multiple levels based on the interaction ranges. By capturing relationships at different levels, we can build more robust, expressive, and efficient models that are capable of capturing both short-term fluctuations and long-term trends in the data. We then propose a new time series transformer backbone (KronTime) by introducing Kronecker-decomposed attention to process such multi-level time series, which sequenti
&lt;/p&gt;</description></item><item><title>GPTRec&#27169;&#22411;&#20351;&#29992;Next-K&#31574;&#30053;&#26469;&#29983;&#25104;&#25512;&#33616;&#65292;&#19982;&#20256;&#32479;&#30340;Top-K&#27169;&#22411;&#19981;&#21516;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32771;&#34385;&#36229;&#20986;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#22797;&#26434;&#39033;&#30446;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04875</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;GPTRec&#19982;&#36229;&#20986;&#20934;&#30830;&#24615;&#30446;&#26631;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04875
&lt;/p&gt;
&lt;p&gt;
GPTRec&#27169;&#22411;&#20351;&#29992;Next-K&#31574;&#30053;&#26469;&#29983;&#25104;&#25512;&#33616;&#65292;&#19982;&#20256;&#32479;&#30340;Top-K&#27169;&#22411;&#19981;&#21516;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32771;&#34385;&#36229;&#20986;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#22797;&#26434;&#39033;&#30446;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#25913;&#32534;&#65292;&#22914;BERT4Rec&#21644;SASRec&#65292;&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102; accuracy-based &#25351;&#26631;&#65292;&#22914;NDCG&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#39033;&#30446;&#35270;&#20026;&#26631;&#35760;&#65292;&#28982;&#21518;&#21033;&#29992;&#35780;&#20998;-&#25490;&#21517;&#26041;&#27861;&#65288;Top-K&#31574;&#30053;&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#39318;&#20808;&#35745;&#31639;&#39033;&#30446;&#24471;&#20998;&#65292;&#28982;&#21518;&#26681;&#25454;&#27492;&#20998;&#25968;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#23545;&#20110;&#20934;&#30830;&#24615;&#25351;&#26631;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#24456;&#38590;&#23558;&#20854;&#29992;&#20110;&#20248;&#21270;&#26356;&#22797;&#26434;&#30340;&#36229;&#20986;&#20934;&#30830;&#24615;&#25351;&#26631;&#65292;&#22914;&#22810;&#26679;&#24615;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516; Next-K &#31574;&#30053;&#30340;GPTRec&#27169;&#22411;&#65292;&#20316;&#20026;Top-K&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#19982;&#20256;&#32479;&#30340;Top-K&#25512;&#33616;&#30456;&#27604;&#65292;Next-K&#20250;&#36880;&#20010;&#39033;&#30446;&#29983;&#25104;&#25512;&#33616;&#65292;&#22240;&#27492;&#65292;&#21487;&#20197;&#32771;&#34385;&#36229;&#20986;&#20934;&#30830;&#24615;&#25351;&#26631;&#20013;&#37325;&#35201;&#30340;&#22797;&#26434;&#39033;&#30446;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04875v1 Announce Type: cross  Abstract: Adaptations of Transformer models, such as BERT4Rec and SASRec, achieve state-of-the-art performance in the sequential recommendation task according to accuracy-based metrics, such as NDCG. These models treat items as tokens and then utilise a score-and-rank approach (Top-K strategy), where the model first computes item scores and then ranks them according to this score. While this approach works well for accuracy-based metrics, it is hard to use it for optimising more complex beyond-accuracy metrics such as diversity. Recently, the GPTRec model, which uses a different Next-K strategy, has been proposed as an alternative to the Top-K models. In contrast with traditional Top-K recommendations, Next-K generates recommendations item-by-item and, therefore, can account for complex item-to-item interdependencies important for the beyond-accuracy measures. However, the original GPTRec paper focused only on accuracy in experiments and needed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04867</link><description>&lt;p&gt;
&#32452;&#38544;&#31169;&#25918;&#22823;&#21644;&#23376;&#25277;&#26679;&#30340;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;(DP)&#20855;&#26377;&#22810;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#22914;&#23545;&#21518;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12289;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#65292;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#30456;&#20114;&#29420;&#31435;&#25512;&#23548;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#22810;&#20010;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#25552;&#20379;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20445;&#35777;&#65292;&#25105;&#20204;&#22312;R&#233;nyi-DP&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#27604;$(\epsilon,\delta)$-DP&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;&#20316;&#20026;&#36825;&#20010;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#19981;&#20165;&#35753;&#25105;&#20204;&#25913;&#36827;&#21644;&#27867;&#21270;&#29616;&#26377;&#30340;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
&lt;/p&gt;</description></item><item><title>&#22823;&#20048;&#36879;&#20551;&#35774;&#25351;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#35757;&#32451;&#23396;&#31435;&#23376;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#65292;&#35843;&#26597;&#32508;&#36848;&#20102;LTH&#29616;&#29366;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.04861</link><description>&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Lottery Ticket Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04861
&lt;/p&gt;
&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;&#25351;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#35757;&#32451;&#23396;&#31435;&#23376;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#65292;&#35843;&#26597;&#32508;&#36848;&#20102;LTH&#29616;&#29366;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;(LTH)&#25351;&#20986;&#65292;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21253;&#21547;&#19968;&#20010;&#39640;&#24230;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#65292;&#20013;&#22870;&#31080;&#65289;&#65292;&#24403;&#20197;&#23396;&#31435;&#26041;&#24335;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LTH&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#24050;&#32463;&#22312;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#23454;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#35299;&#20915;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20363;&#22914;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#24320;&#28304;&#26694;&#26550;&#21644;&#19968;&#33268;&#30340;&#23454;&#39564;&#35774;&#32622;&#23545;LTH&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#20174;&#19981;&#21516;&#35282;&#24230;&#23457;&#35270;&#20197;&#24448;&#20851;&#20110;LTH&#30340;&#30740;&#31350;&#21644;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21015;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#36825;&#39033;&#35843;&#26597;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;LTH&#30340;&#29616;&#29366;&#65292;&#24182;&#24320;&#21457;&#19968;&#20010;&#24471;&#21040;&#22949;&#21892;&#32500;&#25252;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#26368;&#26032;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04861v1 Announce Type: new  Abstract: The Lottery Ticket Hypothesis (LTH) states that a dense neural network model contains a highly sparse subnetwork (i.e., winning tickets) that can achieve even better performance than the original model when trained in isolation. While LTH has been proved both empirically and theoretically in many works, there still are some open issues, such as efficiency and scalability, to be addressed. Also, the lack of open-source frameworks and consensual experimental setting poses a challenge to future research on LTH. We, for the first time, examine previous research and studies on LTH from different perspectives. We also discuss issues in existing works and list potential directions for further exploration. This survey aims to provide an in-depth look at the state of LTH and develop a duly maintained platform to conduct experiments and compare with the most updated baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#26550;&#26500;&#20013;&#20351;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04847</link><description>&lt;p&gt;
&#20351;&#29992;&#26410;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04847
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;&#27169;&#22411;&#26550;&#26500;&#20013;&#20351;&#29992;&#26410;&#32463;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#8220;&#23637;&#24320;&#36845;&#20195;&#8221;&#65288;LU&#65289;&#21644;&#8220;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#8221;&#65288;DEQ&#65289;&#25193;&#23637;&#65292;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#65288;IP&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#20248;&#21270;&#36845;&#20195;&#23637;&#24320;&#20026;&#19968;&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#38469;&#19978;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#20989;&#25968;&#12290;&#23613;&#31649;&#36825;&#20123;&#26550;&#26500;&#30446;&#21069;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27491;&#21521;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#22312;&#35768;&#22810;&#29289;&#29702;&#24212;&#29992;&#20013;&#21463;&#38480;&#20110;&#27169;&#22411;&#31616;&#21270;&#25110;&#20202;&#22120;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#27491;&#21521;&#27169;&#22411;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27491;&#21521;&#27169;&#22411;&#27531;&#24046;&#22359;&#65292;&#20197;&#21305;&#37197;&#27599;&#20010;&#23454;&#20363;&#22312;&#27979;&#37327;&#22495;&#20013;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#24050;&#30693;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26550;&#26500;&#65288;LU&#21644;DEQ&#65289;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#22312;&#36739;&#36731;&#30340;&#26465;&#20214;&#19979;&#25910;&#25947;&#12290;&#23454;&#39564;&#34920;&#26126;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04847v1 Announce Type: new  Abstract: Model-based deep learning methods such as \emph{loop unrolling} (LU) and \emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. The experiments show signi
&lt;/p&gt;</description></item><item><title>UniTable &#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#65292;&#23558;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#30446;&#26631;&#32479;&#19968;&#21040;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04822</link><description>&lt;p&gt;
UniTable: &#26397;&#21521;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04822
&lt;/p&gt;
&lt;p&gt;
UniTable &#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#34920;&#32467;&#26500;&#35782;&#21035;&#65292;&#23558;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#30446;&#26631;&#32479;&#19968;&#21040;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#20256;&#36798;&#30001;&#20154;&#31867;&#21019;&#24314;&#30340;&#38544;&#24335;&#32422;&#23450;&#30340;&#20107;&#23454;&#21644;&#25968;&#37327;&#25968;&#25454;&#65292;&#36825;&#24448;&#24448;&#26159;&#26426;&#22120;&#38590;&#20197;&#35299;&#26512;&#30340;&#12290;&#20197;&#24448;&#20851;&#20110;&#34920;&#32467;&#26500;&#35782;&#21035;&#65288;TSR&#65289;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#21487;&#29992;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#22797;&#26434;&#29305;&#23450;&#20219;&#21153;&#32452;&#21512;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UniTable&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;TSR&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#35757;&#32451;&#33539;&#24335;&#32467;&#21512;&#20102;&#32431;&#31929;&#20687;&#32032;&#32423;&#36755;&#20837;&#30340;&#31616;&#21333;&#24615;&#20197;&#21450;&#26469;&#33258;&#21508;&#31181;&#26410;&#27880;&#37322;&#34920;&#26684;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSP&#65289;&#36171;&#20104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25152;&#26377;&#19977;&#20010;TSR&#20219;&#21153;&#30340;&#35757;&#32451;&#30446;&#26631; - &#25552;&#21462;&#34920;&#32467;&#26500;&#65292;&#21333;&#20803;&#26684;&#20869;&#23481;&#21644;&#21333;&#20803;&#26684;&#36793;&#30028;&#26694;&#65288;bbox&#65289; - &#32479;&#19968;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#35757;&#32451;&#30446;&#26631;&#65306;&#35821;&#35328;&#24314;&#27169;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#31361;&#26174;&#20102;UniTable&#22312;&#22235;&#20010;&#26368;&#22823;&#30340;TSR&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04822v1 Announce Type: cross  Abstract: Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. T
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22522;&#20110;LSTM&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#29289;&#29702;&#27169;&#22411;&#30340;&#31995;&#32479;&#35823;&#24046;&#26469;&#25913;&#21892;&#39123;&#39118;&#39118;&#26292;&#28526;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04818</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;LSTM&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#39640;&#39123;&#39118;&#39118;&#26292;&#28526;&#24314;&#27169;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning for Enhancing Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04818
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;LSTM&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#29289;&#29702;&#27169;&#22411;&#30340;&#31995;&#32479;&#35823;&#24046;&#26469;&#25913;&#21892;&#39123;&#39118;&#39118;&#26292;&#28526;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36807;&#31243;&#30340;&#29289;&#29702;&#27169;&#25311;&#32467;&#26524;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#21463;&#21040;&#29289;&#29702;&#36807;&#31243;&#27169;&#25311;&#30340;&#38480;&#21046;&#20197;&#21450;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#21033;&#29992;&#22522;&#20110;LSTM&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#26469;&#25429;&#25417;&#21644;&#39044;&#27979;&#39123;&#39118;&#20107;&#20214;&#26399;&#38388;&#26469;&#33258;&#27979;&#31449;&#30340;&#23454;&#38469;&#27700;&#20301;&#35266;&#27979;&#19982;&#39118;&#26292;&#28526;&#39044;&#27979;&#27169;&#22411;&#30340;&#31995;&#32479;&#35823;&#24046;&#34892;&#20026;&#12290;&#26412;&#24037;&#20316;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#39044;&#27979;&#29289;&#29702;&#27169;&#22411;&#30340;&#31995;&#32479;&#35823;&#24046;&#65292;&#24182;&#21033;&#29992;&#20854;&#26469;&#20107;&#21518;&#25552;&#39640;&#27169;&#25311;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#27839;&#28023;&#22320;&#21306;&#30340;61&#22330;&#21382;&#21490;&#39118;&#26292;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#22312;&#26657;&#27491;&#39123;&#39118;&#20234;&#24681;&#65288;2022&#24180;&#65289;&#27169;&#25311;&#27700;&#20301;&#25968;&#25454;&#39044;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#22320;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04818v1 Announce Type: new  Abstract: Physics simulation results of natural processes usually do not fully capture the real world. This is caused for instance by limits in what physical processes are simulated and to what accuracy. In this work we propose and analyze the use of an LSTM-based deep learning network machine learning (ML) architecture for capturing and predicting the behavior of the systemic error for storm surge forecast models with respect to real-world water height observations from gauge stations during hurricane events. The overall goal of this work is to predict the systemic error of the physics model and use it to improve the accuracy of the simulation results post factum. We trained our proposed ML model on a dataset of 61 historical storms in the coastal regions of the U.S. and we tested its performance in bias correcting modeled water level data predictions from hurricane Ian (2022). We show that our model can consistently improve the forecasting accur
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; TrafPS&#65292;&#19968;&#31181;&#22522;&#20110; Shapley &#30340;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20132;&#36890;&#39044;&#27979;&#32467;&#26524;&#65292;&#25903;&#25345;&#20132;&#36890;&#31649;&#29702;&#21644;&#22478;&#24066;&#35268;&#21010;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.04812</link><description>&lt;p&gt;
&#22522;&#20110; Shapley &#30340;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861; TrafPS &#29992;&#20110;&#35299;&#37322;&#20132;&#36890;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
TrafPS: A Shapley-based Visual Analytics Approach to Interpret Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; TrafPS&#65292;&#19968;&#31181;&#22522;&#20110; Shapley &#30340;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20132;&#36890;&#39044;&#27979;&#32467;&#26524;&#65292;&#25903;&#25345;&#20132;&#36890;&#31649;&#29702;&#21644;&#22478;&#24066;&#35268;&#21010;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21462;&#24471;&#30340;&#25104;&#23601;&#23637;&#31034;&#20102;&#20854;&#22312;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#39044;&#27979;&#26377;&#21161;&#20110;&#29702;&#35299;&#24773;&#20917;&#24182;&#22312;&#20132;&#36890;&#25511;&#21046;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;DL&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#22522;&#26412;&#27809;&#26377;&#36879;&#26126;&#24230;&#65292;&#26080;&#27861;&#20102;&#35299;&#24213;&#23618;&#26426;&#21046;&#12290;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#23581;&#35797;&#8220;&#25171;&#24320;&#40657;&#30418;&#23376;&#8221;&#24182;&#22686;&#21152;&#29983;&#25104;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#19978;&#30340;&#22797;&#26434;&#27169;&#22411;&#24182;&#21457;&#29616;&#26174;&#33879;&#24433;&#21709;&#20132;&#36890;&#27969;&#37327;&#30340;&#20851;&#38190;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; TrafPS&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#20132;&#36890;&#39044;&#27979;&#32467;&#26524;&#30340;&#35270;&#35273;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#20132;&#36890;&#31649;&#29702;&#21644;&#22478;&#24066;&#35268;&#21010;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#27979;&#37327;&#26041;&#27861;&#65292;&#21306;&#22495; SHAP &#21644;&#36712;&#36857; SHAP&#65292;&#34987;&#25552;&#20986;&#26469;&#37327;&#21270;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04812v1 Announce Type: new  Abstract: Recent achievements in deep learning (DL) have shown its potential for predicting traffic flows. Such predictions are beneficial for understanding the situation and making decisions in traffic control. However, most state-of-the-art DL models are considered "black boxes" with little to no transparency for end users with respect to the underlying mechanisms. Some previous work tried to "open the black boxes" and increase the interpretability of how predictions are generated. However, it still remains challenging to handle complex models on large-scale spatio-temporal data and discover salient spatial and temporal patterns that significantly influence traffic flows. To overcome the challenges, we present TrafPS, a visual analytics approach for interpreting traffic prediction outcomes to support decision-making in traffic management and urban planning. The measurements, region SHAP and trajectory SHAP, are proposed to quantify the impact of
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#37327;&#21270;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#27745;&#26579;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#37325;&#21472;&#12290;</title><link>https://arxiv.org/abs/2403.04811</link><description>&lt;p&gt;
&#37327;&#21270;&#27745;&#26579;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04811
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#37327;&#21270;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#27745;&#26579;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#24182;&#23637;&#31034;&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20154;&#20204;&#23545;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#30340;&#28508;&#22312;&#27745;&#26579;&#26085;&#30410;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#27844;&#28431;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#23545;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#20934;&#30830;&#37327;&#21270;&#20102;&#23427;&#20204;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#37325;&#21472;&#65292;&#36890;&#36807;&#34920;&#38754;&#32423;&#21644;&#35821;&#20041;&#32423;&#21305;&#37197;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19982;&#20844;&#24320;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#65292;&#24182;&#19988;&#27169;&#22411;&#34920;&#29616;&#26174;&#30528;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04811v1 Announce Type: cross  Abstract: While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#22797;&#26434;&#30340;&#24037;&#19994;&#32456;&#31471;&#26465;&#23545;&#35937;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04809</link><description>&lt;p&gt;
&#30740;&#31350;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#32456;&#31471;&#26465;&#23545;&#35937;&#26816;&#27979;&#24037;&#19994;&#24212;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#22797;&#26434;&#30340;&#24037;&#19994;&#32456;&#31471;&#26465;&#23545;&#35937;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#26816;&#26597;&#25110;&#26816;&#27979;&#29305;&#23450;&#23545;&#35937;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#30001;&#20154;&#24037;&#25110;&#32463;&#20856;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;&#22312;&#24037;&#19994;&#29615;&#22659;&#24341;&#20837;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#21487;&#33021;&#25552;&#39640;&#29983;&#20135;&#25928;&#29575;&#24182;&#23454;&#29616;&#26032;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36275;&#22815;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#20351;&#24471;&#36825;&#31867;&#39033;&#30446;&#30340;&#23454;&#26045;&#21464;&#24471;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#20174;3D&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#26631;&#27880;&#36825;&#20123;&#25968;&#25454;&#65292;&#23613;&#31649;&#36825;&#20250;&#23548;&#33268;&#19968;&#20010;&#27169;&#25311;&#21040;&#30495;&#23454;&#39046;&#22495;&#24046;&#36317;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#22797;&#26434;&#30340;&#24037;&#19994;&#32456;&#31471;&#26465;&#23545;&#35937;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#21512;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04809v1 Announce Type: cross  Abstract: In industrial manufacturing, numerous tasks of visually inspecting or detecting specific objects exist that are currently performed manually or by classical image processing methods. Therefore, introducing recent deep learning models to industrial environments holds the potential to increase productivity and enable new applications. However, gathering and labeling sufficient data is often intractable, complicating the implementation of such projects. Hence, image synthesis methods are commonly used to generate synthetic training data from 3D models and annotate them automatically, although it results in a sim-to-real domain gap. In this paper, we investigate the sim-to-real generalization performance of standard object detectors on the complex industrial application of terminal strip object detection. Combining domain randomization and domain knowledge, we created an image synthesis pipeline for automatically generating the training da
&lt;/p&gt;</description></item><item><title>WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04808</link><description>&lt;p&gt;
WaterMax: &#25171;&#30772;LLM&#27700;&#21360;&#21487;&#26816;&#27979;&#24615;-&#31283;&#20581;&#24615;-&#36136;&#37327;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04808
&lt;/p&gt;
&lt;p&gt;
WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#26159;&#38459;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;WaterMax&#30340;&#26032;&#39062;&#27700;&#21360;&#26041;&#26696;&#65292;&#20855;&#26377;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20854;&#26032;&#35774;&#35745;&#19981;&#20250;&#23545;LLM&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#65288;&#19981;&#35843;&#25972;&#26435;&#37325;&#12289;&#23545;&#25968;&#12289;&#28201;&#24230;&#25110;&#37319;&#26679;&#25216;&#26415;&#65289;&#12290;WaterMax&#24179;&#34913;&#20102;&#31283;&#20581;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#27700;&#21360;&#25216;&#26415;&#30456;&#21453;&#65292;&#20174;&#26681;&#26412;&#19978;&#24341;&#21457;&#20102;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20854;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#26368;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19979;&#65292;&#23427;&#32988;&#36807;&#25152;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04808v1 Announce Type: cross  Abstract: Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite.
&lt;/p&gt;</description></item><item><title>&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#24182;&#28608;&#21457;&#20852;&#36259;&#65292;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#21644;&#23558;&#26446;&#32676;&#29702;&#35770;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35762;&#20041;&#21450;&#32534;&#30721;&#25945;&#31243;&#20844;&#24320;&#21487;&#33719;&#21462;</title><link>https://arxiv.org/abs/2403.04807</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#65288;&#30740;&#31350;&#29983;&#35838;&#31243;&#35762;&#20041;&#65289;
&lt;/p&gt;
&lt;p&gt;
Mathematics of Neural Networks (Lecture Notes Graduate Course)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#24182;&#28608;&#21457;&#20852;&#36259;&#65292;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#21644;&#23558;&#26446;&#32676;&#29702;&#35770;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35762;&#20041;&#21450;&#32534;&#30721;&#25945;&#31243;&#20844;&#24320;&#21487;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#26159;&#25105;&#22312;2021&#24180;&#33267;2023&#24180;&#22312;&#22467;&#22240;&#38669;&#28201;&#31185;&#25216;&#22823;&#23398;&#25945;&#25480;&#30340;&#21516;&#21517;&#35838;&#31243;&#30340;&#35762;&#20041;&#12290;&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26088;&#22312;&#28608;&#21457;&#25968;&#23398;&#23398;&#29983;&#23545;&#36827;&#19968;&#27493;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#24863;&#20852;&#36259;&#12290;&#35838;&#31243;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#39318;&#20808;&#26159;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#33324;&#20171;&#32461;&#65292;&#20391;&#37325;&#20110;&#20197;&#24418;&#24335;&#21270;&#25968;&#23398;&#26041;&#24335;&#20171;&#32461;&#35813;&#39046;&#22495;&#12290;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#26446;&#32676;&#21644;&#21516;&#24577;&#31354;&#38388;&#30340;&#29702;&#35770;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#29702;&#24819;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35762;&#20041;&#23613;&#21487;&#33021;&#33258;&#21253;&#21547;&#65292;&#20197;&#20415;&#23545;&#20855;&#26377;&#19968;&#23450;&#25968;&#23398;&#32972;&#26223;&#30340;&#20219;&#20309;&#23398;&#29983;&#37117;&#21487;&#20197;&#29702;&#35299;&#12290;&#35813;&#35838;&#31243;&#36824;&#21253;&#25324;&#19968;&#31995;&#21015;Jupyter&#31508;&#35760;&#26412;&#24418;&#24335;&#30340;&#32534;&#30721;&#25945;&#31243;&#21644;&#20316;&#19994;&#65292;&#21487;&#22312;https://g&#19978;&#20844;&#24320;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04807v1 Announce Type: cross  Abstract: These are the lecture notes that accompanied the course of the same name that I taught at the Eindhoven University of Technology from 2021 to 2023. The course is intended as an introduction to neural networks for mathematics students at the graduate level and aims to make mathematics students interested in further researching neural networks. It consists of two parts: first a general introduction to deep learning that focuses on introducing the field in a formal mathematical way. The second part provides an introduction to the theory of Lie groups and homogeneous spaces and how it can be applied to design neural networks with desirable geometric equivariances. The lecture notes were made to be as self-contained as possible so as to accessible for any student with a moderate mathematics background. The course also included coding tutorials and assignments in the form of a set of Jupyter notebooks that are publicly available at https://g
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.04805</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#31080;&#25454;&#37117;&#26159;&#24179;&#31561;&#30340;&#65292;&#32780;&#25105;&#20204;&#30693;&#36947;&#65306;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#24341;&#23548;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04805
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#23398;&#20064;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#20391;&#37325;&#20110;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#30340;&#20462;&#21098;&#31639;&#27861;&#22312;&#36873;&#25321;&#31526;&#21512;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#26377;&#24847;&#20041;&#27169;&#22411;&#26041;&#38754;&#38754;&#20020;&#31639;&#27861;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DASH&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DASH&#19982;&#29616;&#26377;&#19968;&#33324;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19982;&#29983;&#29289;&#23398;&#19968;&#33268;&#30340;&#25968;&#25454;&#29305;&#23450;&#35265;&#35299;&#12290;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;DASH&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#24456;&#22823;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#32467;&#26500;&#20449;&#24687;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#34893;&#29983;&#31185;&#23398;&#27934;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04805v1 Announce Type: new  Abstract: Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary pruning algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides pruning by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insi
&lt;/p&gt;</description></item><item><title>AttentionStitch&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#24182;&#23558;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#33258;&#21160;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#32534;&#36753;&#30340;&#26080;&#32541;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.04804</link><description>&lt;p&gt;
AttentionStitch&#65306;&#20851;&#27880;&#22914;&#20309;&#35299;&#20915;&#35821;&#38899;&#32534;&#36753;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AttentionStitch: How Attention Solves the Speech Editing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04804
&lt;/p&gt;
&lt;p&gt;
AttentionStitch&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#24182;&#23558;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#33258;&#21160;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#32534;&#36753;&#30340;&#26080;&#32541;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#35821;&#38899;&#29983;&#25104;&#20043;&#22806;&#65292;&#35821;&#38899;&#32534;&#36753;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#32534;&#36753;&#21518;&#30340;&#35821;&#38899;&#26080;&#32541;&#12289;&#19981;&#26131;&#23519;&#35273;&#22320;&#25972;&#21512;&#21040;&#21512;&#25104;&#35821;&#38899;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#32534;&#36753;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#22914;FastSpeech 2&#65292;&#24182;&#22312;&#20854;&#20043;&#19978;&#21152;&#20837;&#21452;&#27880;&#24847;&#21147;&#22359;&#32593;&#32476;&#65292;&#20197;&#33258;&#21160;&#23558;&#21512;&#25104;&#30340;mel&#39057;&#35889;&#22270;&#19982;&#32534;&#36753;&#25991;&#26412;&#30340;mel&#39057;&#35889;&#22270;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22411;&#31216;&#20026;AttentionStitch&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#26469;&#23558;&#38899;&#39057;&#26679;&#26412;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#35828;&#35805;&#32773;&#25968;&#25454;&#38598;&#65288;LJSpeech&#21644;VCTK&#65289;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;AttentionStitch&#27169;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04804v1 Announce Type: cross  Abstract: The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 1
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#21644;&#20849;&#35782;&#39564;&#35777;&#27969;&#31243;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#25915;&#20987;&#24182;&#25552;&#21319;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04803</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#20849;&#35782;&#39564;&#35777;&#27169;&#22411;&#26356;&#26032;&#65292;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04803
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#21644;&#20849;&#35782;&#39564;&#35777;&#27969;&#31243;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#25915;&#20987;&#24182;&#25552;&#21319;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24378;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#25269;&#24481;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#22522;&#20110;&#20849;&#35782;&#30340;&#39564;&#35777;&#27969;&#31243;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#12290;&#36825;&#31181;&#21160;&#24577;&#38408;&#20540;&#35774;&#35745;&#26088;&#22312;&#26681;&#25454;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#23637;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#24322;&#24120;&#26816;&#27979;&#23618;&#65292;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#30340;&#23454;&#26102;&#38656;&#27714;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#21442;&#19982;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#22810;&#25968;&#20849;&#35782;&#25165;&#33021;&#39564;&#35777;&#26356;&#26032;&#65292;&#30830;&#20445;&#21482;&#26377;&#32463;&#36807;&#23457;&#26597;&#21644;&#20849;&#35782;&#30340;&#20462;&#25913;&#25165;&#24212;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CIFAR-10&#21644;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#36731;&#65292;&#22686;&#24378;&#20102;FL&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20381;&#36182;&#24322;&#24120;&#26816;&#27979;&#30340;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04803v1 Announce Type: cross  Abstract: This paper introduces an advanced approach for fortifying Federated Learning (FL) systems against label-flipping attacks. We propose a simplified consensus-based verification process integrated with an adaptive thresholding mechanism. This dynamic thresholding is designed to adjust based on the evolving landscape of model updates, offering a refined layer of anomaly detection that aligns with the real-time needs of distributed learning environments. Our method necessitates a majority consensus among participating clients to validate updates, ensuring that only vetted and consensual modifications are applied to the global model. The efficacy of our approach is validated through experiments on two benchmark datasets in deep learning, CIFAR-10 and MNIST. Our results indicate a significant mitigation of label-flipping attacks, bolstering the FL system's resilience. This method transcends conventional techniques that depend on anomaly detec
&lt;/p&gt;</description></item><item><title>&#19968;&#32500;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33021;&#22815;&#23454;&#29616;&#38750;&#37197;&#23545;&#20449;&#21495;&#21040;&#20449;&#21495;&#30340;&#32763;&#35793;&#65292;&#23558;&#20108;&#32500;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#32500;&#20449;&#21495;&#21040;&#20449;&#21495;&#20219;&#21153;&#65292;&#19988;&#22312;&#39057;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31867;&#20284;&#37197;&#23545;&#20449;&#21495;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04800</link><description>&lt;p&gt;
(&#38750;)&#37197;&#23545;&#20449;&#21495;&#21040;&#20449;&#21495;&#30340;1D&#26465;&#20214;GAN&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
(Un)paired signal-to-signal translation with 1D conditional GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04800
&lt;/p&gt;
&lt;p&gt;
&#19968;&#32500;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33021;&#22815;&#23454;&#29616;&#38750;&#37197;&#23545;&#20449;&#21495;&#21040;&#20449;&#21495;&#30340;&#32763;&#35793;&#65292;&#23558;&#20108;&#32500;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#32500;&#20449;&#21495;&#21040;&#20449;&#21495;&#20219;&#21153;&#65292;&#19988;&#22312;&#39057;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31867;&#20284;&#37197;&#23545;&#20449;&#21495;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#23637;&#31034;&#20102;&#19968;&#32500;&#65288;1D&#65289;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGAN&#65289;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#26550;&#26500;&#33021;&#22815;&#23454;&#29616;&#38750;&#37197;&#23545;&#20449;&#21495;&#21040;&#20449;&#21495;&#65288;"sig2sig"&#65289;&#30340;&#32763;&#35793;&#12290;&#20351;&#29992;&#31616;&#21270;&#30340;CycleGAN&#27169;&#22411;&#65292;&#20855;&#26377;1D&#23618;&#21644;&#26356;&#23485;&#30340;&#21367;&#31215;&#26680;&#65292;&#27169;&#20223;WaveGAN&#23558;&#20108;&#32500;&#65288;2D&#65289;&#22270;&#20687;&#29983;&#25104;&#37325;&#26032;&#26500;&#24314;&#20026;1D&#38899;&#39057;&#29983;&#25104;&#65292;&#25105;&#23637;&#31034;&#20102;&#23558;2D&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;1D&#20449;&#21495;&#21040;&#20449;&#21495;&#30340;&#28145;&#24230;&#21367;&#31215;GAN&#20219;&#21153;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#26080;&#38656;&#23545;&#20316;&#20026;CycleGAN&#24320;&#21457;&#30340;&#20256;&#32479;U-Net&#27169;&#22411;&#21644;&#23545;&#25239;&#26550;&#26500;&#36827;&#34892;&#23454;&#36136;&#24615;&#20462;&#25913;&#12290;&#36890;&#36807;&#36825;&#20010;&#65292;&#25105;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#20010;&#23567;&#30340;&#21487;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#26410;&#34987;1D CycleGAN&#27169;&#22411;&#35265;&#36807;&#19988;&#26080;&#37197;&#23545;&#35757;&#32451;&#30340;&#22024;&#26434;&#27979;&#35797;&#20449;&#21495;&#20174;&#28304;&#22495;&#36716;&#25442;&#20026;&#22312;&#32763;&#35793;&#22495;&#20013;&#31867;&#20284;&#37197;&#23545;&#27979;&#35797;&#20449;&#21495;&#30340;&#20449;&#21495;&#26159;&#21487;&#33021;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#39057;&#29575;&#26041;&#38754;&#65292;&#24182;&#19988;&#25105;&#37327;&#21270;&#20102;&#36825;&#20123;&#24046;&#24322;&#20174;&#30456;&#20851;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04800v1 Announce Type: cross  Abstract: I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal ("sig2sig") translation. Using a simplified CycleGAN model with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-to-image translation task to a 1D signal-to-signal translation task with deep convolutional GANs is possible without substantial modification to the conventional U-Net model and adversarial architecture developed as CycleGAN. With this I show for a small tunable dataset that noisy test signals unseen by the 1D CycleGAN model and without paired training transform from the source domain to signals similar to paired test signals in the translated domain, especially in terms of frequency, and I quantify these differences in terms of correlation an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04798</link><description>&lt;p&gt;
JMI&#22312;SemEval 2024&#20219;&#21153;3&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;GPT&#21644;instruction-tuned Llama&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#30340;&#20004;&#27493;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#65306;&#8220;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31454;&#36187;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#25429;&#25417;&#20154;&#31867;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#38656;&#35201;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#26679;&#24615;&#27169;&#24577;&#30340;&#22797;&#26434;&#24615;&#32473;&#24320;&#21457;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#26041;&#27861;1&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;Llama 2&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#21644;&#21407;&#22240;&#39044;&#27979;&#30340;instruction-tuning&#12290;&#22312;&#26041;&#27861;2&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4V&#36827;&#34892;&#20250;&#35805;&#32423;&#35270;&#39057;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;GPT 3.5&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#27880;&#37322;&#23545;&#35805;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33719;&#24471;&#20102;&#31532;4&#21517;&#65292;&#31995;&#32479;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#19978;&#19979;&#25991;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;LLMs&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04797</link><description>&lt;p&gt;
&#22312;&#20013;&#38388;&#34987;&#21457;&#29616;: &#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#20301;&#32622;&#32534;&#30721;&#26356;&#22909;&#22320;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#19978;&#19979;&#25991;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;LLMs&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#8220;&#20013;&#38388;&#20002;&#22833;&#8221;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#25104;&#21151;&#23454;&#29616;&#20102;LLMs&#23545;&#21253;&#21547;400&#19975;&#20196;&#29260;&#30340;&#31283;&#23450;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#22312;&#35782;&#21035;&#20301;&#20110;&#19978;&#19979;&#25991;&#20013;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25345;&#32493;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#65288;Ms-PoE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LLMs&#22788;&#29702;&#20301;&#20110;&#19978;&#19979;&#25991;&#20013;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#24494;&#35843;&#25110;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#24320;&#38144;&#12290;Ms-PoE&#21033;&#29992;&#20301;&#32622;&#25351;&#25968;&#37325;&#26032;&#32553;&#25918;&#20197;&#20943;&#36731;RoPE&#24341;&#20837;&#30340;&#38271;&#26399;&#34928;&#20943;&#25928;&#24212;&#65292;&#21516;&#26102;&#31934;&#24515;&#20026;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#20998;&#37197;&#19981;&#21516;&#30340;&#32553;&#25918;&#27604;&#20197;&#20445;&#30041;&#39044;&#35757;&#32451;&#38454;&#27573;&#23398;&#21040;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#24418;&#25104;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04797v1 Announce Type: new  Abstract: This paper aims to overcome the "lost-in-the-middle" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04795</link><description>&lt;p&gt;
&#28040;&#38450;&#24037;&#31243;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38024;&#23545;&#39046;&#22495;&#30693;&#35782;&#23545;&#25216;&#26415;&#38382;&#39064;&#30340;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27604;&#36739;&#20004;&#20010;&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;OpenAI&#30340;ChatGPT&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#22312;&#28779;&#28798;&#24037;&#31243;&#39046;&#22495;&#20013;&#35780;&#20272;&#23427;&#20204;&#22788;&#29702;&#19982;&#28040;&#38450;&#23433;&#20840;&#30456;&#20851;&#26597;&#35810;&#30340;&#22238;&#24212;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#12290; &#21019;&#24314;&#24182;&#26816;&#26597;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#31867;&#22411;&#30340;&#28040;&#38450;&#24037;&#31243;&#38382;&#39064;&#21644;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#32467;&#26500;&#28779;&#28798;&#35774;&#35745;&#12289;&#38450;&#28779;&#31574;&#30053;&#12289;&#30095;&#25955;&#12289;&#24314;&#31569;&#27861;&#35268;&#21512;&#35268;&#21644;&#28781;&#28779;&#31995;&#32479;&#31561;&#65288;&#20854;&#20013;&#19968;&#20123;&#31867;&#20284;&#20110;&#28040;&#38450;&#20445;&#25252;&#32771;&#35797;&#65288;FPE&#65289;&#20013;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#12290; &#32467;&#26524;&#26174;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#24615;&#33021;&#19978;&#30340;&#19968;&#20123;&#20851;&#38190;&#24046;&#24322;&#65292;ChatGPT&#34920;&#29616;&#20986;&#30456;&#23545;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290; &#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#31361;&#20986;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#30340;&#21516;&#26102;&#24443;&#24213;&#25913;&#38761;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26174;&#28982;&#65292;&#22312;&#25216;&#26415;&#25104;&#29087;&#21518;&#65292;&#36825;&#39033;&#25216;&#26415;&#23558;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04795v1 Announce Type: cross  Abstract: This communication presents preliminary findings from comparing two recent chatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire engineering by evaluating their responses in handling fire safety related queries. A diverse range of fire engineering questions and scenarios were created and examined, including structural fire design, fire prevention strategies, evacuation, building code compliance, and fire suppression systems (some of which resemble those commonly present in the Fire Protection exam (FPE)). The results reveal some key differences in the performance of the chatbots, with ChatGPT demonstrating a relatively superior performance. Then, this communication highlights the potential for chatbot technology to revolutionize fire engineering practices by providing instant access to critical information while outlining areas for further improvement and research. Evidently, and when it matures, this technology will lik
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#22240;&#26524;&#22522;&#20934;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#38477;&#20302;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04793</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Two-Phase Multi-Split Causal Ensemble Model for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#22240;&#26524;&#22522;&#20934;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#38477;&#20302;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#35768;&#22810;&#23398;&#31185;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#30740;&#31350;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#31639;&#27861;&#37117;&#21516;&#26679;&#36866;&#29992;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#22240;&#26524;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04793v1 Announce Type: cross  Abstract: Causal inference is a fundamental research topic for discovering the cause-effect relationships in many disciplines. However, not all algorithms are equally well-suited for a given dataset. For instance, some approaches may only be able to identify linear relationships, while others are applicable for non-linearities. Algorithms further vary in their sensitivity to noise and their ability to infer causal information from coupled vs. non-coupled time series. Therefore, different algorithms often generate different causal relationships for the same input. To achieve a more robust causal inference result, this publication proposes a novel data-driven two-phase multi-split causal ensemble model to combine the strengths of different causality base algorithms. In comparison to existing approaches, the proposed ensemble method reduces the influence of noise through a data partitioning scheme in the first phase. To achieve this, the data are i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#20197;&#24448;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#24182;&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#35821;&#35328;&#20013;&#34920;&#26126;PaLM2-L&#22312;&#30452;&#25509;&#25512;&#26029;&#20013;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2403.04792</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#22312;&#22810;&#35821;&#35328;LLM&#24212;&#29992;&#20013;&#65292;&#30452;&#25509;&#25512;&#26029;&#33021;&#21542;&#32988;&#36807;&#39044;&#32763;&#35793;&#65311;
&lt;/p&gt;
&lt;p&gt;
Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#20197;&#24448;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#24182;&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#35821;&#35328;&#20013;&#34920;&#26126;PaLM2-L&#22312;&#30452;&#25509;&#25512;&#26029;&#20013;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20027;&#35201;&#20197;&#33521;&#25991;&#20026;&#20013;&#24515;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#22266;&#26377;&#20559;&#35265;&#65292;&#22240;&#27492;&#24050;&#32463;&#26222;&#36941;&#37319;&#29992;&#39044;&#32763;&#35793;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#26029;&#20043;&#21069;&#23558;&#38750;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#33521;&#25991;&#65292;&#20174;&#32780;&#23548;&#33268;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#22312;PaLM2&#27169;&#22411;&#20013;&#30340;&#39044;&#32763;&#35793;&#38656;&#27714;&#65288;Anil&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;108&#31181;&#35821;&#35328;&#21644;6&#20010;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#20219;&#21153;&#65292;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#25490;&#38500;&#22312;&#22806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#39044;&#32763;&#35793;&#33539;&#24335;&#65292;&#31361;&#20986;&#20102;&#22312;PaLM2&#20013;&#30452;&#25509;&#25512;&#26029;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PaLM2-L&#22312;108&#31181;&#35821;&#35328;&#20013;&#30340;94&#31181;&#20013;&#22987;&#32456;&#20248;&#20110;&#39044;&#32763;&#35793;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26356;&#39640;&#25928;&#30340;&#32763;&#35793;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04792v1 Announce Type: new  Abstract: Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more effici
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#22823;&#22411;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#26696;&#20363;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;F1&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.04791</link><description>&lt;p&gt;
LLM&#23545;&#25239;&#24459;&#24072;&#65306;&#22312;&#22823;&#22411;&#33521;&#22269;&#26696;&#20363;&#27861;&#24459;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#30340;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04791
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#22823;&#22411;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#25688;&#35201;&#35009;&#23450;&#26696;&#20363;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#39640;&#25928;&#22320;&#35782;&#21035;&#19982;&#29305;&#23450;&#27861;&#24459;&#38382;&#39064;&#30456;&#20851;&#30340;&#27861;&#38498;&#35009;&#20915;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21162;&#21147;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#20851;&#20110;&#22914;&#20309;&#20174;&#22823;&#37327;&#33521;&#22269;&#27861;&#38498;&#20915;&#23450;&#30340;&#25991;&#38598;&#20013;&#38548;&#31163;&#26696;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#26159;&#25688;&#35201;&#35009;&#23450;&#65289;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#35745;&#31639;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#65288;1&#65289;&#20256;&#32479;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#29983;&#25104;&#30340;&#20851;&#38190;&#23383;&#21644;&#36923;&#36753;&#36816;&#31639;&#31526;&#65292;&#20197;&#21450;&#65288;2&#65289;&#21019;&#26032;&#24615;&#22320;&#23558;Claude 2&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22522;&#20110;&#29305;&#23450;&#20869;&#23481;&#25552;&#31034;&#20998;&#31867;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;356,011&#20221;&#33521;&#22269;&#27861;&#38498;&#21028;&#20915;&#30340;&#21073;&#26725;&#27861;&#23398;&#25991;&#38598;&#65292;&#24182;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#26435;F1&#24471;&#20998;&#20026;0.94&#65292;&#32780;&#20851;&#38190;&#23383;&#30340;&#24471;&#20998;&#20026;0.78&#12290;&#23613;&#31649;&#32463;&#36807;&#36845;&#20195;&#25913;&#36827;&#65292;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#36923;&#36753;&#26410;&#33021;&#25429;&#25417;&#27861;&#24459;&#35821;&#35328;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04791v1 Announce Type: new  Abstract: To undertake computational research of the law, efficiently identifying datasets of court decisions that relate to a specific legal issue is a crucial yet challenging endeavour. This study addresses the gap in the literature working with large legal corpora about how to isolate cases, in our case summary judgments, from a large corpus of UK court decisions. We introduce a comparative analysis of two computational methods: (1) a traditional natural language processing-based approach leveraging expert-generated keywords and logical operators and (2) an innovative application of the Claude 2 large language model to classify cases based on content-specific prompts. We use the Cambridge Law Corpus of 356,011 UK court decisions and determine that the large language model achieves a weighted F1 score of 0.94 versus 0.78 for keywords. Despite iterative refinement, the search logic based on keywords fails to capture nuances in legal language. We 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22312;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#38544;&#31169;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20027;&#21160;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#35780;&#20272;&#21508;&#31181;&#35843;&#25972;&#21518;&#30340;&#32852;&#37030;&#23398;&#20064;&#37197;&#32622;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#24182;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#37325;&#22823;&#38544;&#31169;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.04784</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#38544;&#31169;&#27844;&#38706;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Privacy Leakage in Federated Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22312;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#38544;&#31169;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20027;&#21160;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#35780;&#20272;&#21508;&#31181;&#35843;&#25972;&#21518;&#30340;&#32852;&#37030;&#23398;&#20064;&#37197;&#32622;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#24182;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#37325;&#22823;&#38544;&#31169;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#30340;&#35757;&#32451;&#21644;&#35843;&#20248;&#21327;&#35758;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#23545;FL&#36827;&#34892;&#37325;&#22823;&#20462;&#25913;&#20197;&#36866;&#24212;LLMs&#30340;&#22823;&#35268;&#27169;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;&#20316;&#20026;&#22238;&#24212;&#24050;&#32463;&#24341;&#20837;&#20102;&#23545;&#21327;&#35758;&#30340;&#37325;&#22823;&#35843;&#25972;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#23545;&#36866;&#24212;&#21518;&#30340;FL&#21327;&#35758;&#36827;&#34892;&#20840;&#38754;&#38544;&#31169;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;LLMs&#26102;&#20351;&#29992;FL&#30340;&#38544;&#31169;&#20998;&#26512;&#65292;&#26082;&#20174;&#29702;&#35770;&#35282;&#24230;&#21448;&#20174;&#23454;&#38469;&#35282;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#24102;&#26377;&#26377;&#29702;&#35770;&#25104;&#21151;&#29575;&#20445;&#35777;&#30340;&#20027;&#21160;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;&#35843;&#25972;&#21518;&#30340;FL&#37197;&#32622;&#30340;&#38544;&#31169;&#27844;&#28431;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#36716;&#21270;&#20026;&#23454;&#38469;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;LLMs&#65288;&#21253;&#25324;BERT&#12289;RoBERTa&#12289;DistilBERT&#21644;OpenAI&#30340;&#65289;&#23384;&#22312;&#37325;&#22823;&#30340;&#38544;&#31169;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04784v1 Announce Type: cross  Abstract: With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking.   To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04783</link><description>&lt;p&gt;
AutoDefense: &#22810;Agent LLM &#38450;&#24481;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36947;&#24503;&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20197;&#38450;&#27490;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#29992;&#20110;&#20174;LLMs&#20013;&#36807;&#28388;&#26377;&#23475;&#22238;&#22797;&#12290; &#27492;&#26694;&#26550;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20849;&#21516;&#23436;&#25104;&#38450;&#24481;&#20219;&#21153;&#12290; &#20219;&#21153;&#30340;&#21010;&#20998;&#22686;&#24378;&#20102;LLMs&#30340;&#25972;&#20307;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#20182;&#38450;&#24481;&#32452;&#20214;&#20316;&#20026;&#24037;&#20855;&#38598;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290; AutoDefense &#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#35268;&#27169;&#21644;&#31181;&#31867;&#30340;&#24320;&#28304;LLMs&#20316;&#20026;&#20195;&#29702;&#12290; &#36890;&#36807;&#23545;&#22823;&#37327;&#26377;&#23475;&#21644;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;AutoDefense&#22312;&#25552;&#39640;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04783v1 Announce Type: cross  Abstract: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#26696;&#20197;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#29992;&#25143;&#30340;&#35270;&#35282;&#21019;&#26032;&#20110;&#21307;&#23398;&#22270;&#20687;&#23384;&#20648;&#21644;&#23433;&#20840;&#39046;&#22495;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#21152;&#23494;&#21307;&#23398;&#22270;&#20687;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#26469;&#23454;&#29616;&#23545;&#22810;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#36873;&#25321;&#24615;&#21152;&#23494;&#12290;</title><link>https://arxiv.org/abs/2403.04781</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#27788;Heron&#26144;&#23556;&#30340;&#20998;&#21106;&#25513;&#27169;&#36827;&#34892;&#22810;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#36873;&#25321;&#24615;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Selective Encryption using Segmentation Mask with Chaotic Henon Map for Multidimensional Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#26696;&#20197;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#29992;&#25143;&#30340;&#35270;&#35282;&#21019;&#26032;&#20110;&#21307;&#23398;&#22270;&#20687;&#23384;&#20648;&#21644;&#23433;&#20840;&#39046;&#22495;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#21152;&#23494;&#21307;&#23398;&#22270;&#20687;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#26469;&#23454;&#29616;&#23545;&#22810;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#36873;&#25321;&#24615;&#21152;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04781v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;:&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36164;&#28304;&#20248;&#21270;&#24212;&#35813;&#26159;&#20219;&#20309;&#25216;&#26415;&#25110;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#20351;&#24320;&#21457;&#32773;&#26377;&#26426;&#20250;&#36827;&#34892;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#21270;&#12290;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#29992;&#25143;&#26159;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#24182;&#23558;&#35786;&#26029;&#32467;&#26524;&#25552;&#20379;&#32473;&#24739;&#32773;&#30340;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#12290;&#36825;&#20010;&#26041;&#26696;&#20197;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#29992;&#25143;&#30340;&#35270;&#35282;&#21019;&#26032;&#20110;&#21307;&#23398;&#22270;&#20687;&#23384;&#20648;&#21644;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#26550;&#26500;&#35774;&#35745;&#26377;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65292;&#21363;:&#20998;&#21106;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#12290;&#36825;&#19968;&#26550;&#26500;&#30340;&#35774;&#35745;&#26159;&#22240;&#20026;&#19982;&#20026;&#29305;&#23450;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20960;&#27425;&#23384;&#20648;&#25805;&#20316;&#30456;&#27604;&#65292;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25191;&#34892;&#30340;&#26816;&#32034;&#25805;&#20316;&#25968;&#37327;&#39640;&#24471;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#12290;&#36825;&#20026;&#25105;&#20204;&#30340;&#21019;&#26032;&#25552;&#20379;&#20102;&#20998;&#21106;&#20986;&#21307;&#23398;&#19978;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#36827;&#34892;&#21152;&#23494;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04781v1 Announce Type: cross  Abstract: A user-centric design and resource optimization should be at the center of any technology or innovation. The user-centric perspective gives the developer the opportunity to develop with task-based optimization. The user in the medical image field is a medical professional who analyzes the medical images and gives their diagnosis results to the patient. This scheme, having the medical professional user's perspective, innovates in the area of Medical Image storage and security. The architecture is designed with three main segments, namely: Segmentation, Storage, and Retrieval. This architecture was designed owing to the fact that the number of retrieval operations done by medical professionals was toweringly higher when compared to the storage operations done for some handful number of times for a particular medical image. This gives room for our innovation to segment out the medically indispensable part of the medical image, encrypt it,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04778</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#28431;&#26007;&#30340;&#39640;&#25928;&#20984;&#24046;&#20998;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Difference-of-Convex Solver for Privacy Funnel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#65288;PF&#65289;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20854;&#20984;&#24046;&#20998;&#65288;DC&#65289;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;DC&#20998;&#31163;&#23548;&#33268;&#20102;&#38381;&#24335;&#26356;&#26032;&#26041;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#23545;&#20110;&#24050;&#30693;&#20998;&#24067;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38750;&#36138;&#23146;&#27714;&#35299;&#22120;&#30340;&#25910;&#25947;&#24615;&#65288;&#23616;&#37096;&#31283;&#23450;&#28857;&#65289;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#23427;&#22312;&#34920;&#24449;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;DC&#26041;&#27861;&#27934;&#23519;&#21147;&#36866;&#29992;&#20110;&#20855;&#26377;&#26631;&#35760;&#32463;&#39564;&#26679;&#26412;&#30340;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#28385;&#36275;&#20102;PF&#30340;&#22522;&#26412;Markov&#20851;&#31995;&#65292;&#19982;&#20197;&#24448;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#30456;&#27604;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04778v1 Announce Type: new  Abstract: We propose an efficient solver for the privacy funnel (PF) method, leveraging its difference-of-convex (DC) structure. The proposed DC separation results in a closed-form update equation, which allows straightforward application to both known and unknown distribution settings. For known distribution case, we prove the convergence (local stationary points) of the proposed non-greedy solver, and empirically show that it outperforms the state-of-the-art approaches in characterizing the privacy-utility trade-off. The insights of our DC approach apply to unknown distribution settings where labeled empirical samples are available instead. Leveraging the insights, our alternating minimization solver satisfies the fundamental Markov relation of PF in contrast to previous variational inference-based solvers. Empirically, we evaluate the proposed solver with MNIST and Fashion-MNIST datasets. Our results show that under a comparable reconstruction 
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#27169;&#22411;&#35748;&#20026;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#23545;&#35805;&#21442;&#19982;&#32773;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.04770</link><description>&lt;p&gt;
&#31038;&#20132;&#21462;&#21521;&#65306;&#23545;&#35805;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Social Orientation: A New Feature for Dialogue Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04770
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#27169;&#22411;&#35748;&#20026;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#23545;&#35805;&#21442;&#19982;&#32773;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24773;&#22659;&#19979;&#65292;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#35805;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#24515;&#29702;&#23398;&#30340;Circumplex&#29702;&#35770;&#24314;&#27169;&#20102;&#20250;&#35805;&#21442;&#19982;&#32773;&#30340;&#31038;&#20132;&#21462;&#21521;&#65288;&#20363;&#22914;&#65292;&#28909;&#24773;&#21451;&#22909;&#12289;&#20658;&#24930;&#20919;&#28448;&#65289;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#21644;&#35299;&#37322;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20110;&#31995;&#32479;&#24615;&#22320;&#24212;&#29992;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#26469;&#24314;&#27169;&#23545;&#35805;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#35805;&#35821;&#25968;&#25454;&#38598;&#65292;&#26426;&#22120;&#26631;&#35760;&#20102;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#25552;&#39640;&#20102;&#20219;&#21153;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#35328;&#22522;&#20934;&#20013;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#22914;&#20309;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#20013;&#31038;&#20132;&#20114;&#21160;&#30340;&#32467;&#26524;&#12290;&#26681;&#25454;&#26174;&#31034;&#20986;&#31038;&#20132;&#21462;&#21521;&#26631;&#31614;&#22312;&#23545;&#35805;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29992;&#24615;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04770v1 Announce Type: new  Abstract: There are many settings where it is useful to predict and explain the success or failure of a dialogue. Circumplex theory from psychology models the social orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation participants and can be used to predict and explain the outcome of social interactions. Our work is novel in its systematic application of social orientation tags to modeling conversation outcomes. In this paper, we introduce a new data set of dialogue utterances machine-labeled with social orientation tags. We show that social orientation tags improve task performance, especially in low-resource settings, on both English and Chinese language benchmarks. We also demonstrate how social orientation tags help explain the outcomes of social interactions when used in neural models. Based on these results showing the utility of social orientation tags for dialogue outcome prediction tasks, we release our data sets, co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.04629</link><description>&lt;p&gt;
&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#40657;&#21283;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BO&#26412;&#36523;&#20063;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#65292;&#32570;&#20047;&#25552;&#20379;&#20026;&#20309;&#25552;&#35758;&#35780;&#20272;&#26576;&#20123;&#21442;&#25968;&#30340;&#29702;&#30001;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ShapleyBO&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#21338;&#24328;&#35770;Shapley&#20540;&#35299;&#37322;BO&#25552;&#35758;&#30340;&#26694;&#26550;&#12290;&#23427;&#37327;&#21270;&#20102;&#27599;&#20010;&#21442;&#25968;&#23545;BO&#30340;&#25910;&#33719;&#20989;&#25968;&#30340;&#36129;&#29486;&#12290;&#21033;&#29992;Shapley&#20540;&#30340;&#32447;&#24615;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20687;&#32622;&#20449;&#36793;&#30028;&#36825;&#26679;&#30340;&#21152;&#27861;&#25910;&#33719;&#20989;&#25968;&#25512;&#21160;BO&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ShapleyBO&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#23545;&#20110;&#21208;&#25506;aleatoric&#21644;&#35748;&#35782;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
&lt;/p&gt;</description></item><item><title>IN-N-OUT&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;GNN&#26657;&#20934;&#20559;&#24046;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#31616;&#21333;&#30452;&#35273;&#23454;&#29616;&#26657;&#20934;</title><link>https://arxiv.org/abs/2403.04605</link><description>&lt;p&gt;
In-n-Out: &#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
In-n-Out: Calibrating Graph Neural Networks for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04605
&lt;/p&gt;
&lt;p&gt;
IN-N-OUT&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;GNN&#26657;&#20934;&#20559;&#24046;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#31616;&#21333;&#30452;&#35273;&#23454;&#29616;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#26657;&#20934;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#30340;&#36755;&#20986;&#19981;&#33021;&#21453;&#26144;&#25105;&#20204;&#25171;&#31639;&#39044;&#27979;&#30340;&#20107;&#20214;&#30340;&#30495;&#23454;&#27010;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#28151;&#21512;&#30340;&#34892;&#20026;&#65292;&#21363;&#22312;&#36127;&#39044;&#27979;&#19978;&#21487;&#33021;&#36807;&#20110;&#33258;&#20449;&#65292;&#22312;&#27491;&#39044;&#27979;&#19978;&#21487;&#33021;&#19981;&#22815;&#33258;&#20449;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IN-N-OUT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;IN-N-OUT&#22522;&#20110;&#20004;&#20010;&#31616;&#21333;&#30340;&#30452;&#35273;&#65306;i) &#32473;&#36793;&#26631;&#27880;&#30495;/&#20551;&#26631;&#31614;&#65292;&#21516;&#26102;&#36981;&#24490;GNN&#30340;&#39044;&#27979;&#24212;&#23548;&#33268;&#35813;&#36793;&#23884;&#20837;&#30340;&#24494;&#23567;&#27874;&#21160;&#65307;ii) &#30456;&#21453;&#22320;&#65292;&#22914;&#26524;&#25105;&#20204;&#26631;&#35760;&#30456;&#21516;&#30340;&#36793;&#19982;&#25105;&#20204;&#30340;GNN&#39044;&#27979;&#30456;&#24726;&#65292;&#37027;&#20040;&#23884;&#20837;&#24212;&#35813;&#21457;&#29983;&#26356;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04605v1 Announce Type: new  Abstract: Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substa
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04493</link><description>&lt;p&gt;
&#20351;&#22270;&#20687;&#30495;&#23454;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What makes an image realistic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04493
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#30475;&#36215;&#26469;&#30495;&#23454;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#36824;&#26159;&#35270;&#39057;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#21270;&#29616;&#23454;&#20027;&#20041;&#65292;&#21363;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;&#20174;&#31639;&#27861;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#21333;&#29420;&#19981;&#33021;&#35299;&#20915;&#23427;&#65292;&#20197;&#21450;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#65292;&#19981;&#20687;&#23545;&#25239;&#24615;&#35780;&#35770;&#32773;&#37027;&#26679;&#38656;&#35201;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#29992;&#35780;&#35770;&#32773;&#24182;&#19981;&#31435;&#21363;&#23454;&#29992;&#65292;&#20294;&#23427;&#20204;&#26082;&#21487;&#20197;&#20316;&#20026;&#24341;&#23548;&#23454;&#38469;&#23454;&#29616;&#30340;&#21271;&#26497;&#26143;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03542</link><description>&lt;p&gt;
DPOT: &#33258;&#22238;&#24402;&#21435;&#22122;&#36816;&#31639;&#22120;&#21464;&#25442;&#22120;&#29992;&#20110;&#22823;&#35268;&#27169;PDE&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#26469;&#25552;&#39640;&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#22914;&#38271;&#36712;&#36857;&#12289;&#22810;&#20010;&#23610;&#24230;&#21644;&#19981;&#21516;&#32500;&#24230;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#27867;&#21270;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#28789;&#27963;&#21487;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;10+&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20855;&#26377;&#36229;&#36807;0.5B&#21442;&#25968;&#30340;PDE&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#36229;&#36807;100k&#36712;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;SOTA&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.02746</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#31934;&#30830;&#25351;&#23548;&#30340;&#23398;&#20064;&#65306;&#20174;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#26631;&#31614;&#26356;&#26032;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Paraformer&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65292;&#35774;&#35745;&#20102;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#32508;&#21512;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26159;&#35843;&#26597;&#22320;&#29699;&#34920;&#38754;&#21644;&#35299;&#20915;&#20154;&#31867;&#38754;&#20020;&#30340;&#35768;&#22810;&#25361;&#25112;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22320;&#38754;&#32454;&#33410;&#12289;&#21508;&#31181;&#22320;&#35980;&#21644;&#24191;&#27867;&#22320;&#29702;&#21306;&#22495;&#20869;&#20934;&#30830;&#35757;&#32451;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65288;Paraformer&#65289;&#65292;&#21363;&#20302;&#21040;&#39640;&#32593;&#32476;&#65288;L2HNet&#65289;V2&#65292;&#29992;&#20110;&#22312;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#30340;&#26131;&#33719;&#21382;&#21490;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#25351;&#23548;&#22823;&#35268;&#27169;&#39640;&#20998;&#36776;&#29575;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29616;&#26377;&#30340;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#26041;&#27861;&#26174;&#31034;&#20102;CNN&#22312;&#20445;&#30041;&#23616;&#37096;&#22320;&#38754;&#32454;&#33410;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#21508;&#31181;&#22320;&#35980;&#20013;&#20840;&#23616;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Paraformer&#20013;&#30340;&#24182;&#34892;CNN-Transformer&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#26080;&#38477;&#37319;&#26679;CNN&#20998;&#25903;&#21644;&#19968;&#20010;Transformer&#20998;&#25903;&#65292;&#26469;&#20849;&#21516;&#25429;&#33719;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02746v1 Announce Type: cross  Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual informatio
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.02292</link><description>&lt;p&gt;
Android&#24212;&#29992;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#21313;&#24180;&#22823;&#35268;&#27169;&#36235;&#21183;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65292;&#30740;&#31350;&#20102;&#21313;&#24180;&#38388;&#38544;&#31169;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#36235;&#21183;&#65292;&#21457;&#29616;&#38544;&#31169;&#35780;&#35770;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#65292;&#25506;&#35752;&#20102;&#28909;&#38376;&#21644;&#36880;&#28176;&#20943;&#23569;&#30340;&#38544;&#31169;&#35805;&#39064;&#65292;&#20197;&#21450;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30475;&#27861;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;&#24212;&#29992;&#21830;&#24215;&#19978;1200&#19975;&#26465;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#36825;&#20123;&#35780;&#35770;&#36328;&#36234;&#20102;10&#24180;&#26102;&#38388;&#12290;&#36890;&#36807;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#26102;&#38388;&#12289;&#22269;&#23478;&#12289;&#24212;&#29992;&#31867;&#22411;&#12289;&#19981;&#21516;&#38544;&#31169;&#20027;&#39064;&#20197;&#21450;&#22810;&#31181;&#24773;&#24863;&#32500;&#24230;&#19978;&#26816;&#35270;&#29992;&#25143;&#23545;&#38544;&#31169;&#38382;&#39064;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#25345;&#32493;&#22686;&#38271;&#65292;&#24182;&#25506;&#31350;&#20102;&#19968;&#20123;&#28909;&#38376;&#35805;&#39064;&#65288;&#22914;&#25968;&#25454;&#21024;&#38500;&#21644;&#25968;&#25454;&#31363;&#21462;&#65289;&#65292;&#20197;&#21450;&#19968;&#20123;&#36880;&#28176;&#20943;&#23569;&#30340;&#35805;&#39064;&#65288;&#22914;&#28041;&#21450;&#25935;&#24863;&#26435;&#38480;&#30340;&#38544;&#31169;&#30456;&#20851;&#35780;&#35770;&#65289;&#12290;&#23613;&#31649;&#38544;&#31169;&#35780;&#35770;&#26469;&#33258;200&#22810;&#20010;&#22269;&#23478;&#65292;&#20294;&#26377;33&#20010;&#22269;&#23478;&#25552;&#20379;&#20102;90%&#30340;&#38544;&#31169;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#27599;&#20010;&#22269;&#23478;&#29992;&#25143;&#35780;&#35770;&#30340;&#38544;&#31169;&#20027;&#39064;&#20998;&#24067;&#26469;&#36827;&#34892;&#36328;&#22269;&#23478;&#27604;&#36739;&#65292;&#21457;&#29616;&#22320;&#29702;&#25509;&#36817;&#24182;&#19981;&#24847;&#21619;&#30528;&#38468;&#36817;&#22269;&#23478;&#26377;&#31867;&#20284;&#30340;&#38544;&#31169;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02292v1 Announce Type: new  Abstract: We present an analysis of 12 million instances of privacy-relevant reviews publicly visible on the Google Play Store that span a 10 year period. By leveraging state of the art NLP techniques, we can examine what users have been writing about privacy along multiple dimensions: time, countries, app types, diverse privacy topics, and even across a spectrum of emotions. We find consistent growth of privacy-relevant reviews, and explore topics that are trending (such as Data Deletion and Data Theft), as well as those on the decline (such as privacy-relevant reviews on sensitive permissions). We find that although privacy reviews come from more than 200 countries, 33 countries provide 90% of privacy reviews. We conduct a comparison across countries by examining the distribution of privacy topics a country's users write about, and find that geographic proximity is not a reliable indicator that nearby countries have similar privacy perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#24046;&#20998;&#20998;&#32452;&#21644;&#19968;&#33324;&#20998;&#32452;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#35299;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01192</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#20840;&#23616;&#20248;&#21270;&#30340;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Composite Decomposition Method for Large-Scale Global Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#24046;&#20998;&#20998;&#32452;&#21644;&#19968;&#33324;&#20998;&#32452;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#35299;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#65288;CC&#65289;&#31639;&#27861;&#22522;&#20110;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#20840;&#23616;&#20248;&#21270;&#65288;LSGO&#65289;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20998;&#32452;&#38454;&#27573;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26174;&#33879;&#24433;&#21709;&#20248;&#21270;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#19968;&#33324;&#21487;&#20998;&#31163;&#20998;&#32452;&#65288;GSG&#65289;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#24046;&#20998;&#20998;&#32452;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#24471;&#38750;&#21487;&#21152;&#20998;&#31163;&#20989;&#25968;&#30340;&#20998;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20854;&#23384;&#22312;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#21512;&#21487;&#20998;&#31163;&#20998;&#32452;&#65288;CSG&#65289;&#26041;&#27861;&#65292;&#23558;DG&#21644;GSG&#26080;&#32541;&#25972;&#21512;&#21040;&#19968;&#20010;&#38382;&#39064;&#20998;&#35299;&#26694;&#26550;&#20013;&#65292;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;CSG&#24341;&#20837;&#20102;&#19968;&#20010;&#36880;&#27493;&#20998;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#20934;&#30830;&#20998;&#35299;&#21508;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01192v1 Announce Type: cross  Abstract: Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer strategy, have emerged as the predominant approach to solving large-scale global optimization (LSGO) problems. The efficiency and accuracy of the grouping stage significantly impact the performance of the optimization process. While the general separability grouping (GSG) method has overcome the limitation of previous differential grouping (DG) methods by enabling the decomposition of non-additively separable functions, it suffers from high computational complexity. To address this challenge, this article proposes a composite separability grouping (CSG) method, seamlessly integrating DG and GSG into a problem decomposition framework to utilize the strengths of both approaches. CSG introduces a step-by-step decomposition framework that accurately decomposes various problem types using fewer computational resources. By sequentially identifying additively, multiplic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65288;MC-EIF&#65289;&#30340;&#20840;&#33258;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#33021;&#22815;&#23454;&#29616;&#38024;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#36798;&#21040;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00158</link><description>&lt;p&gt;
&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#23454;&#29616;&#33258;&#21160;&#21270;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automated Efficient Estimation using Monte Carlo Efficient Influence Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65288;MC-EIF&#65289;&#30340;&#20840;&#33258;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#33021;&#22815;&#23454;&#29616;&#38024;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#36798;&#21040;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#28041;&#21450;&#20351;&#29992;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20272;&#35745;&#20302;&#32500;&#32479;&#35745;&#37327;&#12290;&#20960;&#31181;&#26041;&#27861;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#29702;&#35770;&#26469;&#35299;&#20915;&#36825;&#20123;&#20272;&#35745;&#20219;&#21153;&#65292;&#20363;&#22914;&#21435;&#20559;/&#21452;&#37325;&#26497;&#22823;&#20284;&#28982;&#25110;&#26377;&#38024;&#23545;&#24615;&#26368;&#23567;&#25439;&#22833;&#20272;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#25216;&#26415;&#65292;&#21363;\textit{&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;} (MC-EIF)&#65292;&#29992;&#20110;&#36924;&#36817;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#21487;&#24494;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#12290;MC-EIF&#21487;&#33258;&#21160;&#21270;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#32479;&#35745;&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#20197;&#21069;&#38656;&#35201;&#20005;&#26684;&#30340;&#33258;&#23450;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;MC-EIF&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;MC-EIF&#30340;&#20272;&#35745;&#22120;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;$\sqrt{N}$&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#65292;&#20351;&#29992;MC-EIF&#30340;&#20272;&#35745;&#22120;&#19982;&#20351;&#29992;&#35299;&#26512;EIF&#30340;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#19968;&#20010;&#26032;&#39062;&#30340;&#39030;&#28857;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00158v1 Announce Type: cross  Abstract: Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. This paper introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and target functionals that would previously require rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we demonstrate a novel capstone exa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.19095</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#21644;CNN&#38598;&#25104;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#23545;&#20110;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#32467;&#26500;&#20915;&#23450;&#20854;&#21151;&#33021;&#12290;&#34507;&#30333;&#30340;&#20108;&#32423;&#32467;&#26500;&#26159;&#30001;&#34507;&#30333;&#36136;&#30340;&#19968;&#32423;&#32467;&#26500;&#25240;&#21472;&#24418;&#25104;&#30340;&#65292;&#32780;&#34507;&#30333;&#30340;&#19977;&#32423;&#32467;&#26500;&#26159;&#30001;&#20108;&#32423;&#32467;&#26500;&#30340;&#24367;&#26354;&#21644;&#25240;&#21472;&#24418;&#25104;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#34507;&#30333;&#30340;&#20108;&#32423;&#32467;&#26500;&#23545;&#20110;&#25972;&#20307;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#34429;&#28982;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#34507;&#30333;&#36136;&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#34507;&#30333;&#32467;&#26500;&#39044;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#19981;&#36275;&#20197;&#28385;&#36275;&#23545;&#34507;&#30333;&#36136;&#20449;&#24687;&#30340;&#22823;&#37327;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#20108;&#32500;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DstruCCN&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#19968;&#20010;&#30417;&#30563;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19095v1 Announce Type: cross  Abstract: Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neural Networks (CCN) and a supervised Transformer pr
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;Lipschitz&#24615;&#19982;&#31283;&#23450;&#31209;&#30340;&#30740;&#31350;&#20026;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35282;&#24230;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.18863</link><description>&lt;p&gt;
&#27010;&#29575;Lipschitz&#24615;&#21644;&#31283;&#23450;&#31209;&#29992;&#20110;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18863
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;Lipschitz&#24615;&#19982;&#31283;&#23450;&#31209;&#30340;&#30740;&#31350;&#20026;&#27604;&#36739;&#35299;&#37322;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35282;&#24230;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#27169;&#22411;&#22914;&#20170;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#22312;&#30340;&#38382;&#39064;&#26159;&#21738;&#31181;&#35299;&#37322;&#24615;&#27169;&#22411;&#26368;&#26377;&#25928;&#12290;&#27010;&#29575;Lipschitz&#24615;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#19982;&#20107;&#21518;&#35299;&#37322;&#30340;&#36136;&#37327;&#22522;&#26412;&#30456;&#20851;&#12290;&#26412;&#25991;&#22312;&#23545;Integrated Gradients&#12289;LIME&#21644;SmoothGrad&#30340;&#27010;&#29575;Lipschitzness&#36827;&#34892;&#29702;&#35770;&#19979;&#38480;&#35777;&#26126;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20351;&#29992;&#27010;&#29575;Lipschitz&#24615;&#21644;&#24402;&#19968;&#21270;&#30340;&#32874;&#26126;&#24230;&#26469;&#27604;&#36739;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;Lipschitz&#24120;&#25968;&#19982;&#20854;&#31283;&#23450;&#31209;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#31209;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18863v1 Announce Type: new  Abstract: Explainability models are now prevalent within machine learning to address the black-box nature of neural networks. The question now is which explainability model is most effective. Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations. In this work, we prove theoretical lower bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and SmoothGrad. We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models. Further, we prove a link between the local Lipschitz constant of a neural network and its stable rank. We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#35838;&#31243;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17269</link><description>&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#36935;&#35265;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17269
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#35838;&#31243;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;MultiDAG+CL&#65292;&#29992;&#20110;&#20250;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#65292;&#23427;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#38598;&#25104;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#27169;&#22411;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#24212;&#23545;&#24773;&#24863;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35838;&#31243;&#23398;&#20064;&#36890;&#36807;&#36880;&#28176;&#20197;&#26377;&#24847;&#20041;&#30340;&#39034;&#24207;&#21576;&#29616;&#35757;&#32451;&#26679;&#26412;&#26469;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#24773;&#32490;&#21464;&#21270;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;&#22312;IEMOCAP&#21644;MELD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MultiDAG+CL&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17269v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models.
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.13005</link><description>&lt;p&gt;
SzCORE&#65306;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#30315;&#30187;&#31038;&#21306;&#24320;&#28304;&#30740;&#31350;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23478;&#24237;&#21644;&#38271;&#26399;&#33041;&#30005;&#22270;&#30417;&#27979;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#39640;&#36136;&#37327;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#36825;&#20123;&#31639;&#27861;&#39564;&#35777;&#26041;&#27861;&#30340;&#24322;&#36136;&#24615;&#24433;&#21709;&#20102;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35813;&#24322;&#36136;&#24615;&#20027;&#35201;&#28041;&#21450;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#24615;&#33021;&#24230;&#37327;&#31561;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24314;&#31435;EEG&#22522;&#30784;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#39564;&#35777;&#30340;&#26631;&#20934;&#21270;&#12290;&#22522;&#20110;&#29616;&#26377;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#32452;&#20851;&#20110;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;EEG&#25968;&#25454;&#36755;&#20837;&#20869;&#23481;&#12289;&#30315;&#30187;&#27880;&#37322;&#36755;&#20837;&#21644;&#36755;&#20986;&#12289;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#20197;&#21450;&#24615;&#33021;&#24230;&#37327;&#30340;&#24314;&#35758;&#21644;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13005v1 Announce Type: cross  Abstract: The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25918;&#24323;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#21464;&#20307;&#65292;&#36890;&#36807;&#23545;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#26032;&#35299;&#37322;&#65292;TTM&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23398;&#29983;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11148</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Based on Transformed Teacher Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25918;&#24323;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#21464;&#20307;&#65292;&#36890;&#36807;&#23545;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#26032;&#35299;&#37322;&#65292;TTM&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23398;&#29983;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#36830;&#25509;&#36923;&#36753;&#21305;&#37197;&#21644;&#27010;&#29575;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#65292;&#28201;&#24230;&#32553;&#25918;&#22312;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;KD&#20013;&#65292;&#28201;&#24230;&#32553;&#25918;&#34987;&#24212;&#29992;&#20110;&#25945;&#24072;&#30340;logits&#21644;&#23398;&#29983;&#30340;logits&#12290;&#21463;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26412;&#25991;&#25918;&#24323;&#20102;&#22312;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;KD&#21464;&#20307;&#65292;&#31216;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#12290;&#36890;&#36807;&#37325;&#26032;&#35299;&#37322;&#28201;&#24230;&#32553;&#25918;&#20316;&#20026;&#27010;&#29575;&#20998;&#24067;&#30340;&#24130;&#21464;&#25442;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;&#30340;KD&#30456;&#27604;&#65292;TTM&#22312;&#20854;&#30446;&#26631;&#20989;&#25968;&#20013;&#20855;&#26377;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#36825;&#20805;&#24403;&#20102;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#36825;&#31181;&#22266;&#26377;&#30340;&#27491;&#21017;&#21270;&#65292;TTM&#23548;&#33268;&#35757;&#32451;&#33391;&#22909;&#30340;&#23398;&#29983;&#27604;&#21407;&#22987;KD&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11148v1 Announce Type: new  Abstract: As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed proba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#21644;&#29702;&#35299;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06295</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#22810;&#37325;&#25239;&#33740;&#33647;&#29289;&#32784;&#33647;&#24615;&#30340;&#26089;&#26399;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Interpretable Data-Driven Models for Early Prediction of Antimicrobial Multidrug Resistance Using Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#21644;&#29702;&#35299;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#22810;&#27169;&#24577;&#27880;&#20876;&#65292;&#21253;&#25324;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#12290;&#34429;&#28982;MTS&#26159;&#20020;&#24202;&#39044;&#27979;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#20294;&#23558;&#20854;&#19982;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#34701;&#21512;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#25104;&#20026;&#35782;&#21035;&#21644;&#23450;&#20041;&#21307;&#30103;&#39046;&#22495;&#28508;&#22312;&#27169;&#24335;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;DNN&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#36824;&#38656;&#35201;&#22522;&#26412;&#25913;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#22312;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#29702;&#35299;&#39532;&#24503;&#37324;&#35757;&#25289;&#24067;&#25289;&#36798;&#22823;&#23398;&#21307;&#38498;&#65288;&#35199;&#29677;&#29273;&#65289;&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#65288;AMR&#65289;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;&#24739;&#32773;&#30340;&#20010;&#20154;&#36164;&#26009;&#21644;&#21021;&#22987;&#20581;&#24247;&#29366;&#20917;&#20351;&#29992;&#38745;&#24577;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#28436;&#21464;&#36807;&#31243;&#20351;&#29992;MTS&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHR) is an inherently multimodal register of the patient's health status characterized by static data and multivariate time series (MTS). While MTS are a valuable tool for clinical prediction, their fusion with other data modalities can possibly result in more thorough insights and more accurate results. Deep neural networks (DNNs) have emerged as fundamental tools for identifying and defining underlying patterns in the healthcare domain. However, fundamental improvements in interpretability are needed for DNN models to be widely used in the clinical setting. In this study, we present an approach built on a collection of interpretable multimodal data-driven models that may anticipate and understand the emergence of antimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU) of the University Hospital of Fuenlabrada (Madrid, Spain). The profile and initial health status of the patient are modeled using static variables, while the evolution 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03921</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Enhance Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23427;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#24378;&#35843;&#65292;&#29305;&#21035;&#26159;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#26377;&#25928;&#22320;&#24179;&#34913;&#21208;&#25506;&#21644;&#24320;&#21457;&#12290;&#23613;&#31649;&#22312;BO&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24179;&#34913;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24494;&#22937;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;LLAMBO&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19982;BO&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#20351;LLM&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#35780;&#20272;&#25552;&#20986;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#32467;&#21512;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#26469;&#22686;&#24378;&#22522;&#20110;&#27169;&#22411;&#30340;BO&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLAMBO&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#26102;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02245</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20108;&#36827;&#21046;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02245
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#26102;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#36335;&#38754;&#34920;&#38754;&#29366;&#20917;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#31639;&#27861;&#33258;&#21160;&#26816;&#27979;&#20195;&#34920;&#24322;&#24120;&#29366;&#24577;&#65288;&#22914;&#35010;&#32541;&#65289;&#30340;&#20687;&#32032;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30456;&#20851;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#36335;&#38754;&#24322;&#24120;&#21306;&#22495;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#29983;&#25104;&#22120;&#20174;&#24322;&#26500;&#36755;&#20837;&#20013;&#20272;&#35745;&#27010;&#29575;&#29305;&#24449;&#22270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23558;&#20960;&#31181;&#27880;&#24847;&#26426;&#21046;&#32435;&#20837;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#35757;&#32451;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalous pavement surface conditions detection aims to detect pixels representing anomalous states, such as cracks, on pavement surface images automatically by algorithms. Recently, deep learning models have been intensively applied to related topics with outstanding performance. However, most existing deep learning-related solutions rarely achieve a stable performance on diverse datasets. To address this issue, in this work, we propose a deep learning framework based on conditional Generative Adversarial Networks for anomalous region detection on pavement images at the pixel level. In particular, the proposed framework is developed to enhance the generator's ability to estimate the probability feature map from heterogeneous inputs with two training stages and multiscale feature representation. Moreover, several attention mechanisms are incorporated into the proposed framework to mitigate the performance deterioration of model training on severely imbalanced datasets. We implement exp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PC-Winter&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35780;&#20272;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#20197;&#21450;&#35745;&#31639;&#25361;&#25112;&#65292;PC-Winter&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01943</link><description>&lt;p&gt;
&#26377;&#25928;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Precedence-Constrained Winter Value for Effective Graph Data Valuation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PC-Winter&#30340;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35780;&#20272;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#20197;&#21450;&#35745;&#31639;&#25361;&#25112;&#65292;PC-Winter&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#12289;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#21644;&#30830;&#23450;&#20844;&#24179;&#34917;&#20607;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#22312;&#35780;&#20272;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#30340;&#20215;&#20540;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#22270;&#24418;&#25968;&#25454;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20215;&#20540;&#20272;&#35745;&#25104;&#26412;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#24418;&#25968;&#25454;&#20272;&#20540;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;&#20248;&#20808;&#32422;&#26463;&#20908;&#23395;&#20215;&#20540;(Precedence-Constrained Winter, PC-Winter)&#65292;&#20197;&#32771;&#34385;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#23454;&#29616;&#23545;PC-Winter&#30340;&#39640;&#25928;&#36817;&#20284;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-Winter&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is essential for quantifying data's worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the exponential growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, Precedence-Constrained Winter (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25429;&#25417;&#28151;&#21512;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#29992;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#23376;&#38388;&#38548;&#20869;&#23398;&#20064;&#20107;&#20214;&#38388;&#30340;&#20381;&#36182;&#22270;&#65292;&#25552;&#39640;&#20102;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.16083</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#28508;&#22312;&#22270;&#30340;&#31070;&#32463;&#26102;&#24207;&#28857;&#36807;&#31243;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16083
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25429;&#25417;&#28151;&#21512;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#29992;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#23376;&#38388;&#38548;&#20869;&#23398;&#20064;&#20107;&#20214;&#38388;&#30340;&#20381;&#36182;&#22270;&#65292;&#25552;&#39640;&#20102;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#21457;&#29983;&#24448;&#24448;&#34920;&#29616;&#20986;&#33258;&#28608;&#21644;&#20114;&#28608;&#25928;&#24212;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#21270;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36825;&#20123;&#20107;&#20214;&#21160;&#24577;&#20063;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#20855;&#26377;&#26576;&#31181;&#21608;&#26399;&#24615;&#36235;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#36825;&#31181;&#28151;&#21512;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#24207;&#21015;&#30340;&#25972;&#20010;&#26102;&#38388;&#38388;&#38548;&#34987;&#21010;&#20998;&#20026;&#19968;&#32452;&#23376;&#38388;&#38548;&#12290;&#20551;&#35774;&#27599;&#20010;&#23376;&#38388;&#38548;&#20869;&#30340;&#20107;&#20214;&#21160;&#24577;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#23376;&#38388;&#38548;&#20043;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27599;&#20010;&#23376;&#38388;&#38548;&#20013;&#35266;&#23519;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#22270;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20381;&#36182;&#22270;&#26469;&#28040;&#38500;&#36807;&#21435;&#20107;&#20214;&#30340;&#38750;&#36129;&#29486;&#24433;&#21709;&#65292;&#39044;&#27979;&#26410;&#26469;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#19978;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16083v2 Announce Type: replace  Abstract: Continuously-observed event occurrences, often exhibit self- and mutually-exciting effects, which can be well modeled using temporal point processes. Beyond that, these event dynamics may also change over time, with certain periodic trends. We propose a novel variational auto-encoder to capture such a mixture of temporal dynamics. More specifically, the whole time interval of the input sequence is partitioned into a set of sub-intervals. The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals. In particular, we use a sequential latent variable model to learn a dependency graph between the observed dimensions, for each sub-interval. The model predicts the future event times, by using the learned dependency graph to remove the noncontributing influences of past events. By doing so, the proposed model demonstrates its higher accuracy in predicting inter-event times and e
&lt;/p&gt;</description></item><item><title>MixEHR-SurG&#26159;&#19968;&#31181;&#30417;&#30563;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;EHR&#25968;&#25454;&#21644;&#29983;&#23384;&#39118;&#38505;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29983;&#23384;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#26029;&#19982;&#24739;&#32773;&#27515;&#20129;&#30456;&#20851;&#30340;PheCode&#29305;&#23450;&#34920;&#22411;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.13454</link><description>&lt;p&gt;
MixEHR-SurG&#65306;&#19968;&#31181;&#32852;&#21512;&#27604;&#20363;&#21361;&#38505;&#21644;&#24341;&#23548;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25512;&#26029;&#19982;&#27515;&#20129;&#30456;&#20851;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
MixEHR-SurG: a joint proportional hazard and guided topic model for inferring mortality-associated topics from electronic health records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13454
&lt;/p&gt;
&lt;p&gt;
MixEHR-SurG&#26159;&#19968;&#31181;&#30417;&#30563;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;EHR&#25968;&#25454;&#21644;&#29983;&#23384;&#39118;&#38505;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29983;&#23384;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#26029;&#19982;&#24739;&#32773;&#27515;&#20129;&#30456;&#20851;&#30340;PheCode&#29305;&#23450;&#34920;&#22411;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29983;&#23384;&#27169;&#22411;&#35201;&#20040;&#19981;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35201;&#20040;&#24456;&#38590;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MixEHR-SurG&#30340;&#30417;&#30563;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25972;&#21512;&#24322;&#36136;EHR&#25968;&#25454;&#24182;&#24314;&#27169;&#29983;&#23384;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1) &#23558;EHR&#20027;&#39064;&#25512;&#26029;&#19982;Cox&#27604;&#20363;&#39118;&#38505;&#20284;&#28982;&#30456;&#32467;&#21512;&#65307;(2) &#20351;&#29992;PheCode&#27010;&#24565;&#38598;&#25104;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#20027;&#39064;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;&#27599;&#20010;&#20027;&#39064;&#21487;&#20197;&#34987;&#35782;&#21035;&#20026;&#20165;&#19982;&#19968;&#20010;PheCode&#30456;&#20851;&#30340;&#34920;&#22411;&#65307;(3) &#22810;&#27169;&#24577;&#29983;&#23384;&#20027;&#39064;&#25512;&#26029;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29983;&#23384;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#26029;&#19982;&#24739;&#32773;&#27515;&#20129;&#30456;&#20851;&#30340;PheCode&#29305;&#23450;&#34920;&#22411;&#20027;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;EHR&#25968;&#25454;&#38598;&#65288;&#39745;&#21271;&#20811;&#20808;&#22825;&#24615;&#24515;&#33039;&#30149;&#65288;CHD&#65289;&#25968;&#25454;&#21644;&#21253;&#21547;8,211&#21517;&#21463;&#35797;&#32773;&#30340;&#22810;&#20010;&#38376;&#35786;&#32034;&#36180;&#35760;&#24405;&#30340;1,767&#20010;&#21807;&#19968;ICD&#34892;&#20026;&#65289;&#26469;&#35780;&#20272;MixEHR-SurG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13454v2 Announce Type: replace  Abstract: Existing survival models either do not scale to high dimensional and multi-modal data or are difficult to interpret. In this study, we present a supervised topic model called MixEHR-SurG to simultaneously integrate heterogeneous EHR data and model survival hazard. Our contributions are three-folds: (1) integrating EHR topic inference with Cox proportional hazards likelihood; (2) integrating patient-specific topic hyperparameters using the PheCode concepts such that each topic can be identified with exactly one PheCode-associated phenotype; (3) multi-modal survival topic inference. This leads to a highly interpretable survival topic model that can infer PheCode-specific phenotype topics associated with patient mortality. We evaluated MixEHR-SurG using a simulated dataset and two real-world EHR datasets: the Quebec Congenital Heart Disease (CHD) data consisting of 8,211 subjects with 75,187 outpatient claim records of 1,767 unique ICD 
&lt;/p&gt;</description></item><item><title>ERASE&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ERASE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#32534;&#30721;&#29575;&#38477;&#20302;&#26469;&#23398;&#20064;&#20855;&#26377;&#35823;&#24046;&#23481;&#24525;&#24615;&#30340;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22270;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08852</link><description>&lt;p&gt;
ERASE: &#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#23481;&#24525;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ERASE: Error-Resilient Representation Learning on Graphs for Label Noise Tolerance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08852
&lt;/p&gt;
&lt;p&gt;
ERASE&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ERASE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#32534;&#30721;&#29575;&#38477;&#20302;&#26469;&#23398;&#20064;&#20855;&#26377;&#35823;&#24046;&#23481;&#24525;&#24615;&#30340;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22270;&#20219;&#21153;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#36825;&#19968;&#25104;&#23601;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#23454;&#38469;&#19978;&#20351;&#29992;&#32463;&#27982;&#39640;&#25928;&#26469;&#28304;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#21644;&#29992;&#25143;&#26631;&#31614;&#65289;&#33719;&#24471;&#30340;&#26631;&#31614;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26631;&#31614;&#32463;&#24120;&#24102;&#26377;&#22122;&#22768;&#65292;&#24433;&#21709;&#28145;&#24230;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22686;&#24378;&#22270;&#20219;&#21153;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ERASE&#65288;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#23481;&#24525;&#24615;&#65289;&#30340;&#26041;&#27861;&#12290;ERASE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#32534;&#30721;&#29575;&#38477;&#20302;&#26469;&#23398;&#20064;&#20855;&#26377;&#35823;&#24046;&#23481;&#24525;&#24615;&#30340;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#26041;&#27861;&#39044;&#26657;&#27491;&#22122;&#22768;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08852v2 Announce Type: replace  Abstract: Deep learning has achieved remarkable success in graph-related tasks, yet this accomplishment heavily relies on large-scale high-quality annotated datasets. However, acquiring such datasets can be cost-prohibitive, leading to the practical use of labels obtained from economically efficient sources such as web searches and user tags. Unfortunately, these labels often come with noise, compromising the generalization performance of deep networks. To tackle this challenge and enhance the robustness of deep learning models against label noise in graph-based tasks, we propose a method called ERASE (Error-Resilient representation learning on graphs for lAbel noiSe tolerancE). The core idea of ERASE is to learn representations with error tolerance by maximizing coding rate reduction. Particularly, we introduce a decoupled label propagation method for learning representations. Before training, noisy labels are pre-corrected through structural
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.07624</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A dynamical clipping approach with task feedback for Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20248;&#21270;&#21644;&#26426;&#22120;&#20154;&#23398;&#20064;&#31561;&#12290;&#28982;&#32780;&#65292;PPO&#21463;&#21040;&#22266;&#23450;&#21098;&#35009;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#21069;&#27809;&#26377;&#29702;&#35770;&#35777;&#26126;&#26368;&#20339;&#21098;&#35009;&#36793;&#30028;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#29420;&#29305;&#30340;&#21098;&#35009;&#36793;&#30028;&#25130;&#26029;&#26032;&#26087;&#31574;&#30053;&#30340;&#27604;&#29575;&#65292;&#21487;&#20197;&#30830;&#20445;&#31283;&#23450;&#30340;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20339;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22266;&#23450;&#30340;&#21098;&#35009;&#36793;&#30028;&#38480;&#21046;&#20102;agent&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#19968;&#31181;&#21160;&#24577;&#21098;&#35009;&#36793;&#30028;&#20197;&#22686;&#24378;PPO&#30340;&#24615;&#33021;&#26159;&#38750;&#24120;&#26377;&#30410;&#30340;&#12290;&#19982;&#20197;&#24448;&#30340;&#21098;&#35009;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#20013;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#35270;&#20316;RL&#20219;&#21153;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07624v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization (PPO) has been broadly applied to various domains, including Large Language Model (LLM) optimization and Robotics learning, etc. However, PPO is limited by a fixed setting for the clipping bound. Specifically, there is no theoretical proof that the optimal clipping bound remains consistent throughout the entire training process. Truncating the ratio of the new and old policies with a unique clipping bound ensures stable training and can achieve the best training performance. Additionally, previous research suggests that a fixed clipping bound limits the agent's exploration. Therefore, researching a dynamical clipping bound to enhance PPO's performance can be highly beneficial. Different from previous clipping approaches, we consider increasing the maximum cumulative Return in reinforcement learning (RL) tasks as the preference of the RL task, and propose a bi-level proximal policy optimization parad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16609</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#30340;&#29305;&#24449;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Eigenmatrix for unstructured sparse recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#33324;&#24418;&#24335;&#30340;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#21253;&#25324;&#26377;&#29702;&#36924;&#36817;&#12289;&#35889;&#20989;&#25968;&#20272;&#35745;&#12289;&#20613;&#37324;&#21494;&#21453;&#28436;&#12289;&#25289;&#26222;&#25289;&#26031;&#21453;&#28436;&#21644;&#31232;&#30095;&#21453;&#21367;&#31215;&#31561;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#30697;&#38453;&#65292;&#19968;&#31181;&#20855;&#26377;&#25152;&#38656;&#36817;&#20284;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#20026;&#36825;&#20123;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the unstructured sparse recovery problems in a general form. Examples include rational approximation, spectral function estimation, Fourier inversion, Laplace inversion, and sparse deconvolution. The main challenges are the noise in the sample values and the unstructured nature of the sample locations. This paper proposes the eigenmatrix, a data-driven construction with desired approximate eigenvalues and eigenvectors. The eigenmatrix offers a new way for these sparse recovery problems. Numerical results are provided to demonstrate the efficiency of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.16496</link><description>&lt;p&gt;
DPOD&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#21033;&#29992;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#20256;&#25773;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#26159;&#20449;&#24687;&#36807;&#36733;&#26102;&#20195;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#34394;&#20551;&#26032;&#38395;&#21487;&#33021;&#28041;&#21450;&#19981;&#21516;&#39046;&#22495;&#65292;&#22914;&#25919;&#27835;&#12289;&#20307;&#32946;&#12289;&#23089;&#20048;&#31561;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#19981;&#21516;&#39046;&#22495;&#26032;&#38395;&#25991;&#31456;&#23384;&#22312;&#30528;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#39046;&#22495;&#25968;&#25454;&#20016;&#23500;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#24320;&#21457;&#20986;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#37327;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65288;&#22312;&#27492;&#31216;&#20026;&#22810;&#27169;&#24577;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#65288;&#20351;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16496v2 Announce Type: replace  Abstract: The spread of fake news using out-of-context images has become widespread and is a relevant problem in this era of information overload. Such out-of-context fake news may arise across different domains like politics, sports, entertainment, etc. In practical scenarios, an inherent problem of imbalance exists among news articles from such widely varying domains, resulting in a few domains with abundant data, while the rest containing very limited data. Under such circumstances, it is imperative to develop methods which can work in such varying amounts of data setting. In this work, we explore whether out-of-domain data can help to improve out-of-context misinformation detection (termed here as multi-modal fake news detection) of a desired domain, to address this challenging problem. Towards this goal, we propose a novel framework termed DPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to compute generalizable featu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DyG-HAP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;ICU&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2311.15545</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DyG-HAP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;ICU&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30333;&#34507;&#30333;&#23545;&#25351;&#31034;&#36523;&#20307;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#39044;&#27979;&#34880;&#27974;&#30333;&#34507;&#30333;&#27700;&#24179;&#24182;&#30830;&#23450;&#36866;&#24403;&#21058;&#37327;&#26159;&#20127;&#38656;&#35299;&#20915;&#30340;&#20020;&#24202;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#21361;&#37325;&#24739;&#32773;&#20013;&#65292;&#20197;&#20445;&#25345;&#26368;&#20339;&#34880;&#28082;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#24182;&#19981;&#31616;&#21333;&#65292;&#24517;&#39035;&#21033;&#29992;&#29983;&#21270;&#26631;&#24535;&#29289;&#30340;&#21160;&#24577;&#24615;&#20197;&#21450;&#27835;&#30103;&#24739;&#32773;&#30340;&#32463;&#39564;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#65292;&#38477;&#20302;&#27169;&#22411;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Out-of-Distribution Generalized Dynamic Graph&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65288;DyG-HAP&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#22312;&#20303;&#38498;&#26399;&#38388;&#20026;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#30333;&#34507;&#30333;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15545v2 Announce Type: replace-cross  Abstract: Human albumin is essential for indicating the body's overall health. Accurately predicting plasma albumin levels and determining appropriate doses are urgent clinical challenges, particularly in critically ill patients, to maintain optimal blood levels. However, human albumin prediction is non-trivial that has to leverage the dynamics of biochemical markers as well as the experience of treating patients. Moreover, the problem of distribution shift is often encountered in real clinical data, which may lead to a decline in the model prediction performance and reduce the reliability of the model's application. In this paper, we propose a framework named Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction (DyG-HAP), which is able to provide accurate albumin predictions for Intensity Care Unit (ICU) patients during hospitalization. We first model human albumin prediction as a dynamic graph regre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;I-DIDA&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#31354;&#20998;&#24067;&#21464;&#21270;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#31283;&#23450;&#30340;&#19981;&#21464;&#27169;&#24335;&#65292;&#21363;&#32467;&#26500;&#21644;&#29305;&#24449;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#31283;&#23450;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.14255</link><description>&lt;p&gt;
&#20855;&#26377;&#35299;&#32806;&#24178;&#39044;&#21644;&#19981;&#21464;&#25512;&#21160;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Generalized Dynamic Graph Neural Network with Disentangled Intervention and Invariance Promotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;I-DIDA&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#21160;&#24577;&#22270;&#20013;&#30340;&#26102;&#31354;&#20998;&#24067;&#21464;&#21270;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#31283;&#23450;&#30340;&#19981;&#21464;&#27169;&#24335;&#65292;&#21363;&#32467;&#26500;&#21644;&#29305;&#24449;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#31283;&#23450;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DyGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DyGNNs&#26080;&#27861;&#22788;&#29702;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;&#21160;&#24577;&#22270;&#20013;&#26159;&#33258;&#28982;&#23384;&#22312;&#30340;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;DyGNNs&#25152;&#21033;&#29992;&#30340;&#27169;&#24335;&#21487;&#33021;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#23545;&#26631;&#31614;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35299;&#32806;&#24178;&#39044;&#21644;&#19981;&#21464;&#25512;&#21160;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;I-DIDA&#65289;&#65292;&#20197;&#21457;&#29616;&#21644;&#21033;&#29992;&#19981;&#21464;&#27169;&#24335;&#65292;&#21363;&#37027;&#20123;&#32467;&#26500;&#21644;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20445;&#25345;&#31283;&#23450;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#32806;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25429;&#25417;&#21464;&#24322;&#21644;&#19981;&#21464;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#35299;&#32806;&#27169;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#31354;&#24178;&#39044;&#26426;&#21046;&#26469;&#21019;&#24314;&#22810;&#20010;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14255v2 Announce Type: replace  Abstract: Dynamic graph neural networks (DyGNNs) have demonstrated powerful predictive abilities by exploiting graph structural and temporal dynamics. However, the existing DyGNNs fail to handle distribution shifts, which naturally exist in dynamic graphs, mainly because the patterns exploited by DyGNNs may be variant with respect to labels under distribution shifts. In this paper, we propose Disentangled Intervention-based Dynamic graph Attention networks with Invariance Promotion (I-DIDA) to handle spatio-temporal distribution shifts in dynamic graphs by discovering and utilizing invariant patterns, i.e., structures and features whose predictive abilities are stable across distribution shifts. Specifically, we first propose a disentangled spatio-temporal attention network to capture the variant and invariant patterns. By utilizing the disentangled patterns, we design a spatio-temporal intervention mechanism to create multiple interventional 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23558;&#35268;&#21017;&#23398;&#20064;&#21644;&#25237;&#31080;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#19982;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#30830;&#23450;&#24615;&#35268;&#21017;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2311.07323</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#25237;&#31080;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Voting Approach for Explainable Classification with Rule Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23558;&#35268;&#21017;&#23398;&#20064;&#21644;&#25237;&#31080;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#19982;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#30830;&#23450;&#24615;&#35268;&#21017;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#20998;&#31867;&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#36890;&#24120;&#30001;&#26080;&#27861;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#23454;&#29616;&#12290;&#30456;&#21453;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#26679;&#30340;&#24773;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#20998;&#31867;&#22522;&#20110;&#21487;&#29702;&#35299;&#65288;&#19968;&#38454;&#65289;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#35299;&#37322;&#20102;&#25152;&#20316;&#20986;&#30340;&#39044;&#27979;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#36828;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20934;&#30830;&#65288;&#36890;&#24120;&#26174;&#33879;&#65289;&#12290;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#19982;&#65288;&#26080;&#27861;&#35299;&#37322;&#30340;&#65289;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20173;&#20197;&#30830;&#23450;&#24615;&#35268;&#21017;&#30340;&#24418;&#24335;&#25552;&#20379;&#35299;&#37322;&#12290;&#32771;&#34385;&#21040;&#21253;&#25324;&#23545;&#20445;&#38505;&#19994;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20351;&#29992;&#26696;&#20363;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#26222;&#36890;&#30340;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07323v2 Announce Type: replace  Abstract: State-of-the-art results in typical classification tasks are mostly achieved by unexplainable machine learning methods, like deep neural networks, for instance. Contrarily, in this paper, we investigate the application of rule learning methods in such a context. Thus, classifications become based on comprehensible (first-order) rules, explaining the predictions made. In general, however, rule-based classifications are less accurate than state-of-the-art results (often significantly). As main contribution, we introduce a voting approach combining both worlds, aiming to achieve comparable results as (unexplainable) state-of-the-art methods, while still providing explanations in the form of deterministic rules. Considering a variety of benchmark data sets including a use case of significant interest to insurance industries, we prove that our approach not only clearly outperforms ordinary rule learning methods, but also yields results on
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#27668;&#21160;&#21147;&#23398;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;GCM&#65292;&#23637;&#31034;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#26368;&#20339;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#26041;&#27861;&#30456;&#21305;&#25932;&#30340;&#30830;&#23450;&#24615;&#22825;&#27668;&#12289;&#38598;&#21512;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2311.07222</link><description>&lt;p&gt;
&#31070;&#32463;&#27668;&#20505;&#36890;&#29992;&#24490;&#29615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural General Circulation Models for Weather and Climate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#27668;&#21160;&#21147;&#23398;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;GCM&#65292;&#23637;&#31034;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#26368;&#20339;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#26041;&#27861;&#30456;&#21305;&#25932;&#30340;&#30830;&#23450;&#24615;&#22825;&#27668;&#12289;&#38598;&#21512;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#36890;&#29992;&#24490;&#29615;&#27169;&#22411;&#65288;GCMs&#65289;&#26159;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#30340;&#22522;&#30784;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#23567;&#23610;&#24230;&#36807;&#31243;&#65288;&#22914;&#20113;&#24418;&#25104;&#65289;&#35843;&#26657;&#30340;GCMs&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#37325;&#26032;&#20998;&#26512;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#19982;&#30830;&#23450;&#24615;&#22825;&#27668;&#39044;&#25253;&#20013;GCMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07222v3 Announce Type: replace-cross  Abstract: General circulation models (GCMs) are the foundation of weather and climate prediction. GCMs are physics-based simulators which combine a numerical solver for large-scale dynamics with tuned representations for small-scale processes such as cloud formation. Recently, machine learning (ML) models trained on reanalysis data achieved comparable or better skill than GCMs for deterministic weather forecasting. However, these models have not demonstrated improved ensemble forecasts, or shown sufficient stability for long-term weather and climate simulations. Here we present the first GCM that combines a differentiable solver for atmospheric dynamics with ML components, and show that it can generate forecasts of deterministic weather, ensemble weather and climate on par with the best ML and physics-based methods. NeuralGCM is competitive with ML models for 1-10 day forecasts, and with the European Centre for Medium-Range Weather Forec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#35270;&#22270;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#36328;&#35270;&#22270;&#23376;&#38598;&#30340;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21333;&#20010;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2311.04056</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#22810;&#35270;&#22270;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Causal Representation Learning with Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04056
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#35270;&#22270;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#36328;&#35270;&#22270;&#23376;&#38598;&#30340;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21333;&#20010;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#20174;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#35270;&#22270;&#65288;&#22914;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#65289;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#20801;&#35768;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#35270;&#22270;&#26500;&#25104;&#24213;&#23618;&#28508;&#22312;&#21464;&#37327;&#23376;&#38598;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#65292;&#36825;&#20123;&#21464;&#37327;&#21487;&#20197;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#27599;&#20010;&#35270;&#22270;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#36328;&#25152;&#26377;&#20219;&#24847;&#25968;&#37327;&#35270;&#22270;&#23376;&#38598;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#30452;&#33267;&#24179;&#28369;&#21452;&#23556;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22270;&#24418;&#26631;&#20934;&#65292;&#25351;&#31034;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31616;&#21333;&#35268;&#21017;&#30830;&#23450;&#21738;&#20123;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21487;&#35782;&#21035;&#24615;&#20195;&#25968;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#26694;&#26550;&#21644;&#29702;&#35770;&#32467;&#26524;&#32479;&#19968;&#24182;&#25193;&#23637;&#20102;&#20808;&#21069;&#20851;&#20110;&#22810;&#35270;&#22270;&#38750;&#32447;&#24615;ICA&#12289;&#35299;&#32544;&#20197;&#21450;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#20960;&#39033;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#25968;&#23383;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04056v2 Announce Type: replace-cross  Abstract: We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets. Further, we demonstr
&lt;/p&gt;</description></item><item><title>MixMIL&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#65288;GLMM&#65289;&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#21333;&#32454;&#32990;&#25968;&#25454;&#20013;&#30340;&#32454;&#32990;&#24322;&#36136;&#24615;&#65292;&#22312;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2311.02455</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed Models with Multiple Instance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02455
&lt;/p&gt;
&lt;p&gt;
MixMIL&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#65288;GLMM&#65289;&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#21333;&#32454;&#32990;&#25968;&#25454;&#20013;&#30340;&#32454;&#32990;&#24322;&#36136;&#24615;&#65292;&#22312;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#32454;&#32990;&#25968;&#25454;&#20013;&#39044;&#27979;&#24739;&#32773;&#29305;&#24449;&#26377;&#21161;&#20110;&#30830;&#23450;&#19982;&#20581;&#24247;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#32454;&#32990;&#29366;&#24577;&#12290;&#32447;&#24615;&#27169;&#22411;&#21644;&#24179;&#22343;&#32454;&#32990;&#31867;&#22411;&#34920;&#36798;&#36890;&#24120;&#34987;&#20559;&#29233;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#39640;&#25928;&#19988;&#31283;&#20581;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#21333;&#32454;&#32990;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32454;&#32990;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MixMIL&#65292;&#19968;&#20010;&#25972;&#21512;&#20102;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#65288;GLMM&#65289;&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#30340;&#26694;&#26550;&#65292;&#20445;&#30041;&#32447;&#24615;&#27169;&#22411;&#30340;&#20248;&#21183;&#21516;&#26102;&#24314;&#27169;&#32454;&#32990;&#29366;&#24577;&#30340;&#24322;&#36136;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#32454;&#32990;&#23884;&#20837;&#65292;MixMIL&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19982;&#26368;&#36817;&#22312;&#21333;&#32454;&#32990;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#36827;&#23637;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;MixMIL&#22312;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;MIL&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#26032;&#30340;&#20851;&#32852;&#24182;&#38416;&#26126;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#29983;&#29289;&#23398;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02455v2 Announce Type: replace  Abstract: Predicting patient features from single-cell data can help identify cellular states implicated in health and disease. Linear models and average cell type expressions are typically favored for this task for their efficiency and robustness, but they overlook the rich cell heterogeneity inherent in single-cell data. To address this gap, we introduce MixMIL, a framework integrating Generalized Linear Mixed Models (GLMM) and Multiple Instance Learning (MIL), upholding the advantages of linear models while modeling cell state heterogeneity. By leveraging predefined cell embeddings, MixMIL enhances computational efficiency and aligns with recent advancements in single-cell representation learning. Our empirical results reveal that MixMIL outperforms existing MIL models in single-cell datasets, uncovering new associations and elucidating biological mechanisms across different domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20462;&#21098;&#23548;&#33268;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#23376;&#32452;&#35774;&#32622;&#30028;&#38480;&#20197;&#38480;&#21046;&#31264;&#23494;&#21644;&#31232;&#30095;&#27169;&#22411;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2310.20673</link><description>&lt;p&gt;
&#24179;&#34913;&#20043;&#36947;&#65306;&#32422;&#26463;&#31232;&#30095;&#27169;&#22411;&#20013;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Balancing Act: Constraining Disparate Impact in Sparse Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20673
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20462;&#21098;&#23548;&#33268;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#23376;&#32452;&#35774;&#32622;&#30028;&#38480;&#20197;&#38480;&#21046;&#31264;&#23494;&#21644;&#31232;&#30095;&#27169;&#22411;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20462;&#21098;&#26159;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#25110;&#23384;&#20648;&#23481;&#37327;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#31232;&#30095;&#27169;&#22411;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32423;&#21035;&#19978;&#23454;&#29616;&#20102;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26576;&#20123;&#25968;&#25454;&#23376;&#32452;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24230;&#38477;&#33853;&#12290;&#29616;&#26377;&#30340;&#32531;&#35299;&#20462;&#21098;&#24341;&#36215;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#38388;&#25509;&#35299;&#20915;&#38382;&#39064;&#30340;&#26367;&#20195;&#25351;&#26631;&#24182;&#20855;&#26377;&#26377;&#38480;&#30340;&#35299;&#37322;&#24615;&#65292;&#35201;&#20040;&#38543;&#30528;&#21463;&#20445;&#25252;&#23376;&#32452;&#25968;&#37327;&#30340;&#22686;&#22810;&#32780;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20462;&#21098;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#25105;&#20204;&#30340;&#21046;&#23450;&#30028;&#23450;&#20102;&#22312;&#27599;&#20010;&#23376;&#32452;&#20013;&#31264;&#23494;&#21644;&#31232;&#30095;&#27169;&#22411;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#21464;&#21270;&#12290;&#36825;&#20123;&#32422;&#26463;&#30340;&#36873;&#25321;&#20026;&#30830;&#23450;&#20462;&#21098;&#27169;&#22411;&#26159;&#21542;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#19981;&#24179;&#31561;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25104;&#21151;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20673v2 Announce Type: replace  Abstract: Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#39640;&#25928;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2310.18882</link><description>&lt;p&gt;
&#21487;&#24494;&#23398;&#20064;&#24191;&#20041;&#32467;&#26500;&#21270;&#30697;&#38453;&#20197;&#23454;&#29616;&#39640;&#25928;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#39640;&#25928;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#36890;&#36807;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#21462;&#20195;&#23494;&#38598;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#21487;&#24494;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#26435;&#37325;&#30697;&#38453;&#30340;&#39640;&#25928;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#31867;&#26032;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#65292;&#36890;&#36807;&#35843;&#25972;&#32467;&#26500;&#21442;&#25968;&#35206;&#30422;&#20102;&#25991;&#29486;&#20013;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#22522;&#20110;&#39640;&#26031;-&#29380;&#21033;&#20811;&#38647;&#26680;&#30340;&#39057;&#22495;&#21487;&#24494;&#21442;&#25968;&#21270;&#26041;&#26696;&#26469;&#23398;&#20064;&#32467;&#26500;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18882v2 Announce Type: replace-cross  Abstract: This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#65292;&#21487;&#20197;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2310.10486</link><description>&lt;p&gt;
ManyQuadrupeds: &#23398;&#20064;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#65292;&#21487;&#20197;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36816;&#21160;&#31574;&#30053;&#36890;&#24120;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#26426;&#22120;&#20154;&#24418;&#24577;&#12289;&#36136;&#37327;&#21644;&#23610;&#23544;&#12290;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#24517;&#39035;&#38024;&#23545;&#27599;&#21488;&#26032;&#26426;&#22120;&#20154;&#37325;&#22797;&#36827;&#34892;&#65292;&#38656;&#35201;&#37325;&#26032;&#35843;&#25972;&#36229;&#21442;&#25968;&#21644;&#22870;&#21169;&#20989;&#25968;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#27599;&#20010;&#26032;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#23581;&#35797;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#20197;&#36866;&#24212;&#19981;&#21516;&#22823;&#23567;&#30340;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#33258;&#30001;&#24230;&#65288;DoF&#65289;&#21644;&#24418;&#24577;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25110;&#32773;&#36136;&#37327;&#12289;&#24815;&#24615;&#21644;&#23610;&#23544;&#38543;&#26426;&#21270;&#65292;&#36825;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#30340;&#24046;&#24322;&#21253;&#25324;&#65306;&#21487;&#21464;&#25968;&#37327;&#30340;DoF&#65288;&#21363;12&#25110;16&#20010;&#20851;&#33410;&#65289;&#12289;&#19977;&#31181;&#19981;&#21516;&#30340;&#24418;&#24577;&#21644;&#20174;&#36739;&#20302;&#21040;&#36739;&#39640;&#36136;&#37327;&#33539;&#22260;&#30340;&#24191;&#27867;&#36136;&#37327;&#36328;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10486v2 Announce Type: replace-cross  Abstract: Learning a locomotion policy for quadruped robots has traditionally been constrained to a specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. The robot differences encompass: a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item><item><title>TAIL&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#26377;&#25928;&#29575;&#12289;&#25345;&#32493;&#36866;&#24212;&#19981;&#21516;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2310.05905</link><description>&lt;p&gt;
TAIL: &#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05905
&lt;/p&gt;
&lt;p&gt;
TAIL&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#26377;&#25928;&#29575;&#12289;&#25345;&#32493;&#36866;&#24212;&#19981;&#21516;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25511;&#21046;&#39046;&#22495;&#65288;&#22914;&#26426;&#22120;&#20154;&#25216;&#26415;&#65289;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#20026;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#25110;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;TAIL&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#26032;&#25511;&#21046;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#35821;&#35328;&#39046;&#22495;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;TAIL&#20013;&#25506;&#35752;&#20102;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#20363;&#22914;&#29942;&#39048;&#36866;&#37197;&#22120;&#12289;P&#35843;&#25972;&#21644;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#65292;&#20197;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#20026;&#20855;&#26377;&#26377;&#38480;&#28436;&#31034;&#25968;&#25454;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05905v2 Announce Type: replace-cross  Abstract: The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#29305;&#24449;&#37327;&#21270;&#25552;&#20986;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#20063;&#36827;&#34892;&#37327;&#21270;&#65292;&#24471;&#21040;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.05436</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#30340;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#29305;&#24449;&#37327;&#21270;&#25552;&#20986;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#20063;&#36827;&#34892;&#37327;&#21270;&#65292;&#24471;&#21040;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#26426;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#22810;&#39033;&#24335;&#21644;&#20613;&#31435;&#21494;&#29305;&#24449;&#36890;&#24120;&#29992;&#20110;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#26356;&#39640;&#32500;&#31354;&#38388;&#26469;&#20026;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#38750;&#32447;&#24615;&#25193;&#23637;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#22810;&#39033;&#24335;&#21644;&#20613;&#31435;&#21494;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05436v2 Announce Type: replace  Abstract: In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21452;&#33457;&#21644;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65288;SDTLA&#21644;WVBM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25269;&#24481;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#65292;&#25552;&#39640;&#30408;&#21033;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2307.00529</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#38477;&#20302;&#33258;&#31169;&#25366;&#30719;&#21644;&#21452;&#33457;&#25915;&#20987;&#39118;&#38505;&#30340;&#26032;&#26234;&#33021;&#38450;&#24481;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
New intelligent defense systems to reduce the risks of Selfish Mining and Double-Spending attacks using Learning Automata
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21452;&#33457;&#21644;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#30340;&#20840;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65288;SDTLA&#21644;WVBM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25269;&#24481;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#65292;&#25552;&#39640;&#30408;&#21033;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25968;&#23383;&#36135;&#24065;&#20013;&#21452;&#33457;&#21644;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#21452;&#33457;&#26159;&#25351;&#22312;&#25968;&#23383;&#36135;&#24065;&#20132;&#26131;&#20013;&#21516;&#19968;&#36135;&#24065;&#34987;&#22810;&#27425;&#25903;&#20986;&#30340;&#38382;&#39064;&#65292;&#32780;&#33258;&#31169;&#25366;&#30719;&#21017;&#26159;&#26377;&#24847;&#22320;&#25913;&#21464;&#21306;&#22359;&#38142;&#20197;&#22686;&#21152;&#19968;&#20010;&#30719;&#24037;&#25110;&#19968;&#32452;&#30719;&#24037;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#25915;&#20987;&#30340;&#26032;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#23398;&#20064;&#33258;&#21160;&#26426;&#36825;&#19968;&#24378;&#22823;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#21363;SDTLA&#21644;WVBM&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#33258;&#31169;&#25366;&#30719;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SDTLA&#26041;&#27861;&#23558;&#33258;&#31169;&#25366;&#30719;&#30340;&#30408;&#21033;&#38408;&#20540;&#25552;&#39640;&#20102;47&#65285;&#65292;&#32780;WVBM&#26041;&#27861;&#30340;&#34920;&#29616;&#29978;&#33267;&#26356;&#22909;&#65292;&#38750;&#24120;&#25509;&#36817;&#29702;&#24819;&#24773;&#20917;&#65292;&#21363;&#27599;&#20010;&#30719;&#24037;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00529v2 Announce Type: replace-cross  Abstract: In this paper, we address the critical challenges of double-spending and selfish mining attacks in blockchain-based digital currencies. Double-spending is a problem where the same tender is spent multiple times during a digital currency transaction, while selfish mining is an intentional alteration of a blockchain to increase rewards to one miner or a group of miners. We introduce a new attack that combines both these attacks and propose a machine learning-based solution to mitigate the risks associated with them. Specifically, we use the learning automaton, a powerful online learning method, to develop two models, namely the SDTLA and WVBM, which can effectively defend against selfish mining attacks. Our experimental results show that the SDTLA method increases the profitability threshold of selfish mining up to 47$\%$, while the WVBM method performs even better and is very close to the ideal situation where each miner's reven
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;</title><link>https://arxiv.org/abs/2302.03357</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03357
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#27491;&#26679;&#26412;&#23545;&#23545;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#37117;&#26377;&#30410;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#21487;&#33021;&#24433;&#21709;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#36136;&#37327;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#65306;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#23384;&#22312;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#21482;&#23398;&#20064;&#22122;&#22768;&#30340;&#27169;&#24335;&#65288;&#22122;&#22768;&#23545;&#40784;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#20986;&#29616;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#26102;&#65292;&#27169;&#22411;&#20250;&#28010;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#23545;&#40784;&#38750;&#20195;&#34920;&#24615;&#27169;&#24335;&#65288;&#38169;&#35823;&#23545;&#40784;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DBPM&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#21160;&#24577;&#36319;&#36394;&#27599;&#20010;&#27491;&#26679;&#26412;&#23545;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#34892;&#20026;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#27599;&#20010;&#26102;&#26399;&#35782;&#21035;&#28508;&#22312;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03357v2 Announce Type: replace  Abstract: Not all positive pairs are beneficial to time series contrastive learning. In this paper, we study two types of bad positive pairs that can impair the quality of time series representation learned through contrastive learning: the noisy positive pair and the faulty positive pair. We observe that, with the presence of noisy positive pairs, the model tends to simply learn the pattern of noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the model wastes considerable amount of effort aligning non-representative patterns (Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair Mining (DBPM) algorithm, which reliably identifies and suppresses bad positive pairs in time series contrastive learning. Specifically, DBPM utilizes a memory module to dynamically track the training behavior of each positive pair along training process. This allows us to identify potential bad positive pairs at each epoch based
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20854;&#24212;&#29992;&#20110;&#22266;&#20307;&#21147;&#23398;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#38382;&#39064;&#65292;&#20419;&#36827;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2211.12971</link><description>&lt;p&gt;
&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cooperative data-driven modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20854;&#24212;&#29992;&#20110;&#22266;&#20307;&#21147;&#23398;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#38382;&#39064;&#65292;&#20419;&#36827;&#21512;&#20316;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12971v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;: &#22312;&#26426;&#26800;&#39046;&#22495;&#65292;&#22522;&#20110;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#27491;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#19968;&#39046;&#22495;&#19981;&#26029;&#25104;&#29087;&#65292;&#19981;&#21516;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#30340;&#26032;&#25968;&#25454;&#21644;&#27169;&#22411;&#38470;&#32493;&#38382;&#19990;&#65292;&#20026;&#21512;&#20316;&#24314;&#27169;&#24320;&#36767;&#20102;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24403;&#22312;&#26032;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#24536;&#35760;&#22914;&#20309;&#25191;&#34892;&#26087;&#20219;&#21153;&#12290;&#36825;&#31181;&#24773;&#20917;&#38459;&#30861;&#20102;&#21512;&#20316;&#65292;&#22240;&#20026;&#20351;&#29616;&#26377;&#27169;&#22411;&#36866;&#24212;&#26032;&#20219;&#21153;&#20250;&#24433;&#21709;&#20182;&#20154;&#35757;&#32451;&#30340;&#20808;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#27492;&#39318;&#27425;&#23558;&#20854;&#24212;&#29992;&#20110;&#22266;&#20307;&#21147;&#23398;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21382;&#21490;&#30456;&#20851;&#30340;&#22609;&#24615;&#34892;&#20026;&#65292;&#23613;&#31649;&#23427;&#20063;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#20854;&#20182;&#26550;&#26500;&#65288;&#21069;&#39304;&#12289;&#21367;&#31215;&#31561;&#65289;&#21644;&#39044;&#27979;&#20854;&#20182;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12971v2 Announce Type: replace-cross  Abstract: Data-driven modeling in mechanics is evolving rapidly based on recent machine learning advances, especially on artificial neural networks. As the field matures, new data and models created by different groups become available, opening possibilities for cooperative modeling. However, artificial neural networks suffer from catastrophic forgetting, i.e. they forget how to perform an old task when trained on a new one. This hinders cooperation because adapting an existing model for a new task affects the performance on a previous task trained by someone else. The authors developed a continual learning method that addresses this issue, applying it here for the first time to solid mechanics. In particular, the method is applied to recurrent neural networks to predict history-dependent plasticity behavior, although it can be used on any other architecture (feedforward, convolutional, etc.) and to predict other phenomena. This work int
&lt;/p&gt;</description></item><item><title>SRLDA&#26041;&#27861;&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;RLDA&#21644;ILDA&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.03859</link><description>&lt;p&gt;
&#20855;&#26377;&#27874;&#32441;&#21327;&#26041;&#24046;&#27169;&#22411;&#30340;&#35889;&#26657;&#27491;&#21644;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03859
&lt;/p&gt;
&lt;p&gt;
SRLDA&#26041;&#27861;&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;RLDA&#21644;ILDA&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#31216;&#20026;&#35889;&#26657;&#27491;&#21644;&#27491;&#21017;&#21270;LDA&#65288;SRLDA&#65289;&#12290;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#26679;&#26412;&#35889;&#26657;&#27491;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#30340;&#35774;&#35745;&#24605;&#24819;&#12290;&#22312;&#22823;&#32500;&#38543;&#26426;&#30697;&#38453;&#20998;&#26512;&#26694;&#26550;&#30340;&#25903;&#25345;&#19979;&#65292;&#35777;&#26126;&#20102;SRLDA&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#20223;&#30495;&#25968;&#25454;&#20998;&#26512;&#65292;&#35777;&#26126;SRLDA&#20998;&#31867;&#22120;&#20248;&#20110;RLDA&#21644;ILDA&#65292;&#24182;&#25509;&#36817;&#29702;&#35770;&#20998;&#31867;&#22120;&#12290;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SRLDA&#31639;&#27861;&#22312;&#20998;&#31867;&#21644;&#38477;&#32500;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.03859v3 Announce Type: replace-cross  Abstract: This paper proposes an improved linear discriminant analysis called spectrally-corrected and regularized LDA (SRLDA). This method integrates the design ideas of the sample spectrally-corrected covariance matrix and the regularized discriminant analysis. With the support of a large-dimensional random matrix analysis framework, it is proved that SRLDA has a linear classification global optimal solution under the spiked model assumption. According to simulation data analysis, the SRLDA classifier performs better than RLDA and ILDA and is closer to the theoretical classifier. Experiments on different data sets show that the SRLDA algorithm performs better in classification and dimensionality reduction than currently used tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#20989;&#25968;&#22238;&#24402;&#65292;&#22312;&#20272;&#35745;&#35823;&#24046;&#21644;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2205.14545</link><description>&lt;p&gt;
&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Functional Linear Regression of Cumulative Distribution Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#20989;&#25968;&#22238;&#24402;&#65292;&#22312;&#20272;&#35745;&#35823;&#24046;&#21644;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#30740;&#31350;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;CDF&#30340;&#20989;&#25968;&#22238;&#24402;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#26159;&#20174;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;CDF&#22522;&#30784;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20989;&#25968;&#23725;&#22238;&#24402;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#22312;&#21508;&#20010;&#22320;&#26041;&#20934;&#30830;&#20272;&#35745;CDF&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20855;&#26377;$d$&#20010;&#22522;&#30784;&#20989;&#25968;&#30340;$n$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#35774;&#35745;&#12289;&#38543;&#26426;&#35774;&#35745;&#21644;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#35823;&#24046;&#19978;&#30028;&#20026;$\widetilde O(\sqrt{d/n})$&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#21305;&#37197;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65292;&#24314;&#31435;&#20102;CDF&#20989;&#25968;&#22238;&#24402;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#26367;&#20195;&#24809;&#32602;&#20272;&#35745;&#22120;&#28040;&#38500;&#20102;&#38543;&#26426;&#35774;&#35745;&#35774;&#32622;&#20013;&#30340;&#29123;&#28903;&#26102;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#39064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26377;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.14545v3 Announce Type: replace  Abstract: The estimation of cumulative distribution functions (CDF) is an important learning task with a great variety of downstream applications, such as risk assessments in predictions and decision making. In this paper, we study functional regression of contextual CDFs where each data point is sampled from a linear combination of context dependent CDF basis functions. We propose functional ridge-regression-based estimation methods that estimate CDFs accurately everywhere. In particular, given $n$ samples with $d$ basis functions, we show estimation error upper bounds of $\widetilde O(\sqrt{d/n})$ for fixed design, random design, and adversarial context cases. We also derive matching information theoretic lower bounds, establishing minimax optimality for CDF functional regression. Furthermore, we remove the burn-in time in the random design setting using an alternative penalized estimator. Then, we consider agnostic settings where there is a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2204.05275</link><description>&lt;p&gt;
&#35299;&#20915;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26377;&#25928;&#30340;&#31163;&#32447;RL&#24212;&#33021;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#25110;&#20998;&#26512;&#35201;&#20040;&#21463;&#21040;&#27425;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#20135;&#29983;&#39640;&#26114;&#30340;&#28903;&#24405;&#25104;&#26412;&#20197;&#36798;&#21040;&#26679;&#26412;&#26368;&#20248;&#24615;&#65292;&#20174;&#32780;&#23545;&#26679;&#26412;&#21294;&#20047;&#24212;&#29992;&#20013;&#30340;&#39640;&#25928;&#31163;&#32447;RL&#26500;&#25104;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2203.01707</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27979;&#35797;&#24179;&#31283;&#24615;&#21644;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing Stationarity and Change Point Detection in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.01707
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#35768;&#22810;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;RL&#31639;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#31995;&#32479;&#36716;&#25442;&#21644;&#22870;&#21169;&#20989;&#25968;&#38543;&#26102;&#38388;&#20445;&#25345;&#24658;&#23450;&#30340;&#24179;&#31283;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#24179;&#31283;&#24615;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#21487;&#33021;&#34987;&#36829;&#21453;&#65292;&#21253;&#25324;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#31227;&#21160;&#20581;&#24247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#33268;&#30340;&#31243;&#24207;&#65292;&#22522;&#20110;&#39044;&#20808;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#39034;&#24207;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;RL&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#12289;&#20223;&#30495;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#30340;&#26696;&#20363;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.01707v3 Announce Type: replace-cross  Abstract: We consider offline reinforcement learning (RL) methods in possibly nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal Q-function based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimization in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, an
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;</title><link>https://arxiv.org/abs/2110.05365</link><description>&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#38543;&#26426;&#24179;&#28369;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Input-dependent Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.05365
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#33719;&#24471;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#26174;&#33879;&#65292;&#20294;&#35813;&#26041;&#27861;&#23384;&#22312;&#35832;&#22914;&#8220;&#35748;&#35777;&#20934;&#30830;&#24615;&#28689;&#24067;&#8221;&#12289;&#35748;&#35777;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29978;&#33267;&#20844;&#24179;&#24615;&#38382;&#39064;&#31561;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#38519;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36755;&#20837;&#30456;&#20851;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#26041;&#24046;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;&#21322;&#24377;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#24179;&#28369;&#26041;&#24046;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.05365v3 Announce Type: replace-cross  Abstract: Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as "certified accuracy waterfalls", certification vs.\ accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;$O(HS\sqrt{AT})$&#12290;</title><link>https://arxiv.org/abs/2109.03396</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#31639;&#27861;&#21450;&#20854;&#20219;&#24847;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;$O(HS\sqrt{AT})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#65288;PSRL-ZSG&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#20013;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#20026;$O(HS\sqrt{AT})$&#12290;&#20854;&#20013;$H$&#26159;&#20559;&#24046;&#20989;&#25968;&#36328;&#24230;&#30340;&#19978;&#30028;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65292;$A$&#26159;&#32852;&#21512;&#21160;&#20316;&#25968;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#23545;&#25163;&#26080;&#27861;&#25511;&#21046;&#19988;&#21487;&#20197;&#37319;&#21462;&#20219;&#24847;&#20219;&#24847;&#19982;&#21382;&#21490;&#30456;&#20851;&#30340;&#31574;&#30053;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#25913;&#36827;&#20102;&#39759;&#31561;&#20154;(2017)&#22312;&#30456;&#21516;&#20551;&#35774;&#19979;&#30340;&#26368;&#20339;&#36951;&#25022;&#30028;$O(\sqrt[3]{DS^2AT^2})$&#65292;&#24182;&#19988;&#19982;&#22312;$T$&#19978;&#30340;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.03396v2 Announce Type: replace  Abstract: In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. Our regret bound improves on the best existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the same assumption and matches the theoretical lower bound in $T$.
&lt;/p&gt;</description></item><item><title>&#35823;&#24046;&#32531;&#35299;&#30340;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#25351;&#25968;&#32423;&#25104;&#26412;&#38598;&#20013;&#38382;&#39064;&#32780;&#19981;&#22312;&#20854;&#20182;&#22320;&#26041;&#25237;&#20837;&#25351;&#25968;&#32423;&#36164;&#28304;&#65292;&#26576;&#20123;&#31574;&#30053;&#33021;&#22815;&#25913;&#21892;&#26377;&#22122;&#22768;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21487;&#35757;&#24615;&#12290;</title><link>https://arxiv.org/abs/2109.01051</link><description>&lt;p&gt;
&#35823;&#24046;&#32531;&#35299;&#33021;&#25913;&#21892;&#26377;&#22122;&#22768;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21487;&#35757;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Error Mitigation Improve Trainability of Noisy Variational Quantum Algorithms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.01051
&lt;/p&gt;
&lt;p&gt;
&#35823;&#24046;&#32531;&#35299;&#30340;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#25351;&#25968;&#32423;&#25104;&#26412;&#38598;&#20013;&#38382;&#39064;&#32780;&#19981;&#22312;&#20854;&#20182;&#22320;&#26041;&#25237;&#20837;&#25351;&#25968;&#32423;&#36164;&#28304;&#65292;&#26576;&#20123;&#31574;&#30053;&#33021;&#22815;&#25913;&#21892;&#26377;&#22122;&#22768;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21487;&#35757;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#36890;&#24120;&#34987;&#35270;&#20026;&#36817;&#26399;&#33719;&#24471;&#37327;&#23376;&#20248;&#21183;&#30340;&#26368;&#20339;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22122;&#22768;&#21487;&#20197;&#20005;&#37325;&#38480;&#21046;VQAs&#30340;&#21487;&#35757;&#24615;&#65292;&#20363;&#22914;&#36890;&#36807;&#25351;&#25968;&#32423;&#20351;&#25104;&#26412;&#26223;&#35266;&#24179;&#22374;&#21270;&#24182;&#25233;&#21046;&#25104;&#26412;&#26799;&#24230;&#30340;&#24133;&#24230;&#12290;&#35823;&#24046;&#32531;&#35299;&#65288;EM&#65289;&#22312;&#38477;&#20302;&#22122;&#22768;&#23545;&#36817;&#26399;&#35774;&#22791;&#30340;&#24433;&#21709;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#22320;&#20250;&#38382;&#35823;&#24046;&#32531;&#35299;&#26159;&#21542;&#33021;&#25552;&#39640;VQAs&#30340;&#21487;&#35757;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;EM&#31574;&#30053;&#65292;&#26080;&#27861;&#35299;&#20915;&#25351;&#25968;&#32423;&#25104;&#26412;&#38598;&#20013;&#38382;&#39064;&#32780;&#19981;&#22312;&#20854;&#20182;&#22320;&#26041;&#25237;&#20837;&#25351;&#25968;&#32423;&#36164;&#28304;&#12290;&#36825;&#31867;&#31574;&#30053;&#21253;&#25324;&#38646;&#22122;&#22768;&#22806;&#25512;&#12289;&#34394;&#25311;&#33976;&#39311;&#12289;&#27010;&#29575;&#35823;&#24046;&#25269;&#28040;&#21644;&#20811;&#21033;&#31119;&#24503;&#25968;&#25454;&#22238;&#24402;&#31561;&#29305;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;EM&#21327;&#35758;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#25968;&#20540;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20123;&#65288;&#20363;&#22914;&#65292;&#34394;&#25311;&#33976;&#39311;&#65289;&#21487;&#20197;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.01051v2 Announce Type: replace-cross  Abstract: Variational Quantum Algorithms (VQAs) are often viewed as the best hope for near-term quantum advantage. However, recent studies have shown that noise can severely limit the trainability of VQAs, e.g., by exponentially flattening the cost landscape and suppressing the magnitudes of cost gradients. Error Mitigation (EM) shows promise in reducing the impact of noise on near-term devices. Thus, it is natural to ask whether EM can improve the trainability of VQAs. In this work, we first show that, for a broad class of EM strategies, exponential cost concentration cannot be resolved without committing exponential resources elsewhere. This class of strategies includes as special cases Zero Noise Extrapolation, Virtual Distillation, Probabilistic Error Cancellation, and Clifford Data Regression. Second, we perform analytical and numerical analysis of these EM protocols, and we find that some of them (e.g., Virtual Distillation) can ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20182;&#20204;&#22312;&#25991;&#29486;&#20013;&#21807;&#19968;&#21457;&#29616;&#30340;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2102.02649</link><description>&lt;p&gt;
&#26397;&#30528;&#24378;&#21270;&#23398;&#20064;de novo&#22522;&#22240;&#32452;&#32452;&#35013;&#22120;&#36808;&#20986;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
A step toward a reinforcement learning de novo genome assembler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20182;&#20204;&#22312;&#25991;&#29486;&#20013;&#21807;&#19968;&#21457;&#29616;&#30340;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
De novo&#22522;&#22240;&#32452;&#32452;&#35013;&#26159;&#22522;&#22240;&#32452;&#23398;&#20013;&#19968;&#20010;&#30456;&#20851;&#20294;&#35745;&#31639;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;de novo&#32452;&#35013;&#22120;&#24050;&#32463;&#25104;&#21151;&#22320;&#22312;&#20960;&#20010;&#22522;&#22240;&#32452;&#39033;&#30446;&#20013;&#20351;&#29992;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;"&#26368;&#20339;&#32452;&#35013;&#22120;"&#65292;&#32452;&#35013;&#22120;&#30340;&#36873;&#25321;&#21644;&#35774;&#32622;&#20173;&#28982;&#20381;&#36182;&#29983;&#29289;&#20449;&#24687;&#23398;&#19987;&#23478;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#35745;&#31639;&#22797;&#26434;&#38382;&#39064;&#19968;&#26679;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#25104;&#20026;&#24320;&#21457;&#26356;&#20934;&#30830;&#21644;&#33258;&#21160;&#21270;&#32452;&#35013;&#22120;&#30340;&#19968;&#31181;&#26367;&#20195;&#65288;&#25110;&#34917;&#20805;&#65289;&#26041;&#24335;&#12290;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#22797;&#26434;&#27963;&#21160;&#65288;&#22914;&#28216;&#25103;&#65289;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#35813;&#26041;&#27861;&#22312;&#8220;&#30495;&#23454;&#8221;&#38382;&#39064;&#65288;&#22914;DFA&#38382;&#39064;&#65289;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#21807;&#19968;&#20808;&#21069;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.02649v4 Announce Type: replace-cross  Abstract: De novo genome assembly is a relevant but computationally complex task in genomics. Although de novo assemblers have been used successfully in several genomics projects, there is still no 'best assembler', and the choice and setup of assemblers still rely on bioinformatics experts. Thus, as with other computationally complex problems, machine learning may emerge as an alternative (or complementary) way for developing more accurate and automated assemblers. Reinforcement learning has proven promising for solving complex activities without supervision - such games - and there is a pressing need to understand the limits of this approach to 'real' problems, such as the DFA problem. This study aimed to shed light on the application of machine learning, using reinforcement learning (RL), in genome assembly. We expanded upon the sole previous approach found in the literature to solve this problem by carefully exploring the learning as
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#32858;&#31867;&#20013;&#34701;&#21512;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#24403;&#21442;&#25968;$\lambda&gt;1$&#26102;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#65292;&#24403;$\lambda\to 0$&#26102;&#36866;&#29992;&#20110;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/1711.04366</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A unified framework for hard and soft clustering with regularized optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1711.04366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#32858;&#31867;&#20013;&#34701;&#21512;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#24403;&#21442;&#25968;$\lambda&gt;1$&#26102;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#65292;&#24403;$\lambda\to 0$&#26102;&#36866;&#29992;&#20110;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20174;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#38416;&#36848;&#20026;&#19968;&#20010;&#24102;&#26377;&#21442;&#25968;$\lambda\geq 0$&#30340;&#29109;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24403;$\lambda=1$&#26102;&#65292;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#34987;&#23436;&#20840;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#31867;&#31639;&#27861;&#26063;&#20381;&#36182;&#20110;&#20351;&#29992;&#20132;&#26367;&#26368;&#23567;&#21270;&#26469;&#35299;&#20915;&#38750;&#20984;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#24191;&#20041;$\lambda$-EM&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25512;&#26029;&#25351;&#25968;&#26063;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#26368;&#23567;&#21270;&#36807;&#31243;&#20013;&#30340;&#27599;&#19968;&#27493;&#37117;&#26377;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#23454;&#39564;&#31361;&#20986;&#20102;&#37319;&#29992;&#21442;&#25968;$\lambda&gt;1$&#26469;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#20197;&#21450;$\lambda\to 0$&#29992;&#20110;&#20998;&#31867;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1711.04366v2 Announce Type: replace  Abstract: In this paper, we formulate the problem of inferring a Finite Mixture Model from discrete data as an optimal transport problem with entropic regularization of parameter $\lambda\geq 0$. Our method unifies hard and soft clustering, the Expectation-Maximization (EM) algorithm being exactly recovered for $\lambda=1$. The family of clustering algorithm we propose rely on the resolution of nonconvex problems using alternating minimization. We study the convergence property of our generalized $\lambda-$EM algorithms and show that each step in the minimization process has a closed form solution when inferring finite mixture models of exponential families. Experiments highlight the benefits of taking a parameter $\lambda&gt;1$ to improve the inference performance and $\lambda\to 0$ for classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.12924</link><description>&lt;p&gt;
&#23545;&#20110;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#21644;&#21033;&#29992;&#24773;&#20917;&#12290;&#38543;&#30528;&#26862;&#26519;&#28779;&#28798;&#23545;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#23450;&#23621;&#28857;&#30340;&#23041;&#32961;&#26085;&#30410;&#22686;&#21152;&#65292;&#36805;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;SVM&#20197;&#20854;&#24378;&#22823;&#30340;&#20998;&#31867;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#36890;&#36807;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;SVM&#33719;&#24471;&#20102;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#29420;&#29305;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#22914;&#28779;&#28976;&#12289;&#28895;&#38654;&#25110;&#26862;&#26519;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20351;&#29992;SVM&#30340;&#21508;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#20005;&#26684;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#31561;&#21442;&#25968;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#24314;&#27169;&#21644;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10632</link><description>&lt;p&gt;
&#37096;&#20998;&#24050;&#30693;&#22240;&#26524;&#22270;&#19978;&#30340;&#24178;&#39044;&#20844;&#24179;&#24615;: &#19968;&#31181;&#21463;&#38480;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach. (arXiv:2401.10632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#24314;&#27169;&#21644;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#38450;&#27490;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65289;&#23545;&#20010;&#20307;&#25110;&#23376;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#29992;&#20110;&#36890;&#36807;&#22240;&#26524;&#25928;&#24212;&#26469;&#34913;&#37327;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20551;&#35774;&#24050;&#30693;&#30495;&#23454;&#30340;&#22240;&#26524;&#22270;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#24050;&#30693;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#24773;&#20917;&#19979;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;PDAG&#65289;&#26469;&#24314;&#27169;&#20844;&#24179;&#39044;&#27979;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#20013;&#23398;&#20064;&#30340;&#19968;&#31867;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;PDAG&#29992;&#20110;&#34913;&#37327;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26469;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge. The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08977</link><description>&lt;p&gt;
FedLoGe: &#38271;&#23614;&#25968;&#25454;&#19979;&#30340;&#26412;&#22320;&#21644;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#26159;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#30340;&#33539;&#20363;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;Fed-LT&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#33021;&#65292;&#32780;&#24573;&#35270;&#20102;&#26412;&#22320;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24120;&#35268;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#20248;&#21270;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#22312;Fed-LT&#20013;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#65292;&#25552;&#39640;&#26412;&#22320;&#21644;&#36890;&#29992;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20351;&#29992;&#20849;&#20139;&#39592;&#24178;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framewor
&lt;/p&gt;</description></item><item><title>REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08850</link><description>&lt;p&gt;
REValueD: &#23545;&#21487;&#20998;&#35299;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08850
&lt;/p&gt;
&lt;p&gt;
REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#33021;&#30340;&#21160;&#20316;&#25968;&#37327;&#24222;&#22823;&#65292;&#31163;&#25955;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#8212;&#8212;&#20540;&#20998;&#35299;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20540;&#20998;&#35299;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#34429;&#28982;&#21487;&#20197;&#36943;&#21046;Q&#23398;&#20064;&#31639;&#27861;&#22266;&#26377;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#20294;&#20063;&#20250;&#25918;&#22823;&#30446;&#26631;&#26041;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;&#30340;&#38598;&#21512;&#20197;&#20943;&#36731;&#30446;&#26631;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19968;&#20010;&#32500;&#24230;&#19978;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#23545;&#20854;&#20182;&#32500;&#24230;&#19978;&#26368;&#20248;&#21160;&#20316;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31639;&#27861;REValueD&#65292;&#22312;&#32463;&#36807;&#31163;&#25955;&#21270;&#30340;DeepMind&#25511;&#21046;&#22871;&#20214;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24433;&#21709;REValueD&#34920;&#29616;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#25945;&#24072;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#32858;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08732</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#29992;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#25945;&#24072;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#32858;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#36890;&#24120;&#35748;&#20026;&#25945;&#24072;&#30340;&#35282;&#33394;&#26159;&#25552;&#20379;&#29992;&#20110;&#23398;&#29983;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26410;&#30693;&#36125;&#21494;&#26031;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#65288;BCPD&#65289;&#30340;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#65288;MLL&#65289;&#26041;&#27861;&#35757;&#32451;&#25945;&#24072;&#26469;&#33719;&#24471;&#27492;&#20272;&#35745;&#12290;&#20026;&#20102;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#65292;&#26412;&#25991;&#23558;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;BCPD&#30340;&#20272;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#26368;&#22823;CMI&#65288;MCMI&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;MCMI&#20272;&#35745;&#20013;&#65292;&#25945;&#24072;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;&#36890;&#36807;Eigen-CAM&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;CMI&#20540;&#21487;&#20197;&#20351;&#25945;&#24072;&#22312;&#22270;&#20687;&#32858;&#31867;&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;MCMI&#20272;&#35745;&#35757;&#32451;&#30340;&#25945;&#24072;&#32780;&#19981;&#26159;&#36890;&#36807;MLL&#20272;&#35745;&#35757;&#32451;&#30340;&#25945;&#24072;&#65292;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04305</link><description>&lt;p&gt;
&#25512;&#36827;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#65306;&#29992;&#20449;&#24687;&#35770;&#30452;&#35273;&#32479;&#19968;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Advancing Deep Active Learning &amp; Data Subset Selection: Unifying Principles with Information-Theory Intuitions. (arXiv:2401.04305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#36890;&#36807;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#12290;&#20027;&#21160;&#23398;&#20064;&#25552;&#39640;&#20102;&#26631;&#31614;&#25928;&#29575;&#65292;&#32780;&#20027;&#21160;&#37319;&#26679;&#21017;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26631;&#31614;&#33719;&#21462;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#35757;&#32451;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#8220;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#8221;&#20197;&#22806;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#32570;&#20047;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#21407;&#21017;&#22522;&#30784;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#35770;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30446;&#26631;&#21450;&#20854;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21147;&#27714;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17110</link><description>&lt;p&gt;
LLM4DyG&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#35299;&#20915;&#21160;&#24577;&#22270;&#19978;&#30340;&#38382;&#39064;&#21527;?
&lt;/p&gt;
&lt;p&gt;
LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#26102;&#20195;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25506;&#32034;LLMs&#22312;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21160;&#24577;&#22270;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#23427;&#20204;&#25429;&#25417;&#20102;&#32593;&#32476;&#28436;&#21270;&#27169;&#24335;&#12290;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;Web&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#19978;&#35780;&#20272;LLMs&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4DyG&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20061;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#32771;&#34385;&#20102;LLMs&#22312;&#26102;&#24577;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#25968;&#25454;&#32479;&#35745;&#12289;&#25552;&#31034;&#25216;&#26415;&#21644;LLMs&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.06045</link><description>&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#24615;&#23545;&#27969;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#20005;&#37325;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#32654;&#22269;&#26412;&#22303;&#20005;&#37325;&#22825;&#27668;&#65288;&#40857;&#21367;&#39118;&#12289;&#20912;&#38649;&#21644;&#22823;&#39118;&#38453;&#65289;&#30340;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21518;&#22788;&#29702;&#23545;&#27969;&#20801;&#35768;&#27169;&#22411;&#65288;CAM&#65289;&#30340;&#39044;&#27979;&#12290;CGANs&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#30830;&#23450;&#24615;CAM&#39044;&#27979;&#20013;&#21019;&#24314;&#21512;&#25104;&#38598;&#25104;&#25104;&#21592;&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;CNN&#22788;&#29702;&#20197;&#20272;&#35745;&#20005;&#37325;&#22825;&#27668;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24555;&#36895;&#21047;&#26032;&#65288;HRRR&#65289;1-24&#23567;&#26102;&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#21450;&#26292;&#39118;&#39044;&#35686;&#20013;&#24515;&#65288;SPC&#65289;&#30340;&#20005;&#37325;&#22825;&#27668;&#25253;&#21578;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;2021&#24180;&#30340;HRRR&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#32771;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05446</link><description>&lt;p&gt;
RetSeg: &#22522;&#20110;&#20445;&#30041;&#26426;&#21046;&#30340;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30456;&#27604;&#65292;&#22312;&#24687;&#32905;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#32858;&#28966;&#20110;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#65292;ViTs&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#23398;&#22270;&#20687;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#19988;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#21464;&#25442;&#22120;&#20013;&#22266;&#26377;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#36866;&#24212;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23610;&#23544;&#21644;&#20998;&#36776;&#29575;&#65292;&#20026;&#20256;&#32479;&#30340;CNNs&#25152;&#19981;&#20855;&#22791;&#30340;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21464;&#25442;&#22120;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#32780;&#38754;&#20020;&#30528;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#31350;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07138</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#30450;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20174;&#28151;&#21512;&#20449;&#21495;&#20013;&#20998;&#31163;&#20986;&#28304;&#20449;&#21495;&#21644;&#28151;&#21512;&#31995;&#32479;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#28151;&#21512;&#31995;&#32479;&#21644;&#28304;&#20449;&#21495;&#20570;&#20986;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#28151;&#21512;&#30340;BSS&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#28982;&#29305;&#24449;&#23376;&#31354;&#38388;&#19987;&#38376;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23436;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#24378;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36755;&#20837;&#35299;&#30721;&#25104;&#22810;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#29420;&#32534;&#30721;&#31354;&#38388;&#65292;&#28982;&#21518;&#22312;&#35299;&#30721;&#22120;&#20869;&#37325;&#26032;&#28151;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#37325;&#26500;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#65292;&#21363;&#23631;&#34109;&#38500;&#19968;&#20010;&#32534;&#30721;&#22806;&#30340;&#25152;&#26377;&#32534;&#30721;&#65292;&#20351;&#24471;&#35299;&#30721;&#22120;&#33021;&#22815;&#20272;&#35745;&#28304;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32534;&#30721;&#20043;&#38388;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of blind source separation (BSS) involves separating sources from a mixture without prior knowledge of the sources or the mixing system. This is a challenging problem that often requires making restrictive assumptions about both the mixing system and the sources. In this paper, we propose a novel method for addressing BSS of non-linear mixtures by leveraging the natural feature subspace specialization ability of multi-encoder autoencoders with fully self-supervised learning without strong priors. During the training phase, our method unmixes the input into the separate encoding spaces of the multi-encoder network and then remixes these representations within the decoder for a reconstruction of the input. Then to perform source inference, we introduce a novel encoding masking technique whereby masking out all but one of the encodings enables the decoder to estimate a source signal. To this end, we also introduce a so-called pathway separation loss that encourages sparsity betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.16208</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;q-learning&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;q-learning&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;&#19982;Jia&#21644;Zhou&#65288;2022c&#65289;&#30340;&#21333;&#20010;&#20195;&#29702;&#25511;&#21046;&#38382;&#39064;&#19981;&#21516;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20351;&#24471;q&#20989;&#25968;&#30340;&#23450;&#20041;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#28982;&#20135;&#29983;&#20004;&#31181;&#19981;&#21516;q&#20989;&#25968;&#30340;&#24773;&#20917;&#65306;&#65288;i&#65289;&#34987;&#31216;&#20026;&#38598;&#25104;q&#20989;&#25968;&#65288;&#29992;$q$&#34920;&#31034;&#65289;&#65292;&#20316;&#20026;Gu&#12289;Guo&#12289;Wei&#21644;Xu&#65288;2023&#65289;&#24341;&#20837;&#30340;&#38598;&#25104;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#21487;&#20197;&#36890;&#36807;&#28041;&#21450;&#27979;&#35797;&#31574;&#30053;&#30340;&#24369;&#38789;&#26465;&#20214;&#36827;&#34892;&#23398;&#20064;&#65307;&#65288;ii&#65289;&#20316;&#20026;&#31574;&#30053;&#25913;&#36827;&#36845;&#20195;&#20013;&#25152;&#20351;&#29992;&#30340;&#23454;&#36136;q&#20989;&#25968;&#65288;&#29992;$q_e$&#34920;&#31034;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;q&#20989;&#25968;&#22312;&#25152;&#26377;&#27979;&#35797;&#31574;&#30053;&#19979;&#36890;&#36807;&#31215;&#20998;&#34920;&#31034;&#30456;&#20851;&#32852;&#12290;&#22522;&#20110;&#38598;&#25104;q&#20989;&#25968;&#30340;&#24369;&#38789;&#26465;&#20214;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#23398;&#20064;&#20004;&#20010;q&#20989;&#25968;&#20197;&#35299;&#20915;Mckean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the q-learning, recently coined as the continuous-time counterpart of Q-learning by Jia and Zhou (2022c), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2022c), the mean-field interaction of agents render the definition of q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023) that can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition of the integrated q-function and our proposed searching method of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.13773</link><description>&lt;p&gt;
&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#36817;&#37051;&#31639;&#27861;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22788;&#29702;&#20102;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#21363;&#19981;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#24403;&#19982;&#24555;&#36895;&#25968;&#25454;&#32467;&#26500;&#65288;&#21487;&#33021;&#26159;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#22914;&#23548;&#33322;&#32593;&#32476;&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#39640;&#25928;-&#27599;&#27425;&#35797;&#39564;&#30340;&#36816;&#34892;&#26102;&#38388;&#23545;&#21160;&#20316;&#25968;&#21644;&#35797;&#39564;&#25968;&#21576;&#23545;&#25968;&#22810;&#39033;&#24335;&#22686;&#38271;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#20934;&#32447;&#24615;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.10715</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy Heterogeneous-Agent Mirror Learning. (arXiv:2306.10715v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10715
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#22312;&#21512;&#20316;&#21338;&#24328;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#36229;&#21442;&#25968;&#33030;&#24369;&#24615;&#21644;&#25910;&#25947;&#20110;&#27425;&#20248;&#32435;&#20160;&#22343;&#34913;&#30340;&#39118;&#38505;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#65292;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;MEHAML&#26694;&#26550;&#23548;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#12290;MEHAML&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#24320;&#21457;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#30340;MEHAML&#25193;&#23637;&#26469;&#23637;&#31034;&#65292;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#20102;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample inefficiency, brittleness regarding hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium. To resolve these issues, in this paper, we propose a novel theoretical framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), that leverages the maximum entropy principle to design maximum entropy MARL actor-critic algorithms. We prove that algorithms derived from the MEHAML framework enjoy the desired properties of the monotonic improvement of the joint maximum entropy objective and the convergence to quantal response equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a MEHAML extension of the widely used RL algorithm, HASAC (for soft actor-critic), which shows significant improvements in exploration and robustness on three challenging benchmark
&lt;/p&gt;</description></item><item><title>ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04527</link><description>&lt;p&gt;
ContriMix&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#26080;&#30417;&#30563;&#20869;&#23481;&#23646;&#24615;&#20998;&#31163;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04527
&lt;/p&gt;
&lt;p&gt;
ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#31561;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ContriMix&#65292;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#20998;&#31163;&#20986;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#29983;&#29289;&#23398;&#20869;&#23481;&#21644;&#25216;&#26415;&#21464;&#24322;&#65292;&#24182;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25163;&#24037; fine-tuning &#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102; ContriMix &#30340;&#26377;&#25928;&#24615;&#65292;&#21462;&#24471;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#20266;&#26631;&#27880;&#26041;&#27861;&#36890;&#36807;&#25552;&#31034;&#35843;&#21442;&#25913;&#36827;CLIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#20266;&#26631;&#31614;&#26469;&#20248;&#21270;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01669</link><description>&lt;p&gt;
&#29992;CLIP&#22686;&#24378;CLIP: &#25506;&#32034;&#26377;&#38480;&#26631;&#27880;&#25552;&#31034;&#35843;&#21442;&#30340;&#20266;&#26631;&#27880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning. (arXiv:2306.01669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#20266;&#26631;&#27880;&#26041;&#27861;&#36890;&#36807;&#25552;&#31034;&#35843;&#21442;&#25913;&#36827;CLIP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#20266;&#26631;&#31614;&#26469;&#20248;&#21270;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#22914;CLIP&#65292;&#20197;&#20248;&#21270;&#20854;&#24615;&#33021;&#24448;&#24448;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#38556;&#30861;&#26159;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#20266;&#26631;&#27880;&#65288;&#21363;&#38750;&#26631;&#27880;&#25968;&#25454;&#30340;&#21551;&#21457;&#24335;&#26631;&#31614;&#65289;&#36890;&#36807;&#25552;&#31034;&#35843;&#21442;&#25913;&#36827;CLIP&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#20266;&#26631;&#27880;&#26041;&#27861;&#20250;&#22312;&#26377;&#26631;&#27880;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20026;&#26080;&#26631;&#27880;&#25968;&#25454;&#29983;&#25104;&#26631;&#31614;&#12290;VLM&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#20351;&#8220;&#31532;&#20108;&#20195;&#8221;&#20266;&#26631;&#27880;&#26041;&#27861;&#19981;&#38656;&#35201;&#22312;&#26377;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#20266;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#26469;&#28304;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#23558;&#21322;&#30417;&#30563;&#12289;&#36807;&#28193;&#38646;&#26679;&#26412;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#23398;&#20064;&#33539;&#24335;&#35270;&#20026;&#20248;&#21270;&#30456;&#21516;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#33021;&#22815;&#23454;&#29616;&#36866;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#33539;&#24335;&#30340;&#22810;&#21151;&#33021;&#22521;&#35757;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#30340;&#26041;&#24335;&#26469;&#25506;&#32034;&#36825;&#20123;&#22521;&#35757;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915; CLIP &#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a ``second generation'' of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18231</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#22797;&#21046;&#36825;&#20010;&#25104;&#21151;&#21364;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#32473;&#23450;&#27604;&#29305;&#29575;&#19979;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#36890;&#36807; FID &#20998;&#25968;&#35780;&#20272;&#65292;&#34920;&#29616;&#36229;&#36234;&#20102; PO-ELIC &#21644; HiFiC &#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#20294;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20197; MSE &#20026;&#30446;&#26631;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#36827;&#19968;&#27493;&#22522;&#20110;&#20998;&#25968;&#30340;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#23454;&#29616;&#32454;&#33410;&#24456;&#37325;&#35201;&#65292;&#26368;&#20339;&#35774;&#35745;&#20915;&#31574;&#21487;&#33021;&#19982;&#20856;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2303.10139</link><description>&lt;p&gt;
Distill n' Explain&#65306;&#20351;&#29992;&#31616;&#21333;&#26367;&#20195;&#27169;&#22411;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distill n' Explain: explaining graph neural networks using simple surrogates. (arXiv:2303.10139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#25214;&#21040;&#20445;&#25345;&#39044;&#27979;&#30340;&#22270;&#23376;&#32467;&#26500;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#30001;&#20110;GNN&#30340;&#22797;&#26434;&#24615;&#65288;&#20363;&#22914;&#65292;&#23618;&#25968;&#65289;&#32780;&#23548;&#33268;&#35299;&#37322;&#30340;&#25104;&#26412;&#19978;&#21319;&#12290;&#22240;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;DnX&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#26367;&#20195;&#30340;GNN&#12290;&#28982;&#21518;&#65292;DnX&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#26469;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;FastDnX&#65292;&#36825;&#26159;DnX&#30340;&#26356;&#24555;&#29256;&#26412;&#65292;&#23427;&#21033;&#29992;&#20102;&#25105;&#20204;&#26367;&#20195;&#27169;&#22411;&#30340;&#32447;&#24615;&#20998;&#35299;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2302.08434</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On marginal feature attributions of tree-based models. (arXiv:2302.08434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24378;&#22823;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;&#38598;&#25104;&#31561;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#26399;&#26395;&#30340;&#23616;&#37096;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20363;&#22914;&#36793;&#38469;&#65288;&#24178;&#39044;&#65289;Shapley&#12289;Owen&#25110;Banzhaf&#20540;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#27169;&#22411;&#30495;&#23454;&#19988;&#23454;&#29616;&#19981;&#21464;&#65292;&#21363;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#12290;&#36890;&#36807;&#25552;&#20379;&#20004;&#20010;&#65288;&#20855;&#26377;&#30456;&#20284;&#32479;&#35745;&#24615;&#36136;&#30340;&#65289;&#20915;&#31574;&#26641;&#26469;&#23545;&#27604;&#36825;&#19968;&#28857;&#65292;&#36825;&#20004;&#20010;&#20915;&#31574;&#26641;&#35745;&#31639;&#23436;&#20840;&#30456;&#21516;&#30340;&#20989;&#25968;&#65292;&#20294;&#8220;&#36335;&#24452;&#30456;&#20851;&#8221;&#30340;TreeSHAP&#26041;&#27861;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25490;&#24207;&#65292;&#32780;&#36793;&#38469;Shapley&#20540;&#37325;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#26469;&#24110;&#21161;&#35745;&#31639;&#23427;&#20204;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#24471;&#21040;&#32447;&#24615;&#21338;&#24328;&#20540;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;&#26576;&#20010;&#24120;&#25968;&#21306;&#38388;&#20869;&#26159;&#31616;&#21333;&#30340;&#65288;&#20998;&#27573;&#24120;&#25968;&#65289;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their power and ease of use, tree-based machine learning models, such as random forests and gradient-boosted tree ensembles, have become very popular. To interpret them, local feature attributions based on marginal expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values, may be employed. Such methods are true to the model and implementation invariant, i.e. dependent only on the input-output function of the model. We contrast this with the popular TreeSHAP algorithm by presenting two (statistically similar) decision trees that compute the exact same function for which the "path-dependent" TreeSHAP yields different rankings of features, whereas the marginal Shapley values coincide. Furthermore, we discuss how the internal structure of tree-based models may be leveraged to help with computing their marginal feature attributions according to a linear game value. One important observation is that these are simple (piecewise-constant) functions with respect to a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.15856</link><description>&lt;p&gt;
&#36229;&#36234;&#38598;&#21512;&#24179;&#22343;&#20540;&#65306;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#24615;&#39044;&#27979;&#20013;&#65292;&#23545;&#20110;&#20851;&#38190;&#27668;&#20505;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#21644;&#38477;&#27700;&#65289;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#22320;&#34920;&#23395;&#33410;&#24615;&#26102;&#38388;&#23610;&#24230;&#39044;&#27979;&#19968;&#30452;&#23384;&#22312;&#30528;&#24046;&#36317;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#25512;&#36827;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#65288;SSF&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#29289;&#29702;&#22522;&#20110;&#27169;&#22411;&#38598;&#21512;&#30340;&#24179;&#22343;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#28982;&#32780;&#38598;&#21512;&#39044;&#27979;&#20013;&#21253;&#21547;&#20102;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#30340;&#20449;&#24687;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#21512;&#22343;&#20540;&#12290;&#20854;&#27425;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20851;&#27880;&#24179;&#22343;&#24615;&#33021;&#65292;&#28982;&#32780;&#23545;&#20110;&#35745;&#21010;&#21644;&#20943;&#28798;&#30446;&#30340;&#26469;&#35828;&#65292;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#31532;&#19977;&#65292;&#27668;&#20505;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#20010;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#32780;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32771;&#34385;&#20102;&#21709;&#24212;&#30340;&#31354;&#38388;&#21487;&#21464;&#24615;&#12290;&#27169;&#22411;&#22534;&#21472;&#21487;&#20197;&#32531;&#35299;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30456;&#20851;&#25991;&#29486;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03464</link><description>&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Quantum Reinforcement Learning. (arXiv:2211.03464v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30456;&#20851;&#25991;&#29486;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26412;&#25991;&#23558;&#25552;&#20379;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#20294;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#24050;&#32463;&#21487;&#29992;&#30340;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#65292;&#36825;&#20123;&#35774;&#22791;&#21253;&#25324;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65292;&#23427;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#20805;&#24403;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#20110;&#26410;&#26469;&#23481;&#38169;&#30828;&#20214;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#20463;&#30640;&#20197;&#21450;&#23545;&#25991;&#29486;&#20013;&#37096;&#20998;&#20869;&#23481;&#30340;&#24635;&#32467;&#21644;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum reinforcement learning is an emerging field at the intersection of quantum computing and machine learning. While we intend to provide a broad overview of the literature on quantum reinforcement learning (our interpretation of this term will be clarified below), we put particular emphasis on recent developments. With a focus on already available noisy intermediate-scale quantum devices, these include variational quantum circuits acting as function approximators in an otherwise classical reinforcement learning setting. In addition, we survey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some of which come with a provable quantum advantage. We provide both a birds-eye-view of the field, as well as summaries and reviews for selected parts of the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07290</link><description>&lt;p&gt;
&#21452;&#25511;&#21046;&#21464;&#37327;&#21152;&#36895;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#26694;&#26550;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#39640;&#26041;&#24046;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#24046;&#26469;&#33258;&#20004;&#20010;&#38543;&#26426;&#28304;&#65306;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#21464;&#37327;&#20165;&#35299;&#20915;&#33945;&#29305;&#21345;&#32599;&#22122;&#22768;&#65292;&#32780;&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#36890;&#24120;&#20165;&#35299;&#20915;&#25968;&#25454;&#23376;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#8221;&#25511;&#21046;&#21464;&#37327;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#20004;&#31181;&#22122;&#22768;&#28304;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#35748;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#26041;&#24046;&#21644;&#22312;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2201.00087</link><description>&lt;p&gt;
&#38745;&#24687;&#26102;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#25345;&#32493;&#21516;&#35843;&#29366;&#24577;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v3 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38745;&#24687;&#29366;&#24577;&#19979;&#20154;&#31867;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#19981;&#21516;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#32500;&#24230;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#24577;&#21464;&#21270;&#30340;&#20154;&#31867;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#32593;&#32476;&#20043;&#38388;&#30340;&#25299;&#25169;&#36317;&#31163;&#36827;&#34892;&#24809;&#32602;&#65292;&#23558;&#21160;&#24577;&#21464;&#21270;&#30340;&#33041;&#32593;&#32476;&#32858;&#31867;&#20026;&#25299;&#25169;&#19978;&#19981;&#21516;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20272;&#35745;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#21160;&#24577;&#21464;&#21270;&#30340;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#29983;&#23376;&#30740;&#31350;&#35774;&#35745;&#25506;&#35752;&#20102;&#33041;&#32593;&#32476;&#30340;&#25972;&#20307;&#25299;&#25169;&#26159;&#21542;&#20855;&#26377;&#36951;&#20256;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new data driven topological data analysis (TDA) approach for estimating state spaces in dynamically changing human functional brain networks of human. Our approach penalizes the topological distance between networks and clusters dynamically changing brain networks into topologically distinct states. Our method takes into account the temporal dimension of the data through the Wasserstein distance between networks. Our method is shown to outperform the widely used k-means clustering often used in estimating the state space in brain networks. The method is applied to more accurately determine the state spaces of dynamically changing functional brain networks. Subsequently, we address the question of whether the overall topology of brain networks is a heritable feature using the twin study design.
&lt;/p&gt;</description></item></channel></rss>