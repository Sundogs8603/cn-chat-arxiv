<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#35752;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#38519;&#38449;&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01314</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Test Selection Methods for Deep Neural Networks. (arXiv:2308.01314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#35752;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#38519;&#38449;&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25910;&#38598;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#21147;&#65292;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#26631;&#35760;&#24037;&#20316;&#37327;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#21482;&#38656;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#21363;&#21487;&#28385;&#36275;&#27979;&#35797;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#25253;&#36947;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#26041;&#27861;&#21482;&#22312;&#31616;&#21333;&#24773;&#26223;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#20363;&#22914;&#65292;&#22312;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#36825;&#35753;&#25105;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#24635;&#26159;&#21487;&#38752;&#30340;&#21527;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#22833;&#36133;&#30340;&#26102;&#38388;&#21644;&#31243;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#26500;&#24314;&#26041;&#27861;&#65292;&#30830;&#23450;&#20102;&#26469;&#33258;&#39030;&#32423;&#20250;&#35758;&#30340;11&#31181;&#36873;&#25321;&#26041;&#27861;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20116;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#26377;&#20004;&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#20174;&#32463;&#39564;&#19978;&#30830;&#35748;&#36825;&#20123;&#38519;&#38449;&#30340;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#38519;&#38449;&#22914;&#20309;&#30772;&#22351;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>Lode Encoder&#26159;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#24182;&#32467;&#21512;&#29992;&#25143;&#35774;&#35745;&#65292;&#29983;&#25104;&#26356;&#21152;&#31526;&#21512;&#35774;&#35745;&#39118;&#26684;&#30340;&#20851;&#21345;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01312</link><description>&lt;p&gt;
Lode Encoder: AI&#32422;&#26463;&#19979;&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lode Encoder: AI-constrained co-creativity. (arXiv:2308.01312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01312
&lt;/p&gt;
&lt;p&gt;
Lode Encoder&#26159;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#24182;&#32467;&#21512;&#29992;&#25143;&#35774;&#35745;&#65292;&#29983;&#25104;&#26356;&#21152;&#31526;&#21512;&#35774;&#35745;&#39118;&#26684;&#30340;&#20851;&#21345;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Lode Encoder&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#32463;&#20856;&#24179;&#21488;&#30410;&#26234;&#28216;&#25103;Lode Runner&#20026;&#22522;&#30784;&#30340;&#21019;&#24847;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;Lode Runner&#20851;&#21345;&#36827;&#34892;&#35757;&#32451;&#26469;&#29983;&#25104;&#19982;&#29992;&#25143;&#35774;&#35745;&#39118;&#26684;&#26356;&#30456;&#20284;&#30340;&#29256;&#26412;&#12290;Lode Encoder&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#24314;&#35758;&#26469;&#26500;&#24314;&#21644;&#32534;&#36753;&#20851;&#21345;&#12290;&#20026;&#20102;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#35813;&#31995;&#32479;&#19981;&#21253;&#21547;&#20256;&#32479;&#30340;&#32534;&#36753;&#24037;&#20855;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#31995;&#32479;&#35774;&#35745;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#21450;&#31995;&#32479;&#28436;&#36827;&#21644;&#29992;&#25143;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Lode Encoder, a gamified mixed-initiative level creation system for the classic platform-puzzle game Lode Runner. The system is built around several autoencoders which are trained on sets of Lode Runner levels. When fed with the user's design, each autoencoder produces a version of that design which is closer in style to the levels that it was trained on. The Lode Encoder interface allows the user to build and edit levels through 'painting' from the suggestions provided by the autoencoders. Crucially, in order to encourage designers to explore new possibilities, the system does not include more traditional editing tools. We report on the system design and training procedure, as well as on the evolution of the system itself and user tests.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#30340;&#20219;&#21153;&#65292;&#21363;&#25512;&#33616;&#20165;&#21253;&#21547;&#26032;&#29289;&#21697;&#30340;&#31726;&#23376;&#12290;&#20316;&#32773;&#21457;&#29616;&#29616;&#26377;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#26377;&#38480;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21452;&#21521;&#21464;&#25442;&#31726;&#23376;&#25512;&#33616;&#27169;&#22411;&#65288;BTBR&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01308</link><description>&lt;p&gt;
&#36974;&#30422;&#21644;&#20132;&#25442;&#24207;&#21015;&#24314;&#27169;&#65306;&#29992;&#20110;&#26434;&#36135;&#36141;&#29289;&#20013;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping. (arXiv:2308.01308v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01308
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#30340;&#20219;&#21153;&#65292;&#21363;&#25512;&#33616;&#20165;&#21253;&#21547;&#26032;&#29289;&#21697;&#30340;&#31726;&#23376;&#12290;&#20316;&#32773;&#21457;&#29616;&#29616;&#26377;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#26377;&#38480;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21452;&#21521;&#21464;&#25442;&#31726;&#23376;&#25512;&#33616;&#27169;&#22411;&#65288;BTBR&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#26159;&#22522;&#20110;&#24050;&#36141;&#31726;&#23376;&#24207;&#21015;&#39044;&#27979;&#19979;&#19968;&#32452;&#29289;&#21697;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#22312;&#26434;&#36135;&#36141;&#29289;&#29615;&#22659;&#20013;&#12290;&#22312;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#20013;&#65292;&#21306;&#20998;&#37325;&#22797;&#29289;&#21697;&#21644;&#25506;&#32034;&#29289;&#21697;&#26159;&#24456;&#26377;&#29992;&#30340;&#65292;&#21363;&#29992;&#25143;&#24050;&#32463;&#28040;&#36153;&#36807;&#30340;&#29289;&#21697;&#21644;&#29992;&#25143;&#26410;&#28040;&#36153;&#36807;&#30340;&#29289;&#21697;&#12290;&#22823;&#37096;&#20998;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#30340;&#30740;&#31350;&#35201;&#20040;&#24573;&#30053;&#36825;&#31181;&#21306;&#20998;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#37325;&#22797;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#30340;&#20219;&#21153;&#65292;&#21363;&#25512;&#33616;&#20165;&#21253;&#21547;&#26032;&#29289;&#21697;&#30340;&#31726;&#23376;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#35780;&#20272;&#37117;&#24456;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#19979;&#19968;&#20010;&#31726;&#23376;&#25512;&#33616;&#26041;&#27861;&#22312;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19979;&#19968;&#20010;&#26032;&#31726;&#23376;&#25512;&#33616;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21452;&#21521;&#21464;&#25442;&#31726;&#23376;&#25512;&#33616;&#27169;&#22411;&#65288;BTBR&#65289;&#65292;&#23427;&#19987;&#27880;&#20110;&#30452;&#25509;&#24314;&#27169;&#31726;&#23376;&#24207;&#21015;&#20013;&#30340;&#26032;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets. It is a recommendation task that has been widely studied, especially in the context of grocery shopping. In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before. Most NBR work either ignores this distinction or focuses on repeat items. We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation. We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t. the NNBR task. To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeli
&lt;/p&gt;</description></item><item><title>BRNES&#26159;&#19968;&#20010;&#26032;&#30340;MARL&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#37051;&#23621;&#21306;&#22495;&#21644;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#38450;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01274</link><description>&lt;p&gt;
BRNES: &#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#38544;&#31169;&#24863;&#30693;&#30340;&#32463;&#39564;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems. (arXiv:2308.01274v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01274
&lt;/p&gt;
&lt;p&gt;
BRNES&#26159;&#19968;&#20010;&#26032;&#30340;MARL&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#37051;&#23621;&#21306;&#22495;&#21644;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#38450;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32463;&#39564;&#20849;&#20139;&#65288;ES&#65289;&#22312;&#39038;&#38382;-&#21463;&#21149;&#21578;&#26694;&#26550;&#20013;&#21152;&#36895;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20294;&#23581;&#35797;&#23558;ES&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36804;&#20170;&#20026;&#27490;&#20173;&#20381;&#36182;&#20110;&#20540;&#24471;&#20449;&#36182;&#30340;&#29615;&#22659;&#65292;&#24182;&#24573;&#30053;&#20102;&#23545;&#25239;&#24615;&#25805;&#32437;&#21644;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19968;&#20123;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#20882;&#20805;&#39038;&#38382;&#21521;&#21463;&#21149;&#21578;&#25552;&#20379;&#34394;&#20551;&#24314;&#35758;&#65292;&#24182;&#20005;&#37325;&#38477;&#20302;&#25972;&#20307;&#23398;&#20064;&#24615;&#33021;&#12290;&#32780;&#19988;&#65292;&#19968;&#20010;&#25512;&#29702;&#25915;&#20987;&#32773;&#20882;&#20805;&#21463;&#21149;&#21578;&#32773;&#21487;&#33021;&#36827;&#34892;&#22810;&#27425;&#26597;&#35810;&#20197;&#25512;&#26029;&#39038;&#38382;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#20351;&#25972;&#20010;ES&#36807;&#31243;&#22312;&#38544;&#31169;&#27844;&#28431;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#21644;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MARL&#26694;&#26550;&#65288;BRNES&#65289;&#65292;&#23427;&#22312;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#20013;&#21551;&#21457;&#24335;&#22320;&#20026;&#27599;&#20010;&#21463;&#21149;&#21578;&#32773;&#36873;&#25321;&#19968;&#20010;&#21160;&#24577;&#37051;&#23621;&#21306;&#22495;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#20943;&#23569;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#20195;&#29702;&#30340;&#31169;&#20154;&#20449;&#24687;&#30340;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and overlooked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01271</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#27010;&#29575;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC. (arXiv:2308.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#65288;cSGHMC&#65289;&#30340;&#23454;&#29992;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#35774;&#23450;&#20808;&#39564;&#65292;&#24182;&#20351;&#29992;cSGHMC&#36817;&#20284;&#34920;&#31034;&#22810;&#32500;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#36793;&#32536;&#21270;&#36825;&#20123;&#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;SVHN&#21644;CIFAR-10&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#12289;&#22270;&#20687;&#36716;&#25442;&#12289;&#33258;&#25105;&#30417;&#30563;&#21644;&#20998;&#35299;&#34920;&#31034;&#26041;&#27861;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#23558;&#30693;&#35782;&#20174;&#26631;&#35760;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#30456;&#20851;&#20294;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#65292;&#20026;&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01265</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives. (arXiv:2308.01265v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#12289;&#22270;&#20687;&#36716;&#25442;&#12289;&#33258;&#25105;&#30417;&#30563;&#21644;&#20998;&#35299;&#34920;&#31034;&#26041;&#27861;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#23558;&#30693;&#35782;&#20174;&#26631;&#35760;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#30456;&#20851;&#20294;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#65292;&#20026;&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#30417;&#30563;&#23398;&#20064;&#19978;&#65292;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#25216;&#26415;&#65292;&#20197;&#23558;&#30693;&#35782;&#20174;&#26631;&#35760;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#30456;&#20851;&#20294;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;UDA&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20135;&#29983;&#20102;&#24191;&#27867;&#30340;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#12289;&#22270;&#20687;&#36716;&#25442;&#12289;&#33258;&#25105;&#30417;&#30563;&#21644;&#20998;&#35299;&#34920;&#31034;&#26041;&#27861;&#31561;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#35780;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#28145;&#24230;UDA&#26041;&#27861;&#30340;&#30740;&#31350;&#25991;&#29486;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24403;&#21069;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;UDA&#30740;&#31350;&#20998;&#20026;&#20845;&#31867;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#30340;&#29305;&#28857;&#36827;&#19968;&#27493;&#32454;&#20998;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has demonstrated remarkable performance across various tasks in medical imaging. However, these approaches primarily focus on supervised learning, assuming that the training and testing data are drawn from the same distribution. Unfortunately, this assumption may not always hold true in practice. To address these issues, unsupervised domain adaptation (UDA) techniques have been developed to transfer knowledge from a labeled domain to a related but unlabeled domain. In recent years, significant advancements have been made in UDA, resulting in a wide range of methodologies, including feature alignment, image translation, self-supervision, and disentangled representation methods, among others. In this paper, we provide a comprehensive literature review of recent deep UDA approaches in medical imaging from a technical perspective. Specifically, we categorize current UDA research in medical imaging into six groups and further divide them into finer subcategories based on the d
&lt;/p&gt;</description></item><item><title>Tirtha&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#30340;&#22270;&#20687;&#24182;&#21019;&#24314;&#23427;&#20204;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#36825;&#20010;&#24179;&#21488;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#25216;&#26415;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#29305;&#28857;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.01246</link><description>&lt;p&gt;
Tirtha -- &#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#22270;&#20687;&#24182;&#21019;&#24314;&#21382;&#21490;&#36951;&#22336;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01246
&lt;/p&gt;
&lt;p&gt;
Tirtha&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#24179;&#21488;&#65292;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#30340;&#22270;&#20687;&#24182;&#21019;&#24314;&#23427;&#20204;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#36825;&#20010;&#24179;&#21488;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#25216;&#26415;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#29305;&#28857;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#20445;&#23384;&#25991;&#21270;&#36951;&#20135;&#65288;CH&#65289;&#36951;&#22336;&#23545;&#20110;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#33258;&#28982;&#28798;&#23475;&#25110;&#20154;&#31867;&#27963;&#21160;&#30340;&#25439;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#21019;&#24314;&#25991;&#21270;&#36951;&#20135;&#36951;&#22336;&#30340;&#19977;&#32500;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25968;&#23383;&#21270;&#20445;&#23384;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#24471;&#30410;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25668;&#24433;&#27979;&#37327;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#30340;&#35774;&#22791;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#23384;&#22312;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19968;&#20010;&#24320;&#25918;&#30340;&#19977;&#32500;&#27169;&#22411;&#23384;&#20648;&#24211;&#38459;&#30861;&#20102;&#23545;&#36951;&#20135;&#30340;&#30740;&#31350;&#21644;&#20844;&#20247;&#21442;&#19982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tirtha&#65292;&#19968;&#20010;&#29992;&#20110;&#20247;&#21253;&#25991;&#21270;&#36951;&#22336;&#22270;&#20687;&#21644;&#19977;&#32500;&#27169;&#22411;&#21019;&#24314;&#30340;&#32593;&#32476;&#24179;&#21488;&#12290;Tirtha&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#21644;&#22810;&#35270;&#22270;&#31435;&#20307;&#65288;MVS&#65289;&#25216;&#26415;&#12290;&#23427;&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#65292;&#21487;&#20197;&#38543;&#30528;&#25668;&#24433;&#27979;&#37327;&#30340;&#36827;&#23637;&#32780;&#24341;&#20837;&#26032;&#25216;&#26415;&#12290;Tirtha&#21487;&#20197;&#36890;&#36807;https://tirtha.niser.ac.in&#20316;&#20026;&#32593;&#39029;&#30028;&#38754;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20381;&#36182;&#29992;&#25143;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#39044;&#27979;&#29992;&#25143;&#34892;&#20026;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#24615;&#33021;&#24182;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#39046;&#22495;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01231</link><description>&lt;p&gt;
&#21457;&#25381;&#19978;&#19979;&#25991;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#39044;&#27979;&#27169;&#22411;&#22686;&#24378;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models. (arXiv:2308.01231v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20381;&#36182;&#29992;&#25143;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#39044;&#27979;&#29992;&#25143;&#34892;&#20026;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#24615;&#33021;&#24182;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#39046;&#22495;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;&#19978;&#19979;&#25991;&#39044;&#27979;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#29992;&#25143;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#32780;&#19981;&#32771;&#34385;&#29289;&#21697;&#26412;&#36523;&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#26469;&#30830;&#23450;&#29992;&#25143;&#34892;&#20026;&#65288;&#22914;&#28857;&#20987;&#25110;&#36716;&#21270;&#65289;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;&#35757;&#32451;&#36741;&#21161;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#26469;&#20272;&#35745;&#28857;&#20987;&#27010;&#29575;&#65292;&#24182;&#23558;&#20854;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;CTR&#39044;&#27979;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#19994;&#21153;&#25351;&#26631;&#19978;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#32780;&#23545;&#26381;&#21153;&#25104;&#26412;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#65292;&#20294;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#35268;&#27169;&#21830;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01223</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#24605;&#32771;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#27979;&#35797;&#26159;&#25552;&#39640;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#23545;&#32763;&#35793;&#21518;&#30340;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#36890;&#24120;&#26159;&#22312;&#22823;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#30475;&#19981;&#21040;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#32763;&#35793;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#26469;&#20811;&#26381;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#23545;5&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#25105;&#32763;&#35793;&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#25512;&#29702;&#65292;&#35777;&#26126;&#20102;&#24403;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;&#20854;&#22810;&#35821;&#35328;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/juletx/self-translate &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#26469;&#30740;&#31350;&#21307;&#30103;&#39046;&#22495;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#26631;&#27880;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#28857;&#21644;&#30495;&#27491;&#30340;&#32570;&#38519;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.01220</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;ScrutinAI&#36827;&#34892;DNN&#24615;&#33021;&#30340;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case. (arXiv:2308.01220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01220
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#26469;&#30740;&#31350;&#21307;&#30103;&#39046;&#22495;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#26631;&#27880;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#28857;&#21644;&#30495;&#27491;&#30340;&#32570;&#38519;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#25903;&#25345;&#20154;&#24037;&#20998;&#26512;&#24072;&#20132;&#20114;&#24335;&#22320;&#30740;&#31350;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#27880;&#36136;&#37327;&#12290;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#29615;&#22659;&#20013;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#38656;&#35201;&#28145;&#20837;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38750;&#24120;&#26114;&#36149;&#12290;&#36890;&#24120;&#65292;&#25968;&#25454;&#38598;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#32676;&#20307;&#30340;&#24847;&#35265;&#36827;&#34892;&#26631;&#27880;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#26469;&#20998;&#26512;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#26631;&#27880;&#21464;&#24322;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;ScrutinAI&#26377;&#21161;&#20110;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#23558;&#30001;&#26631;&#27880;&#36136;&#37327;&#19981;&#21516;&#25110;&#32570;&#22833;&#24341;&#36215;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#32570;&#28857;&#19982;&#30495;&#27491;&#30340;&#32570;&#38519;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#35814;&#32454;&#26816;&#26597;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#39045;&#20869;&#20986;&#34880;&#30340;&#25972;&#20307;&#26816;&#27979;&#21644;&#20122;&#22411;&#20043;&#38388;&#26356;&#32454;&#24494;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;softmax&#21644;&#24179;&#38754;&#20998;&#31867;&#22120;&#65292;&#23618;&#27425;&#21270;softmax&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01210</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Global Hierarchical Neural Networks using Hierarchical Softmax. (arXiv:2308.01210v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;softmax&#21644;&#24179;&#38754;&#20998;&#31867;&#22120;&#65292;&#23618;&#27425;&#21270;softmax&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#26469;&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#23618;&#27425;&#21270;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20855;&#26377;&#33258;&#28982;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#24120;&#35268;softmax&#65292;&#23618;&#27425;&#21270;softmax&#22312;&#23439;F1&#21644;&#23439;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#23618;&#27425;&#21270;softmax&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24494;&#20934;&#30830;&#29575;&#21644;&#23439;&#31934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework in which hierarchical softmax is used to create a global hierarchical classifier. The approach is applicable for any classification task where there is a natural hierarchy among classes. We show empirical results on four text classification datasets. In all datasets the hierarchical softmax improved on the regular softmax used in a flat classifier in terms of macro-F1 and macro-recall. In three out of four datasets hierarchical softmax achieved a higher micro-accuracy and macro-precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20010;&#24615;&#21270;&#26102;&#38388;&#34928;&#20943;&#20989;&#25968;&#30340;&#33258;&#36866;&#24212;&#21327;&#21516;&#36807;&#28388;&#37329;&#34701;&#20135;&#21697;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#25552;&#20379;&#21487;&#38752;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#23458;&#25143;&#21644;&#20135;&#21697;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20449;&#21495;&#65292;&#22788;&#29702;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.01208</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#26102;&#38388;&#34928;&#20943;&#20989;&#25968;&#30340;&#33258;&#36866;&#24212;&#21327;&#21516;&#36807;&#28388;&#37329;&#34701;&#20135;&#21697;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation. (arXiv:2308.01208v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20010;&#24615;&#21270;&#26102;&#38388;&#34928;&#20943;&#20989;&#25968;&#30340;&#33258;&#36866;&#24212;&#21327;&#21516;&#36807;&#28388;&#37329;&#34701;&#20135;&#21697;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#25552;&#20379;&#21487;&#38752;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#23458;&#25143;&#21644;&#20135;&#21697;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20449;&#21495;&#65292;&#22788;&#29702;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20551;&#35774;&#21382;&#21490;&#25968;&#25454;&#26159;&#19981;&#21464;&#30340;&#65292;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26102;&#38388;&#25935;&#24863;&#29615;&#22659;&#20013;&#25552;&#20379;&#21487;&#38752;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#20551;&#35774;&#22312;&#37329;&#34701;&#39046;&#22495;&#23588;&#20854;&#26377;&#38382;&#39064;&#65292;&#22240;&#20026;&#37329;&#34701;&#20135;&#21697;&#30340;&#20272;&#20540;&#19981;&#26029;&#21464;&#21270;&#65292;&#23548;&#33268;&#23458;&#25143;&#20852;&#36259;&#39057;&#32321;&#36716;&#31227;&#12290;&#36825;&#20123;&#28436;&#21464;&#30340;&#20852;&#36259;&#21487;&#20197;&#36890;&#36807;&#36807;&#21435;&#23458;&#25143;-&#20135;&#21697;&#20132;&#20114;&#20013;&#24635;&#32467;&#20986;&#26469;&#65292;&#20854;&#25928;&#29992;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#22240;&#23458;&#25143;&#32780;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#30456;&#20851;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20010;&#24615;&#21270;&#34928;&#20943;&#20989;&#25968;&#33258;&#36866;&#24212;&#22320;&#25240;&#20215;&#36828;&#31163;&#30340;&#23458;&#25143;-&#20135;&#21697;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#22788;&#29702;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#23458;&#25143;&#21644;&#20135;&#21697;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20449;&#21495;&#26469;&#20135;&#29983;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#19987;&#26377;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical recommender systems often assume that historical data are stationary and fail to account for the dynamic nature of user preferences, limiting their ability to provide reliable recommendations in time-sensitive settings. This assumption is particularly problematic in finance, where financial products exhibit continuous changes in valuations, leading to frequent shifts in client interests. These evolving interests, summarized in the past client-product interactions, see their utility fade over time with a degree that might differ from one client to another. To address this challenge, we propose a time-dependent collaborative filtering algorithm that can adaptively discount distant client-product interactions using personalized decay functions. Our approach is designed to handle the non-stationarity of financial data and produce reliable recommendations by modeling the dynamic collaborative signals between clients and products. We evaluate our method using a proprietary dataset 
&lt;/p&gt;</description></item><item><title>BiERL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01207</link><description>&lt;p&gt;
BiERL: &#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01207
&lt;/p&gt;
&lt;p&gt;
BiERL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#65292;&#20294;&#26159;&#22312;&#19981;&#20180;&#32454;&#35843;&#25972;&#36229;&#21442;&#25968;&#65288;&#21363;&#20803;&#21442;&#25968;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#20250;&#38754;&#20020;&#19981;&#36275;&#30340;&#25506;&#32034;&#25110;&#27169;&#22411;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#65288;BiERL&#65289;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#20813;&#21435;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#38656;&#35201;&#20808;&#26377;&#39046;&#22495;&#30693;&#35782;&#25110;&#26114;&#36149;&#20248;&#21270;&#36807;&#31243;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#20803;&#32423;&#26550;&#26500;&#65292;&#23558;&#20869;&#37096;&#32423;&#21035;&#30340;&#36827;&#21270;&#32463;&#39564;&#23884;&#20837;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#31181;&#32676;&#34920;&#31034;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#21487;&#34892;&#30340;&#20803;&#32423;&#36866;&#24212;&#24230;&#20989;&#25968;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;MuJoCo&#21644;Box2D&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;BiERL&#20248;&#20110;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25552;&#39640;&#29616;&#20195;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#35770;&#65292;&#20026;&#32463;&#39564;&#20016;&#23500;&#30340;RS&#24037;&#31243;&#24072;&#25552;&#20379;&#23454;&#29992;&#32463;&#39564;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#20854;&#20182;RS&#12290;</title><link>http://arxiv.org/abs/2308.01204</link><description>&lt;p&gt;
&#25552;&#21319;&#29616;&#20195;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Methodologies for Improving Modern Industrial Recommender Systems. (arXiv:2308.01204v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25552;&#39640;&#29616;&#20195;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#35770;&#65292;&#20026;&#32463;&#39564;&#20016;&#23500;&#30340;RS&#24037;&#31243;&#24072;&#25552;&#20379;&#23454;&#29992;&#32463;&#39564;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#20854;&#20182;RS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#26159;&#19968;&#39033;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#12289;&#30005;&#23376;&#21830;&#21153;&#12289;&#23089;&#20048;&#31561;&#39046;&#22495;&#12290;RS&#30830;&#23454;&#26159;&#35768;&#22810;&#27969;&#34892;APP&#65288;&#22914;YouTube&#12289;Tik Tok&#12289;&#23567;&#32418;&#20070;&#12289;&#21716;&#21737;&#21716;&#21737;&#31561;&#65289;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25552;&#39640;&#29616;&#20195;&#24037;&#19994;RS&#30340;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#20026;&#32463;&#39564;&#20016;&#23500;&#30340;RS&#24037;&#31243;&#24072;&#25552;&#20379;&#24110;&#21161;&#65292;&#20182;&#20204;&#27491;&#22312;&#21162;&#21147;&#25913;&#21892;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#22914;&#29992;&#25143;&#30041;&#23384;&#21644;&#20351;&#29992;&#26102;&#38271;&#12290;&#26412;&#25991;&#20013;&#20998;&#20139;&#30340;&#32463;&#39564;&#24050;&#32463;&#22312;&#19968;&#20123;&#30495;&#23454;&#30340;&#24037;&#19994;RS&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19988;&#21487;&#33021;&#36866;&#29992;&#20110;&#20854;&#20182;RS&#12290;&#22823;&#37096;&#20998;&#20869;&#23481;&#37117;&#22522;&#20110;&#19994;&#30028;&#32463;&#39564;&#65292;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender system (RS) is an established technology with successful applications in social media, e-commerce, entertainment, and more. RSs are indeed key to the success of many popular APPs, such as YouTube, Tik Tok, Xiaohongshu, Bilibili, and others. This paper explores the methodology for improving modern industrial RSs. It is written for experienced RS engineers who are diligently working to improve their key performance indicators, such as retention and duration. The experiences shared in this paper have been tested in some real industrial RSs and are likely to be generalized to other RSs as well. Most contents in this paper are industry experience without publicly available references.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#22270;&#35757;&#32451;&#65292;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.01197</link><description>&lt;p&gt;
GNN4FR:&#19968;&#31181;&#26080;&#25439;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNN4FR: A Lossless GNN-based Federated Recommendation Framework. (arXiv:2308.01197v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#22270;&#35757;&#32451;&#65292;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#25910;&#38598;&#29992;&#25143;&#21644;&#30456;&#24212;&#29289;&#21697;&#20043;&#38388;&#30340;&#20010;&#20154;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#20013;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#21487;&#33021;&#36829;&#21453;GDPR&#31561;&#38544;&#31169;&#27861;&#24459;&#12290;&#33267;&#20170;&#27809;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#22270;&#65292;&#21516;&#26102;&#21448;&#19981;&#27844;&#38706;&#27599;&#20010;&#29992;&#25143;&#30340;&#31169;&#20154;&#20132;&#20114;&#25968;&#25454;&#65288;&#21363;&#20182;&#25110;&#22905;&#30340;&#23376;&#22270;&#65289;&#12290;&#26412;&#25991;&#39318;&#27425;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26032;&#22411;&#26080;&#25439;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#23436;&#25972;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#22270;&#35757;&#32451;&#65292;&#20351;&#35757;&#32451;&#36807;&#31243;&#31561;&#20215;&#20110;&#30456;&#24212;&#30340;&#38750;&#32852;&#37030;&#23545;&#24212;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LightGCN&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20854;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained wide popularity in recommender systems due to their capability to capture higher-order structure information among the nodes of users and items. However, these methods need to collect personal interaction data between a user and the corresponding items and then model them in a central server, which would break the privacy laws such as GDPR. So far, no existing work can construct a global graph without leaking each user's private interaction data (i.e., his or her subgraph). In this paper, we are the first to design a novel lossless federated recommendation framework based on GNN, which achieves full-graph training with complete high-order structure information, enabling the training process to be equivalent to the corresponding un-federated counterpart. In addition, we use LightGCN to instantiate an example of our framework and show its equivalence.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.01196</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#36879;&#26126;&#30340;&#25512;&#33616;&#31995;&#32479;: &#29992;&#20110;&#35299;&#37322;&#24615;&#30340;&#36125;&#21494;&#26031;&#22270;&#20687;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#24120;&#25351;&#23548;&#29992;&#25143;&#25214;&#21040;&#30456;&#20851;&#30340;&#20869;&#23481;&#25110;&#20135;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#21644;&#20844;&#27665;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65307;&#20010;&#24615;&#21270;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#25512;&#33616;&#25552;&#20379;&#29702;&#30001;&#12290;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#29992;&#25143;&#21019;&#24314;&#30340;&#35270;&#35273;&#20869;&#23481;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#36873;&#39033;&#65292;&#26377;&#28508;&#21147;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#35299;&#37322;&#25512;&#33616;&#26102;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#21487;&#25345;&#32493;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#30340;&#30899;&#25490;&#25918;&#37327;&#19982;&#23427;&#20204;&#34987;&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20351;&#29992;&#30340;&#26367;&#20195;&#23398;&#20064;&#30446;&#26631;&#19982;&#25490;&#21517;&#26368;&#26377;&#25928;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#23458;&#25143;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#39044;&#27979;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#21644;&#21830;&#21697;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#23384;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#28040;&#36153;&#34892;&#20026;&#21644;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#31867;&#21035;&#31890;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.01195</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#8220;&#20877;&#27425;&#36141;&#20080;&#8221;&#25512;&#33616;&#20013;&#30340;&#31867;&#21035;&#39057;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Category Frequency prediction for Buy It Again recommendations. (arXiv:2308.01195v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#23458;&#25143;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#39044;&#27979;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#21644;&#21830;&#21697;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#23384;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#28040;&#36153;&#34892;&#20026;&#21644;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#31867;&#21035;&#31890;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20877;&#27425;&#36141;&#20080;&#8221;&#65288;BIA&#65289;&#25512;&#33616;&#23545;&#20110;&#38646;&#21806;&#21830;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#33258;&#24049;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#25552;&#20379;&#21487;&#33021;&#20877;&#27425;&#36141;&#20080;&#30340;&#21830;&#21697;&#25512;&#33616;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#32593;&#31449;&#21442;&#19982;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;BIA&#30740;&#31350;&#20998;&#26512;&#20102;&#23458;&#25143;&#22312;&#21830;&#21697;&#31890;&#24230;&#19978;&#30340;&#20010;&#24615;&#21270;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31867;&#21035;&#30340;&#27169;&#22411;&#21487;&#33021;&#26356;&#21512;&#36866;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#8221;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21253;&#25324;&#20102;&#20010;&#24615;&#21270;&#31867;&#21035;&#27169;&#22411;&#65288;PC&#27169;&#22411;&#65289;&#21644;&#31867;&#21035;&#20869;&#20010;&#24615;&#21270;&#21830;&#21697;&#27169;&#22411;&#65288;IC&#27169;&#22411;&#65289;&#12290;PC&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#31867;&#21035;&#21015;&#34920;&#65292;&#26174;&#31034;&#20102;&#23458;&#25143;&#21487;&#33021;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#12290;IC&#27169;&#22411;&#22312;&#31867;&#21035;&#20869;&#23545;&#21830;&#21697;&#36827;&#34892;&#25490;&#21517;&#65292;&#26174;&#31034;&#20102;&#23458;&#25143;&#22312;&#31867;&#21035;&#20869;&#21487;&#33021;&#28040;&#36153;&#30340;&#21830;&#21697;&#12290;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#20351;&#29992;&#29983;&#23384;&#27169;&#22411;&#25429;&#25417;&#20135;&#21697;&#30340;&#19968;&#33324;&#28040;&#36153;&#29575;&#12290;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#20102;&#28040;&#36153;&#36235;&#21183;&#12290;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#31867;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20844;&#24335;&#12289;&#38590;&#20197;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#26080;&#20449;&#24687;&#20808;&#39564;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01184</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#26631;&#31614;&#20808;&#39564;&#30340;&#38544;&#24335;&#21028;&#21035;&#36924;&#36817;&#36827;&#34892;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20844;&#24335;&#12289;&#38590;&#20197;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#26080;&#20449;&#24687;&#20808;&#39564;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;&#23613;&#31649;&#21028;&#21035;&#27169;&#22411;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#35757;&#32451;&#36807;&#31243;&#32780;&#22312;&#35813;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20998;&#35299;&#24178;&#20928;&#21644;&#22122;&#22768;&#26631;&#31614;&#65292;&#24182;&#25913;&#21892;&#26631;&#31614;&#36716;&#25442;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26041;&#27861;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#20844;&#24335;&#26469;&#26368;&#22823;&#21270;&#22122;&#22768;&#26631;&#31614;&#21644;&#25968;&#25454;&#30340;&#32852;&#21512;&#20284;&#28982;&#65292;&#36825;&#21482;&#38388;&#25509;&#20248;&#21270;&#20102;&#19982;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#30456;&#20851;&#30340;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#24456;&#38590;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20542;&#21521;&#20110;&#20351;&#29992;&#26080;&#20449;&#24687;&#30340;&#24178;&#20928;&#26631;&#31614;&#20808;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#20851;&#32852;&#25968;&#25454;&#21644;&#24178;&#20928;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20351;&#29992;&#21028;&#21035;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#38544;&#24335;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.01170</link><description>&lt;p&gt;
&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#23398;&#20064;&#20351;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#33021;&#22815;&#21453;&#20107;&#23454;&#22320;&#25512;&#29702;&#26410;&#25191;&#34892;&#30340;&#31574;&#30053;&#65292;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#24605;&#24819;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#36825;&#20004;&#20010;&#22312;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#22240;&#32032;&#32467;&#21512;&#26102;&#65292;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#23601;&#26159;&#33261;&#21517;&#26157;&#33879;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#12290;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#65288;GTD&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#33268;&#21629;&#19977;&#20803;&#32452;&#30340;&#19968;&#31181;&#24378;&#22823;&#24037;&#20855;&#12290;&#23427;&#30340;&#25104;&#21151;&#26159;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#22797;&#21046;&#25110;Fenchel&#23545;&#20598;&#38388;&#25509;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#32780;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21482;&#38656;&#22312;&#36880;&#28176;&#22686;&#21152;&#30340;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#20351;&#29992;&#20004;&#20010;&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;GTD&#19968;&#26679;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#20294;&#25682;&#24323;&#20102;GTD&#30340;&#39069;&#22806;&#26435;&#37325;&#12290;&#25105;&#20204;&#25152;&#20184;&#20986;&#30340;&#21807;&#19968;&#20195;&#20215;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20869;&#23384;&#21576;&#23545;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28176;&#36817;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#20854;&#20013;&#25910;&#25947;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;</title><link>http://arxiv.org/abs/2308.01157</link><description>&lt;p&gt;
LLMs&#29702;&#35299;&#29627;&#29827;&#30418;&#27169;&#22411;&#65292;&#21457;&#29616;&#24778;&#21916;&#24182;&#25552;&#20986;&#20462;&#22797;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01157
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23558;&#22797;&#26434;&#32467;&#26524;&#20998;&#35299;&#20026;&#21333;&#19968;&#21464;&#37327;&#30340;&#22270;&#34920;&#31034;&#32452;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25972;&#20010;&#27169;&#22411;&#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#24212;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#33258;&#21160;&#23436;&#25104;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#26816;&#27979;&#19982;&#20808;&#21069;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24322;&#24120;&#65292;&#25551;&#36848;&#24322;&#24120;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#21435;&#38500;&#24322;&#24120;&#30340;&#20462;&#22797;&#24314;&#35758;&#12290;&#25105;&#20204;&#20351;&#29992;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#22810;&#20010;&#31034;&#20363;&#26469;&#35777;&#26126;LLMs&#30340;&#36825;&#20123;&#26032;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;$\texttt{TalkToEBM}$&#21253;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#36827;&#34892;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#21270;&#20102;&#26679;&#26412;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.01140</link><description>&lt;p&gt;
DySTreSS: &#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#28201;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#21270;&#20102;&#26679;&#26412;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31639;&#27861;&#20013;&#65292;&#35832;&#22914;SimCLR&#12289;MoCo&#31561;&#65292;&#24179;&#34913;&#20004;&#20010;&#35821;&#20041;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#21560;&#24341;&#21147;&#21644;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#26679;&#26412;&#20043;&#38388;&#30340;&#25490;&#26021;&#21147;&#30340;&#20219;&#21153;&#20027;&#35201;&#21463;&#21040;&#30828;&#36127;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26681;&#25454;&#22256;&#38590;&#31243;&#24230;&#26045;&#21152;&#24809;&#32602;&#65292;&#20294;&#28201;&#24230;&#36229;&#21442;&#25968;&#26159;&#35843;&#33410;&#24809;&#32602;&#21644;&#22343;&#21248;&#24615;&#19982;&#23481;&#24525;&#24230;&#20043;&#38388;&#26435;&#34913;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#25913;&#36827;SSL&#20013;InfoNCE&#25439;&#22833;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#28201;&#24230;&#36229;&#21442;&#25968;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20313;&#24358;&#30456;&#20284;&#24230;&#20381;&#36182;&#30340;&#28201;&#24230;&#35843;&#25972;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#20248;&#21270;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#22343;&#21248;&#24615;&#21644;&#23481;&#24525;&#24230;&#24230;&#37327;&#65292;&#20197;&#30740;&#31350;&#20313;&#24358;&#30456;&#20284;&#24230;&#31354;&#38388;&#20013;&#26356;&#22909;&#20248;&#21270;&#30340;&#26368;&#20339;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#23616;&#37096;&#21644;&#20840;&#23616;&#34892;&#20026;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#38544;&#31169;&#20998;&#37197;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25252;&#26799;&#24230;&#24182;&#26368;&#23567;&#21270;&#20248;&#21270;&#38169;&#35823;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#35843;&#25972;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.01139</link><description>&lt;p&gt;
&#21160;&#24577;&#38544;&#31169;&#20998;&#37197;&#29992;&#20110;&#20855;&#26377;&#22797;&#21512;&#30446;&#26631;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives. (arXiv:2308.01139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#38544;&#31169;&#20998;&#37197;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25252;&#26799;&#24230;&#24182;&#26368;&#23567;&#21270;&#20248;&#21270;&#38169;&#35823;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#35843;&#25972;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#20984;&#20294;&#21487;&#33021;&#38750;&#24179;&#28369;&#38382;&#39064;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#26799;&#24230;&#65292;&#20197;&#20813;&#21463;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#30340;&#27844;&#28431;&#12290;&#35813;&#31639;&#27861;&#22312;&#20849;&#20139;&#20449;&#24687;&#20013;&#28155;&#21152;&#20154;&#24037;&#22122;&#22768;&#20197;&#30830;&#20445;&#38544;&#31169;&#65292;&#24182;&#21160;&#24577;&#20998;&#37197;&#26102;&#21464;&#30340;&#22122;&#22768;&#26041;&#24046;&#65292;&#20197;&#26368;&#23567;&#21270;&#20248;&#21270;&#38169;&#35823;&#30340;&#19978;&#30028;&#65292;&#21516;&#26102;&#31526;&#21512;&#39044;&#23450;&#20041;&#30340;&#38544;&#31169;&#39044;&#31639;&#38480;&#21046;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20219;&#24847;&#22823;&#30340;&#20294;&#26377;&#38480;&#27425;&#25968;&#30340;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#25928;&#29992;&#65292;&#24182;&#28040;&#38500;&#20102;&#35843;&#25972;&#36845;&#20195;&#27425;&#25968;&#30340;&#38656;&#27714;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a locally differentially private federated learning algorithm for strongly convex but possibly nonsmooth problems that protects the gradients of each worker against an honest but curious server. The proposed algorithm adds artificial noise to the shared information to ensure privacy and dynamically allocates the time-varying noise variance to minimize an upper bound of the optimization error subject to a predefined privacy budget constraint. This allows for an arbitrarily large but finite number of iterations to achieve both privacy protection and utility up to a neighborhood of the optimal solution, removing the need for tuning the number of iterations. Numerical results show the superiority of the proposed algorithm over state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01138</link><description>&lt;p&gt;
&#33021;&#21542;&#36716;&#31227;&#22122;&#22768;&#27169;&#24335;&#65311;&#20351;&#29992;&#29983;&#25104;&#26696;&#20363;&#30340;&#22810;&#29615;&#22659;&#39057;&#35889;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#27700;&#36136;&#26816;&#27979;&#20013;&#65292;&#39057;&#35889;&#20998;&#26512;&#31995;&#32479;&#26088;&#22312;&#26816;&#27979;&#27745;&#26579;&#29289;&#30340;&#31867;&#22411;&#21644;&#27987;&#24230;&#65292;&#24182;&#20351;&#30417;&#31649;&#26426;&#26500;&#33021;&#22815;&#21450;&#26102;&#22238;&#24212;&#27745;&#26579;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#39057;&#35889;&#25968;&#25454;&#30340;&#27979;&#35797;&#35774;&#22791;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#20250;&#21463;&#21040;&#22797;&#26434;&#30340;&#22122;&#22768;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20998;&#26512;&#27169;&#22411;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19981;&#21516;&#29615;&#22659;&#20013;&#26631;&#20934;&#27700;&#26679;&#21697;&#30340;&#39057;&#35889;&#20316;&#20026;&#26696;&#20363;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#22122;&#22768;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20351;&#22122;&#22768;&#27169;&#24335;&#33021;&#22815;&#24212;&#29992;&#20110;&#26410;&#30693;&#26679;&#21697;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24517;&#28982;&#23384;&#22312;&#30340;&#26679;&#26412;&#32423;&#22522;&#32447;&#22122;&#22768;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#33719;&#21462;&#21482;&#22312;&#25968;&#25454;&#38598;&#32423;&#29615;&#22659;&#22122;&#22768;&#19978;&#26377;&#24046;&#24322;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26679;&#26412;&#23545;&#26679;&#26412;&#30340;&#26696;&#20363;&#24211;&#65292;&#25490;&#38500;&#20102;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#33016;&#37096;CT&#25195;&#25551;&#30340;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#37325;&#24314;&#21644;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#23569;&#37327;&#21307;&#30103;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#30149;&#21464;&#65292;&#20197;&#26356;&#22909;&#22320;&#27010;&#25324;&#21644;&#35782;&#21035;&#26089;&#26399;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2308.01137</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#33016;&#37096;CT&#25195;&#25551;&#30340;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#37325;&#24314;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans. (arXiv:2308.01137v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#33016;&#37096;CT&#25195;&#25551;&#30340;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#37325;&#24314;&#21644;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#23569;&#37327;&#21307;&#30103;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#30149;&#21464;&#65292;&#20197;&#26356;&#22909;&#22320;&#27010;&#25324;&#21644;&#35782;&#21035;&#26089;&#26399;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#21644;COVID-19&#22312;&#19990;&#30028;&#19978;&#20855;&#26377;&#26368;&#39640;&#30340;&#24739;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#20043;&#19968;&#12290;&#23545;&#20110;&#21307;&#29983;&#26469;&#35828;&#65292;&#26089;&#26399;&#30142;&#30149;&#30340;&#30149;&#21464;&#35782;&#21035;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#23569;&#37327;&#21307;&#30103;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#65288;&#22914;&#30149;&#21464;&#65289;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23398;&#20064;&#26356;&#22909;&#22320;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#37325;&#24314;&#21644;&#26816;&#27979;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#26816;&#27979;&#28155;&#21152;&#21040;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20154;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#26816;&#26597;&#20102;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#39592;&#24178;&#32593;&#32476;&#21644;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer and covid-19 have one of the highest morbidity and mortality rates in the world. For physicians, the identification of lesions is difficult in the early stages of the disease and time-consuming. Therefore, multi-task learning is an approach to extracting important features, such as lesions, from small amounts of medical data because it learns to generalize better. We propose a novel multi-task framework for classification, segmentation, reconstruction, and detection. To the best of our knowledge, we are the first ones who added detection to the multi-task solution. Additionally, we checked the possibility of using two different backbones and different loss functions in the segmentation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;XBL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22810;&#25968;&#25454;&#28304;&#24341;&#20837;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01119</link><description>&lt;p&gt;
&#22312;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#28040;&#38500;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;XBL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22810;&#25968;&#25454;&#28304;&#24341;&#20837;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#23545;&#20110;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24517;&#39035;&#25215;&#35748;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#22810;&#26679;&#24615;&#26412;&#36136;&#19978;&#24341;&#20837;&#20102;&#24847;&#22806;&#30340;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#32908;&#39592;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#26159;&#22312;&#38738;&#26149;&#26399;&#35266;&#23519;&#21040;&#30340;&#39592;&#39612;&#25104;&#29087;&#24341;&#36215;&#30340;&#39592;&#39612;&#29983;&#38271;&#12290;&#25105;&#20204;&#20351;&#29992;COVID-19&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30001;&#20110;&#24847;&#22806;&#28151;&#28102;&#21306;&#22495;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#65288;XBL&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#35299;&#37322;&#26469;&#20132;&#20114;&#24335;&#22320;&#28040;&#38500;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20351;&#20854;&#36229;&#36234;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#25972;&#21512;&#20132;&#20114;&#24335;&#29992;&#25143;&#21453;&#39304;&#12289;&#31934;&#30830;&#30340;region-of-interest&#36880;&#28176;&#28040;&#35114;&#31561;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image classification models are frequently trained using training datasets derived from multiple data sources. While leveraging multiple data sources is crucial for achieving model generalization, it is important to acknowledge that the diverse nature of these sources inherently introduces unintended confounders and other challenges that can impact both model accuracy and transparency. A notable confounding factor in medical image classification, particularly in musculoskeletal image classification, is skeletal maturation-induced bone growth observed during adolescence. We train a deep learning model using a Covid-19 chest X-ray dataset and we showcase how this dataset can lead to spurious correlations due to unintended confounding regions. eXplanation Based Learning (XBL) is a deep learning approach that goes beyond interpretability by utilizing model explanations to interactively unlearn spurious correlations. This is achieved by integrating interactive user feedback, specifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.01097</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#30001;&#20110;&#26410;&#26469;&#23039;&#21183;&#30340;&#38543;&#26426;&#21644;&#19981;&#35268;&#21017;&#24615;&#36136;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24448;&#24448;&#38590;&#20197;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#30340;&#26102;&#31354;&#34920;&#31034;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#39592;&#26550;&#33410;&#28857;&#30340;&#26102;&#22495;&#21644;&#31354;&#22495;&#20381;&#36182;&#24615;&#26159;&#19981;&#21516;&#30340;&#12290;&#26102;&#22495;&#20851;&#31995;&#25429;&#25417;&#21040;&#38543;&#26102;&#38388;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#31354;&#22495;&#20851;&#31995;&#25551;&#36848;&#20102;&#36523;&#20307;&#32467;&#26500;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#22686;&#37327;&#20449;&#24687;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#23427;&#35299;&#32806;&#20102;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#20102;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#25299;&#25169;&#22330;&#26223;&#20013;&#33258;&#21160;&#20272;&#35745;&#21333;&#24212;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#30456;&#26426;&#26657;&#20934;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;IoU&#24230;&#37327;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.01086</link><description>&lt;p&gt;
&#22797;&#26434;&#25299;&#25169;&#22330;&#26223;&#20013;&#30340;&#21333;&#24212;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Homography Estimation in Complex Topological Scenes. (arXiv:2308.01086v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01086
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#25299;&#25169;&#22330;&#26223;&#20013;&#33258;&#21160;&#20272;&#35745;&#21333;&#24212;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#30456;&#26426;&#26657;&#20934;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;IoU&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#35270;&#39057;&#21644;&#22270;&#20687;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20174;&#20132;&#36890;&#20998;&#26512;&#21040;&#29359;&#32618;&#26816;&#27979;&#12290;&#23545;&#20110;&#22823;&#22810;&#25968;&#20998;&#26512;&#24212;&#29992;&#65292;&#22806;&#37096;&#30456;&#26426;&#26657;&#20934;&#25968;&#25454;&#37117;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25668;&#20687;&#22836;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#26465;&#20214;&#21644;&#23567;&#22411;&#30456;&#26426;&#31227;&#21160;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#33258;&#21160;&#37325;&#26032;&#26657;&#20934;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#30340;&#33258;&#21160;&#30456;&#26426;&#26657;&#20934;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#25668;&#20687;&#26426;&#35774;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23450;&#21046;&#30340;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#65288;STN&#65289;&#30340;&#23454;&#29616;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#21644;2014&#24180;&#19990;&#30028;&#26479;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#25552;&#39640;IoU&#24230;&#37327;&#25351;&#26631;&#39640;&#36798;12%&#12290;
&lt;/p&gt;
&lt;p&gt;
Surveillance videos and images are used for a broad set of applications, ranging from traffic analysis to crime detection. Extrinsic camera calibration data is important for most analysis applications. However, security cameras are susceptible to environmental conditions and small camera movements, resulting in a need for an automated re-calibration method that can account for these varying conditions. In this paper, we present an automated camera-calibration process leveraging a dictionary-based approach that does not require prior knowledge on any camera settings. The method consists of a custom implementation of a Spatial Transformer Network (STN) and a novel topological loss function. Experiments reveal that the proposed method improves the IoU metric by up to 12% w.r.t. a state-of-the-art model across five synthetic datasets and the World Cup 2014 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21033;&#29992;&#25552;&#21319;&#20551;&#35774;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#21644;&#20351;&#29992;&#36763;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#65292;&#20445;&#25345;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01084</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#20108;&#27425;&#36763;&#34920;&#31034;&#30340;&#25968;&#25454;&#39537;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems. (arXiv:2308.01084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21033;&#29992;&#25552;&#21319;&#20551;&#35774;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#21644;&#20351;&#29992;&#36763;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#65292;&#20445;&#25345;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#25552;&#21319;&#20551;&#35774;&#65292;&#21363;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21487;&#20197;&#20889;&#25104;&#20855;&#26377;&#31435;&#26041;&#21704;&#23494;&#39039;&#37327;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24471;&#21040;&#22312;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#19979;&#20855;&#26377;&#21704;&#23494;&#39039;&#32467;&#26500;&#30340;&#20108;&#27425;&#21160;&#21147;&#23398;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#24191;&#20041;&#20301;&#32622;&#21644;&#21160;&#37327;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20108;&#27425;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#36763;&#33258;&#32534;&#30721;&#22120;&#24378;&#21046;&#23454;&#26045;&#21704;&#23494;&#39039;&#32467;&#26500;&#12290;&#24378;&#21046;&#23454;&#26045;&#30340;&#21704;&#23494;&#39039;&#32467;&#26500;&#34920;&#29616;&#20986;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#65292;&#32780;&#31435;&#26041;&#21704;&#23494;&#39039;&#20989;&#25968;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#20302;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;&#20302;&#32500;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#39640;&#38454;&#21464;&#25442;&#30340;&#22352;&#26631;&#31995;&#65292;&#32780;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#25152;&#38656;&#29305;&#24615;&#30340;&#20302;&#38454;&#22352;&#26631;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20302;&#32500;&#21644;&#39640;&#32500;&#30340;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#38190;&#30424;&#22768;&#23398;&#20391;&#20449;&#36947;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#22312;&#26234;&#33021;&#25163;&#26426;&#40614;&#20811;&#39118;&#19978;&#30340;&#27169;&#22411;&#23545;&#31508;&#35760;&#26412;&#30005;&#33041;&#30340;&#25353;&#38190;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992;Zoom&#36719;&#20214;&#35760;&#24405;&#30340;&#25353;&#38190;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#38190;&#30424;&#20998;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#29616;&#25104;&#35774;&#22791;&#21644;&#31639;&#27861;&#36827;&#34892;&#36825;&#31181;&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#19968;&#31995;&#21015;&#32531;&#35299;&#26041;&#27861;&#26469;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#36825;&#20123;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01074</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#38190;&#30424;&#22768;&#23398;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards. (arXiv:2308.01074v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#38190;&#30424;&#22768;&#23398;&#20391;&#20449;&#36947;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#22312;&#26234;&#33021;&#25163;&#26426;&#40614;&#20811;&#39118;&#19978;&#30340;&#27169;&#22411;&#23545;&#31508;&#35760;&#26412;&#30005;&#33041;&#30340;&#25353;&#38190;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992;Zoom&#36719;&#20214;&#35760;&#24405;&#30340;&#25353;&#38190;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#38190;&#30424;&#20998;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#29616;&#25104;&#35774;&#22791;&#21644;&#31639;&#27861;&#36827;&#34892;&#36825;&#31181;&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#19968;&#31995;&#21015;&#32531;&#35299;&#26041;&#27861;&#26469;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#36825;&#20123;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#40614;&#20811;&#39118;&#30340;&#26222;&#21450;&#20197;&#21450;&#20010;&#20154;&#35774;&#22791;&#19978;&#22312;&#32447;&#26381;&#21153;&#30340;&#20852;&#36215;&#65292;&#22768;&#23398;&#20391;&#20449;&#36947;&#25915;&#20987;&#23545;&#38190;&#30424;&#26500;&#25104;&#20102;&#27604;&#20197;&#24448;&#26356;&#22823;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#26234;&#33021;&#25163;&#26426;&#38598;&#25104;&#30340;&#40614;&#20811;&#39118;&#19978;&#23454;&#29616;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;&#31508;&#35760;&#26412;&#30005;&#33041;&#30340;&#25353;&#38190;&#12290;&#24403;&#20351;&#29992;&#38468;&#36817;&#25163;&#26426;&#35760;&#24405;&#30340;&#25353;&#38190;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95%&#65292;&#26159;&#22312;&#19981;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25152;&#35265;&#21040;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#24403;&#20351;&#29992;&#35270;&#39057;&#20250;&#35758;&#36719;&#20214;Zoom&#35760;&#24405;&#30340;&#25353;&#38190;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;93%&#65292;&#20026;&#35813;&#23186;&#20171;&#30340;&#26032;&#32426;&#24405;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#36890;&#36807;&#29616;&#25104;&#35774;&#22791;&#21644;&#31639;&#27861;&#36827;&#34892;&#36825;&#20123;&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31995;&#21015;&#32531;&#35299;&#26041;&#27861;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#36825;&#20123;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent developments in deep learning, the ubiquity of micro-phones and the rise in online services via personal devices, acoustic side channel attacks present a greater threat to keyboards than ever. This paper presents a practical implementation of a state-of-the-art deep learning model in order to classify laptop keystrokes, using a smartphone integrated microphone. When trained on keystrokes recorded by a nearby phone, the classifier achieved an accuracy of 95%, the highest accuracy seen without the use of a language model. When trained on keystrokes recorded using the video-conferencing software Zoom, an accuracy of 93% was achieved, a new best for the medium. Our results prove the practicality of these side channel attacks via off-the-shelf equipment and algorithms. We discuss a series of mitigation methods to protect users against these series of attacks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#24037;&#20855;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2308.01071</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;: &#35780;&#20272;&#19982;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion. (arXiv:2308.01071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01071
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#24037;&#20855;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#26159;&#25968;&#25454;&#31185;&#23398;&#21644;&#30693;&#35782;&#24037;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38543;&#30528;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;TSC&#31639;&#27861;&#12290;&#38500;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#12289;&#21306;&#38388;&#12289;&#24418;&#29366;&#29305;&#24449;&#12289;&#23383;&#20856;&#12289;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25110;&#32773;&#28151;&#21512;&#38598;&#25104;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22806;&#65292;&#36817;&#24180;&#26469;&#36824;&#35774;&#35745;&#20102;&#19968;&#20123;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#26080;&#30417;&#30563;&#20449;&#24687;&#25688;&#35201;&#32479;&#35745;&#29305;&#24449;&#30340;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20294;&#24456;&#23569;&#26377;&#36825;&#20123;&#29305;&#24449;&#24037;&#31243;&#24037;&#20855;&#22312;TSC&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;TSC&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;TSC&#36807;&#31243;&#26469;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;TSC&#31639;&#27861;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification (TSC) has received much attention in the past two decades and is still a crucial and challenging problem in data science and knowledge engineering. Indeed, along with the increasing availability of time series data, many TSC algorithms have been suggested by the research community in the literature. Besides state-of-the-art methods based on similarity measures, intervals, shapelets, dictionaries, deep learning methods or hybrid ensemble methods, several tools for extracting unsupervised informative summary statistics, aka features, from time series have been designed in the recent years. Originally designed for descriptive analysis and visualization of time series with informative and interpretable features, very few of these feature engineering tools have been benchmarked for TSC problems and compared with state-of-the-art TSC algorithms in terms of predictive performance. In this article, we aim at filling this gap and propose a simple TSC process to evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.01070</link><description>&lt;p&gt;
&#24403;&#35299;&#26512;&#24335;&#24494;&#31215;&#20998;&#30772;&#35299;AdaBoost&#23494;&#30721;&#26102;
&lt;/p&gt;
&lt;p&gt;
When Analytic Calculus Cracks AdaBoost Code. (arXiv:2308.01070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#23398;&#20064;&#20013;&#30340;&#22686;&#24378;&#21407;&#29702;&#28041;&#21450;&#23558;&#22810;&#20010;&#24369;&#20998;&#31867;&#22120;&#32452;&#21512;&#20197;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#30340;&#20998;&#31867;&#22120;&#12290;AdaBoost&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#23436;&#32654;&#20363;&#23376;&#12290;&#25105;&#20204;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;AdaBoost&#24182;&#19981;&#30495;&#27491;&#26159;&#19968;&#20010;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#20004;&#31867;&#38382;&#39064;&#26469;&#36827;&#34892;&#65292;&#20197;&#19977;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#29305;&#27530;&#24773;&#20917;&#20026;&#20363;&#65292;&#24182;&#19982;Python&#24211;scikit-learn&#20013;AdaBoost&#31639;&#27861;&#30340;&#23454;&#29616;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#20013;&#26816;&#27979;&#32452;&#32423;&#21035;&#30340;&#24322;&#24120;&#65292;&#24182;&#21033;&#29992;&#25299;&#25169;&#27169;&#24335;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01063</link><description>&lt;p&gt;
&#22312;&#32452;&#32423;&#21035;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#19968;&#31181;&#22686;&#24378;&#25299;&#25169;&#27169;&#24335;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach. (arXiv:2308.01063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#20013;&#26816;&#27979;&#32452;&#32423;&#21035;&#30340;&#24322;&#24120;&#65292;&#24182;&#21033;&#29992;&#25299;&#25169;&#27169;&#24335;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#37329;&#34701;&#23433;&#20840;&#21644;&#29983;&#29289;&#21270;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#20027;&#35201;&#20851;&#27880;&#21306;&#20998;&#22270;&#20013;&#30340;&#20010;&#20307;&#23454;&#20307;&#65288;&#33410;&#28857;&#25110;&#22270;&#65289;&#65292;&#24573;&#35270;&#20102;&#22270;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#32452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#21517;&#20026;&#32452;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;Gr-GAD&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#31181;&#26469;&#23450;&#20301;&#23646;&#20110;&#21487;&#33021;&#24322;&#24120;&#32452;&#30340;&#38170;&#23450;&#33410;&#28857;&#65292;&#36890;&#36807;&#25429;&#25417;&#38271;&#31243;&#19981;&#19968;&#33268;&#24615;&#12290;&#38543;&#21518;&#65292;&#32452;&#37319;&#26679;&#34987;&#29992;&#26469;&#37319;&#26679;&#20505;&#36873;&#32452;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#25299;&#25169;&#27169;&#24335;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;TPGCL&#65289;&#26041;&#27861;&#20013;&#12290;TPGCL&#21033;&#29992;&#32452;&#30340;&#25299;&#25169;&#27169;&#24335;&#20316;&#20026;&#32447;&#32034;&#20026;&#27599;&#20010;&#20505;&#36873;&#32452;&#29983;&#25104;&#23884;&#20837;&#65292;&#20174;&#32780;&#21306;&#20998;&#19981;&#21516;&#30340;&#24322;&#24120;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has achieved success and has been widely applied in various domains, such as fraud detection, cybersecurity, finance security, and biochemistry. However, existing graph anomaly detection algorithms focus on distinguishing individual entities (nodes or graphs) and overlook the possibility of anomalous groups within the graph. To address this limitation, this paper introduces a novel unsupervised framework for a new task called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that belong to potential anomaly groups by capturing long-range inconsistencies. Subsequently, group sampling is employed to sample candidate groups, which are then fed into the proposed Topology Pattern-based Graph Contrastive Learning (TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to generate embeddings for each candidate group and thus distinct anomaly groups. The ex
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01054</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01054
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#65288;SSNL&#65289;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#21487;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#26102;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;SSNL&#25311;&#21512;&#19968;&#20010;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#21253;&#25324;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#25110;&#21464;&#20998;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;SSNL&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#21253;&#21547;&#26080;&#20449;&#24687;&#25968;&#25454;&#32500;&#24230;&#25110;&#20301;&#20110;&#36739;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;SSNL&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#23427;&#36890;&#24120;&#20248;&#20110;&#22312;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#29616;&#20195;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#19968;&#39033;&#26469;&#33258;&#22825;&#20307;&#29289;&#29702;&#23398;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20363;&#23376;&#19978;&#23545;&#30913;&#22330;&#27169;&#22411;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19981;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#24179;&#22374;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#20998;&#24067;&#24635;&#36136;&#37327;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#25968;&#25454;&#20998;&#24067;&#20998;&#26512;&#12290;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#20004;&#20010;&#32473;&#23450;&#27979;&#24230;&#20043;&#38388;&#36317;&#31163;&#30340;&#26368;&#20339;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01039</link><description>&lt;p&gt;
&#35745;&#31639;&#19981;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163; - &#24179;&#22374;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Computing the Distance between unbalanced Distributions -- The flat Metric. (arXiv:2308.01039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19981;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#24179;&#22374;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#20998;&#24067;&#24635;&#36136;&#37327;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#25968;&#25454;&#20998;&#24067;&#20998;&#26512;&#12290;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#20004;&#20010;&#32473;&#23450;&#27979;&#24230;&#20043;&#38388;&#36317;&#31163;&#30340;&#26368;&#20339;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#20219;&#24847;&#32500;&#24230;&#35745;&#31639;&#24179;&#22374;&#24230;&#37327;&#30340;&#23454;&#29616;&#12290;&#24179;&#22374;&#24230;&#37327;&#65292;&#20063;&#31216;&#20026;&#21452;&#36793;&#30028;Lipschitz&#36317;&#31163;&#65292;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;Wasserstein&#36317;&#31163;W1&#25512;&#24191;&#21040;&#20102;&#20998;&#24067;&#24635;&#36136;&#37327;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;&#36825;&#23545;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#20998;&#26512;&#20013;&#65292;&#26679;&#26412;&#22823;&#23567;&#37325;&#35201;&#25110;&#32773;&#24402;&#19968;&#21270;&#19981;&#21487;&#33021;&#30340;&#24773;&#20917;&#20855;&#26377;&#29305;&#27530;&#30340;&#24847;&#20041;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#23450;&#23454;&#29616;&#20004;&#20010;&#32473;&#23450;&#27979;&#24230;&#20043;&#38388;&#36317;&#31163;&#30340;&#26368;&#20339;&#27979;&#35797;&#20989;&#25968;&#12290;&#25105;&#20204;&#29305;&#21035;&#27880;&#37325;&#23454;&#29616;&#20102;&#20174;&#29420;&#31435;&#35757;&#32451;&#30340;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#25104;&#23545;&#36317;&#31163;&#30340;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#35777;&#26126;&#20102;&#36755;&#20986;&#30340;&#36136;&#37327;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#26377;&#23454;&#38469;&#30495;&#20540;&#30340;&#23454;&#39564;&#20197;&#21450;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an implementation to compute the flat metric in any dimension. The flat metric, also called dual bounded Lipschitz distance, generalizes the well-known Wasserstein distance W1 to the case that the distributions are of unequal total mass. This is of particular interest for unbalanced optimal transport tasks and for the analysis of data distributions where the sample size is important or normalization is not possible. The core of the method is based on a neural network to determine on optimal test function realizing the distance between two given measures. Special focus was put on achieving comparability of pairwise computed distances from independently trained networks. We tested the quality of the output in several experiments where ground truth was available as well as with simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#32032;&#26469;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#20197;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#26368;&#21518;&#65292;&#24341;&#20837;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20197;&#21516;&#26102;&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01030</link><description>&lt;p&gt;
&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#30340;&#19977;&#20010;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Three Factors to Improve Out-of-Distribution Detection. (arXiv:2308.01030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#32032;&#26469;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#20197;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#26368;&#21518;&#65292;&#24341;&#20837;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20197;&#21516;&#26102;&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#20316;&#20026;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#65288;ACC&#65289;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65288;AUROC&#12289;FPR&#12289;AUPR&#65289;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#65288;i&#65289;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#21487;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#65288;ii&#65289;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#22312;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#24341;&#20837;&#25105;&#20204;&#30340;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for training can improve OOD detection performance with minimal impact on accuracy; (iii) The introduction of our novel supervised contrastive learning can simultaneously improve OOD detection performance and the accuracy of the network. By incorporating all three factors, our approach enhances both accuracy and OOD detection performance by addressing the trade-off between classification and OOD detection. Our method achieves improvements over previous approaches in both performance metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#23454;&#26102;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.01028</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximizing Success Rate of Payment Routing using Non-stationary Bandits. (arXiv:2308.01028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#23454;&#26102;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#37096;&#32626;&#65292;&#20197;&#26681;&#25454;&#26368;&#36817;&#30340;&#20132;&#26131;&#21382;&#21490;&#30830;&#23450;&#25509;&#36817;&#26368;&#20248;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23556;&#32447;&#30340;&#26032;&#22411;&#36335;&#30001;&#26381;&#21153;&#26550;&#26500;&#65292;&#36890;&#36807;&#26368;&#20248;&#30340;&#25193;&#23637;&#36172;&#21338;&#24335;&#25903;&#20184;&#36335;&#30001;&#26469;&#22788;&#29702;&#27599;&#31186;&#36229;&#36807;10000&#27425;&#30340;&#20132;&#26131;&#37327;&#65292;&#24182;&#31526;&#21512;Payment Card Industry Data Security Standard (PCI DSS) &#30340;&#31995;&#32479;&#35774;&#35745;&#35201;&#27714;&#21644;&#29983;&#24577;&#32422;&#26463;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#33258;&#23450;&#20041;&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;&#22810;&#20010;&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#23545;&#27604;&#22810;&#20010;&#38750;&#24179;&#31283;&#36172;&#21338;&#26041;&#27861;&#24182;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#34394;&#25311;&#20307;&#32946;&#24179;&#21488;Dream11&#30340;&#25903;&#20184;&#20132;&#26131;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#23454;&#26102;&#23454;&#39564;&#12290;&#22312;&#23454;&#26102;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#22987;&#32456;&#25552;&#39640;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>MDT3D&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24050;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#21463;&#30417;&#30563;&#30340;LiDAR&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#20256;&#24863;&#22120;&#37197;&#32622;&#30340;&#26032;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01000</link><description>&lt;p&gt;
MDT3D&#65306;LiDAR&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#27867;&#21270;&#30340;&#22810;&#25968;&#25454;&#38598;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization. (arXiv:2308.01000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01000
&lt;/p&gt;
&lt;p&gt;
MDT3D&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24050;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#21463;&#30417;&#30563;&#30340;LiDAR&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#20256;&#24863;&#22120;&#37197;&#32622;&#30340;&#26032;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#19968;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#21463;&#30417;&#30563;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#29615;&#22659;&#21644;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#29992;&#20110;&#24494;&#35843;&#25110;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#20351;&#29992;&#29305;&#23450;&#28857;&#20998;&#24067;&#30340;&#28304;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#26102;&#26174;&#31034;&#20986;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20915;&#23450;&#21033;&#29992;&#22810;&#20010;&#24050;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;MDT3D&#26041;&#27861;&#26469;&#22686;&#21152;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#20256;&#24863;&#22120;&#37197;&#32622;&#30340;&#26032;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26631;&#27880;&#24046;&#36317;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#31895;&#26631;&#31614;&#30340;&#26032;&#26631;&#31614;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#25968;&#25454;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00994</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65306;&#22522;&#20110;&#25968;&#25454;&#35282;&#24230;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#29983;&#27963;&#22312;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#28023;&#27915;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#19981;&#20363;&#22806;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#34920;&#29616;&#20986;&#19968;&#31181;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#29616;&#35937;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#24102;&#26469;&#20102;&#39118;&#38505;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#21518;&#26524;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#37492;&#20110;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#20043;&#21069;&#20351;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#31639;&#27861;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;&#36825;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#22312;CIFAR100-LT&#12289;ImageNet100-LT&#12289;UTKFace&#21644;Waterbird&#31561;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#24182;&#19981;&#22768;&#31216;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#38382;&#39064;&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26080;&#20851;&#27491;&#21017;&#21270;&#22120;WDER&#65292;&#36890;&#36807;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WDER&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00989</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#22810;&#26679;&#24615;&#22686;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26080;&#20851;&#27491;&#21017;&#21270;&#22120;WDER&#65292;&#36890;&#36807;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WDER&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23558;&#19981;&#21516;&#23618;&#27425;&#30340;&#23376;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#33258;&#21160;&#21457;&#29616;&#23376;&#31574;&#30053;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#29983;&#25104;&#23376;&#31574;&#30053;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#23545;&#22810;&#26679;&#24615;&#30340;&#32771;&#34385;&#25110;&#20351;&#29992;&#24369;&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#31216;&#20026;Wasserstein&#22810;&#26679;&#24615;&#22686;&#24378;&#27491;&#21017;&#21270;&#22120;&#65288;WDER&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21160;&#20316;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#26469;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;WDER&#21487;&#20197;&#36731;&#26494;&#22320;&#34701;&#20837;&#21040;&#29616;&#26377;&#26041;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19981;&#20462;&#25913;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;WDER&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#20102;WDER&#30340;&#36866;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.00978</link><description>&lt;p&gt;
&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Certified Multi-Fidelity Zeroth-Order Optimization. (arXiv:2308.00978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36817;&#20284;&#27700;&#24179;&#65288;&#20195;&#20215;&#19981;&#21516;&#65289;&#19978;&#35780;&#20272;&#20989;&#25968;$f$&#65292;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#20302;&#30340;&#20195;&#20215;&#20248;&#21270;$f$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\emph{&#35748;&#35777;}&#31639;&#27861;&#65292;&#23427;&#20204;&#39069;&#22806;&#35201;&#27714;&#36755;&#20986;&#19968;&#20010;&#23545;&#20248;&#21270;&#35823;&#24046;&#30340;&#25968;&#25454;&#39537;&#21160;&#19978;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#20197;&#31639;&#27861;&#21644;&#35780;&#20272;&#29615;&#22659;&#20043;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#24418;&#24335;&#26469;&#24418;&#24335;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#22312;&#20219;&#24847;Lipschitz&#20989;&#25968;$f$&#19978;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;$f$&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#31034;&#20363;&#35299;&#20915;&#20102;&#26377;&#22122;&#22768;&#65288;&#38543;&#26426;&#65289;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#31181;&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20113;&#20013;&#33258;&#20027;&#21644;&#20445;&#23494;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#38598;&#25104;&#21516;&#24577;&#21152;&#23494;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#20445;&#25252;&#33258;&#20027;&#35745;&#31639;&#30340;&#26426;&#23494;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20113;&#20013;&#36827;&#34892;&#25345;&#32493;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#31649;&#29702;&#65292;&#32780;&#19981;&#38656;&#29992;&#25143;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2308.00963</link><description>&lt;p&gt;
&#23558;&#21516;&#24577;&#21152;&#23494;&#21644;&#21487;&#20449;&#25191;&#34892;&#25216;&#26415;&#38598;&#25104;&#21040;&#20113;&#20013;&#65292;&#23454;&#29616;&#33258;&#20027;&#21644;&#20445;&#23494;&#30340;&#27169;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud. (arXiv:2308.00963v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#31181;&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20113;&#20013;&#33258;&#20027;&#21644;&#20445;&#23494;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#38598;&#25104;&#21516;&#24577;&#21152;&#23494;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#20445;&#25252;&#33258;&#20027;&#35745;&#31639;&#30340;&#26426;&#23494;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20113;&#20013;&#36827;&#34892;&#25345;&#32493;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#31649;&#29702;&#65292;&#32780;&#19981;&#38656;&#29992;&#25143;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26222;&#21450;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65288;&#21253;&#25324;&#27169;&#22411;&#35757;&#32451;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#29702;&#65289;&#22806;&#21253;&#32473;&#20113;&#25104;&#20026;&#19968;&#31181;&#36235;&#21183;&#12290;&#36890;&#36807;&#22806;&#21253;&#65292;&#38500;&#20102;&#21033;&#29992;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25552;&#20379;&#30340;&#24191;&#27867;&#21644;&#21487;&#25193;&#23637;&#30340;&#36164;&#28304;&#65292;&#22914;&#26524;&#20113;&#26381;&#21153;&#22120;&#33021;&#22815;&#20195;&#34920;&#29992;&#25143;&#33258;&#20027;&#31649;&#29702;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#37027;&#20040;&#23545;&#29992;&#25143;&#26469;&#35828;&#20063;&#20250;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#39044;&#26399;&#26159;&#19968;&#20010;&#38271;&#26399;&#25345;&#32493;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#29992;&#25143;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#21442;&#19982;&#26102;&#65292;&#36825;&#20010;&#29305;&#24615;&#23588;&#20854;&#31361;&#20986;&#12290;&#30001;&#20110;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#32771;&#34385;&#65292;&#24076;&#26395;&#33258;&#20027;&#23398;&#20064;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#26041;&#26696;&#65292;&#23454;&#29616;&#22312;&#20113;&#20013;&#33258;&#20027;&#21644;&#20445;&#23494;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#21516;&#24577;&#21152;&#23494;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#25216;&#26415;&#21487;&#20197;&#20445;&#25252;&#33258;&#20027;&#35745;&#31639;&#30340;&#26426;&#23494;&#24615;&#65292;&#20294;&#26159;...
&lt;/p&gt;
&lt;p&gt;
With the popularity of cloud computing and machine learning, it has been a trend to outsource machine learning processes (including model training and model-based inference) to cloud. By the outsourcing, other than utilizing the extensive and scalable resource offered by the cloud service provider, it will also be attractive to users if the cloud servers can manage the machine learning processes autonomously on behalf of the users. Such a feature will be especially salient when the machine learning is expected to be a long-term continuous process and the users are not always available to participate. Due to security and privacy concerns, it is also desired that the autonomous learning preserves the confidentiality of users' data and models involved. Hence, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in cloud. Homomorphic encryption and trusted execution environment technology can protect confidentiality for autonomous computation, bu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00957</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;(&#20998;&#32452;)&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference with Differentially Private (Clustered) Outcomes. (arXiv:2308.00957v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#23454;&#39564;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#21482;&#26377;&#22312;&#21442;&#19982;&#32773;&#21516;&#24847;&#36879;&#38706;&#20182;&#20204;&#21487;&#33021;&#25935;&#24863;&#30340;&#21709;&#24212;&#26102;&#25165;&#21487;&#34892;&#12290;&#22312;&#30830;&#20445;&#38544;&#31169;&#30340;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#38544;&#31169;&#20445;&#35777;&#24230;&#37327;&#65292;&#21487;&#20197;&#40723;&#21169;&#21442;&#19982;&#32773;&#20998;&#20139;&#21709;&#24212;&#32780;&#19981;&#20250;&#38754;&#20020;&#21435;&#21311;&#21517;&#21270;&#30340;&#39118;&#38505;&#12290;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#20250;&#21521;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#38899;&#26469;&#23454;&#29616;&#36825;&#31181;&#38544;&#31169;&#20445;&#35777;&#65292;&#36825;&#20250;&#22686;&#21152;&#22823;&#22810;&#25968;&#32479;&#35745;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#31934;&#30830;&#27979;&#37327;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#22256;&#38590;&#65306;&#20174;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#23384;&#22312;&#30528;&#22266;&#26377;&#30340;&#38544;&#31169;-&#26041;&#24046;&#26435;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#30340;&#36739;&#20302;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#21033;&#29992;&#25968;&#25454;&#30340;&#20219;&#20309;&#32473;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#21516;&#26102;&#20173;&#28982;&#20801;&#35768;&#23545;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26263;&#22495;&#20013;&#30340;&#35838;&#31243;&#24341;&#23548;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#36880;&#27493;&#23558;&#40657;&#30418;&#23376;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#65292;&#21516;&#26102;&#21033;&#29992;&#24178;&#20928;/&#22122;&#22768;&#25968;&#25454;&#20998;&#21106;&#30340;&#26131;&#38590;&#23398;&#20064;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#38454;&#27573;&#21644;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.00956</link><description>&lt;p&gt;
&#26263;&#22495;&#20013;&#30340;&#35838;&#31243;&#24341;&#23548;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Curriculum Guided Domain Adaptation in the Dark. (arXiv:2308.00956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26263;&#22495;&#20013;&#30340;&#35838;&#31243;&#24341;&#23548;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#36880;&#27493;&#23558;&#40657;&#30418;&#23376;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#65292;&#21516;&#26102;&#21033;&#29992;&#24178;&#20928;/&#22122;&#22768;&#25968;&#25454;&#20998;&#21106;&#30340;&#26131;&#38590;&#23398;&#20064;&#29305;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#38454;&#27573;&#21644;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#26263;&#22495;&#20013;&#30340;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#23558;&#40657;&#30418;&#23376;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20219;&#20309;&#28304;&#25968;&#25454;&#25110;&#28304;&#27169;&#22411;&#21442;&#25968;&#12290;&#24403;&#21069;&#26041;&#27861;&#23558;&#20174;&#28304;&#27169;&#22411;&#24471;&#21040;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#30340;&#22122;&#22768;&#39044;&#27979;&#36716;&#21270;&#20026;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#20351;&#29992;&#20256;&#32479;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#36866;&#24212;&#20043;&#21069;&#23558;&#30446;&#26631;&#26679;&#26412;&#20998;&#20026;&#24178;&#20928;&#21644;&#22122;&#22768;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#24178;&#20928;/&#22122;&#22768;&#25968;&#25454;&#20998;&#21106;&#30340;&#26131;&#38590;&#23398;&#20064;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#19981;&#26159;&#31471;&#21040;&#31471;&#30340;&#65292;&#38656;&#35201;&#21333;&#29420;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#21021;&#22987;&#28909;&#36523;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35838;&#31243;&#40657;&#30418;&#36866;&#24212;&#65288;CABB&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#36880;&#27493;&#24341;&#23548;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#36880;&#27493;&#22320;&#36866;&#24212;&#30446;&#26631;&#22495;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the source model to the target model, and/or separate clean/noisy target samples before adapting using traditional noisy label learning algorithms. However, these methods do not utilize the easy-to-hard learning nature of the clean/noisy data splits. Also, none of the existing methods are end-to-end, and require a separate fine-tuning stage and an initial warmup stage. In this work, we present Curriculum Adaptation for Black-Box (CABB) which provides a curriculum guided adaptation approach to gradually 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00951</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#21040;&#36719;&#24615;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#19987;&#23478;&#27169;&#22411;(MoEs)&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;MoEs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#20002;&#22833;&#26631;&#35760;&#12289;&#26080;&#27861;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#25110;&#26080;&#25928;&#30340;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Soft MoE&#65292;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;Transformer&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;MoEs&#30340;&#20248;&#28857;&#12290;Soft MoE&#36890;&#36807;&#21521;&#27599;&#20010;&#19987;&#23478;&#20256;&#36882;&#25152;&#26377;&#36755;&#20837;&#26631;&#35760;&#30340;&#19981;&#21516;&#21152;&#26435;&#32452;&#21512;&#26469;&#25191;&#34892;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#12290;&#19982;&#20854;&#20182;MoE&#20316;&#21697;&#19968;&#26679;&#65292;Soft MoE&#20013;&#30340;&#19987;&#23478;&#21482;&#22788;&#29702;&#19968;&#37096;&#20998;&#65288;&#32452;&#21512;&#30340;&#65289;&#26631;&#35760;&#65292;&#20197;&#22312;&#36739;&#20302;&#30340;&#25512;&#29702;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#65292;Soft MoE&#22312;&#26631;&#20934;Transformer&#65288;ViTs&#65289;&#21644;&#27969;&#34892;&#30340;MoE&#21464;&#20307;&#65288;Tokens Choice&#21644;Experts Choice&#65289;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;Soft MoE-Base/16&#30340;&#25512;&#29702;&#25104;&#26412;&#27604;ViT-Huge/14&#20302;10.5&#20493;&#65288;&#22681;&#38047;&#26102;&#38388;&#38477;&#20302;&#20102;5.7&#20493;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DC-Net&#30340;&#20998;&#35299;&#32806;&#21512;&#32593;&#32476;&#26469;&#24212;&#23545;&#36229;&#22768;&#22270;&#20687;&#20013;&#20934;&#30830;&#30149;&#21464;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#32806;&#21512;&#23376;&#32593;&#32476;&#20197;&#21450;&#26174;&#33879;&#24615;&#20808;&#39564;&#34701;&#21512;&#36827;&#34892;&#22788;&#29702;&#65292;&#21253;&#25324;&#21306;&#22495;&#29305;&#24449;&#32858;&#21512;&#21644;&#20851;&#31995;&#24863;&#30693;&#34920;&#31034;&#34701;&#21512;&#31561;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00947</link><description>&lt;p&gt;
&#23545;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30149;&#21464;&#20998;&#21106;&#36827;&#34892;&#20998;&#35299;&#21644;&#32806;&#21512;&#30340;&#26174;&#33879;&#24615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images. (arXiv:2308.00947v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DC-Net&#30340;&#20998;&#35299;&#32806;&#21512;&#32593;&#32476;&#26469;&#24212;&#23545;&#36229;&#22768;&#22270;&#20687;&#20013;&#20934;&#30830;&#30149;&#21464;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#32806;&#21512;&#23376;&#32593;&#32476;&#20197;&#21450;&#26174;&#33879;&#24615;&#20808;&#39564;&#34701;&#21512;&#36827;&#34892;&#22788;&#29702;&#65292;&#21253;&#25324;&#21306;&#22495;&#29305;&#24449;&#32858;&#21512;&#21644;&#20851;&#31995;&#24863;&#30693;&#34920;&#31034;&#34701;&#21512;&#31561;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#22270;&#20687;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#30456;&#37051;&#32452;&#32455;&#65288;&#21363;&#32972;&#26223;&#65289;&#19982;&#30149;&#21464;&#21306;&#22495;&#65288;&#21363;&#21069;&#26223;&#65289;&#20855;&#26377;&#30456;&#20284;&#30340;&#24378;&#24230;&#29978;&#33267;&#21253;&#21547;&#26356;&#20016;&#23500;&#30340;&#32441;&#29702;&#27169;&#24335;&#65292;&#36825;&#32473;&#20934;&#30830;&#30340;&#30149;&#21464;&#20998;&#21106;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;DC-Net&#30340;&#20998;&#35299;&#32806;&#21512;&#32593;&#32476;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20998;&#31163;-&#34701;&#21512;&#65288;&#21069;&#26223;-&#32972;&#26223;&#65289;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;DC-Net&#30001;&#20998;&#35299;&#21644;&#32806;&#21512;&#23376;&#32593;&#32476;&#32452;&#25104;&#65292;&#21069;&#32773;&#23558;&#21407;&#22987;&#22270;&#20687;&#21021;&#27493;&#20998;&#35299;&#20026;&#21069;&#26223;&#21644;&#32972;&#26223;&#26174;&#33879;&#24615;&#22270;&#65292;&#28982;&#21518;&#22312;&#26174;&#33879;&#24615;&#20808;&#39564;&#34701;&#21512;&#30340;&#36741;&#21161;&#19979;&#65292;&#30001;&#21518;&#32773;&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#32806;&#21512;&#23376;&#32593;&#32476;&#28041;&#21450;&#19977;&#20010;&#34701;&#21512;&#31574;&#30053;&#26041;&#38754;&#65306;1&#65289;&#21306;&#22495;&#29305;&#24449;&#32858;&#21512;&#65288;&#36890;&#36807;&#32534;&#30721;&#22120;&#20013;&#30340;&#21487;&#24494;&#20998;&#19978;&#19979;&#25991;&#27744;&#21270;&#31639;&#23376;&#65289;&#20197;&#33258;&#36866;&#24212;&#22320;&#20445;&#30041;&#20855;&#26377;&#26356;&#22823;&#24863;&#21463;&#37326;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#32454;&#33410;&#36827;&#34892;&#23610;&#23544;&#38477;&#20302;&#65307;2&#65289;&#20851;&#31995;&#24863;&#30693;&#34920;&#31034;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex scenario of ultrasound image, in which adjacent tissues (i.e., background) share similar intensity with and even contain richer texture patterns than lesion region (i.e., foreground), brings a unique challenge for accurate lesion segmentation. This work presents a decomposition-coupling network, called DC-Net, to deal with this challenge in a (foreground-background) saliency map disentanglement-fusion manner. The DC-Net consists of decomposition and coupling subnets, and the former preliminarily disentangles original image into foreground and background saliency maps, followed by the latter for accurate segmentation under the assistance of saliency prior fusion. The coupling subnet involves three aspects of fusion strategies, including: 1) regional feature aggregation (via differentiable context pooling operator in the encoder) to adaptively preserve local contextual details with the larger receptive field during dimension reduction; 2) relation-aware representation fusion (via
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#39044;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#19977;&#20010;&#38454;&#27573;&#30340;&#25903;&#25345;&#20197;&#21450;&#22312;&#30456;&#20301;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#24635;&#32467;&#20102;DL&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#26395;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;DL&#25552;&#39640;&#30456;&#20301;&#24674;&#22797;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00942</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the use of deep learning for phase recovery. (arXiv:2308.00942v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#39044;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#19977;&#20010;&#38454;&#27573;&#30340;&#25903;&#25345;&#20197;&#21450;&#22312;&#30456;&#20301;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#24635;&#32467;&#20102;DL&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#26395;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;DL&#25552;&#39640;&#30456;&#20301;&#24674;&#22797;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#26159;&#25351;&#20174;&#20809;&#24378;&#27979;&#37327;&#20540;&#20013;&#35745;&#31639;&#20986;&#20809;&#22330;&#30340;&#30456;&#20301;&#12290;&#20174;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#12289;&#30456;&#24178;&#34893;&#23556;&#25104;&#20687;&#21040;&#33258;&#36866;&#24212;&#20809;&#23398;&#65292;&#30456;&#20301;&#24674;&#22797;&#23545;&#20110;&#37325;&#24314;&#29289;&#20307;&#30340;&#25240;&#23556;&#29575;&#20998;&#24067;&#25110;&#22320;&#24418;&#20197;&#21450;&#26657;&#27491;&#25104;&#20687;&#31995;&#32479;&#30340;&#20687;&#24046;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#20026;&#35745;&#31639;&#25104;&#20687;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25903;&#25345;&#65292;&#20026;&#21508;&#31181;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20256;&#32479;&#30340;&#30456;&#20301;&#24674;&#22797;&#26041;&#27861;&#65292;&#28982;&#21518;&#20174;&#39044;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#19977;&#20010;&#38454;&#27573;&#20171;&#32461;&#20102;DL&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;DL&#22312;&#30456;&#20301;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;DL&#22312;&#30456;&#20301;&#24674;&#22797;&#20013;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#26395;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;DL&#25552;&#39640;&#30456;&#20301;&#24674;&#22797;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase recovery (PR) refers to calculating the phase of the light field from its intensity measurements. As exemplified from quantitative phase imaging and coherent diffraction imaging to adaptive optics, PR is essential for reconstructing the refractive index distribution or topography of an object and correcting the aberration of an imaging system. In recent years, deep learning (DL), often implemented through deep neural networks, has provided unprecedented support for computational imaging, leading to more efficient solutions for various PR problems. In this review, we first briefly introduce conventional methods for PR. Then, we review how DL provides support for PR from the following three stages, namely, pre-processing, in-processing, and post-processing. We also review how DL is used in phase image processing. Finally, we summarize the work in DL for PR and outlook on how to better use DL to improve the reliability and efficiency in PR. Furthermore, we present a live-updating re
&lt;/p&gt;</description></item><item><title>QUANT&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26497;&#31616;&#21306;&#38388;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#30340;&#29305;&#24449;&#65288;&#20998;&#20301;&#25968;&#65289;&#12289;&#22266;&#23450;&#21306;&#38388;&#21644;&#19968;&#20010;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#29616;&#26377;&#26368;&#20934;&#30830;&#30340;&#21306;&#38388;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;UCR&#23384;&#26723;&#30340;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#21333;&#20010;CPU&#26680;&#24515;&#30340;&#24635;&#35745;&#31639;&#26102;&#38388;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#65289;&#19981;&#36229;&#36807;15&#20998;&#38047;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00928</link><description>&lt;p&gt;
QUANT&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26497;&#31616;&#21306;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QUANT: A Minimalist Interval Method for Time Series Classification. (arXiv:2308.00928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00928
&lt;/p&gt;
&lt;p&gt;
QUANT&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#26497;&#31616;&#21306;&#38388;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#30340;&#29305;&#24449;&#65288;&#20998;&#20301;&#25968;&#65289;&#12289;&#22266;&#23450;&#21306;&#38388;&#21644;&#19968;&#20010;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#65292;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#29616;&#26377;&#26368;&#20934;&#30830;&#30340;&#21306;&#38388;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;UCR&#23384;&#26723;&#30340;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#21333;&#20010;CPU&#26680;&#24515;&#30340;&#24635;&#35745;&#31639;&#26102;&#38388;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#65289;&#19981;&#36229;&#36807;15&#20998;&#38047;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20351;&#29992;&#19968;&#31181;&#29305;&#24449;&#31867;&#22411;&#65288;&#20998;&#20301;&#25968;&#65289;&#12289;&#22266;&#23450;&#21306;&#38388;&#21644;&#19968;&#20010;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26368;&#20934;&#30830;&#30340;&#21306;&#38388;&#26041;&#27861;&#24179;&#22343;&#20934;&#30830;&#29575;&#30456;&#21516;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#21306;&#38388;&#26041;&#27861;&#30340;&#31934;&#31616;&#34920;&#31034;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;UCR&#23384;&#26723;&#30340;&#25193;&#23637;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;CPU&#26680;&#24515;&#30340;&#24635;&#35745;&#31639;&#26102;&#38388;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#65289;&#19981;&#36229;&#36807;15&#20998;&#38047;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that it is possible to achieve the same accuracy, on average, as the most accurate existing interval methods for time series classification on a standard set of benchmark datasets using a single type of feature (quantiles), fixed intervals, and an 'off the shelf' classifier. This distillation of interval-based approaches represents a fast and accurate method for time series classification, achieving state-of-the-art accuracy on the expanded set of 142 datasets in the UCR archive with a total compute time (training and inference) of less than 15 minutes using a single CPU core.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#27169;&#22411;&#22312;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#21512;&#25104;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#27169;&#25311;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.00924</link><description>&lt;p&gt;
&#19981;&#26029;&#36866;&#24212;&#38470;&#22320;&#24433;&#20687;&#20013;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#27169;&#22411;&#22312;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20316;&#32773;&#21512;&#25104;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#27169;&#25311;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#20943;&#23569;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65288;&#28304;&#22495;&#65289;&#19982;&#20351;&#29992;&#25968;&#25454;&#65288;&#30446;&#26631;&#22495;&#65289;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#24403;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#31354;&#20013;&#24179;&#21488;&#19978;&#26102;&#65292;&#23427;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#36935;&#21040;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#35780;&#20272;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#25193;&#22823;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#29616;&#26377;&#30340;&#31354;&#20013;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20174;&#23454;&#38469;&#22270;&#20687;&#20013;&#21512;&#25104;&#20102;&#20004;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#29983;&#25104;&#20102;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#36830;&#32493;&#25110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65306;&#19968;&#20010;&#22522;&#32447;&#26631;&#20934;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#21644;&#20004;&#20010;&#36830;&#32493;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#27599;&#27425;&#21482;&#33021;&#35775;&#38382;&#19968;&#23567;&#37096;&#20998;&#25110;&#19968;&#20010;&#25209;&#27425;&#30340;&#30446;&#26631;&#25968;&#25454;&#65292;&#24182;&#19988;&#36866;&#24212;&#21482;&#21457;&#29983;&#22312;&#19968;&#27425;&#25968;&#25454;&#36845;&#20195;&#20013;&#12290;&#19981;&#26029;&#36866;&#24212;&#30340;&#32422;&#26463;&#21644;&#36880;&#28176;&#24694;&#21270;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#32467;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#38024;&#23545;&#26410;&#26631;&#35760;&#23608;&#20307;&#35299;&#21078;&#32452;&#32455;&#36827;&#34892;&#34394;&#25311;&#26579;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#33258;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#19982;&#20256;&#32479;&#26579;&#33394;&#26679;&#26412;&#30456;&#21305;&#37197;&#30340;&#20142;&#22330;&#22270;&#20687;&#65292;&#21435;&#38500;&#20102;&#33258;&#28342;&#24341;&#36215;&#30340;&#20005;&#37325;&#26579;&#33394;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2308.00920</link><description>&lt;p&gt;
&#26410;&#26631;&#35760;&#23608;&#20307;&#35299;&#21078;&#32452;&#32455;&#30340;&#34394;&#25311;&#32452;&#32455;&#23398;&#26579;&#33394;
&lt;/p&gt;
&lt;p&gt;
Virtual histological staining of unlabeled autopsy tissue. (arXiv:2308.00920v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#38024;&#23545;&#26410;&#26631;&#35760;&#23608;&#20307;&#35299;&#21078;&#32452;&#32455;&#36827;&#34892;&#34394;&#25311;&#26579;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#33258;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#19982;&#20256;&#32479;&#26579;&#33394;&#26679;&#26412;&#30456;&#21305;&#37197;&#30340;&#20142;&#22330;&#22270;&#20687;&#65292;&#21435;&#38500;&#20102;&#33258;&#28342;&#24341;&#36215;&#30340;&#20005;&#37325;&#26579;&#33394;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#23398;&#26816;&#26597;&#26159;&#23608;&#20307;&#35299;&#21078;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32452;&#32455;&#21270;&#23398;&#26579;&#33394;&#26041;&#27861;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22240;&#23608;&#20307;&#32452;&#32455;&#24310;&#36831;&#22266;&#23450;&#24341;&#36215;&#30340;&#33258;&#28342;&#25152;&#33268;&#26579;&#33394;&#36136;&#37327;&#36739;&#24046;&#65292;&#20197;&#21450;&#35206;&#30422;&#22823;&#38754;&#31215;&#32452;&#32455;&#25152;&#38656;&#30340;&#21270;&#23398;&#26579;&#33394;&#31243;&#24207;&#36164;&#28304;&#23494;&#38598;&#22411;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#12289;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#22312;&#20840;&#29699;&#21355;&#29983;&#21361;&#26426;&#20013;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#26381;&#21153;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#20250;&#26356;&#21152;&#26126;&#26174;&#65292;&#23548;&#33268;&#32452;&#32455;&#22266;&#23450;&#30340;&#36827;&#19968;&#27493;&#24310;&#36831;&#21644;&#26356;&#20005;&#37325;&#30340;&#26579;&#33394;&#20266;&#24433;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#23545;&#23608;&#20307;&#35299;&#21078;&#32452;&#32455;&#30340;&#34394;&#25311;&#26579;&#33394;&#65292;&#24182;&#34920;&#26126;&#35757;&#32451;&#26377;&#32032;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#23558;&#26080;&#26631;&#35760;&#23608;&#20307;&#35299;&#21078;&#32452;&#32455;&#20999;&#29255;&#30340;&#33258;&#33639;&#20809;&#22270;&#20687;&#36716;&#25442;&#20026;&#19982;&#21516;&#19968;&#26679;&#26412;&#30340;&#34880;&#32418;&#32032;&#21644;&#21994;&#26579;&#33394;&#65288;H&#65286;E&#65289;&#30456;&#21305;&#37197;&#30340;&#20142;&#22330;&#31561;&#25928;&#22270;&#20687;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#33258;&#28342;&#23548;&#33268;&#30340;&#20005;&#37325;&#26579;&#33394;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histological examination is a crucial step in an autopsy; however, the traditional histochemical staining of post-mortem samples faces multiple challenges, including the inferior staining quality due to autolysis caused by delayed fixation of cadaver tissue, as well as the resource-intensive nature of chemical staining procedures covering large tissue areas, which demand substantial labor, cost, and time. These challenges can become more pronounced during global health crises when the availability of histopathology services is limited, resulting in further delays in tissue fixation and more severe staining artifacts. Here, we report the first demonstration of virtual staining of autopsy tissue and show that a trained neural network can rapidly transform autofluorescence images of label-free autopsy tissue sections into brightfield equivalent images that match hematoxylin and eosin (H&amp;E) stained versions of the same samples, eliminating autolysis-induced severe staining artifacts inhere
&lt;/p&gt;</description></item><item><title>VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00904</link><description>&lt;p&gt;
VLUCI: &#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00904
&lt;/p&gt;
&lt;p&gt;
VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32463;&#27982;&#23398;&#31561;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#21435;&#28151;&#28102;&#21644;&#21453;&#20107;&#23454;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;&#20294;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#25197;&#26354;&#20102;&#22240;&#26524;&#25512;&#26029;&#24182;&#24433;&#21709;&#20102;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#65288;VLUCI&#65289;&#65292;&#23427;&#29983;&#25104;&#20102;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;VLUCI&#25918;&#26494;&#20102;&#22823;&#22810;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#30340;&#26080;&#28151;&#28102;&#20551;&#35774;&#12290;&#36890;&#36807;&#35299;&#32806;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#21644;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;VLUCI&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21464;&#37327;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#25143;&#21487;&#25511;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#20107;&#21518;&#21644;&#20107;&#21069;&#35299;&#37322;&#65292;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#35299;&#37322;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#20174;&#32780;&#23450;&#21046;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#28385;&#24847;&#24230;&#21644;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.00894</link><description>&lt;p&gt;
&#29992;&#25143;&#21487;&#25511;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#36890;&#36807;&#20107;&#21518;&#21644;&#20107;&#21069;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations. (arXiv:2308.00894v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#25143;&#21487;&#25511;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#20107;&#21518;&#21644;&#20107;&#21069;&#35299;&#37322;&#65292;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#35299;&#37322;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#20174;&#32780;&#23450;&#21046;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#28385;&#24847;&#24230;&#21644;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#26469;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#32570;&#20047;&#29992;&#25143;&#21487;&#25511;&#24615;&#65292;&#23548;&#33268;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25552;&#39640;&#29992;&#25143;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#25143;&#21487;&#25511;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#26080;&#32541;&#38598;&#25104;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#25552;&#20379;&#20107;&#21518;&#21644;&#20107;&#21069;&#35299;&#37322;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#19982;&#36825;&#20123;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#26469;&#23450;&#21046;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#20010;&#21487;&#25511;&#24615;&#23646;&#24615;&#65306;&#21487;&#25511;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;MovieLens&#21644;Yelp&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems utilize users' historical behaviors to generate personalized recommendations. However, these systems often lack user controllability, leading to diminished user satisfaction and trust in the systems. Acknowledging the recent advancements in explainable recommender systems that enhance users' understanding of recommendation mechanisms, we propose leveraging these advancements to improve user controllability. In this paper, we present a user-controllable recommender system that seamlessly integrates explainability and controllability within a unified framework. By providing both retrospective and prospective explanations through counterfactual reasoning, users can customize their control over the system by interacting with these explanations.  Furthermore, we introduce and assess two attributes of controllability in recommendation systems: the complexity of controllability and the accuracy of controllability. Experimental evaluations on MovieLens and Yelp datas
&lt;/p&gt;</description></item><item><title>Tango&#26159;&#19968;&#20010;&#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35268;&#21017;&#26469;&#20445;&#25345;&#20934;&#30830;&#24230;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.00890</link><description>&lt;p&gt;
Tango: &#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00890
&lt;/p&gt;
&lt;p&gt;
Tango&#26159;&#19968;&#20010;&#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35268;&#21017;&#26469;&#20445;&#25345;&#20934;&#30830;&#24230;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22312;&#20851;&#38190;&#30340;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#37327;&#21270;&#34987;&#24191;&#27867;&#29992;&#20110;&#21152;&#36895;GNN&#35745;&#31639;&#65292;&#20294;&#37327;&#21270;&#35757;&#32451;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#37327;&#21270;GNN&#35757;&#32451;&#31995;&#32479;&#24448;&#24448;&#27604;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#26377;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21407;&#22240;&#26377;&#20108;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#20934;&#30830;&#24230;&#38382;&#39064;&#23548;&#33268;&#20102;&#36807;&#22810;&#30340;&#24320;&#38144;&#65292;&#65288;&#20108;&#65289;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#25152;&#23637;&#31034;&#30340;&#20248;&#21270;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Tango&#65292;&#23427;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;GPU&#19978;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#25928;&#30340;&#35268;&#21017;&#26469;&#22312;&#37327;&#21270;GNN&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#20934;&#30830;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Tango&#19982;&#27969;&#34892;&#30340;Deep Graph Library&#65288;DGL&#65289;&#31995;&#32479;&#38598;&#25104;&#65292;&#24182;&#23637;&#31034;&#20854;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its
&lt;/p&gt;</description></item><item><title>&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FGNN&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#25429;&#25417;&#39640;&#38454;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20855;&#26377;&#27604;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00887</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Factor Graph Neural Networks. (arXiv:2308.00887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00887
&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FGNN&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#25429;&#25417;&#39640;&#38454;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#21644;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20855;&#26377;&#27604;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#28608;&#22686;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#25670;&#33073;&#20102;PGM&#30340;&#26576;&#20123;&#38480;&#21046;&#12290;GNN&#26088;&#22312;&#25552;&#20379;&#34920;&#36798;&#23398;&#20064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#36793;&#38469;&#25110;&#26368;&#21487;&#33021;&#30340;&#37197;&#32622;&#65292;&#22240;&#27492;&#22312;&#20449;&#24687;&#27969;&#35268;&#21017;&#30340;&#36873;&#25321;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#24102;&#26469;&#28789;&#24863;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21644;&#23398;&#20064;&#21464;&#37327;/&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#25805;&#20316;&#22312;k&#20803;&#33410;&#28857;&#19978;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#39640;&#38454;GNN&#38656;&#35201;&#22686;&#21152;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#22788;&#29702;&#39640;&#38454;&#24352;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FGNN&#65289;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#39640;&#38454;&#20851;&#31995;&#20197;&#36827;&#34892;&#25512;&#29702;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approxima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#26102;&#20934;&#30830;&#35780;&#20998;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#38598;&#23454;&#26102;&#30140;&#30171;&#35780;&#20998;&#21644;&#20869;&#32986;&#23618;&#27963;&#21160;&#25968;&#25454;&#65292;&#22312;&#30140;&#30171;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00886</link><description>&lt;p&gt;
&#29992;&#36830;&#32493;&#20250;&#35805;&#20013;&#30340;&#23454;&#26102;&#20934;&#30830;&#35780;&#20998;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#65306;&#22522;&#20110;&#23458;&#35266;&#39592;&#39612;&#32908;&#30140;&#30171;&#24378;&#24230;&#39044;&#27979;&#30340;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction. (arXiv:2308.00886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#26102;&#20934;&#30830;&#35780;&#20998;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#38598;&#23454;&#26102;&#30140;&#30171;&#35780;&#20998;&#21644;&#20869;&#32986;&#23618;&#27963;&#21160;&#25968;&#25454;&#65292;&#22312;&#30140;&#30171;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#35757;&#32451;&#20102;&#20027;&#35266;&#33258;&#25105;&#25253;&#21578;&#35780;&#20998;&#21518;&#65292;&#30001;&#20110;&#23454;&#26102;&#30140;&#30171;&#20307;&#39564;&#21644;&#21518;&#32493;&#35760;&#24405;&#24471;&#20998;&#20043;&#38388;&#30340;&#26174;&#33879;&#21464;&#24322;&#65292;&#24456;&#38590;&#20934;&#30830;&#22320;&#23458;&#35266;&#20998;&#31867;&#30140;&#30171;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#35774;&#22791;&#65292;&#29992;&#20110;&#33719;&#21462;&#23454;&#26102;&#36830;&#32493;&#20250;&#35805;&#20013;&#30340;&#30140;&#30171;&#35780;&#20998;&#21644;&#33258;&#20027;&#31070;&#32463;&#31995;&#32479;&#35843;&#33410;&#30340;&#20869;&#32986;&#23618;&#27963;&#21160;&#65288;EDA&#65289;&#25968;&#25454;&#12290;&#23454;&#39564;&#25307;&#21215;N = 24&#21517;&#21463;&#35797;&#32773;&#65292;&#36827;&#34892;&#20102;&#36816;&#21160;&#21518;&#24490;&#29615;&#38459;&#22622;&#65288;PECO&#65289;&#20280;&#23637;&#24341;&#36215;&#30340;&#19981;&#36866;&#12290;&#21463;&#35797;&#32773;&#25968;&#25454;&#23384;&#20648;&#22312;&#23450;&#21046;&#30340;&#30140;&#30171;&#24179;&#21488;&#20013;&#65292;&#20415;&#20110;&#25552;&#21462;&#26102;&#22495;EDA&#29305;&#24449;&#21644;&#20250;&#35805;&#20013;&#30340;&#20934;&#30830;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#36824;&#20174;&#27599;&#20010;&#21463;&#35797;&#32773;&#25910;&#38598;&#20102;&#23454;&#39564;&#21518;&#30340;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#65288;VAS&#65289;&#35780;&#20998;&#12290;&#20998;&#21035;&#20351;&#29992;&#30456;&#24212;&#30340;&#23458;&#35266;EDA&#29305;&#24449;&#32467;&#21512;&#20250;&#35805;&#20013;&#24471;&#20998;&#21644;&#20250;&#35805;&#21518;&#24471;&#20998;&#65292;&#35757;&#32451;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;10&#20493;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#36827;&#34892;&#20102;&#23439;&#35266;&#24179;&#22343;&#21151;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models trained on subjective self-report scores struggle to objectively classify pain accurately due to the significant variance between real-time pain experiences and recorded scores afterwards. This study developed two devices for acquisition of real-time, continuous in-session pain scores and gathering of ANS-modulated endodermal activity (EDA).The experiment recruited N = 24 subjects who underwent a post-exercise circulatory occlusion (PECO) with stretch, inducing discomfort. Subject data were stored in a custom pain platform, facilitating extraction of time-domain EDA features and in-session ground truth scores. Moreover, post-experiment visual analog scale (VAS) scores were collected from each subject. Machine learning models, namely Multi-layer Perceptron (MLP) and Random Forest (RF), were trained using corresponding objective EDA features combined with in-session scores and post-session scores, respectively. Over a 10-fold cross-validation, the macro-avera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2308.00864</link><description>&lt;p&gt;
PeRP&#65306;&#36890;&#36807;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#23454;&#29616;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#20197;&#32531;&#35299;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#34892;&#21160;&#26469;&#32531;&#35299;&#25317;&#22581;&#65292;&#20174;&#32780;&#25913;&#21892;&#36890;&#21220;&#26102;&#38388;&#21644;&#29123;&#27833;&#25104;&#26412;&#31561;&#20247;&#22810;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20551;&#35774;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#38431;&#20855;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20998;&#27573;&#24120;&#25968;&#65288;PC&#65289;&#31574;&#30053;&#36890;&#36807;&#32467;&#26500;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#65292;&#20197;&#25552;&#20379;&#32473;&#20154;&#31867;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;PC&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#39550;&#39542;&#21592;&#34892;&#20026;&#30456;&#20284;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PC&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#29305;&#24449;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#65292;&#21363;PeRP&#12290;PeRP&#24314;&#35758;&#39550;&#39542;&#21592;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#30340;&#26041;&#24335;&#34892;&#39542;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26080;&#30417;&#30563;&#22320;&#25512;&#26029;&#39550;&#39542;&#21592;&#22914;&#20309;&#36981;&#24490;&#25351;&#20196;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#31574;&#30053;&#19982;&#39550;&#39542;&#21592;&#29305;&#24449;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00858</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#38543;&#26426;&#36807;&#31243;&#26469;&#29702;&#35299;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes. (arXiv:2308.00858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#65288;&#28145;&#23618;&#65289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#21644;&#23398;&#20064;&#21160;&#24577;&#65292;&#37319;&#29992;&#25968;&#23398;&#25277;&#35937;&#21644;&#27169;&#22411;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#36825;&#20123;&#24037;&#20855;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#31616;&#21270;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#20419;&#36827;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36804;&#20170;&#20026;&#27490;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#65288;&#28145;&#23618;&#65289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38408;&#20540;&#33410;&#28857;&#30340;&#28608;&#27963;&#27169;&#24335;&#24314;&#27169;&#20026;&#38543;&#26426;&#36807;&#31243;&#12290;&#25105;&#20204;&#20165;&#20851;&#27880;&#28608;&#27963;&#39057;&#29575;&#65292;&#21033;&#29992;&#29992;&#20110;&#30495;&#23454;&#31070;&#32463;&#20803;&#23574;&#23792;&#20449;&#21495;&#30340;&#31070;&#32463;&#31185;&#23398;&#25216;&#26415;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#21462;&#23574;&#23792;&#27963;&#21160;&#24182;&#20351;&#29992;&#31526;&#21512;&#27850;&#26494;&#20998;&#24067;&#30340;&#21040;&#36798;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#26816;&#26597;&#26469;&#33258;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#25311;&#21512;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
To gain a deeper understanding of the behavior and learning dynamics of (deep) artificial neural networks, it is valuable to employ mathematical abstractions and models. These tools provide a simplified perspective on network performance and facilitate systematic investigations through simulations. In this paper, we propose utilizing the framework of stochastic processes, which has been underutilized thus far.  Our approach models activation patterns of thresholded nodes in (deep) artificial neural networks as stochastic processes. We focus solely on activation frequency, leveraging neuroscience techniques used for real neuron spike trains. During a classification task, we extract spiking activity and use an arrival process following the Poisson distribution.  We examine observed data from various artificial neural networks in image recognition tasks, fitting the proposed model's assumptions. Through this, we derive parameters describing activation patterns in each network. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#32959;&#30244;&#20998;&#21106;&#20013;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#21512;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30456;&#20284;&#24615;&#26435;&#37325;&#32858;&#21512;&#26041;&#27861;&#65288;SimAgg&#65289;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20998;&#21106;&#33021;&#21147;&#65292;&#24182;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#20570;&#20986;&#20102;&#39069;&#22806;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.00856</link><description>&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#32959;&#30244;&#20998;&#21106;&#20013;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#21512;&#30340;&#24046;&#20998;&#38544;&#31169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation. (arXiv:2308.00856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#32959;&#30244;&#20998;&#21106;&#20013;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#21512;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30456;&#20284;&#24615;&#26435;&#37325;&#32858;&#21512;&#26041;&#27861;&#65288;SimAgg&#65289;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20998;&#21106;&#33021;&#21147;&#65292;&#24182;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#20570;&#20986;&#20102;&#39069;&#22806;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20844;&#27491;&#30340;&#20840;&#23616;&#27169;&#22411;&#26469;&#20445;&#25252;&#20010;&#20307;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#19981;&#21516;&#23458;&#25143;&#25968;&#25454;&#26102;&#21487;&#33021;&#24341;&#20837;&#23433;&#20840;&#39118;&#38505;&#65292;&#20174;&#32780;&#21487;&#33021;&#21361;&#21450;&#38544;&#31169;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25193;&#23637;&#20102;&#30456;&#20284;&#24615;&#26435;&#37325;&#32858;&#21512;&#26041;&#27861;&#65288;SimAgg&#65289;&#21040;DP-SimAgg&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#24046;&#20998;&#38544;&#31169;&#30456;&#20284;&#24615;&#21152;&#26435;&#32858;&#21512;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;DP-SimAgg&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#20998;&#21106;&#33021;&#21147;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#38544;&#31169;&#20445;&#25252;&#23618;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#20197;&#35745;&#31639;&#24615;&#33021;&#20026;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292;&#35777;&#26126;&#20102;DP-SimAgg&#20351;..
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning approach that safeguards privacy by creating an impartial global model while respecting the privacy of individual client data. However, the conventional FL method can introduce security risks when dealing with diverse client data, potentially compromising privacy and data integrity. To address these challenges, we present a differential privacy (DP) federated deep learning framework in medical image segmentation. In this paper, we extend our similarity weight aggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private similarity-weighted aggregation algorithm for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only enhances model segmentation capabilities but also provides an additional layer of privacy preservation. Extensive benchmarking and evaluation of our framework, with computational performance as a key consideration, demonstrate that DP-SimAgg enables a
&lt;/p&gt;</description></item><item><title>&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#20316;&#32773;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#21644;&#26368;&#26032;&#28044;&#29616;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00855</link><description>&lt;p&gt;
&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades. (arXiv:2308.00855v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00855
&lt;/p&gt;
&lt;p&gt;
&#23545;&#36807;&#21435;&#20845;&#21313;&#24180;&#38388;&#20855;&#26377;&#39640;&#24341;&#29992;&#21644;&#37325;&#35201;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#20316;&#32773;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#21644;&#26368;&#26032;&#28044;&#29616;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#20013;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25512;&#21160;&#20854;&#20182;&#24863;&#20852;&#36259;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20102;&#35299;&#39640;&#24341;&#29992;&#35770;&#25991;&#30340;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#23450;&#20851;&#38190;&#36235;&#21183;&#12289;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#32773;&#20197;&#21450;&#36804;&#20170;&#20026;&#27490;&#25152;&#20570;&#20986;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#39640;&#24341;&#29992;&#26426;&#22120;&#23398;&#20064;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20221;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;1959&#24180;&#21040;2022&#24180;&#30340;&#22810;&#24180;&#38388;&#20869;&#65292;&#22791;&#21463;&#25512;&#23815;&#30340;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#39640;&#24341;&#29992;&#35770;&#25991;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#25991;&#29486;&#35745;&#37327;&#25216;&#26415;&#23545;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#24341;&#29992;&#20998;&#26512;&#12289;&#21512;&#33879;&#20998;&#26512;&#12289;&#20851;&#38190;&#35789;&#20998;&#26512;&#21644;&#20986;&#29256;&#36235;&#21183;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#35770;&#25991;&#12289;&#39640;&#24341;&#29992;&#30340;&#20316;&#32773;&#20197;&#21450;&#21512;&#20316;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#28909;&#38376;&#30740;&#31350;&#20027;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#36817;&#23835;&#36215;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently 
&lt;/p&gt;</description></item><item><title>CASSINI&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#38598;&#32676;&#30340;&#32593;&#32476;&#24863;&#30693;&#20316;&#19994;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#20146;&#21644;&#22270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20316;&#19994;&#30340;&#39640;&#25928;&#35843;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;&#21644;&#23614;&#37096;&#23436;&#25104;&#26102;&#38388;&#30340;&#25552;&#21319;&#65292;&#20197;&#21450;ECN&#26631;&#35760;&#25968;&#25454;&#21253;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.00852</link><description>&lt;p&gt;
CASSINI&#65306;&#26426;&#22120;&#23398;&#20064;&#38598;&#32676;&#20013;&#30340;&#32593;&#32476;&#24863;&#30693;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters. (arXiv:2308.00852v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00852
&lt;/p&gt;
&lt;p&gt;
CASSINI&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#38598;&#32676;&#30340;&#32593;&#32476;&#24863;&#30693;&#20316;&#19994;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#20146;&#21644;&#22270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20316;&#19994;&#30340;&#39640;&#25928;&#35843;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;&#21644;&#23614;&#37096;&#23436;&#25104;&#26102;&#38388;&#30340;&#25552;&#21319;&#65292;&#20197;&#21450;ECN&#26631;&#35760;&#25968;&#25454;&#21253;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CASSINI&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#38598;&#32676;&#30340;&#32593;&#32476;&#24863;&#30693;&#20316;&#19994;&#35843;&#24230;&#22120;&#12290;CASSINI&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#25277;&#35937;&#65292;&#20197;&#32771;&#34385;&#19981;&#21516;&#20316;&#19994;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#25918;&#32622;&#22312;&#32593;&#32476;&#38142;&#36335;&#19978;&#12290;&#20026;&#27492;&#65292;CASSINI&#20351;&#29992;&#20102;&#19968;&#20010;&#20146;&#21644;&#22270;&#65292;&#25214;&#21040;&#19968;&#31995;&#21015;&#30340;&#26102;&#38388;&#20559;&#31227;&#20540;&#26469;&#35843;&#25972;&#19968;&#32452;&#20316;&#19994;&#30340;&#36890;&#20449;&#38454;&#27573;&#65292;&#20351;&#24471;&#20849;&#20139;&#21516;&#19968;&#32593;&#32476;&#38142;&#36335;&#30340;&#20316;&#19994;&#30340;&#36890;&#20449;&#27169;&#24335;&#30456;&#20114;&#20132;&#38169;&#12290;&#22312;&#19968;&#20010;24&#21488;&#26381;&#21153;&#22120;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#20351;&#29992;13&#31181;&#24120;&#35265;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;ML&#35843;&#24230;&#22120;&#30456;&#27604;&#65292;CASSINI&#23558;&#20316;&#19994;&#30340;&#24179;&#22343;&#21644;&#23614;&#37096;&#23436;&#25104;&#26102;&#38388;&#20998;&#21035;&#25552;&#39640;&#20102;1.6&#20493;&#21644;2.5&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CASSINI&#23558;&#38598;&#32676;&#20013;ECN&#26631;&#35760;&#30340;&#25968;&#25454;&#21253;&#25968;&#37327;&#20943;&#23569;&#20102;&#26368;&#22810;33&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CASSINI, a network-aware job scheduler for machine learning (ML) clusters. CASSINI introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, CASSINI uses an affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs, such that the communication patterns of jobs sharing the same network link are interleaved with each other. Experiments with 13 common ML models on a 24-server testbed demonstrate that compared to the state-of-the-art ML schedulers, CASSINI improves the average and tail completion time of jobs by up to 1.6x and 2.5x, respectively. Moreover, we show that CASSINI reduces the number of ECN marked packets in the cluster by up to 33x.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00824</link><description>&lt;p&gt;
&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25512;&#23548;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20219;&#24847;&#26377;&#38480;&#22823;&#23567;&#21442;&#25968;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31934;&#30830;&#34920;&#31034;&#19982;&#33879;&#21517;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#30456;&#23545;&#20110;NTK&#21644;&#20854;&#20182;&#38750;&#31934;&#30830;&#36335;&#24452;&#26680;&#20844;&#24335;&#30340;&#36817;&#20284;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#36924;&#30495;&#32593;&#32476;&#30340;&#26680;&#21040;&#26426;&#22120;&#31934;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#31934;&#30830;&#26680;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22914;&#20309;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00788</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#20171;&#32461;&#65306;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20123;&#28608;&#21160;&#20154;&#24515;&#30340;&#21457;&#23637;&#20013;&#21344;&#25454;&#20102;&#20013;&#24515;&#33310;&#21488;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;BLO&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#65288;&#21363;&#19978;&#23618;&#21644;&#19979;&#23618;&#65289;&#65292;&#20854;&#20013;&#35299;&#20915;&#19978;&#23618;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#19979;&#23618;&#38382;&#39064;&#12290;BLO&#20043;&#25152;&#20197;&#21463;&#21040;&#27426;&#36814;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#22240;&#20026;&#23427;&#22312;&#24314;&#27169;&#28041;&#21450;&#20248;&#21270;&#23884;&#22871;&#30446;&#26631;&#20989;&#25968;&#30340;SP&#21644;ML&#31561;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#12290;BLO&#30340;&#26174;&#33879;&#24212;&#29992;&#33539;&#22260;&#20174;&#26080;&#32447;&#31995;&#32479;&#30340;&#36164;&#28304;&#20998;&#37197;&#21040;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#19968;&#31867;&#22312;SP&#21644;ML&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#21487;&#35299;BLO&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31867;BLO&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#27010;&#24565;&#30340;&#27010;&#36848;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#12289;&#26631;&#20934;&#31639;&#27861;&#65288;&#21253;&#25324;&#23427;&#20204;&#30340;&#20248;&#21270;&#21407;&#29702;&#21644;&#23454;&#38469;&#23454;&#29616;&#26041;&#27861;&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#33021;&#22815;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be levera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#24212;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#24863;&#30693;&#21644;&#22810;&#38376;&#38480;&#916;&#35843;&#21046;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.00787</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#24179;&#21488;&#19978;&#35780;&#20272;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition. (arXiv:2308.00787v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#24212;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#24863;&#30693;&#21644;&#22810;&#38376;&#38480;&#916;&#35843;&#21046;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#21644;&#20302;&#24310;&#36831;&#26159;&#35774;&#35745;&#21487;&#31359;&#25140;&#24335;AI&#36741;&#21161;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#37325;&#35201;&#35201;&#27714;&#65292;&#30001;&#20110;&#30005;&#27744;&#25805;&#20316;&#21644;&#38381;&#29615;&#21453;&#39304;&#30340;&#20005;&#26684;&#38480;&#21046;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#21387;&#32553;&#20197;&#28385;&#36275;&#20005;&#26684;&#30340;&#36793;&#32536;&#35201;&#27714;&#65292;&#20294;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24863;&#30693;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#22266;&#26377;&#30340;&#33021;&#25928;&#21644;&#22312;&#38750;&#24120;&#20302;&#24310;&#36831;&#19979;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#30340;&#25163;&#33109;&#36816;&#21160;&#20256;&#24863;&#22120;&#12290;&#37319;&#29992;&#22810;&#38376;&#38480;&#916;&#35843;&#21046;&#26041;&#27861;&#23558;&#36755;&#20837;&#20256;&#24863;&#22120;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#24207;&#21015;&#65292;&#20197;&#23558;&#31995;&#32479;&#36716;&#31227;&#21040;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#33033;&#20914;&#24207;&#21015;&#36755;&#20837;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural n
&lt;/p&gt;</description></item><item><title>DYMOND&#26159;&#19968;&#31181;&#32771;&#34385;&#20102;&#21160;&#24577;&#21464;&#21270;&#21644;&#33410;&#28857;&#22312;&#22270;&#26696;&#20013;&#35282;&#33394;&#30340;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;</title><link>http://arxiv.org/abs/2308.00770</link><description>&lt;p&gt;
DYMOND: DYnamic MOtif-NoDes&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DYMOND: DYnamic MOtif-NoDes Network Generative Model. (arXiv:2308.00770v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00770
&lt;/p&gt;
&lt;p&gt;
DYMOND&#26159;&#19968;&#31181;&#32771;&#34385;&#20102;&#21160;&#24577;&#21464;&#21270;&#21644;&#33410;&#28857;&#22312;&#22270;&#26696;&#20013;&#35282;&#33394;&#30340;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24050;&#32463;&#34987;&#30830;&#23450;&#20026;&#32593;&#32476;&#32467;&#26500;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36229;&#36234;&#20102;&#25104;&#23545;&#36830;&#25509;&#65292;&#25429;&#25417;&#20102;&#36830;&#25509;&#21644;&#27963;&#21160;&#20013;&#30340;&#38271;&#31243;&#30456;&#20851;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24456;&#23569;&#26377;&#32771;&#34385;&#39640;&#38454;&#32593;&#32476;&#32467;&#26500;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29978;&#33267;&#26356;&#23569;&#30340;&#20851;&#27880;&#22312;&#21160;&#24577;&#22270;&#27169;&#22411;&#20013;&#20351;&#29992;&#21160;&#26426;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#20005;&#26684;&#36890;&#36807;&#36793;&#32536;&#28155;&#21152;&#22686;&#38271;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38745;&#24577;&#22270;&#32467;&#26500;&#24230;&#37327;&#26469;&#35780;&#20272;&#27169;&#22411;--&#36825;&#24182;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#32593;&#32476;&#30340;&#26102;&#38388;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DYnamic MOtif-NoDes (DYMOND)&#8212;&#8212;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;(i)&#25972;&#20307;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#20351;&#29992;&#26102;&#38388;&#21160;&#26426;&#27963;&#21160;&#21644;(ii)&#33410;&#28857;&#22312;&#22270;&#26696;&#20013;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#19968;&#20010;&#33410;&#28857;&#22312;&#26964;&#24418;&#20013;&#22788;&#20110;&#26530;&#32445;&#35282;&#33394;&#65292;&#32780;&#20854;&#20313;&#20004;&#20010;&#33410;&#28857;&#25198;&#28436;&#20174;&#23646;&#35282;&#33394;&#65289;&#12290;&#25105;&#20204;&#23558;DYMOND&#19982;&#19977;&#20010;&#23454;&#38469;&#32593;&#32476;&#19978;&#30340;&#21160;&#24577;&#22270;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Motifs, which have been established as building blocks for network structure, move beyond pair-wise connections to capture longer-range correlations in connections and activity. In spite of this, there are few generative graph models that consider higher-order network structures and even fewer that focus on using motifs in models of dynamic graphs. Most existing generative models for temporal graphs strictly grow the networks via edge addition, and the models are evaluated using static graph structure metrics -- which do not adequately capture the temporal behavior of the network. To address these issues, in this work we propose DYnamic MOtif-NoDes (DYMOND) -- a generative model that considers (i) the dynamic changes in overall graph structure using temporal motif activity and (ii) the roles nodes play in motifs (e.g., one node plays the hub role in a wedge, while the remaining two act as spokes). We compare DYMOND to three dynamic graph generative model baselines on real-world network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21040;&#35780;&#35770;&#39033;&#26816;&#32034;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;BERT&#23884;&#20837;&#65292;&#20197;&#34701;&#21512;&#26597;&#35810;&#21644;&#35780;&#35770;&#24471;&#20998;&#24182;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.00762</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;BERT&#24494;&#35843;&#29992;&#20110;&#22522;&#20110;&#34701;&#21512;&#30340;&#35780;&#35770;&#39033;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21040;&#35780;&#35770;&#39033;&#26816;&#32034;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;BERT&#23884;&#20837;&#65292;&#20197;&#34701;&#21512;&#26597;&#35810;&#21644;&#35780;&#35770;&#24471;&#20998;&#24182;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#20351;&#29992;&#25143;&#33021;&#22815;&#34920;&#36798;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#29992;&#25143;&#35780;&#35770;&#20869;&#23481;&#20063;&#21576;&#29190;&#28856;&#24335;&#22686;&#38271;&#65292;&#36825;&#21487;&#20197;&#20351;&#29992;&#25143;&#26356;&#22909;&#22320;&#25214;&#21040;&#19982;&#36825;&#20123;&#34920;&#36798;&#24615;&#26597;&#35810;&#21305;&#37197;&#30340;&#39184;&#21381;&#12289;&#20070;&#31821;&#25110;&#30005;&#24433;&#31561;&#29289;&#21697;&#12290;&#34429;&#28982;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;(IR)&#26041;&#27861;&#20026;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#21305;&#37197;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#25193;&#23637;&#21040;&#35780;&#20272;&#39033;&#26816;&#32034;(RIR)&#20219;&#21153;&#65292;&#20854;&#20013;&#26597;&#35810;-&#35780;&#35770;&#24471;&#20998;&#24517;&#39035;&#32858;&#21512;(&#25110;&#34701;&#21512;)&#25104;&#29289;&#21697;&#32423;&#24471;&#20998;&#36827;&#34892;&#25490;&#21517;&#12290;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;RIR&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#23545;BERT&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#31070;&#32463;IR&#26041;&#27861;&#25193;&#23637;&#21040;RIR&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#38656;&#35201;&#36873;&#25321;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#32780;&#25105;&#20204;&#30340;&#39033;-&#35780;&#35770;&#25968;&#25454;&#30340;&#29420;&#29305;&#20108;&#32423;&#32467;&#26500;&#32467;&#21512;&#20803;&#25968;&#25454;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.00755</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#26159;&#19968;&#31181;&#27169;&#22411;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#26469;&#27604;&#36739;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#24615;&#21035;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#20284;&#20046;&#25918;&#22823;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25918;&#22823;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#39064;&#36890;&#24120;&#21253;&#21547;&#26126;&#30830;&#30340;&#24615;&#21035;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#30340;&#25552;&#31034;&#21017;&#19981;&#21253;&#21547;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20559;&#35265;&#24230;&#37327;&#12290;&#19968;&#26086;&#25105;&#20204;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#20351;&#29992;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25918;&#22823;&#29616;&#35937;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35828;&#26126;&#20102;&#27604;&#36739;&#27169;&#22411;&#21644;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24403;&#21069;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#20998;&#26512;&#24341;&#29992;&#35745;&#25968;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30740;&#31350;&#36235;&#21183;&#12290;&#30740;&#31350;&#21457;&#29616;&#24341;&#29992;&#35745;&#25968;&#26159;&#30830;&#23450;&#36235;&#21183;&#30340;&#26368;&#30456;&#20851;&#22240;&#32032;&#65292;&#21516;&#26102;NSF&#36164;&#21161;&#21644;&#19987;&#21033;&#23545;&#28909;&#38376;&#35805;&#39064;&#30340;&#24433;&#21709;&#22312;&#36880;&#28176;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2308.00733</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#30340;&#26144;&#23556;&#65306;&#36235;&#21183;&#65292;&#24433;&#21709;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mapping Computer Science Research: Trends, Influences, and Predictions. (arXiv:2308.00733v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24403;&#21069;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#20998;&#26512;&#24341;&#29992;&#35745;&#25968;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30740;&#31350;&#36235;&#21183;&#12290;&#30740;&#31350;&#21457;&#29616;&#24341;&#29992;&#35745;&#25968;&#26159;&#30830;&#23450;&#36235;&#21183;&#30340;&#26368;&#30456;&#20851;&#22240;&#32032;&#65292;&#21516;&#26102;NSF&#36164;&#21161;&#21644;&#19987;&#21033;&#23545;&#28909;&#38376;&#35805;&#39064;&#30340;&#24433;&#21709;&#22312;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24403;&#21069;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#35843;&#26597;&#20102;&#23548;&#33268;&#36825;&#20123;&#39046;&#22495;&#20986;&#29616;&#30340;&#22240;&#32032;&#12290;&#21033;&#29992;&#21253;&#21547;&#35770;&#25991;&#12289;&#24341;&#29992;&#21644;&#36164;&#37329;&#20449;&#24687;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21253;&#25324;&#20915;&#31574;&#26641;&#21644;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#20869;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#30740;&#31350;&#39046;&#22495;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#30830;&#23450;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#36235;&#21183;&#20013;&#65292;&#30740;&#31350;&#35770;&#25991;&#20013;&#24341;&#29992;&#30340;&#21442;&#32771;&#25991;&#29486;&#25968;&#37327;&#65288;&#24341;&#29992;&#35745;&#25968;&#65289;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#24341;&#29992;&#35745;&#25968;&#25104;&#20026;&#25512;&#21160;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#36235;&#21183;&#30340;&#26368;&#30456;&#20851;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;NSF&#36164;&#21161;&#21644;&#19987;&#21033;&#23545;&#28909;&#38376;&#35805;&#39064;&#30340;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#22686;&#21152;&#12290;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#36235;&#21183;&#26041;&#38754;&#20248;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;&#36890;&#36807;&#36229;&#36234;&#38543;&#26426;&#29468;&#27979;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#35782;&#21035;&#30740;&#31350;&#36235;&#21183;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the current trending research areas in the field of Computer Science (CS) and investigates the factors contributing to their emergence. Leveraging a comprehensive dataset comprising papers, citations, and funding information, we employ advanced machine learning techniques, including Decision Tree and Logistic Regression models, to predict trending research areas. Our analysis reveals that the number of references cited in research papers (Reference Count) plays a pivotal role in determining trending research areas making reference counts the most relevant factor that drives trend in the CS field. Additionally, the influence of NSF grants and patents on trending topics has increased over time. The Logistic Regression model outperforms the Decision Tree model in predicting trends, exhibiting higher accuracy, precision, recall, and F1 score. By surpassing a random guess baseline, our data-driven approach demonstrates higher accuracy and efficacy in identifying trending
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#29109;&#19982;&#37325;&#26500;&#35823;&#24046;&#26799;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#20013;&#21033;&#29992;&#26799;&#24230;&#29109;&#33021;&#22815;&#23454;&#29616;1-2&#65285;&#30340;&#36895;&#29575;&#33410;&#30465;&#65292;&#36825;&#26159;&#29420;&#31435;&#20110;&#20854;&#20182;&#25913;&#36827;&#26041;&#27861;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.00725</link><description>&lt;p&gt;
&#28508;&#22312;&#28418;&#31227;:&#29109;&#26799;&#24230;&#26377;&#21161;&#20110;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26799;&#24230;&#29109;&#19982;&#37325;&#26500;&#35823;&#24046;&#26799;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#20013;&#21033;&#29992;&#26799;&#24230;&#29109;&#33021;&#22815;&#23454;&#29616;1-2&#65285;&#30340;&#36895;&#29575;&#33410;&#30465;&#65292;&#36825;&#26159;&#29420;&#31435;&#20110;&#20854;&#20182;&#25913;&#36827;&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#36890;&#36807;&#20960;&#21313;&#24180;&#30340;&#25163;&#24037;&#24037;&#31243;&#21162;&#21147;&#24320;&#21457;&#30340;&#20256;&#32479;&#21387;&#32553;&#25216;&#26415;&#65292;&#31471;&#21040;&#31471;&#22270;&#20687;/&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#21464;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#21487;&#35757;&#32451;&#30340;&#32534;&#35299;&#30721;&#22120;&#30456;&#23545;&#20256;&#32479;&#25216;&#26415;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#20363;&#22914;&#26131;&#20110;&#36866;&#24212;&#24863;&#30693;&#22833;&#30495;&#24230;&#37327;&#26631;&#20934;&#21644;&#22312;&#29305;&#23450;&#39046;&#22495;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#27809;&#26377;&#21033;&#29992;&#35299;&#30721;&#22120;&#20013;&#29109;&#30340;&#26799;&#24230;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#29109;&#26799;&#24230;&#65288;&#35299;&#30721;&#22120;&#31471;&#21487;&#29992;&#65289;&#19982;&#37325;&#26500;&#35823;&#24046;&#30340;&#26799;&#24230;&#65288;&#35299;&#30721;&#22120;&#31471;&#19981;&#21487;&#29992;&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#21387;&#32553;&#26041;&#27861;&#19978;&#20351;&#29992;&#36825;&#20010;&#26799;&#24230;&#65292;&#20174;&#32780;&#22312;&#30456;&#21516;&#36136;&#37327;&#19979;&#23454;&#29616;1-2&#65285;&#30340;&#36895;&#29575;&#33410;&#30465;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#25913;&#36827;&#26041;&#27861;&#27491;&#20132;&#65292;&#24182;&#24102;&#26469;&#29420;&#31435;&#30340;&#36895;&#29575;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>&#22312;&#27809;&#26377;&#26799;&#24230;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;ADAM&#31639;&#27861;&#22312;&#22266;&#23450;&#27493;&#38271;&#26102;&#20250;&#21457;&#25955;&#65292;&#32780;&#19981;&#21463;&#26041;&#27861;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.00720</link><description>&lt;p&gt;
ADAM&#31639;&#27861;&#22312;&#22266;&#23450;&#27493;&#38271;&#19979;&#30340;&#21457;&#25955;&#65306;&#19968;&#20010;(&#38750;&#24120;)&#31616;&#21333;&#30340;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example. (arXiv:2308.00720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00720
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#26799;&#24230;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;ADAM&#31639;&#27861;&#22312;&#22266;&#23450;&#27493;&#38271;&#26102;&#20250;&#21457;&#25955;&#65292;&#32780;&#19981;&#21463;&#26041;&#27861;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#19968;&#32500;&#20989;&#25968;&#65292;&#20854;&#26799;&#24230;&#26159;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#65292;&#24403;&#22312;&#26799;&#24230;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21407;&#28857;&#24320;&#22987;&#24212;&#29992;ADAM&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#36825;&#20010;&#20989;&#25968;&#26102;&#65292;&#35813;&#31639;&#27861;&#20197;&#24658;&#23450;&#30340;&#27493;&#38271;&#21457;&#25955;&#12290;&#19981;&#31649;&#26041;&#27861;&#21442;&#25968;&#22914;&#20309;&#36873;&#25321;&#65292;&#37117;&#20250;&#20986;&#29616;&#21457;&#25955;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
A very simple unidimensional function with Lipschitz continuous gradient is constructed such that the ADAM algorithm with constant stepsize, started from the origin, diverges when applied to minimize this function in the absence of noise on the gradient. Divergence occurs irrespective of the choice of the method parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20809;&#26463;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#24314;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#27169;&#22411;&#30340;&#32452;&#21512;&#36827;&#34892;&#39044;&#27979;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;85.8%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00718</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20809;&#26463;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beam Detection Based on Machine Learning Algorithms. (arXiv:2308.00718v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20809;&#26463;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#24314;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#27169;&#22411;&#30340;&#32452;&#21512;&#36827;&#34892;&#39044;&#27979;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;85.8%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31934;&#30830;&#30830;&#23450;&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#26463;&#22312;&#23631;&#24149;&#19978;&#30340;&#20301;&#32622;&#12290;&#22312;&#22522;&#20110;VGG16&#27169;&#22411;&#30340;&#33258;&#24314;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#36801;&#31227;&#35757;&#32451;&#12290;&#23558;&#20013;&#38388;&#23618;&#30340;&#36755;&#20986;&#20316;&#20026;&#29305;&#24449;&#20256;&#36882;&#32473;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20010;&#24207;&#21015;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;85.8&#65285;&#30340;&#27491;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The positions of free electron laser beams on screens are precisely determined by a sequence of machine learning models. Transfer training is conducted in a self-constructed convolutional neural network based on VGG16 model. Output of intermediate layers are passed as features to a support vector regression model. With this sequence, 85.8% correct prediction is achieved on test data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31867;COVID-19 CT&#22270;&#20687;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;96.99%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00715</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#33258;&#21160;COVID-19 CT&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31867;COVID-19 CT&#22270;&#20687;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;96.99%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30340;&#36805;&#36895;&#20256;&#25773;&#38656;&#35201;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#24050;&#25104;&#20026;&#26816;&#27979;&#35813;&#30142;&#30149;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;COVID-19 CT&#25195;&#25551;&#20998;&#31867;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;Xception&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26032;&#35774;&#35745;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21152;&#26435;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#27599;&#20010;&#36890;&#36947;&#20013;&#30340;&#20449;&#24687;&#21306;&#22495;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#29992;&#20110;COVID-19&#26816;&#27979;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;COVID-19 CT&#25195;&#25551;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#20986;96.99%&#30340;&#38750;&#24120;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20854;&#20182;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#20026;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25239;&#20987;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#30340;&#19981;&#25032;&#21162;&#21147;&#20570;&#20986;&#36129;&#29486;&#65292;&#25552;&#20379;&#26377;&#24076;&#26395;&#21644;&#21450;&#26102;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#31867;&#28608;&#27963;&#22270;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#26679;&#26412;&#30340;&#31867;&#28608;&#27963;&#22270;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#36807;&#31243;&#20351;&#24471;&#20998;&#26512;&#24072;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#21644;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00710</link><description>&lt;p&gt;
&#23454;&#29616;&#32858;&#21512;&#30340;&#31867;&#28608;&#27963;&#22270;&#21487;&#29992;&#20110;&#20998;&#26512;&#31867;&#29305;&#24449;&#30340;&#25972;&#20307;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#31867;&#28608;&#27963;&#22270;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#26679;&#26412;&#30340;&#31867;&#28608;&#27963;&#22270;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#36807;&#31243;&#20351;&#24471;&#20998;&#26512;&#24072;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#21644;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#39640;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#39118;&#38505;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#26080;&#27861;&#20351;&#29992;&#65292;&#38500;&#38750;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#20851;&#27880;&#20110;&#35299;&#37322;&#31867;&#20284;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#21487;&#35270;&#21270;&#25968;&#25454;&#26679;&#26412;&#20013;&#27599;&#20010;&#29305;&#24449;&#23545;&#20998;&#31867;&#30340;&#36129;&#29486;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#26679;&#26412;&#30340;CAMs&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#20351;&#20998;&#26512;&#24072;&#33021;&#22815;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;CAM&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#20197;&#19968;&#20010;&#26041;&#24418;&#26631;&#35760;&#34920;&#31034;&#27599;&#20010;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#65292;&#26041;&#24418;&#30340;&#39068;&#33394;&#34920;&#31034;&#35813;&#29305;&#24449;&#30340;&#20998;&#31867;&#24433;&#21709;&#21147;&#65292;&#22635;&#20805;&#26041;&#24418;&#30340;&#22823;&#23567;&#34920;&#31034;&#35813;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled squ
&lt;/p&gt;</description></item><item><title>DeepTSF&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#26694;&#26550;&#65292;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#38761;&#26032;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00709</link><description>&lt;p&gt;
DeepTSF&#65306;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;
&lt;/p&gt;
&lt;p&gt;
DeepTSF: Codeless machine learning operations for time series forecasting. (arXiv:2308.00709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00709
&lt;/p&gt;
&lt;p&gt;
DeepTSF&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#26694;&#26550;&#65292;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#38761;&#26032;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepTSF&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#65288;MLOps&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#26080;&#20195;&#30721;&#24314;&#27169;&#26469;&#38761;&#26032;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;DeepTSF&#33258;&#21160;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#20351;&#20854;&#25104;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;MLops&#24037;&#31243;&#24072;&#21442;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#39044;&#27979;&#30340;&#29702;&#24819;&#24037;&#20855;&#12290;DeepTSF&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#35774;&#35745;&#19982;&#29616;&#26377;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#65292;&#25552;&#20379;&#22686;&#24378;&#30340;&#29983;&#20135;&#21147;&#21644;&#20860;&#23481;&#24615;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20854;&#20182;&#39640;&#32423;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21069;&#31471;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#65292;&#36890;&#36807;&#26377;&#35265;&#22320;&#30340;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#23454;&#29616;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;DeepTSF&#36824;&#36890;&#36807;&#36523;&#20221;&#31649;&#29702;&#21644;&#35775;&#38382;&#25480;&#26435;&#26426;&#21046;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#22312;I-NERGY&#39033;&#30446;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DeepTSF&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DeepTSF, a comprehensive machine learning operations (MLOps) framework aiming to innovate time series forecasting through workflow automation and codeless modeling. DeepTSF automates key aspects of the ML lifecycle, making it an ideal tool for data scientists and MLops engineers engaged in machine learning (ML) and deep learning (DL)-based forecasting. DeepTSF empowers users with a robust and user-friendly solution, while it is designed to seamlessly integrate with existing data analysis workflows, providing enhanced productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists, as well as other higher-level stakeholders, enabling comprehensive understanding through insightful visualizations and evaluation metrics. DeepTSF also prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already proven DeepTSF's ef
&lt;/p&gt;</description></item><item><title>VeriGen&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;Verilog&#20195;&#30721;&#12290;&#23427;&#22312;&#29983;&#25104;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#21644;&#35821;&#27861;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;GPT-3.5-turbo&#27169;&#22411;&#24182;&#19988;&#26174;&#31034;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00708</link><description>&lt;p&gt;
VeriGen: &#19968;&#31181;&#29992;&#20110;Verilog&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VeriGen: A Large Language Model for Verilog Code Generation. (arXiv:2308.00708v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00708
&lt;/p&gt;
&lt;p&gt;
VeriGen&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;Verilog&#20195;&#30721;&#12290;&#23427;&#22312;&#29983;&#25104;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#21644;&#35821;&#27861;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;GPT-3.5-turbo&#27169;&#22411;&#24182;&#19988;&#26174;&#31034;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#30828;&#20214;&#35774;&#35745;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;Verilog&#20195;&#30721;&#65292;Verilog&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#29992;&#20110;&#35774;&#35745;&#21644;&#24314;&#27169;&#25968;&#23383;&#31995;&#32479;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Verilog&#25945;&#26448;&#32534;&#35793;&#30340;Verilog&#25968;&#25454;&#38598;&#19978;&#23545;&#39044;&#20808;&#23384;&#22312;&#30340;LLMs&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#27979;&#35797;&#22871;&#20214;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;Verilog&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#65292;&#35813;&#22871;&#20214;&#21253;&#21547;&#33258;&#23450;&#20041;&#38382;&#39064;&#38598;&#21644;&#27979;&#35797;&#24037;&#20316;&#21488;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30340;&#24320;&#28304;CodeGen-16B&#27169;&#22411;&#22312;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;&#39046;&#20808;&#30340;GPT-3.5-turbo&#27169;&#22411;&#65292;&#25972;&#20307;&#25552;&#39640;&#20102;1.1&#65285;&#12290;&#22312;&#23545;&#26356;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#38382;&#39064;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#22312;&#19982;GPT-3.5-turbo&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#35821;&#27861;&#27491;&#30830;Verilog&#20195;&#30721;&#26041;&#38754;&#25552;&#39640;&#20102;41&#65285;&#65292;&#31361;&#26174;&#20102;&#23567;&#22411;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of small
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;&#31639;&#27861; (AMBS) &#26469;&#39564;&#35777;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#32473;&#23450;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#30456;&#27604;&#65292;AMBS&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#22312;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00707</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;
&lt;/p&gt;
&lt;p&gt;
Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;&#31639;&#27861; (AMBS) &#26469;&#39564;&#35777;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#32473;&#23450;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#30456;&#27604;&#65292;AMBS&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#22312;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064; (RL) &#22312;&#21508;&#20010;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558; RL &#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#26368;&#22823;&#21270;&#26631;&#20934; RL &#30446;&#26631;&#19981;&#33021;&#20445;&#35777;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109; (AMBS)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29702;&#24615;&#21069;&#30651;&#23631;&#34109;&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#23398;&#20064;&#30340; RL &#31574;&#30053;&#30456;&#23545;&#20110;&#19968;&#32452;&#32473;&#23450;&#30340;&#23433;&#20840;&#32422;&#26463;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#23545;&#31995;&#32479;&#30340;&#23433;&#20840;&#30456;&#20851;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20026; AMBS &#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#22312;&#19968;&#32452;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>DiviML&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#32534;&#35793;&#22120;&#32423;&#21035;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00127</link><description>&lt;p&gt;
DiviML: &#19968;&#31181;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00127
&lt;/p&gt;
&lt;p&gt;
DiviML&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#32534;&#35793;&#22120;&#32423;&#21035;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#24322;&#26500;&#26550;&#26500;&#65292;&#24182;&#24320;&#22987;&#21253;&#25324;&#29992;&#20110;&#32593;&#32476;&#12289;&#35270;&#39057;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#19987;&#29992;&#30828;&#20214;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#30340;&#24322;&#26500;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32534;&#35793;&#22120;&#32423;&#21035;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20998;&#21306;&#26144;&#23556;&#21040;&#22810;&#20010;&#20114;&#32852;&#30828;&#20214;&#35774;&#22791;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;DNN&#32534;&#35793;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#38598;&#25104;&#20102;&#19968;&#20010;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#30340;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27169;&#22359;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#30028;&#20844;&#24335;&#26469;&#35780;&#20272;&#21551;&#21457;&#24335;&#35299;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30001;CPU&#21644;&#20004;&#20010;&#35774;&#22791;&#32452;&#25104;&#30340;&#24322;&#26500;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#65292;&#20248;&#21270;&#20256;&#32479;&#30340;DNNs&#21644;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#21040;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di
&lt;/p&gt;</description></item><item><title>LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.16648</link><description>&lt;p&gt;
LLMs4OL: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16648
&lt;/p&gt;
&lt;p&gt;
LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLMs4OL&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26412;&#20307;&#23398;&#20064;&#65288;OL&#65289;&#12290;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30693;&#35782;&#39046;&#22495;&#20013;&#25429;&#25417;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;LLMs4OL&#33539;&#24335;&#30740;&#31350;&#20102;&#20197;&#19979;&#20551;&#35774;&#65306;\textit{LLMs&#33021;&#21542;&#26377;&#25928;&#24212;&#29992;&#23427;&#20204;&#30340;&#35821;&#35328;&#27169;&#24335;&#25429;&#25417;&#33021;&#21147;&#21040;OL&#20013;&#65292;&#36825;&#28041;&#21450;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;?} &#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLM&#27169;&#22411;&#26063;&#32676;&#65292;&#38024;&#23545;&#19977;&#20010;&#20027;&#35201;&#30340;OL&#20219;&#21153;&#65306;&#26415;&#35821;&#31867;&#22411;&#21010;&#20998;&#12289;&#23618;&#32423;&#21457;&#29616;&#21644;&#38750;&#23618;&#32423;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#28085;&#30422;&#20102;&#26412;&#20307;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#21253;&#25324;WordNet&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#30693;&#35782;&#12289;GeoNames&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#21644;UMLS&#20013;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
&lt;/p&gt;</description></item><item><title>Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16039</link><description>&lt;p&gt;
Okapi: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16039
&lt;/p&gt;
&lt;p&gt;
Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#39044;&#26399;&#23545;&#40784;&#65292;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#30446;&#21069;&#24050;&#24212;&#29992;&#20110;&#29983;&#20135;&#26368;&#20339;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#26368;&#36817;&#36824;&#25512;&#20986;&#20102;&#21508;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;Alpaca&#12289;Vicuna&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#20165;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#29699;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#24433;&#21709;&#21147;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#25506;&#32034;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;&#30340;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#21482;&#20351;&#29992;&#20102;SFT&#20316;&#20026;&#25351;&#20196;&#35843;&#20248;&#30340;&#21807;&#19968;&#26041;&#27861;&#12290;&#36825;&#24050;&#32463;&#23384;&#22312;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13055</link><description>&lt;p&gt;
MARIO: &#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#25552;&#39640;OOD&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#22495;&#22806;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#24773;&#20917;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#26377;&#26631;&#31614;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20063;&#26174;&#31034;&#20986;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARIO&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#22270;&#23545;&#27604;&#26041;&#27861;&#65292;&#20811;&#26381;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65306;(i)&#20449;&#24687;&#29942;&#39048;(IB)&#21407;&#21017;&#29992;&#20110;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#65292;(ii)&#19981;&#21464;&#24615;&#21407;&#21017;&#37319;&#29992;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26469;&#33719;&#24471;&#19981;&#21464;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;OOD&#27867;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32676;&#20307;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;QO-ES&#21644;QDO-ES&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36138;&#23146;&#30340;&#38598;&#25104;&#36873;&#25321;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;QO-ES&#21644;QDO-ES&#36890;&#24120;&#20248;&#20110;&#36138;&#23146;&#30340;&#38598;&#25104;&#36873;&#25321;&#65292;&#23613;&#31649;&#21482;&#26377;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22810;&#26679;&#24615;&#23545;&#20110;&#21518;&#26399;&#38598;&#25104;&#21487;&#33021;&#26377;&#30410;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.08364</link><description>&lt;p&gt;
Q(D)O-ES: &#29992;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21518;&#26399;&#38598;&#25104;&#36873;&#25321;&#30340;&#22522;&#20110;&#32676;&#20307;&#30340;&#36136;&#37327;(&#22810;&#26679;&#24615;)&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML. (arXiv:2307.08364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32676;&#20307;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;QO-ES&#21644;QDO-ES&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36138;&#23146;&#30340;&#38598;&#25104;&#36873;&#25321;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;QO-ES&#21644;QDO-ES&#36890;&#24120;&#20248;&#20110;&#36138;&#23146;&#30340;&#38598;&#25104;&#36873;&#25321;&#65292;&#23613;&#31649;&#21482;&#26377;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22810;&#26679;&#24615;&#23545;&#20110;&#21518;&#26399;&#38598;&#25104;&#21487;&#33021;&#26377;&#30410;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#22312;&#21518;&#26399;&#32467;&#21512;&#27169;&#22411;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#24120;&#36890;&#36807;&#36138;&#23146;&#30340;&#38598;&#25104;&#36873;&#25321;(GES)&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;GES&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#25191;&#34892;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#36138;&#23146;&#25628;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32676;&#20307;&#30340;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;QO-ES&#21644;QDO-ES&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;GES&#36827;&#34892;&#27604;&#36739;&#12290;&#34429;&#28982;QO-ES&#20165;&#38024;&#23545;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#20248;&#21270;&#65292;QDO-ES&#36824;&#32771;&#34385;&#20102;&#32676;&#20307;&#20869;&#38598;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20445;&#25345;&#19968;&#32452;&#34920;&#29616;&#33391;&#22909;&#19988;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#65292;&#36825;&#26159;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#21270;&#20248;&#21270;&#30340;&#24605;&#24819;&#12290;&#20351;&#29992;AutoML&#22522;&#20934;&#20013;&#30340;71&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;QO-ES&#21644;QDO-ES&#36890;&#24120;&#20248;&#20110;GES&#65292;&#23613;&#31649;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#21482;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#22810;&#26679;&#24615;&#23545;&#20110;&#21518;&#26399;&#38598;&#25104;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated machine learning (AutoML) systems commonly ensemble models post hoc to improve predictive performance, typically via greedy ensemble selection (GES). However, we believe that GES may not always be optimal, as it performs a simple deterministic greedy search. In this work, we introduce two novel population-based ensemble selection methods, QO-ES and QDO-ES, and compare them to GES. While QO-ES optimises solely for predictive performance, QDO-ES also considers the diversity of ensembles within the population, maintaining a diverse set of well-performing ensembles during optimisation based on ideas of quality diversity optimisation. The methods are evaluated using 71 classification datasets from the AutoML benchmark, demonstrating that QO-ES and QDO-ES often outrank GES, albeit only statistically significant on validation data. Our results further suggest that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01482</link><description>&lt;p&gt;
Nexus sine qua non&#65306;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#36830;&#25509;&#30340;&#26102;&#31354;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19981;&#20165;&#26377;&#21161;&#20110;&#20174;&#19994;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#21152;&#28145;&#25105;&#20204;&#23545;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#25104;&#20026;&#23398;&#20064;&#26102;&#31354;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;STGNNs&#30340;&#26550;&#26500;&#24448;&#24448;&#36890;&#36807;&#22534;&#21472;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#23618;&#27425;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#35774;&#35745;&#30340;&#27169;&#22411;&#21487;&#33021;&#22810;&#20313;&#25110;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#32473;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#20195;STGNNs&#30340;&#35774;&#35745;&#65292;&#24182;&#30830;&#23450;&#23545;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#39044;&#27979;&#22120;&#26377;&#25152;&#36129;&#29486;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23436;&#20840;&#30001;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23450;&#20041;&#65292;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#65292;&#27809;&#26377;&#20219;&#20309;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#65292;&#20363;&#22914;TCNs&#65292;RNNs&#21644;Transformers&#12290;&#36890;&#36807;&#23454;&#35777;&#37325;&#26032;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15865</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20132;&#25442;&#20449;&#24687;&#26469;&#20272;&#35745;&#20174;&#20854;&#31169;&#19979;&#35266;&#23519;&#30340;&#26679;&#26412;&#20013;&#26410;&#30693;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#20294;&#20182;&#20204;&#20063;&#38754;&#20020;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#26696;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#21644;&#32593;&#32476;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#65292;&#21516;&#26102;&#28385;&#36275;&#20195;&#29702;&#30340;&#38544;&#31169;&#38656;&#27714;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#36234;&#20182;&#20204;&#26412;&#22320;&#38468;&#36817;&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#21442;&#19982;&#30340;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25110;&#38543;&#26102;&#38388;&#22312;&#32447;&#33719;&#21462;&#30340;&#31169;&#26377;&#20449;&#21495;&#20013;&#20272;&#35745;&#23436;&#25972;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#24182;&#20445;&#25252;&#20854;&#20449;&#21495;&#21644;&#32593;&#32476;&#38468;&#36817;&#30340;&#38544;&#31169;&#12290;&#36825;&#26159;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#23454;&#29616;&#30340;&#65292;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#20132;&#25442;&#30340;&#20272;&#35745;&#25968;&#25454;&#20013;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.15557</link><description>&lt;p&gt;
&#31616;&#21333;&#25104;&#21151;&#30340;&#27493;&#39588;&#65306;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#20844;&#29702;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15557
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#31639;&#27861;&#34917;&#20607;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#34917;&#20607;&#26041;&#27861;&#25214;&#21040;&#28385;&#36275;&#26576;&#20123;&#26399;&#26395;&#30340;&#28857;&#38598;&#65292;&#20363;&#22914;&#22312;&#22522;&#30784;&#22240;&#26524;&#22270;&#20013;&#30340;&#24178;&#39044;&#65292;&#25110;&#32773;&#26368;&#23567;&#21270;&#20195;&#20215;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#32467;&#26500;&#26377;&#24191;&#27867;&#30340;&#20102;&#35299;&#65292;&#22312;&#20960;&#20010;&#39046;&#22495;&#20013;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#29702;&#21270;&#21512;&#29702;&#21270;&#30340;&#26041;&#27861;&#65292;Stepwise Explainable Paths (StEP)&#65292;&#29992;&#20110;&#35745;&#31639;&#22522;&#20110;&#26041;&#21521;&#30340;&#31639;&#27861;&#34917;&#20607;&#12290;&#25105;&#20204;&#23545;StEP&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#12290;StEP&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#20960;&#20010;&#24050;&#24314;&#31435;&#30340;&#34917;&#20607;&#26041;&#27861;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel data-driven framework for algorithmic recourse that offers users interventions to change their predicted outcome. Existing approaches to compute recourse find a set of points that satisfy some desiderata -- e.g. an intervention in the underlying causal graph, or minimizing a cost function. Satisfying these criteria, however, requires extensive knowledge of the underlying model structure, often an unrealistic amount of information in several domains. We propose a data-driven, computationally efficient approach to computing algorithmic recourse. We do so by suggesting directions in the data manifold that users can take to change their predicted outcome. We present Stepwise Explainable Paths (StEP), an axiomatically justified framework to compute direction-based algorithmic recourse. We offer a thorough empirical and theoretical investigation of StEP. StEP offers provable privacy and robustness guarantees, and outperforms the state-of-the-art on several established reco
&lt;/p&gt;</description></item><item><title>TeleViT&#26159;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23395;&#33410;&#24615;&#37326;&#28779;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#65292;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10940</link><description>&lt;p&gt;
TeleViT: &#30005;&#32852;&#39537;&#21160;&#30340;Transformer&#25913;&#36827;&#20102;&#23395;&#33410;&#24615;&#37326;&#28779;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting. (arXiv:2306.10940v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10940
&lt;/p&gt;
&lt;p&gt;
TeleViT&#26159;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23395;&#33410;&#24615;&#37326;&#28779;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#65292;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#21152;&#21095;&#65292;&#37326;&#28779;&#38382;&#39064;&#26085;&#30410;&#24694;&#21270;&#65292;&#38656;&#35201;&#37319;&#21462;&#20808;&#36827;&#30340;&#20027;&#21160;&#25514;&#26045;&#36827;&#34892;&#26377;&#25928;&#30340;&#32531;&#35299;&#12290;&#25552;&#21069;&#20960;&#21608;&#29978;&#33267;&#20960;&#20010;&#26376;&#39044;&#27979;&#37326;&#28779;&#23545;&#20110;&#35745;&#21010;&#26862;&#26519;&#29123;&#26009;&#31649;&#29702;&#12289;&#36164;&#28304;&#37319;&#36141;&#21644;&#37197;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#24517;&#39035;&#37319;&#29992;&#33021;&#22815;&#32771;&#34385;&#22320;&#29699;&#31995;&#32479;&#22266;&#26377;&#30340;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;&#35760;&#24518;&#25928;&#24212;&#21644;&#30005;&#32852;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#65288;TeleViT&#65289;&#65292;&#33021;&#22815;&#23558;&#22320;&#29699;&#35270;&#20026;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#31995;&#32479;&#65292;&#23558;&#31934;&#32454;&#30340;&#23616;&#37096;&#23610;&#24230;&#36755;&#20837;&#19982;&#20840;&#29699;&#23610;&#24230;&#36755;&#20837;&#65288;&#22914;&#27668;&#20505;&#25351;&#25968;&#21644;&#31895;&#31890;&#24230;&#30340;&#20840;&#29699;&#21464;&#37327;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;TeleViT&#22312;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#39044;&#27979;&#31383;&#21475;&#19979;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfires are increasingly exacerbated as a result of climate change, necessitating advanced proactive measures for effective mitigation. It is important to forecast wildfires weeks and months in advance to plan forest fuel management, resource procurement and allocation. To achieve such accurate long-term forecasts at a global scale, it is crucial to employ models that account for the Earth system's inherent spatio-temporal interactions, such as memory effects and teleconnections. We propose a teleconnection-driven vision transformer (TeleViT), capable of treating the Earth as one interconnected system, integrating fine-grained local-scale inputs with global-scale inputs, such as climate indices and coarse-grained global variables. Through comprehensive experimentation, we demonstrate the superiority of TeleViT in accurately predicting global burned area patterns for various forecasting windows, up to four months in advance. The gain is especially pronounced in larger forecasting wind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05150</link><description>&lt;p&gt;
&#26114;&#36149;&#23884;&#22871;&#28784;&#30418;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#28784;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#30001;&#40657;&#31665;&#21644;&#30333;&#31665;&#20989;&#25968;&#32452;&#25104;&#30340;&#23884;&#22871;&#20989;&#25968;&#12290;&#32473;&#20986;&#20102;&#36825;&#31181;&#28784;&#30418;&#38382;&#39064;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#28784;&#30418;&#20248;&#21270;&#20844;&#24335;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#40657;&#31665;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30456;&#20284;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#20294;&#20056;&#20197;&#20381;&#36182;&#20110;&#25152;&#32771;&#34385;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#30340;&#24120;&#25968;&#20056;&#39033;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#24773;&#20917;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;&#24120;&#29992;&#30340;&#26680;&#20989;&#25968;&#65292;&#21518;&#24724;&#36793;&#30028;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#21040;&#26368;&#20248;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#40657;&#31665;&#20248;&#21270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#28784;&#30418;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#23545;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21644;&#38750;&#26631;&#20934;&#21270;&#30340;&#23383;&#20856;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#20984;&#33021;&#37327;&#26223;&#35266;&#20013;&#36941;&#21382;&#65292;&#35299;&#20915;&#20102;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01940</link><description>&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#23545;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;QUBO&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling binary sparse coding QUBO models using a spiking neuromorphic processor. (arXiv:2306.01940v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#23545;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21644;&#38750;&#26631;&#20934;&#21270;&#30340;&#23383;&#20856;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#20984;&#33021;&#37327;&#26223;&#35266;&#20013;&#36941;&#21382;&#65292;&#35299;&#20915;&#20102;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#22270;&#20687;&#30340;&#31232;&#30095;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24352;&#22270;&#20687;&#21644;&#19968;&#20010;&#36807;&#23436;&#22791;&#30340;&#12289;&#38750;&#27491;&#20132;&#30340;&#22522;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#31232;&#30095;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#65292;&#25351;&#31034;&#20986;&#26368;&#23567;&#30340;&#19968;&#32452;&#22522;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#21152;&#22312;&#19968;&#36215;&#33021;&#22815;&#26368;&#22909;&#22320;&#37325;&#26500;&#32473;&#23450;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#20197;&#37325;&#26500;&#35823;&#24046;&#19978;&#30340;$L_2$&#25439;&#22833;&#21644;&#20108;&#36827;&#21046;&#21521;&#37327;&#19978;&#30340;$L_0$&#65288;&#25110;&#31561;&#20215;&#22320;&#65292;$L_1$&#65289;&#25439;&#22833;&#26469;&#25551;&#36848;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#25152;&#35859;&#30340;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#36890;&#24120;&#24456;&#38590;&#25214;&#21040;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21644;&#38750;&#26631;&#20934;&#21270;&#30340;&#23383;&#20856;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36798;&#21040;&#25152;&#38656;&#30340;&#31232;&#30095;&#32423;&#21035;&#26469;&#26368;&#22909;&#22320;&#21305;&#37197;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#20984;&#33021;&#37327;&#26223;&#35266;&#20013;&#36941;&#21382;&#65292;&#23558;&#20108;&#36827;&#21046;&#31232;&#30095;&#32534;&#30721;&#38382;&#39064;&#35299;&#20915;&#22312;Loihi 1&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#12290;&#25105;&#20204;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#31867;&#21035;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing a sparse binary representation of an image. To be precise, given an image and an overcomplete, non-orthonormal basis, we aim to find a sparse binary vector indicating the minimal set of basis vectors that when added together best reconstruct the given input. We formulate this problem with an $L_2$ loss on the reconstruction error, and an $L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing sparsity. This yields a so-called Quadratic Unconstrained Binary Optimization (QUBO) problem, whose solution is generally NP-hard to find. The contribution of this work is twofold. First, the method of unsupervised and unnormalized dictionary feature learning for a desired sparsity level to best match the data is presented. Second, the binary sparse coding problem is then solved on the Loihi 1 neuromorphic chip by the use of stochastic networks of neurons to traverse the non-convex energy landscape. The solutions are benchmarked against the class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13399</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#22823;&#35268;&#27169;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30005;&#21830;&#24212;&#29992;&#20013;&#35270;&#35273;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#21333;&#27169;&#24577;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20135;&#21697;&#20869;&#23481;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#30005;&#21830;&#25512;&#33616;&#12289;&#25628;&#32034;&#21644;&#24191;&#21578;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#21644;&#23545;&#27604;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26377;&#25928;&#24494;&#35843;&#22823;&#35268;&#27169;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#22810;&#31181;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25552;&#20379;&#35270;&#35273;&#34920;&#31034;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#20026;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#35270;&#35273;&#30456;&#20284;&#24191;&#21578;&#25512;&#33616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#35270;&#35273;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31163;&#32447;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#30456;&#20284;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.01666</link><description>&lt;p&gt;
BrainNPT&#65306;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#31867;&#30340;Transformer&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33041;&#25104;&#20687;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#26223;&#30340;&#29305;&#24449;&#23398;&#20064;&#25913;&#36827;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;Transformer&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#37325;&#28857;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;BrainNPT&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&lt;cls&gt;&#26631;&#35760;&#20316;&#20026;&#20998;&#31867;&#23884;&#20837;&#21521;&#37327;&#65292;&#20197;&#20415;&#20110;Transformer&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#33719;&#33041;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#65292;&#29992;&#20110;BrainNPT&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged &lt;cls&gt; token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
&lt;/p&gt;</description></item><item><title>ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.00365</link><description>&lt;p&gt;
ReLBOT&#65306;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#26368;&#23567;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#24378;&#21270;&#23398;&#20064;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00365
&lt;/p&gt;
&lt;p&gt;
ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#12290;&#24403;&#26234;&#33021;&#24314;&#31569;&#25237;&#20837;&#20351;&#29992;&#26102;&#65292;&#27809;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#23384;&#22312;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#24403;RL&#20195;&#29702;&#26368;&#21021;&#25506;&#32034;&#20854;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#23427;&#21487;&#33021;&#20250;&#32473;&#24314;&#31569;&#23621;&#27665;&#24102;&#26469;&#37325;&#22823;&#19981;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLBOT&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#32467;&#21512;&#28145;&#24230;RL&#65292;&#20174;&#29616;&#26377;&#30340;&#20248;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#30693;&#35782;&#21040;&#26032;&#25237;&#20837;&#20351;&#29992;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28909;&#36523;&#26399;&#23545;&#24314;&#31569;&#29289;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#26524;&#65292;&#28909;&#36523;&#26399;&#30340;&#25345;&#32493;&#26102;&#38388;&#21487;&#25552;&#39640;6.2&#20493;&#65292;&#24182;&#19988;&#39044;&#27979;&#26041;&#24046;&#21487;&#25552;&#39640;132&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05527</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30830;&#23450;&#24615;&#30446;&#26631;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65306;&#26356;&#24555;&#65292;&#26356;&#31934;&#30830;&#65292;&#26356;&#40657;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#65288;ADVI&#65289;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#20195;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#24555;&#36895;&#26131;&#29992;&#30340;&#21518;&#39564;&#36817;&#20284;&#26041;&#27861;&#12290;&#28982;&#32780;&#23427;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#32570;&#20047;&#26126;&#30830;&#30340;&#25910;&#25947;&#26631;&#20934;&#65292;&#24182;&#19988;&#38656;&#35201;&#35843;&#25972;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;ADVI&#32487;&#25215;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#36739;&#24046;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;DADVI&#29992;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;MFVB&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#36825;&#19968;&#25216;&#26415;&#22312;&#38543;&#26426;&#20248;&#21270;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#8220;&#26679;&#26412;&#24179;&#22343;&#36817;&#20284;&#8221;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36817;&#20284;&#20294;&#30830;&#23450;&#30340;&#30446;&#26631;&#65292;DADVI&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#32780;&#19988;&#19982;&#26631;&#20934;&#22343;&#20540;&#22330;ADVI&#19981;&#21516;&#30340;&#26159;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#22351;&#24773;&#20917;&#29702;&#35770;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#65292;DADVI&#21644;SAA&#21487;&#20197;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32511;&#33394;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#21644;&#33021;&#28304;&#31649;&#29702;&#31574;&#30053;&#26469;&#20943;&#23569;&#30899;&#36275;&#36857;&#65292;&#20351;&#24471;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#26356;&#21152;&#29615;&#20445;&#12290;</title><link>http://arxiv.org/abs/2303.14604</link><description>&lt;p&gt;
&#32511;&#33394;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Green Federated Learning. (arXiv:2303.14604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32511;&#33394;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#21644;&#33021;&#28304;&#31649;&#29702;&#31574;&#30053;&#26469;&#20943;&#23569;&#30899;&#36275;&#36857;&#65292;&#20351;&#24471;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#26356;&#21152;&#29615;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#26159;&#30001;&#36234;&#26469;&#36234;&#22823;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#25512;&#21160;&#30340;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#22312;&#25351;&#25968;&#19978;&#22686;&#38271;&#65288;&#20174;2015&#24180;&#21040;2022&#24180;&#27599;10&#20010;&#26376;&#32763;&#19968;&#20493;&#65289;&#65292;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#30899;&#36275;&#36857;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#29992;&#20998;&#25955;&#23454;&#20307;&#30340;&#25968;&#25454;&#35757;&#32451;&#38598;&#20013;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#26102;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#21516;&#26102;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#30899;&#36275;&#36857;&#12290;&#19982;&#21487;&#38752;&#22320;&#21033;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25112;&#30053;&#24615;&#25968;&#25454;&#20013;&#24515;&#30456;&#27604;&#65292;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#33021;&#21033;&#29992;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#25968;&#20159;&#20010;&#20855;&#26377;&#19981;&#21516;&#33021;&#28304;&#26469;&#28304;&#30340;&#32456;&#31471;&#35774;&#22791;&#12290;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#30899;&#36275;&#36857;&#34987;&#35270;&#20026;&#38500;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#20854;&#20182;&#25351;&#26631;&#22806;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#26631;&#20934;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#21644;&#33021;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#23558;&#30899;&#36275;&#36857;&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#32511;&#33394;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of AI is fueled by increasingly large and computationally intensive machine learning models and datasets. As a consequence, the amount of compute used in training state-of-the-art models is exponentially increasing (doubling every 10 months between 2015 and 2022), resulting in a large carbon footprint. Federated Learning (FL) - a collaborative machine learning technique for training a centralized model using data of decentralized entities - can also be resource-intensive and have a significant carbon footprint, particularly when deployed at scale. Unlike centralized AI that can reliably tap into renewables at strategically placed data centers, cross-device FL may leverage as many as hundreds of millions of globally distributed end-user devices with diverse energy sources. Green AI is a novel and important research area where carbon footprint is regarded as an evaluation criterion for AI, alongside accuracy, convergence speed, and other metrics. In this paper, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12653</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20581;&#22766;&#20840;&#24687;&#27627;&#31859;&#27874;&#27874;&#26463;&#25104;&#24418;
&lt;/p&gt;
&lt;p&gt;
Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22825;&#32447;&#38453;&#21015;&#30340;&#27874;&#26463;&#25104;&#24418;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;5G&#21644;&#21363;&#23558;&#25512;&#20986;&#30340;6G&#20013;&#65292;&#22240;&#27492;&#21508;&#31181;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#12289;&#39640;&#32423;&#20248;&#21270;&#31639;&#27861;&#31561;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#21069;&#30740;&#31350;&#26041;&#26696;&#20013;&#20854;&#24615;&#33021;&#30456;&#24403;&#21560;&#24341;&#20154;&#65292;&#20294;&#36890;&#24120;&#24403;&#29615;&#22659;&#25110;&#25968;&#25454;&#38598;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#36805;&#36895;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20855;&#26377;&#24378;&#22823;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#27874;&#26463;&#25104;&#24418;&#32593;&#32476;&#26159;&#26234;&#33021;&#26080;&#32447;&#36890;&#20449;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#27874;&#26463;&#25104;&#24418;&#33258;&#30417;&#30563;&#32593;&#32476;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#28151;&#21512;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#32593;&#32476;&#22312;&#32463;&#20856;&#30340;DeepMIMO&#21644;&#26032;&#30340;WAIR-D&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21407;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#32763;&#35793;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beamforming with large-scale antenna arrays has been widely used in recent years, which is acknowledged as an important part in 5G and incoming 6G. Thus, various techniques are leveraged to improve its performance, e.g., deep learning, advanced optimization algorithms, etc. Although its performance in many previous research scenarios with deep learning is quite attractive, usually it drops rapidly when the environment or dataset is changed. Therefore, designing effective beamforming network with strong robustness is an open issue for the intelligent wireless communications. In this paper, we propose a robust beamforming self-supervised network, and verify it in two kinds of different datasets with various scenarios. Simulation results show that the proposed self-supervised network with hybrid learning performs well in both classic DeepMIMO and new WAIR-D dataset with the strong robustness under the various environments. Also, we present the principle to explain the rationality of this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#21457;&#20102;&#19968;&#31181;&#25972;&#21512;&#24739;&#32773;ICU&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#26469;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24739;&#32773;&#29366;&#20917;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2303.04630</link><description>&lt;p&gt;
&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#30340;&#20020;&#24202;&#36827;&#31243;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65306;&#20174;&#27431;&#27954;&#37325;&#30151;&#30417;&#25252;&#23460;&#25968;&#25454;&#20013;&#25366;&#25496;&#24739;&#32773;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data. (arXiv:2303.04630v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#21457;&#20102;&#19968;&#31181;&#25972;&#21512;&#24739;&#32773;ICU&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#26469;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#24212;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24739;&#32773;&#29366;&#20917;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#24739;&#32773;&#29366;&#20917;&#34920;&#36848;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#20010;&#20307;&#21270;&#27835;&#30103;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24314;&#27169;&#31574;&#30053;&#65292;&#25972;&#21512;&#20303;&#38498;&#26399;&#38388;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#20026;&#27599;&#21517;TBI&#24739;&#32773;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20303;&#38498;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#36827;&#23637;&#12290;&#30740;&#31350;&#38543;&#26426;&#36873;&#25321;&#20102;1550&#21517;&#26469;&#33258;&#27431;&#27954;65&#20010;&#20013;&#24515;&#12289;19&#20010;&#22269;&#23478;&#30340;TBI&#24739;&#32773;&#65292;&#25552;&#21462;&#20102;&#22312;ICU&#20303;&#38498;&#26399;&#38388;&#25110;&#20043;&#21069;&#25910;&#38598;&#30340;&#25152;&#26377;1166&#20010;&#21464;&#37327;&#65292;&#20197;&#21450;6&#20010;&#26376;&#30340;Glasgow Outcome Scale-Extended&#65288;GOSE&#65289;&#21151;&#33021;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#25152;&#26377;&#21464;&#37327;&#30340;&#20196;&#29260;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65288;&#21253;&#25324;&#32570;&#22833;&#25968;&#25454;&#65289;&#26144;&#23556;&#21040;&#27599;2&#20010;&#23567;&#26102;&#30340;GOSE&#39044;&#21518;&#12290;&#36890;&#36807;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Somers' Dxy&#35780;&#20272;&#20102;GOSE&#30340;&#26085;&#24120;&#24046;&#24322;&#30340;&#26657;&#20934;&#21644;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;TimeSHAP&#26469;&#35745;&#31639;&#21464;&#37327;&#21450;&#20808;&#21069;&#26102;&#38388;&#23545;&#27599;&#20010;&#30149;&#20363;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods to characterise the evolving condition of traumatic brain injury (TBI) patients in the intensive care unit (ICU) do not capture the context necessary for individualising treatment. We aimed to develop a modelling strategy which integrates all data stored in medical records to produce an interpretable disease course for each TBI patient's ICU stay. From a prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI patients, we extracted all 1,166 variables collected before or during ICU stay as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended (GOSE). We trained recurrent neural network models to map a token-embedded time series representation of all variables (including missing data) to an ordinal GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated calibration and the explanation of ordinal variance in GOSE with Somers' Dxy. Furthermore, we applied TimeSHAP to calculate the contribution of variables and prior ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#29616;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01584</link><description>&lt;p&gt;
&#36827;&#21270;&#22686;&#24378;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#29616;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#25216;&#26415;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#36741;&#21161;&#38454;&#27573;&#65288;&#20063;&#31216;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#65289;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24433;&#21709;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#32422;&#26463;&#26465;&#20214;&#19979;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#65292;&#24182;&#27979;&#37327;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20013;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#26579;&#33394;&#20307;&#20013;&#32534;&#30721;&#19981;&#21516;&#32452;&#21512;&#30340;&#22686;&#24378;&#25805;&#20316;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#21270;&#20248;&#21270;&#26426;&#21046;&#23547;&#27714;&#26368;&#20248;&#30340;&#22686;&#24378;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#37322;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and expla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01486</link><description>&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding plasticity in neural networks. (arXiv:2303.01486v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22833;&#21435;&#21487;&#22609;&#24615;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#28145;&#24230;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#24182;&#38750;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20445;&#25345;&#21487;&#22609;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#26159;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#26681;&#25454;&#26032;&#20449;&#24687;&#26356;&#25913;&#20854;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21363;&#20351;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#20063;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#21435;&#21487;&#22609;&#24615;&#65292;&#20294;&#39537;&#21160;&#36825;&#31181;&#29616;&#35937;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#28145;&#24230;&#29702;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#20197;&#24341;&#23548;&#26410;&#26469;&#23545;&#26377;&#38024;&#23545;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#19982;&#25439;&#22833;&#26799;&#24230;&#26354;&#29575;&#30340;&#21464;&#21270;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#36890;&#24120;&#21457;&#29983;&#22312;&#26080;&#39281;&#21644;&#21333;&#20803;&#25110;&#21457;&#25955;&#26799;&#24230;&#33539;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#19968;&#20123;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#35774;&#35745;&#36873;&#25321;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#20445;&#25345;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#24178;&#39044;&#25514;&#26045;&#22312;&#19968;&#31995;&#21015;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25928;&#29992;&#65292;&#35777;&#26126;&#23427;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00102</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00102
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#25805;&#32437;&#26032;&#38395;&#26159;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#36943;&#21046;&#20854;&#20256;&#25773;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#35823;&#23548;&#20449;&#24687;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#27979;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22312;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#26377;&#23475;&#35758;&#31243;&#36825;&#19968;&#37325;&#35201;&#25361;&#25112;&#26041;&#38754;&#32570;&#20047;&#25237;&#36164;&#65307;&#35782;&#21035;&#26377;&#23475;&#35758;&#31243;&#23545;&#20110;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#28508;&#22312;&#29616;&#23454;&#21361;&#23475;&#30340;&#26032;&#38395;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#23457;&#26597;&#21046;&#24230;&#23384;&#22312;&#30495;&#23454;&#30340;&#25285;&#24551;&#65292;&#26377;&#23475;&#35758;&#31243;&#26816;&#27979;&#22120;&#24517;&#39035;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;NewsAgendas&#30340;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35758;&#31243;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#19982;&#40657;&#30418;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#21644;&#35746;&#21333;&#20998;&#27573;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#21152;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2301.08360</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#65306;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#21644;&#35746;&#21333;&#20998;&#27573;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#21152;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#33655;&#20848;&#30005;&#21147;&#24066;&#22330;&#65292;&#30001;&#26085;&#21069;&#24066;&#22330;&#21644;&#21363;&#26102;&#24179;&#34913;&#24066;&#22330;&#32452;&#25104;&#65292;&#24182;&#19988;&#36816;&#20316;&#26041;&#24335;&#31867;&#20284;&#25293;&#21334;&#12290;&#30001;&#20110;&#20379;&#38656;&#27874;&#21160;&#65292;&#36890;&#24120;&#23384;&#22312;&#19981;&#24179;&#34913;&#23548;&#33268;&#20004;&#20010;&#24066;&#22330;&#20215;&#26684;&#19981;&#21516;&#65292;&#20174;&#32780;&#25552;&#20379;&#22871;&#21033;&#26426;&#20250;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#21452;&#20195;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27431;&#27954;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#30340;&#21452;&#23618;&#20223;&#30495;&#19982;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#36890;&#36807;&#27169;&#20223;&#30005;&#21147;&#20132;&#26131;&#21592;&#30340;&#20132;&#26131;&#34892;&#20026;&#26469;&#34701;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#26469;&#27169;&#20223;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#35774;&#35745;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#31995;&#32479;&#65292;&#25913;&#21892;&#35757;&#32451;&#20013;&#30340;&#25910;&#25947;&#24615;&#24182;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35746;&#21333;&#20998;&#27573;&#22686;&#21152;&#20102;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28436;&#31034;&#20102;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30005;&#21147;&#22871;&#21033;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&amp;L). Our study demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2212.05206</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#26032;&#20852;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20687;GPT-3&#36825;&#26679;&#30340;LLMs&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30452;&#35273;&#24778;&#20154;&#30456;&#20284;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30001;&#27492;&#24102;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#38519;&#20837;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35748;&#30693;&#21453;&#24605;&#27979;&#35797;&#65288;CRT&#65289;&#20197;&#21450;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#38169;&#35273;&#26469;&#25506;&#32034;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#30740;&#31350;LLMs&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20182;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#32858;&#31867;&#26041;&#27861;&#65292;&#28165;&#26224;&#22320;&#35782;&#21035;&#20102;&#32858;&#21512;&#29289;&#29076;&#20307;&#30340;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65292;&#24182;&#25581;&#31034;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#25429;&#25417;&#32858;&#21512;&#29289;&#38142;&#34892;&#20026;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.14220</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32858;&#21512;&#29289;&#29076;&#20307;&#29627;&#29827;&#36716;&#21464;&#30340;&#35782;&#21035;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-driven identification and analysis of the glass transition in polymer melts. (arXiv:2211.14220v2 [cond-mat.soft] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#32858;&#31867;&#26041;&#27861;&#65292;&#28165;&#26224;&#22320;&#35782;&#21035;&#20102;&#32858;&#21512;&#29289;&#29076;&#20307;&#30340;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65292;&#24182;&#25581;&#31034;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#25429;&#25417;&#32858;&#21512;&#29289;&#38142;&#34892;&#20026;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#29627;&#29827;&#36716;&#21464;&#30340;&#24615;&#36136;&#20197;&#21450;&#20934;&#30830;&#20272;&#35745;&#32858;&#21512;&#29289;&#26448;&#26009;&#30340;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#20173;&#28982;&#26159;&#23454;&#39564;&#21644;&#29702;&#35770;&#32858;&#21512;&#29289;&#31185;&#23398;&#20013;&#30340;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#32454;&#33410;&#65292;&#24182;&#32771;&#34385;&#21333;&#20010;&#38142;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#23427;&#28165;&#26970;&#22320;&#35782;&#21035;&#20102;&#24369;&#21322;&#21018;&#24615;&#32858;&#21512;&#29289;&#29076;&#20307;&#30340;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#12290;&#36890;&#36807;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#21363;&#20351;&#20174;&#30456;&#23545;&#30701;&#26102;&#38388;&#36712;&#36857;&#20013;&#20063;&#33021;&#22815;&#35782;&#21035;&#20986;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65292;&#22312;&#36825;&#20123;&#36712;&#36857;&#20013;&#65292;&#21333;&#20307;&#30340;&#20301;&#31227;&#36798;&#21040;&#20102;Rouse&#31867;&#20284;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#25429;&#25417;&#21040;&#30340;&#27874;&#21160;&#21453;&#26144;&#20102;&#38142;&#30340;&#34892;&#20026;&#21464;&#21270;&#65306;&#20174;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#20197;&#19978;&#30340;&#26500;&#35937;&#37325;&#25490;&#21040;&#28201;&#24230;&#20197;&#19979;&#30340;&#23567;&#37325;&#25490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#30452;&#25509;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the nature of glass transition, as well as precise estimation of the glass transition temperature for polymeric materials, remain open questions in both experimental and theoretical polymer sciences. We propose a data-driven approach, which utilizes the high-resolution details accessible through the molecular dynamics simulation and considers the structural information of individual chains. It clearly identifies the glass transition temperature of polymer melts of weakly semiflexible chains. By combining principal component analysis and clustering, we identify the glass transition temperature in the asymptotic limit even from relatively short-time trajectories, which just reach into the Rouse-like monomer displacement regime. We demonstrate that fluctuations captured by the principal component analysis reflect the change in a chain's behaviour: from conformational rearrangement above to small rearrangements below the glass transition temperature. Our approach is straightf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21457;&#29616;&#20854;&#19982;&#23616;&#37096;&#20960;&#20309;&#12289;&#27491;&#21017;&#24615;&#20197;&#21450;&#38160;&#24230;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#30028;&#35299;&#22312;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2211.08043</link><description>&lt;p&gt;
Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65306;&#23616;&#37096;&#20960;&#20309; vs &#27491;&#21017;&#24615; vs &#38160;&#24230;
&lt;/p&gt;
&lt;p&gt;
The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. (arXiv:2211.08043v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21457;&#29616;&#20854;&#19982;&#23616;&#37096;&#20960;&#20309;&#12289;&#27491;&#21017;&#24615;&#20197;&#21450;&#38160;&#24230;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#30028;&#35299;&#22312;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#38236;&#20687;&#19979;&#38477;&#21040;&#38236;&#20687;-&#36817;&#31471;&#21450;&#20854;&#20048;&#35266;&#21464;&#31181;&#30340;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#65292;&#20316;&#20026;&#19968;&#31181;&#20381;&#36182;&#20110;&#23450;&#20041;&#26041;&#27861;&#30340;&#36817;&#31471;&#26144;&#23556;&#25152;&#23548;&#33268;&#30340;&#23616;&#37096;&#20960;&#20309;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#20851;&#27880;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#23616;&#37096;&#35299;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#32473;&#23450;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21462;&#20915;&#20110;&#20854;&#30456;&#20851;&#30340;Legendre&#25351;&#25968;&#65292;&#36825;&#20010;&#27010;&#24565;&#24230;&#37327;&#20102;&#25509;&#36817;&#35299;&#30340;Bregman&#20989;&#25968;&#65288;&#27431;&#20960;&#37324;&#24471;&#12289;&#29109;&#25110;&#20854;&#20182;&#20989;&#25968;&#65289;&#30340;&#22686;&#38271;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#30028;&#35299;&#22312;&#20855;&#26377;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#31163;&#24773;&#20917;&#65306;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#19968;&#33324;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#24403;&#26041;&#27861;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#31181;&#20108;&#20998;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#24046;&#21035;&#26356;&#21152;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the last-iterate convergence rate of Bregman proximal methods from mirror descent to mirror-prox and its optimistic variants - as a function of the local geometry induced by the prox-mapping defining the method. For generality, we focus on local solutions of constrained, non-monotone variational inequalities, and we show that the convergence rate of a given method depends sharply on its associated Legendre exponent, a notion that measures the growth rate of the underlying Bregman function (Euclidean, entropic, or other) near a solution. In particular, we show that boundary solutions exhibit a stark separation of regimes between methods with a zero and non-zero Legendre exponent: the former converge at a linear rate, while the latter converge, in general, sublinearly. This dichotomy becomes even more pronounced in linearly constrained problems where methods with entropic regularization achieve a linear convergence rate along sharp directions, compared to convergence in a fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#21442;&#25968;&#21270;&#19981;&#21487;&#30693;&#19988;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21644;&#20998;&#24067;&#36716;&#25442;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01258</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#21442;&#25968;&#21270;&#19981;&#21487;&#30693;&#19988;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21644;&#20998;&#24067;&#36716;&#25442;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#38480;&#26080;&#27861;&#35299;&#37322;&#24433;&#21709;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30001;&#20110;&#36825;&#20123;&#30028;&#38480;&#36890;&#24120;&#23545;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#23427;&#20204;&#23481;&#26131;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#26080;&#27861;&#32771;&#34385;&#21040;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20381;&#36182;&#20110;&#25968;&#25454;&#31354;&#38388;&#20013;&#39044;&#27979;&#20989;&#25968;&#30340;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#24615;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23545;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#19968;&#20123;&#23567;&#30340;&#20462;&#25913;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#19978;&#21487;&#20197;&#33719;&#24471;&#21152;&#36895;&#30340;&#36895;&#29575;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#20855;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#20998;&#26512;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#32467;&#26524;&#26174;&#31034;&#30028;&#38480;&#20540;&#26159;
&lt;/p&gt;
&lt;p&gt;
Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.08549</link><description>&lt;p&gt;
&#22269;&#38469;&#31354;&#38388;&#31449;&#33258;&#21160;&#32039;&#24613;&#26080;&#23576;&#35299;&#20915;&#26041;&#26696;: &#24102;&#26377;Bi-GRU&#30340;(AED-ISS)
&lt;/p&gt;
&lt;p&gt;
Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;PM2.5&#25110;PM0.3&#38382;&#39064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#39063;&#31890;&#29289;&#19981;&#20165;&#23545;&#29615;&#22659;&#21644;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#32780;&#19988;&#23545;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#30340;&#20202;&#22120;&#20063;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22242;&#38431;&#26088;&#22312;&#23558;&#21508;&#31181;&#39063;&#31890;&#29289;&#27987;&#24230;&#19982;&#30913;&#22330;&#12289;&#28287;&#24230;&#12289;&#21152;&#36895;&#24230;&#12289;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;CO2&#27987;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;(EWS)&#65292;&#33021;&#22815;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#65292;&#20197;&#20445;&#25252;&#20182;&#20204;&#22312;&#26576;&#20123;&#23454;&#39564;&#20013;&#30340;&#20202;&#22120;&#65292;&#25110;&#32773;&#25552;&#39640;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65307;&#27492;&#22806;&#65292;&#25152;&#26500;&#24314;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#30340;&#21407;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23454;&#29616;Bi-GRU(&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;)&#31639;&#27861;&#65292;&#25910;&#38598;&#36807;&#21435;90&#20998;&#38047;&#30340;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#36229;&#36807;2.5&#24494;&#31859;&#30340;&#39063;&#31890;&#29289;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rising attention for the issue of PM2.5 or PM0.3, particulate matters have become not only a potential threat to both the environment and human, but also a harming existence to instruments onboard International Space Station (ISS). Our team is aiming to relate various concentration of particulate matters to magnetic fields, humidity, acceleration, temperature, pressure and CO2 concentration. Our goal is to establish an early warning system (EWS), which is able to forecast the levels of particulate matters and provides ample reaction time for astronauts to protect their instruments in some experiments or increase the accuracy of the measurements; In addition, the constructed model can be further developed into a prototype of a remote-sensing smoke alarm for applications related to fires. In this article, we will implement the Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for past 90 minutes and predict the levels of particulates which over 2.5 micromete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2210.07420</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#35745;&#21010;&#31283;&#20581;&#30340;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26434;&#20081;&#38382;&#39064;&#65292;&#22810;&#20010;&#21018;&#24615;&#20984;&#22810;&#36793;&#24418;&#29289;&#20307;&#38543;&#26426;&#25918;&#32622;&#22312;&#19968;&#20010;&#24179;&#38754;&#34920;&#38754;&#19978;&#65292;&#24517;&#39035;&#20351;&#29992;&#21333;&#20010;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#25235;&#21462;&#26041;&#24335;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36816;&#36755;&#21040;&#35013;&#31665;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#25705;&#25830;&#26469;&#22686;&#21152;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#35745;&#21010;&#31283;&#20581;&#30340;&#22810;&#29289;&#20307;&#25235;&#21462;&#12290;&#22312;&#29289;&#29702;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#20110;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#21457;&#29616;&#25104;&#21151;&#29575;&#22686;&#21152;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;&#19982;&#21333;&#20010;&#29289;&#20307;&#25235;&#21462;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;3.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20851;&#32852;&#25968;&#37327;&#19978;&#20855;&#26377;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.07229</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;
&lt;/p&gt;
&lt;p&gt;
Mass-Editing Memory in a Transformer. (arXiv:2210.07229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20851;&#32852;&#25968;&#37327;&#19978;&#20855;&#26377;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20351;&#29992;&#26032;&#30340;&#35760;&#24518;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#65292;&#20197;&#26367;&#25442;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#28155;&#21152;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#20027;&#35201;&#20165;&#38480;&#20110;&#26356;&#26032;&#21333;&#20010;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;MEMIT&#65292;&#19968;&#31181;&#30452;&#25509;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#20851;&#32852;&#65292;&#23545;&#20110;GPT-J(6B)&#21644;GPT-NeoX(20B)&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://memit.baulab.info&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.04635</link><description>&lt;p&gt;
FaDIn: &#38024;&#23545;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#26680;&#30340;Hawkes&#36807;&#31243;&#30340;&#24555;&#36895;&#31163;&#25955;&#21270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels. (arXiv:2210.04635v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#26159;&#24314;&#27169;&#20107;&#20214;&#25968;&#25454;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;&#22312;&#25152;&#26377;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;Hawkes&#36807;&#31243;&#34987;&#35777;&#26126;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#30340;&#36866;&#24403;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#25351;&#25968;&#25110;&#38750;&#21442;&#25968;&#26680;&#26102;&#12290;&#23613;&#31649;&#38750;&#21442;&#25968;&#26680;&#26159;&#19968;&#31181;&#36873;&#25321;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#32780;&#25351;&#25968;&#26680;&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#65292;&#23545;&#20110;&#31435;&#21363;&#35302;&#21457;&#26356;&#22810;&#20107;&#20214;&#30340;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#25928;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#20272;&#35745;&#24310;&#36831;&#30340;&#24212;&#29992;&#65288;&#22914;&#31070;&#32463;&#31185;&#23398;&#65289;&#65292;&#23427;&#20204;&#19981;&#22826;&#36866;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#25903;&#25345;&#30340;&#19968;&#33324;&#21442;&#25968;&#26680;&#36827;&#34892;TPP&#25512;&#29702;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#21033;&#29992;&#20107;&#20214;&#30340;&#31163;&#25955;&#21270;&#30340;&#24555;&#36895;$\ell_2$&#26799;&#24230;&#27714;&#35299;&#22120;&#12290;&#22312;&#29702;&#35770;&#19978;&#25903;&#25345;&#31163;&#25955;&#21270;&#30340;&#20351;&#29992;&#21518;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26032;&#26041;&#27861;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal point processes (TPP) are a natural tool for modeling event-based data. Among all TPP models, Hawkes processes have proven to be the most widely used, mainly due to their adequate modeling for various applications, particularly when considering exponential or non-parametric kernels. Although non-parametric kernels are an option, such models require large datasets. While exponential kernels are more data efficient and relevant for specific applications where events immediately trigger more events, they are ill-suited for applications where latencies need to be estimated, such as in neuroscience. This work aims to offer an efficient solution to TPP inference using general parametric kernels with finite support. The developed solution consists of a fast $\ell_2$ gradient-based solver leveraging a discretized version of the events. After theoretically supporting the use of discretization, the statistical and computational efficiency of the novel approach is demonstrated through va
&lt;/p&gt;</description></item><item><title>FedDef&#26159;&#19968;&#31181;&#38450;&#24481;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#26799;&#24230;&#27844;&#38706;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38544;&#31169;&#35780;&#20272;&#25351;&#26631;&#21644;&#23545;&#25239;&#25915;&#20987;&#26469;&#35780;&#20272;FL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04052</link><description>&lt;p&gt;
FedDef: &#38754;&#21521;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
FedDef: Defense Against Gradient Leakage in Federated Learning-based Network Intrusion Detection Systems. (arXiv:2210.04052v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04052
&lt;/p&gt;
&lt;p&gt;
FedDef&#26159;&#19968;&#31181;&#38450;&#24481;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#26799;&#24230;&#27844;&#38706;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38544;&#31169;&#35780;&#20272;&#25351;&#26631;&#21644;&#23545;&#25239;&#25915;&#20987;&#26469;&#35780;&#20272;FL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#24322;&#24120;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20197;&#26816;&#27979;&#24694;&#24847;&#27969;&#37327;&#12290;&#20026;&#20102;&#25193;&#22823;&#22522;&#20110;DL&#30340;&#26041;&#27861;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#29992;&#25143;&#22312;&#23562;&#37325;&#20010;&#20307;&#25968;&#25454;&#38544;&#31169;&#30340;&#22522;&#30784;&#19978;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23545;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#19979;FL-based NIDSs&#23545;&#29616;&#26377;&#38544;&#31169;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;FL-based NIDSs&#35774;&#35745;&#30340;&#38544;&#31169;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#65288;1&#65289;&#38544;&#31169;&#35780;&#20998;&#65292;&#29992;&#20110;&#35780;&#20272;&#20351;&#29992;&#37325;&#26500;&#25915;&#20987;&#24674;&#22797;&#30340;&#27969;&#37327;&#29305;&#24449;&#19982;&#21407;&#22987;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23545;&#25239;&#25915;&#20987;&#19979;&#20351;&#29992;&#24674;&#22797;&#30340;&#27969;&#37327;&#23545;&#25239;NIDSs&#30340;&#36867;&#36991;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#20445;&#25252;&#65292;&#30456;&#24212;&#30340;&#23545;&#25239;&#27969;&#37327;&#29978;&#33267;&#21487;&#20197;&#35268;&#36991;SOTA NIDS Kitsune&#12290;&#20026;&#20102;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#24182;&#26500;&#24314;&#26356;&#21152;&#40065;&#26834;&#30340;FL&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods have been widely applied to anomaly-based network intrusion detection system (NIDS) to detect malicious traffic. To expand the usage scenarios of DL-based methods, federated learning (FL) allows multiple users to train a global model on the basis of respecting individual data privacy. However, it has not yet been systematically evaluated how robust FL-based NIDSs are against existing privacy attacks under existing defenses. To address this issue, we propose two privacy evaluation metrics designed for FL-based NIDSs, including (1) privacy score that evaluates the similarity between the original and recovered traffic features using reconstruction attacks, and (2) evasion rate against NIDSs using adversarial attack with the recovered traffic. We conduct experiments to illustrate that existing defenses provide little protection and the corresponding adversarial traffic can even evade the SOTA NIDS Kitsune. To defend against such attacks and build a more robust FL
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#22312;&#22270;&#20687;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#29983;&#25104;&#26080;&#25928;&#35270;&#22270;&#21644;&#19981;&#21487;&#38752;&#30456;&#20284;&#24615;&#23545;&#30340;&#38480;&#21046;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13964</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13964
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#22312;&#22270;&#20687;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#29983;&#25104;&#26080;&#25928;&#35270;&#22270;&#21644;&#19981;&#21487;&#38752;&#30456;&#20284;&#24615;&#23545;&#30340;&#38480;&#21046;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;Graph Contrastive Learning&#65292;GCL&#65289;&#24050;&#25104;&#20026;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20027;&#35201;&#20511;&#37492;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#29702;&#65306;&#36890;&#36807;&#25351;&#23450;&#23436;&#20840;&#30456;&#20284;&#30340;&#23545;&#26469;&#24314;&#27169;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#65292;&#36825;&#31181;&#33539;&#24335;&#36935;&#21040;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#38480;&#21046;&#65306;&#65288;1&#65289;&#29983;&#25104;&#30340;&#35270;&#22270;&#30340;&#26377;&#25928;&#24615;&#26080;&#27861;&#24471;&#21040;&#20445;&#35777;&#65306;&#22270;&#25200;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#36829;&#21453;&#35821;&#20041;&#21644;&#22270;&#25968;&#25454;&#20869;&#22312;&#25299;&#25169;&#24615;&#36136;&#30340;&#26080;&#25928;&#35270;&#22270;&#65307;&#65288;2&#65289;&#22312;&#22270;&#35270;&#22270;&#20013;&#25351;&#23450;&#23436;&#20840;&#30456;&#20284;&#30340;&#23545;&#26159;&#19981;&#21487;&#38752;&#30340;&#65306;&#23545;&#20110;&#25277;&#35937;&#21644;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#22270;&#25968;&#25454;&#65292;&#20154;&#20204;&#24456;&#38590;&#30452;&#35266;&#22320;&#20915;&#23450;&#32477;&#23545;&#30340;&#30456;&#20284;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;GCL&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#65306;GCL&#26159;&#21542;&#33021;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21407;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has emerged as a promising approach in the realm of graph self-supervised learning. Prevailing GCL methods mainly derive from the principles of contrastive learning in the field of computer vision: modeling invariance by specifying absolutely similar pairs. However, when applied to graph data, this paradigm encounters two significant limitations: (1) the validity of the generated views cannot be guaranteed: graph perturbation may produce invalid views against semantics and intrinsic topology of graph data; (2) specifying absolutely similar pairs in the graph views is unreliable: for abstract and non-Euclidean graph data, it is difficult for humans to decide the absolute similarity and dissimilarity intuitively. Despite the notable performance of current GCL methods, these challenges necessitate a reevaluation: Could GCL be more effectively tailored to the intrinsic properties of graphs, rather than merely adopting principles from computer vision? In res
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProMix&#30340;&#26032;&#39062;LNL&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#12290;&#37319;&#29992;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#26469;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;SSL&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10276</link><description>&lt;p&gt;
ProMix&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10276
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProMix&#30340;&#26032;&#39062;LNL&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#12290;&#37319;&#29992;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#26469;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;SSL&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#65288;LNL&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#65292;&#22240;&#20026;&#24102;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25968;&#25454;&#30456;&#23545;&#36739;&#20415;&#23452;&#26131;&#24471;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37319;&#29992;&#29305;&#23450;&#30340;&#36873;&#25321;&#26426;&#21046;&#26469;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#65292;&#28982;&#21518;&#24212;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#27493;&#39588;&#20027;&#35201;&#25552;&#20379;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#21644;&#36275;&#22815;&#22909;&#30340;&#28165;&#27905;&#23376;&#38598;&#65292;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#24178;&#20928;&#26679;&#26412;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LNL&#26694;&#26550;ProMix&#65292;&#35797;&#22270;&#36890;&#36807;&#26368;&#22823;&#21270;&#24178;&#20928;&#26679;&#26412;&#30340;&#25928;&#29992;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#39640;&#32622;&#20449;&#24230;&#36873;&#25321;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#37027;&#20123;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#24471;&#20998;&#21644;&#19982;&#32473;&#23450;&#26631;&#31614;&#21305;&#37197;&#39044;&#27979;&#30340;&#31034;&#20363;&#65292;&#20197;&#21160;&#24577;&#25193;&#23637;&#22522;&#30784;&#24178;&#20928;&#26679;&#26412;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#36807;&#24230;&#36873;&#25321;&#28165;&#27905;&#26679;&#26412;&#38598;&#31243;&#24207;&#30340;&#28508;&#22312;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SSL&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26102;&#24471;&#21040;&#24179;&#34913;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.02231</link><description>&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20559;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#21033;&#30410;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#36712;&#36857;&#27573;&#23545;&#20043;&#38388;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#24120;&#20551;&#35774;&#36825;&#20123;&#20154;&#31867;&#20559;&#22909;&#20165;&#30001;&#37096;&#20998;&#22238;&#25253;&#26469;&#20915;&#23450;&#65292;&#21363;&#27599;&#20010;&#36712;&#36857;&#27573;&#19978;&#30340;&#22870;&#21169;&#24635;&#21644;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20551;&#35774;&#23384;&#22312;&#32570;&#38519;&#65292;&#25552;&#20986;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#30001;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#26469;&#20915;&#23450;&#65292;&#36951;&#25022;&#26159;&#19968;&#31181;&#34913;&#37327;&#36712;&#36857;&#27573;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#20559;&#31163;&#31243;&#24230;&#30340;&#24230;&#37327;&#12290;&#22312;&#26681;&#25454;&#36951;&#25022;&#29983;&#25104;&#30340;&#26080;&#31351;&#22810;&#20010;&#20559;&#22909;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#35782;&#21035;&#21040;&#19982;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#31561;&#20215;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20197;&#21069;&#30340;&#37096;&#20998;&#22238;&#25253;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#32570;&#20047;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#23646;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.01255</link><description>&lt;p&gt;
&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#39640;&#32500;&#25193;&#25955;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25968;&#23398;&#24314;&#27169;&#24037;&#20855;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#37329;&#34701;&#21040;&#35745;&#31639;&#21270;&#23398;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#26631;&#20934;&#25968;&#20540;&#25216;&#26415;&#36890;&#24120;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23450;&#20041;&#22312;&#39640;&#32500;&#22495;&#19978;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#23450;&#24120;&#25193;&#25955;&#26041;&#31243;&#12290;&#21463;&#39640;&#32500;&#31232;&#30095;&#20989;&#25968;&#36924;&#36817;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21387;&#32553;&#24863;&#30693;&#21644;&#35889;&#33394;&#25955;&#30340;&#24605;&#24819;&#65292;&#29992;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#20195;&#26367;&#20102;&#32467;&#26500;&#21270;&#33394;&#25955;&#32593;&#26684;&#30340;&#20351;&#29992;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65288;&#22914;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#21644;&#8467;^1&#26368;&#23567;&#21270;&#65289;&#26469;&#36817;&#20284;PDE&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Partial Differential Equations (PDEs) are a popular mathematical modelling tool, with applications ranging from finance to computational chemistry. However, standard numerical techniques for solving these PDEs are typically affected by the curse of dimensionality. In this work, we tackle this challenge while focusing on stationary diffusion equations defined over a high-dimensional domain with periodic boundary conditions. Inspired by recent progress in sparse function approximation in high dimensions, we propose a new method called compressive Fourier collocation. Combining ideas from compressive sensing and spectral collocation, our method replaces the use of structured collocation grids with Monte Carlo sampling and employs sparse recovery techniques, such as orthogonal matching pursuit and $\ell^1$ minimization, to approximate the Fourier coefficients of the PDE solution. We conduct a rigorous theoretical analysis showing that the approximation error of the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#26469;&#35299;&#20915;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20851;&#31995;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13492</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#23398;&#20064;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sparse Graph Learning from Spatiotemporal Time Series. (arXiv:2205.13492v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#26469;&#35299;&#20915;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20851;&#31995;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#26480;&#20986;&#25104;&#26524;&#34920;&#26126;&#65292;&#20851;&#31995;&#32422;&#26463;&#20026;&#31070;&#32463;&#39044;&#27979;&#26550;&#26500;&#24341;&#20837;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#34920;&#24449;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#20851;&#31995;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#20174;&#32780;&#20351;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22312;&#21518;&#32493;&#22788;&#29702;&#38454;&#27573;&#20013;&#20351;&#29992;&#21738;&#20010;&#20851;&#31995;&#22270;&#25104;&#20026;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26377;&#29702;&#35770;&#20381;&#25454;&#30340;&#27010;&#29575;&#35780;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20219;&#21153;&#25972;&#20010;&#36807;&#31243;&#30340;&#24615;&#33021;&#26469;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35780;&#20998;&#26799;&#24230;&#20272;&#35745;&#30340;&#24041;&#22266;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26799;&#24230;&#20272;&#35745;&#22120;&#35843;&#25972;&#20026;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#24322;&#27493;&#36873;&#39033;&#25191;&#34892;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.15925</link><description>&lt;p&gt;
&#24322;&#27493;&#12289;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#24322;&#27493;&#36873;&#39033;&#25191;&#34892;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#21327;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#29366;&#24577;&#30340;&#20013;&#22830;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#20302;&#32423;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#22312;&#20855;&#26377;&#22823;&#35268;&#27169;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#65292;&#23558;MAPG&#26041;&#27861;&#25193;&#23637;&#20026;&#20351;&#29992;&#26356;&#39640;&#32423;&#21035;&#30340;&#21160;&#20316;&#65288;&#20063;&#31216;&#20026;&#36873;&#39033;&#65289;&#20197;&#25552;&#39640;&#31574;&#30053;&#25628;&#32034;&#25928;&#29575;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#26426;&#22120;&#20154;&#36873;&#39033;&#25191;&#34892;&#36890;&#24120;&#26159;&#24322;&#27493;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#27493;&#39588;&#36873;&#25321;&#24182;&#23436;&#25104;&#23427;&#20204;&#30340;&#36873;&#39033;&#12290;&#36825;&#20351;&#24471;MAPG&#26041;&#27861;&#24456;&#38590;&#25512;&#23548;&#20986;&#19968;&#20010;&#20013;&#22830;&#31574;&#30053;&#24182;&#35780;&#20272;&#20854;&#26799;&#24230;&#65292;&#22240;&#20026;&#20013;&#22830;&#31574;&#30053;&#24635;&#26159;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#36873;&#25321;&#26032;&#30340;&#36873;&#39033;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks thro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#20256;&#36755;&#25110;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#38480;&#21046;&#35813;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05877</link><description>&lt;p&gt;
&#12298;&#34394;&#26500;&#30340;&#32763;&#36716;&#65306;&#26080;&#25968;&#25454;&#30340;&#20013;&#27602;&#32852;&#37030;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#20256;&#36755;&#25110;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#38480;&#21046;&#35813;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#25915;&#20987;&#21487;&#20197;&#20005;&#37325;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#38480;&#21046;&#36825;&#31181;&#26032;&#20852;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#27169;&#24335;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#23450;&#21521;&#25915;&#20987;&#23545;&#35768;&#22810;&#22330;&#26223;&#26469;&#35828;&#37117;&#19981;&#23454;&#38469;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#25915;&#20987;&#32773;&#30693;&#36947;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#27599;&#20010;&#26356;&#26032;&#65292;&#25110;&#32773;&#25915;&#20987;&#32773;&#25317;&#26377;&#22823;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#26469;&#27169;&#20223;&#33391;&#24615;&#21442;&#19982;&#26041;&#30340;&#26356;&#26032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#20256;&#36755;&#65292;&#20063;&#26080;&#38656;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;DFA&#30340;&#21464;&#20307;&#65292;&#21363;DFA-R&#21644;DFA-G&#65292;&#23427;&#20204;&#22312;&#22914;&#20309;&#26435;&#34913;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DFA-R&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#19968;&#20010;&#24694;&#24847;&#25968;&#25454;&#23618;&#26469;&#26368;&#23567;&#21270;&#20840;&#23616;&#27169;&#22411;&#25152;&#26377;&#36755;&#20986;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#32780;DFA-G&#21017;&#36890;&#36807;&#20132;&#20114;&#24335;&#35757;&#32451;&#24694;&#24847;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attacks on Federated Learning (FL) can severely reduce the quality of the generated models and limit the usefulness of this emerging learning paradigm that enables on-premise decentralized learning. However, existing untargeted attacks are not practical for many scenarios as they assume that i) the attacker knows every update of benign clients, or ii) the attacker has a large dataset to locally train updates imitating benign parties. In this paper, we propose a data-free untargeted attack (DFA) that synthesizes malicious data to craft adversarial models without eavesdropping on the transmission of benign clients at all or requiring a large quantity of task-specific training data. We design two variants of DFA, namely DFA-R and DFA-G, which differ in how they trade off stealthiness and effectiveness. Specifically, DFA-R iteratively optimizes a malicious data layer to minimize the prediction confidence of all outputs of the global model, whereas DFA-G interactively trains a malicious dat
&lt;/p&gt;</description></item><item><title>&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;Successor Representations (SR)&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#26469;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2110.15701</link><description>&lt;p&gt;
&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15701
&lt;/p&gt;
&lt;p&gt;
&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;Successor Representations (SR)&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#26469;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#28304;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21450;&#20854;&#25193;&#23637;&#30340;&#32487;&#25215;&#29305;&#24449;&#65288;SF&#65289;&#26159;&#22312;&#22870;&#21169;&#20989;&#25968;&#22312;&#20219;&#21153;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#39046;&#22495;&#20013;&#26174;&#33879;&#30340;&#36801;&#31227;&#26426;&#21046;&#12290;&#23427;&#20204;&#37325;&#26032;&#35780;&#20272;&#20808;&#21069;&#23398;&#20064;&#31574;&#30053;&#22312;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#39044;&#26399;&#22238;&#25253;&#65292;&#20197;&#20256;&#36882;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;SF&#26694;&#26550;&#36890;&#36807;&#23558;&#22870;&#21169;&#32447;&#24615;&#20998;&#35299;&#20026;&#32487;&#25215;&#29305;&#24449;&#21644;&#22870;&#21169;&#26435;&#37325;&#21521;&#37327;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;SR&#65292;&#24182;&#20801;&#35768;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;&#20294;&#36825;&#26679;&#20570;&#30340;&#20195;&#20215;&#26159;&#22870;&#21169;&#20989;&#25968;&#19982;&#32487;&#25215;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#23384;&#22312;&#36825;&#31181;&#32447;&#24615;&#20851;&#31995;&#30340;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SR&#34920;&#36798;&#26041;&#24335;&#65292;&#21363;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#65292;&#31216;&#20026;&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#12290;&#20851;&#38190;&#26159;&#65292;SFR&#21487;&#20197;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;
&lt;/p&gt;
&lt;p&gt;
Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2110.12351</link><description>&lt;p&gt;
&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#28041;&#21450;&#20855;&#26377;&#27010;&#29575;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#21442;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20272;&#35745;&#12290;&#19982;&#20808;&#20272;&#35745;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#20998;&#24067;&#28982;&#21518;&#22522;&#20110;&#20272;&#35745;&#20248;&#21270;&#30446;&#26631;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#30452;&#25509;&#24314;&#27169;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#19982;&#19978;&#19979;&#25991;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#29992;&#19982;&#19979;&#28216;&#20248;&#21270;&#38382;&#39064;&#19968;&#33268;&#30340;&#30446;&#26631;&#20272;&#35745;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;ICEO&#26041;&#27861;&#22312;&#36866;&#24230;&#35268;&#21017;&#26465;&#20214;&#19979;&#26159;&#28176;&#36827;&#19968;&#33268;&#30340;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#20123;&#25512;&#24191;&#30028;&#38480;&#24418;&#24335;&#30340;&#26377;&#38480;&#24615;&#33021;&#20445;&#35777;&#12290;&#35745;&#31639;&#19978;&#65292;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems involve uncertain parameters with probability distributions that can be estimated using contextual feature information. In contrast to the standard approach of first estimating the distribution of uncertain parameters and then optimizing the objective based on the estimation, we propose an integrated conditional estimation-optimization (ICEO) framework that estimates the underlying conditional distribution of the random parameter while considering the structure of the optimization problem. We directly model the relationship between the conditional distribution of the random parameter and the contextual features, and then estimate the probabilistic model with an objective that aligns with the downstream optimization problem. We show that our ICEO approach is asymptotically consistent under moderate regularity conditions and further provide finite performance guarantees in the form of generalization bounds. Computationally, performing estimation with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#20013;&#22830;&#21270;&#21305;&#37197;&#22312;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#20004;&#36793;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#20013;&#22830;&#21270;&#25237;&#36164;&#26426;&#21046;&#38590;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#20026;&#29702;&#35299;&#22312;&#32447;&#24179;&#21488;&#19978;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#39034;&#24207;&#36129;&#29486;&#21160;&#24577;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2105.02589</link><description>&lt;p&gt;
&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#20013;&#22830;&#21270;&#21305;&#37197;&#22312;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#20004;&#36793;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bandit based centralized matching in two-sided markets for peer to peer lending. (arXiv:2105.02589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#20013;&#22830;&#21270;&#21305;&#37197;&#22312;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#20004;&#36793;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#20013;&#22830;&#21270;&#25237;&#36164;&#26426;&#21046;&#38590;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#20026;&#29702;&#35299;&#22312;&#32447;&#24179;&#21488;&#19978;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#39034;&#24207;&#36129;&#29486;&#21160;&#24577;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#36793;&#22312;&#32447;&#24179;&#21488;&#20013;&#30340;&#39034;&#24207;&#31609;&#27454;&#36890;&#36807;&#20381;&#27425;&#24102;&#26469;&#28508;&#22312;&#30340;&#36129;&#29486;&#32773;&#65292;&#27599;&#20010;&#36129;&#29486;&#32773;&#30340;&#20915;&#31574;&#37117;&#20250;&#24433;&#21709;&#24066;&#22330;&#20013;&#30340;&#20854;&#20182;&#36129;&#29486;&#32773;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28857;&#23545;&#28857;&#20511;&#36151;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#22312;&#32447;&#24179;&#21488;&#19978;&#28857;&#23545;&#28857;&#20511;&#36151;&#30340;&#39034;&#24207;&#36129;&#29486;&#21160;&#24577;&#19968;&#30452;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#36825;&#20123;&#24179;&#21488;&#20013;&#30340;&#20013;&#22830;&#21270;&#25237;&#36164;&#26426;&#21046;&#20351;&#24471;&#24456;&#38590;&#29702;&#35299;&#20511;&#27454;&#20154;&#22312;&#20219;&#20309;&#26102;&#21051;&#37117;&#38754;&#20020;&#30340;&#26469;&#33258;&#21333;&#19968;&#20511;&#27454;&#20154;&#30340;&#38544;&#21547;&#31454;&#20105;&#12290;&#21305;&#37197;&#24066;&#22330;&#26159;&#19968;&#31181;&#23558;&#20195;&#29702;&#20154;&#36827;&#34892;&#37197;&#23545;&#30340;&#27169;&#22411;&#65292;&#20004;&#36793;&#20195;&#29702;&#20154;&#22312;&#20132;&#26131;&#26041;&#38754;&#30340;&#20559;&#22909;&#37197;&#23545;&#21487;&#20197;&#23454;&#29616;&#24066;&#22330;&#30340;&#20998;&#25955;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25237;&#36164;&#32773;&#25110;&#25918;&#36151;&#20154;&#36824;&#38754;&#20020;&#22522;&#20110;&#20511;&#27454;&#20154;&#20559;&#22909;&#30340;&#25237;&#36164;&#38480;&#21046;&#30340;&#20004;&#36793;&#24179;&#21488;&#19978;&#30340;&#25237;&#36164;&#35774;&#35745;&#12290;&#36825;&#31181;&#24773;&#20917;&#38500;&#20102;&#24050;&#26377;&#30340;&#20511;&#27454;&#20154;&#31454;&#20105;&#22806;&#65292;&#36824;&#22312;&#25918;&#36151;&#20154;&#20043;&#38388;&#21019;&#36896;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#31454;&#20105;&#65292;
&lt;/p&gt;
&lt;p&gt;
Sequential fundraising in two sided online platforms enable peer to peer lending by sequentially bringing potential contributors, each of whose decisions impact other contributors in the market. However, understanding the dynamics of sequential contributions in online platforms for peer lending has been an open ended research question. The centralized investment mechanism in these platforms makes it difficult to understand the implicit competition that borrowers face from a single lender at any point in time. Matching markets are a model of pairing agents where the preferences of agents from both sides in terms of their preferred pairing for transactions can allow to decentralize the market. We study investment designs in two sided platforms using matching markets when the investors or lenders also face restrictions on the investments based on borrower preferences. This situation creates an implicit competition among the lenders in addition to the existing borrower competition, especia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#21644;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#30340;&#20869;&#37096;&#24046;&#24322;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#24182;&#36991;&#20813;&#20102;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#21644;&#39069;&#22806;&#30340;&#27880;&#37322;&#20803;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.10401</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification. (arXiv:2104.10401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#21644;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#30340;&#20869;&#37096;&#24046;&#24322;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#24182;&#36991;&#20813;&#20102;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#21644;&#39069;&#22806;&#30340;&#27880;&#37322;&#20803;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20877;&#35782;&#21035;&#36890;&#36807;&#21306;&#20998;&#30456;&#21516;&#21644;&#19981;&#21516;&#36710;&#36742;&#30340;&#22270;&#20687;&#26469;&#23454;&#29616;&#35782;&#21035;&#12290;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#19979;&#30456;&#21516;&#36710;&#36742;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20869;&#37096;&#24046;&#24322;&#20197;&#21450;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#23384;&#22312;&#32454;&#24494;&#30340;&#22806;&#37096;&#24046;&#24322;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#31354;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#35270;&#35282;&#24863;&#30693;&#25110;&#37096;&#20998;&#29305;&#23450;&#29305;&#24449;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#25110;&#38656;&#35201;&#26114;&#36149;&#30340;&#39069;&#22806;&#27880;&#37322;&#20803;&#25968;&#25454;&#65288;&#22914;&#20851;&#38190;&#28857;&#65289;&#26469;&#25552;&#39640;&#36136;&#37327;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#27934;&#23519;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#23450;&#35270;&#35282;&#25110;&#36710;&#36742;&#37096;&#20214;&#30340;&#22810;&#27880;&#24847;&#21147;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#27880;&#24847;&#21147;&#20998;&#25903;&#30340;&#25968;&#37327;&#21644;&#24615;&#36136;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20877;&#35782;&#21035;&#20219;&#21153;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#36710;&#36742;&#20877;&#35782;&#21035;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle re-identification helps in distinguishing between images of the same and other vehicles. It is a challenging process because of significant intra-instance differences between identical vehicles from different views and subtle inter-instance differences between similar vehicles. To solve this issue, researchers have extracted view-aware or part-specific features via spatial attention mechanisms, which usually result in noisy attention maps or otherwise require expensive additional annotation for metadata, such as key points, to improve the quality. Meanwhile, based on the researchers' insights, various handcrafted multi-attention architectures for specific viewpoints or vehicle parts have been proposed. However, this approach does not guarantee that the number and nature of attention branches will be optimal for real-world re-identification tasks. To address these problems, we proposed a new vehicle re-identification network based on a multiple soft attention mechanism for captu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2010.08657</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.08657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#38754;&#23545;&#19968;&#31995;&#21015;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#23398;&#20064;&#26032;&#31867;&#21035;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#31867;&#21035;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24120;&#24120;&#20250;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#25928;&#30340;&#26041;&#27861;&#21033;&#29992;&#23384;&#20648;&#22312;&#19968;&#20010;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#21516;&#26102;&#25193;&#23637;&#26368;&#32456;&#20998;&#31867;&#22120;&#33410;&#28857;&#20197;&#23481;&#32435;&#26032;&#30340;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#22266;&#23450;&#20998;&#31867;&#22120;&#26367;&#20195;&#20102;&#25193;&#23637;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#38454;&#27573;&#24320;&#22987;&#23601;&#21463;&#21040;&#20998;&#31867;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#19982;&#26631;&#20934;&#25193;&#23637;&#20998;&#31867;&#22120;&#30456;&#21453;&#65292;&#36825;&#26679;&#20570;&#26377;&#20197;&#19979;&#22909;&#22788;&#65306;(a)&#26410;&#26469;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#30340;&#19968;&#24320;&#22987;&#23601;&#33021;&#30475;&#21040;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#30340;&#27491;&#26679;&#26412;&#65307;(b)&#33021;&#22815;&#23398;&#20064;&#19981;&#38543;&#30528;&#26032;&#31867;&#21035;&#30340;&#21152;&#20837;&#32780;&#25913;&#21464;&#20854;&#20960;&#20309;&#37197;&#32622;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1706.03762</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23601;&#26159;&#19968;&#20999;&#65288;arXiv:1706.03762v6 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1706.03762
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#35201;&#30340;&#24207;&#21015;&#36716;&#25442;&#27169;&#22411;&#22522;&#20110;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#37197;&#32622;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#36824;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;Transformer&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19981;&#20877;&#20351;&#29992;&#24490;&#29615;&#21644;&#21367;&#31215;&#12290;&#22312;&#20004;&#20010;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;WMT 2014&#33521;&#35793;&#24503;&#20219;&#21153;&#19978;&#36798;&#21040;28.4&#30340;BLEU&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26368;&#22909;&#32467;&#26524;&#65288;&#21253;&#25324;&#38598;&#25104;&#27169;&#22411;&#65289;&#25552;&#39640;&#20102;2&#20010;BLEU&#20998;&#12290;&#22312;WMT 2014&#33521;&#35793;&#27861;&#20219;&#21153;&#19978;&#65292;&#22312;8&#20010;GPU&#19978;&#35757;&#32451;&#20102;3.5&#22825;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;41.8&#30340;&#21333;&#27169;&#22411;&#26368;&#26032;BLEU&#20998;&#25968;&#65292;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;&#25991;&#29486;&#20013;&#26368;&#22909;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
&lt;/p&gt;</description></item></channel></rss>