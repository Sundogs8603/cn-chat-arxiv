<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03022</link><description>&lt;p&gt;
Decision ConvFormer: MetaFormer&#20013;&#30340;&#26412;&#22320;&#36807;&#28388;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#24050;&#32463;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03022
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;Decision Transformer&#65288;DT&#65289;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;DT&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#19981;&#36866;&#21512;&#25429;&#25417;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20381;&#36182;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;DT&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MetaFormer&#26550;&#26500;&#30340;&#26032;&#22411;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#31216;&#20026;Decision ConvFormer&#65288;DC&#65289;&#12290;DC&#37319;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#20316;&#20026;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;RL&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20851;&#32852;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;DC&#22312;&#21508;&#31181;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20197;&#21450;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03016</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31163;&#25955;&#20989;&#25968;&#26469;&#29702;&#35299;&#21464;&#21387;&#22120;&#21644;LLM&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions. (arXiv:2310.03016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03016
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20197;&#21450;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35268;&#33539;&#21270;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#22312;&#23454;&#29616;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#30340;&#38480;&#21046;&#20197;&#21450;&#23427;&#20204;&#23398;&#20064;&#20854;&#20182;&#24418;&#24335;&#31639;&#27861;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20063;&#19981;&#26126;&#30830;&#12290;&#27492;&#22806;&#65292;&#23578;&#19981;&#28165;&#26970;&#20174;&#36825;&#20123;&#35268;&#33539;&#21270;&#35774;&#32622;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#26159;&#21542;&#33021;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21521;&#36825;&#20123;&#38382;&#39064;&#36808;&#36827;&#65306;&#65288;a&#65289;&#22312;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#24067;&#23572;&#20989;&#25968;&#31867;&#30340;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#21464;&#21387;&#22120;&#20960;&#20046;&#21487;&#20197;&#19982;&#8220;&#36739;&#31616;&#21333;&#8221;&#20219;&#21153;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#8220;&#36739;&#22797;&#26434;&#8221;&#20219;&#21153;&#19978;&#20854;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find th
&lt;/p&gt;</description></item><item><title>SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03013</link><description>&lt;p&gt;
SemiReward: &#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03013
&lt;/p&gt;
&lt;p&gt;
SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#21644;&#20266;&#26631;&#31614;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#21306;&#20998;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#36991;&#20813;&#30830;&#35777;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#38480;&#21046;&#20110;&#39044;&#23450;&#20041;&#30340;&#26041;&#26696;&#25110;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#31574;&#30053;&#65292;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#26631;&#31614;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#26694;&#26550;&#65288;SemiReward&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#20197;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#21644;&#22330;&#26223;&#19979;&#19982;&#20027;&#27969;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20351;&#29992;&#12290;&#20026;&#20102;&#20943;&#23569;&#30830;&#35777;&#20559;&#35265;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#19977;&#31181;&#27169;&#24577;&#30340;13&#20010;&#26631;&#20934;&#21322;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;SemiReward&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21160;&#24577;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#20197;&#21450;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#32500;&#28151;&#21512;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#21563;&#21512;&#12290;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#20250;&#22312;&#27599;&#19968;&#23618;&#21457;&#29983;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#20250;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;</title><link>http://arxiv.org/abs/2310.03010</link><description>&lt;p&gt;
&#39640;&#32500;&#24230; SGD &#19982;&#26032;&#20852;&#30340;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#30456;&#21563;&#21512;
&lt;/p&gt;
&lt;p&gt;
High-dimensional SGD aligns with emerging outlier eigenspaces. (arXiv:2310.03010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21160;&#24577;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#20197;&#21450;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#32500;&#28151;&#21512;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#21563;&#21512;&#12290;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#20250;&#22312;&#27599;&#19968;&#23618;&#21457;&#29983;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#20250;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#23545;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#22810;&#31867;&#39640;&#32500;&#28151;&#21512;&#21644;1&#25110;2&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#20856;&#22411;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#36805;&#36895;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#30456;&#21563;&#21512;&#12290;&#27492;&#22806;&#65292;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#21457;&#29983;&#22312;&#27599;&#19968;&#23618;&#65292;&#26368;&#21518;&#19968;&#23618;&#30340;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28436;&#21270;&#65292;&#24182;&#19988;&#22312;SGD&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;&#36825;&#20026;&#36807;&#21435;&#21313;&#24180;&#20013;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#28023;&#26862;&#30697;&#38453;&#21644;&#20449;&#24687;&#30697;&#38453;&#30340;&#35889;&#30340;&#24191;&#27867;&#25968;&#20540;&#30740;&#31350;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#38382;&#39064;&#65292;&#32531;&#35299;&#20102;VQ&#38754;&#20020;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.03004</link><description>&lt;p&gt;
&#36719;&#20984;&#37327;&#21270;&#65306;&#29992;&#20984;&#20248;&#21270;&#37325;&#26032;&#24605;&#32771;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization. (arXiv:2310.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#38382;&#39064;&#65292;&#32531;&#35299;&#20102;VQ&#38754;&#20020;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#29992;&#20110;&#25552;&#21462;&#20449;&#24687;&#24615;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#30340;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#12290;VQ&#23884;&#20837;&#27169;&#22411;&#22312;&#21253;&#25324;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;VQ&#20316;&#20026;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;K-means&#31639;&#27861;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#21333;&#20010;&#30721;&#20070;&#21521;&#37327;&#23558;&#36755;&#20837;&#36827;&#34892;&#37327;&#21270;&#12290;&#23613;&#31649;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#35813;&#25216;&#26415;&#38754;&#20020;&#23454;&#38469;&#25361;&#25112;&#65292;&#21253;&#25324;&#30721;&#20070;&#23849;&#28291;&#12289;&#19981;&#21487;&#21306;&#20998;&#24615;&#21644;&#26377;&#25439;&#21387;&#32553;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;VQ&#30340;&#30452;&#25509;&#26367;&#20195;&#12290;SCQ&#30340;&#24037;&#20316;&#26041;&#24335;&#31867;&#20284;&#20110;&#21487;&#24494;&#20984;&#20248;&#21270;&#65288;DCO&#65289;&#23618;&#65306;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#25105;&#20204;&#27714;&#35299;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#12290;&#22312;&#21453;&#21521;&#20256;&#36882;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#21521;&#35299;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#21033;&#29992;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;SCQ&#20248;&#21270;&#26494;&#24347;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;CIFAR&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31163;&#24515;&#27893;&#22312;&#22810;&#30456;&#27969;&#19979;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.03001</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22810;&#30456;&#27969;&#19979;&#31163;&#24515;&#27893;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks. (arXiv:2310.03001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31163;&#24515;&#27893;&#22312;&#22810;&#30456;&#27969;&#19979;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28508;&#27893;&#65288;ESP&#65289;&#30001;&#20110;&#20854;&#39640;&#27969;&#37327;&#21644;&#22686;&#21387;&#65292;&#26159;&#27833;&#27668;&#24037;&#19994;&#20013;&#31532;&#20108;&#24120;&#29992;&#30340;&#20154;&#24037;&#25552;&#21319;&#35774;&#22791;&#12290;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#22810;&#30456;&#27969;&#21160;&#65292;&#36825;&#20123;&#27969;&#20307;&#36890;&#24120;&#21253;&#21547;&#28867;&#31867;&#12289;&#27700;&#21644;/&#25110;&#27785;&#31215;&#29289;&#30340;&#28151;&#21512;&#29289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#20250;&#24418;&#25104;&#20083;&#28082;&#65292;&#23427;&#26159;&#30001;&#20004;&#31181;&#19981;&#20114;&#28342;&#27969;&#20307;&#32452;&#25104;&#30340;&#28082;&#28082;&#27969;&#21160;&#65292;&#20854;&#26377;&#25928;&#31896;&#24230;&#21644;&#23494;&#24230;&#19982;&#21333;&#29420;&#30340;&#21333;&#30456;&#27969;&#21160;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#24314;&#27169;ESP&#31995;&#32479;&#23545;&#20110;&#20248;&#21270;&#27833;&#30000;&#29983;&#20135;&#21644;&#23454;&#26045;&#25511;&#21046;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#32463;&#27982;&#21407;&#22240;&#65292;&#23454;&#26102;&#21644;&#30452;&#25509;&#27979;&#37327;&#27969;&#20307;&#21644;&#31995;&#32479;&#29305;&#24615;&#36890;&#24120;&#26159;&#19981;&#21487;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#33324;&#32771;&#34385;&#38388;&#25509;&#26041;&#27861;&#26469;&#20272;&#35745;&#31995;&#32479;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#20851;&#38190;&#30340;&#31995;&#32479;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical submersible pumps (ESP) are the second most used artificial lifting equipment in the oil and gas industry due to their high flow rates and boost pressures. They often have to handle multiphase flows, which usually contain a mixture of hydrocarbons, water, and/or sediments. Given these circumstances, emulsions are commonly formed. It is a liquid-liquid flow composed of two immiscible fluids whose effective viscosity and density differ from the single phase separately. In this context, accurate modeling of ESP systems is crucial for optimizing oil production and implementing control strategies. However, real-time and direct measurement of fluid and system characteristics is often impractical due to time constraints and economy. Hence, indirect methods are generally considered to estimate the system parameters. In this paper, we formulate a machine learning model based on Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters. In order to study the effic
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02995</link><description>&lt;p&gt;
IBCL&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#29992;&#20110;&#20219;&#21153;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning. (arXiv:2310.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02995
&lt;/p&gt;
&lt;p&gt;
IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36830;&#32493;&#23398;&#20064;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20808;&#21069;&#30340;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#23384;&#22312;&#22810;&#20010;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#35299;&#20915;&#20102;&#19981;&#21516;&#30340;&#20219;&#21153;&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#35752;&#35770;&#20102;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#19982;&#20559;&#22909;&#25968;&#37327;&#25104;&#27604;&#20363;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#29978;&#33267;&#26159;&#26080;&#38480;&#22810;&#20010;&#20559;&#22909;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#36127;&#25285;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Imprecise Bayesian Continual Learning (IBCL)&#12290;&#22312;&#26032;&#20219;&#21153;&#20986;&#29616;&#26102;&#65292;IBCL(1)&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#26356;&#26032;&#30693;&#35782;&#24211;&#65292;(2)&#33719;&#24471;&#20102;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#20219;&#21153;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is,
&lt;/p&gt;</description></item><item><title>&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.02994</link><description>&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02994
&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#65288;MPP&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#12290;MPP&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20849;&#20139;&#23884;&#20837;&#21644;&#24402;&#19968;&#21270;&#31574;&#30053;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#30340;&#23383;&#27573;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28041;&#21450;&#27969;&#20307;&#21147;&#23398;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#33021;&#22815;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#26469;&#20248;&#21270;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#27714;&#35299;&#36807;&#31243;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.02987</link><description>&lt;p&gt;
&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#22312;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions. (arXiv:2310.02987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#26041;&#24046;&#20943;&#23569;&#30340; Halpern &#36845;&#20195;&#26469;&#20248;&#21270;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#27714;&#35299;&#36807;&#31243;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#23545;&#25239;&#31283;&#20581;&#24615;&#25110;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24341;&#21457;&#20102;&#35299;&#20915;&#21338;&#24328;&#22343;&#34913;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#21487;&#35745;&#31639;&#36924;&#36817;&#35823;&#24046;&#30340;&#26041;&#27861;&#38750;&#24120;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#30340;&#32456;&#27490;&#20934;&#21017;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#25311;&#24191;&#27867;&#31867;&#21035;&#22343;&#34913;&#38382;&#39064;&#30340;&#26377;&#38480;&#21644;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340; Halpern &#36845;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#20943;&#23569;&#33719;&#24471;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#22312;&#26377;&#38480;&#21644;&#30340; $n$ &#20010;&#32452;&#25104;&#25805;&#20316;&#31526;&#20013;&#65292;&#8220;&#24179;&#22343;&#8221;&#22320;&#26159;&#20114;&#34917;&#21327;&#21516;&#25110;Lipschitz&#36830;&#32493;&#21644;&#21333;&#35843;&#65292;&#21442;&#25968;&#20026; $L$&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#39044;&#27979;&#20102;&#26368;&#21518;&#30340;&#36845;&#20195;&#21644;&#19968;&#20010;&#65288;compu&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which $n$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter $L$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (compu
&lt;/p&gt;</description></item><item><title>&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#20013;&#26029;&#25110;&#19981;&#21487;&#29992;&#23548;&#33268;&#30340;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#26080;&#27861;&#36827;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02986</link><description>&lt;p&gt;
&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#25506;&#32034;&#20013;&#26029;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#23545;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios. (arXiv:2310.02986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02986
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#20013;&#26029;&#25110;&#19981;&#21487;&#29992;&#23548;&#33268;&#30340;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#26080;&#27861;&#36827;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20351;&#24471;&#23398;&#20064;&#36164;&#28304;&#21644;&#20915;&#31574;&#33021;&#21147;&#21487;&#20197;&#20998;&#24067;&#22312;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#25110;&#33410;&#28857;&#19978;&#65292;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#29305;&#24615;&#65292;&#23427;&#27491;&#36805;&#36895;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#23398;&#20064;&#36807;&#31243;&#30340;&#20247;&#21253;&#26426;&#21046;&#20351;&#24471;&#31995;&#32479;&#22312;&#19968;&#20123;&#33410;&#28857;&#21463;&#21040;&#24433;&#21709;&#25110;&#26029;&#24320;&#36830;&#25509;&#26102;&#20173;&#28982;&#21487;&#20197;&#32487;&#32493;&#36816;&#36716;&#12290;&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#21644;&#38598;&#20013;&#24335;&#31995;&#32479;&#21487;&#33021;&#20250;&#20013;&#26029;&#25110;&#23436;&#20840;&#19981;&#21487;&#29992;&#65292;&#36825;&#38459;&#30861;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#36827;&#34892;&#26631;&#20934;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#20174;&#38598;&#20013;&#24335;&#21040;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#36807;&#28193;&#24341;&#20837;&#20102;&#23398;&#20064;&#36807;&#31243;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#22270;&#25299;&#25169;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#21363;&#20351;&#26159;&#28857;&#23545;&#28857;&#36890;&#20449;&#20063;&#23481;&#26131;&#20986;&#29616;&#31361;&#28982;&#30340;&#21464;&#21270;&#65292;&#22914;&#35774;&#22791;&#32791;&#23613;&#30005;&#27744;&#25110;&#26029;&#24320;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully decentralized learning enables the distribution of learning resources and decision-making capabilities across multiple user devices or nodes, and is rapidly gaining popularity due to its privacy-preserving and decentralized nature. Importantly, this crowdsourcing of the learning process allows the system to continue functioning even if some nodes are affected or disconnected. In a disaster scenario, communication infrastructure and centralized systems may be disrupted or completely unavailable, hindering the possibility of carrying out standard centralized learning tasks in these settings. Thus, fully decentralized learning can help in this case. However, transitioning from centralized to peer-to-peer communications introduces a dependency between the learning process and the topology of the communication graph among nodes. In a disaster scenario, even peer-to-peer communications are susceptible to abrupt changes, such as devices running out of battery or getting disconnected fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02977</link><description>&lt;p&gt;
T$^3$Bench&#65306;&#26631;&#27880;&#30446;&#21069;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;arXiv:2310.02977v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02977
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#20248;&#21270;NeRF&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;3D&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#24320;&#25918;&#24615;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#20027;&#35266;&#26696;&#20363;&#30740;&#31350;&#21644;&#29992;&#25143;&#23454;&#39564;&#35777;&#26126;&#20854;&#32467;&#26524;&#65292;&#20174;&#32780;&#22312;&#23450;&#37327;&#19978;&#22238;&#31572;&#8220;&#25991;&#26412;&#21040;3D&#30340;&#24403;&#21069;&#36827;&#23637;&#22914;&#20309;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;T$^3$Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19977;&#20010;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#19987;&#38376;&#20026;3D&#29983;&#25104;&#32780;&#35774;&#35745;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;3D&#20869;&#23481;&#20135;&#29983;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#36136;&#37327;&#24230;&#37327;&#32467;&#21512;&#20102;&#22810;&#35270;&#22270;&#25991;&#26412;-&#22270;&#20687;&#20998;&#25968;&#21644;&#21306;&#22495;&#21367;&#31215;&#20197;&#26816;&#27979;&#36136;&#37327;&#21644;&#35270;&#35282;&#19981;&#19968;&#33268;&#24615;&#12290;&#23545;&#40784;&#24230;&#37327;&#20351;&#29992;&#22810;&#35270;&#22270;&#23383;&#24149;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;e
&lt;/p&gt;
&lt;p&gt;
Recent methods in text-to-3D leverage powerful pretrained diffusion models to optimize NeRF. Notably, these methods are able to produce high-quality 3D scenes without training on 3D data. Due to the open-ended nature of the task, most studies evaluate their results with subjective case studies and user experiments, thereby presenting a challenge in quantitatively addressing the question: How has current progress in Text-to-3D gone so far? In this paper, we introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing diverse text prompts of three increasing complexity levels that are specially designed for 3D generation. To assess both the subjective quality and the text alignment, we propose two automatic metrics based on multi-view images produced by the 3D contents. The quality metric combines multi-view text-image scores and regional convolution to detect quality and view inconsistency. The alignment metric uses multi-view captioning and Large Language Model (LLM) e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.02975</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#27874;&#27573;&#30340;&#23436;&#20840;&#33258;&#36866;&#24212;&#36951;&#25022;&#26368;&#23567;&#21270;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#23614;&#20998;&#24067;&#22312;&#37329;&#34701;&#21040;&#30005;&#20449;&#31561;&#22810;&#31181;&#29615;&#22659;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;&#34429;&#28982;&#22312;&#27425;&#39640;&#26031;&#25110;&#26377;&#30028;&#25903;&#25745;&#22870;&#21169;&#19979;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#37325;&#23614;&#20998;&#24067;&#19978;&#30340;&#23398;&#20064;&#21482;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#38543;&#26426;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#22312;&#20551;&#35774;&#20998;&#24067;&#26377;&#26377;&#30028;&#26368;&#22823;&#38454;&#30340;&#26377;&#38480;&#30697;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#36825;&#20123;&#30697;&#34987;&#24120;&#25968;u&#19968;&#33268;&#26377;&#30028;&#65292;&#23545;&#20110;&#26576;&#20010;&#949;&#8712;(0,1]&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#21482;&#25552;&#20379;&#38656;&#35201;&#36825;&#20004;&#20010;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#36825;&#26159;&#26631;&#20934;&#35774;&#32622;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#20195;&#29702;&#23545;&#949;&#21644;u&#22343;&#19981;&#30693;&#26195;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26159;&#23384;&#22312;&#20195;&#20215;&#30340;&#65292;&#24182;&#24341;&#20837;&#23545;&#20110;&#20219;&#20309;&#33258;&#36866;&#24212;&#31639;&#27861;&#36951;&#25022;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#29305;&#23450;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.02970</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#20301;&#32622;-&#26041;&#21521;&#31354;&#38388;&#20013;&#20849;&#20139;&#26435;&#37325;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#34920;&#36798;&#21147;&#24378;&#30340;SE$(n)$&#31561;&#21464;&#32593;&#32476;&#12290;&#20182;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20960;&#20309;&#20248;&#21270;&#30340;&#36793;&#23646;&#24615;&#65292;&#24182;&#23558;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22788;&#29702;&#30456;&#21516;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#20182;&#20204;&#22312;&#22788;&#29702;3D&#28857;&#20113;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36873;&#25321;&#20102;$\mathbb{R}^3 {\times} S^2$&#20316;&#20026;&#26368;&#20339;&#30340;&#22788;&#29702;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#25512;&#23548;&#20986;&#29992;&#20110;&#28789;&#27963;&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#8220;&#20960;&#20309;&#20248;&#21270;&#36793;&#23646;&#24615;&#8221;&#12290;&#25105;&#20204;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#20849;&#20139;&#24418;&#24335;&#21270;&#20026;&#23545;&#31561;&#22320;&#22788;&#29702;&#24182;&#19988;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#30340;&#28857;&#23545;&#30340;&#28040;&#24687;&#20989;&#25968;&#20849;&#20139;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#31561;&#20215;&#31867;&#65292;&#36825;&#20123;&#31561;&#20215;&#31867;&#22312;&#32676;&#20013;&#36827;&#34892;&#21464;&#25442;&#26102;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#25512;&#23548;&#20986;&#21807;&#19968;&#26631;&#35782;&#36825;&#20123;&#31867;&#21035;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#23646;&#24615;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#26435;&#37325;&#20849;&#20139;&#12290;&#20316;&#20026;&#35813;&#29702;&#35770;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31561;&#21464;&#32676;&#21367;&#31215;&#32593;&#32476;&#26469;&#22788;&#29702;3D&#28857;&#20113;&#12290;&#21516;&#24577;&#31354;&#38388;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#22914;&#20309;&#22312;&#20301;&#32622;$\mathbb{R}^3$&#12289;&#20301;&#32622;&#21644;&#26041;&#21521;$\mathbb{R}^3 {\times} S^2$&#30340;&#21516;&#24577;&#31354;&#38388;&#20197;&#21450;&#32676;SE$(3)$&#19978;&#30340;&#29305;&#24449;&#22270;&#19978;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;&#22312;&#36825;&#20123;&#36873;&#25321;&#20013;&#65292;$\mathbb{R}^3 {\times} S^2$&#26159;&#19968;&#20010;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#26041;&#21521;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the theory of homogeneous spaces we derive \textit{geometrically optimal edge attributes} to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$ itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to the ability to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22278;&#38181;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#26469;&#36741;&#21161;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02969</link><description>&lt;p&gt;
&#21452;&#22278;&#38181;&#20195;&#29702;&#29992;&#20110;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Dual Conic Proxies for AC Optimal Power Flow. (arXiv:2310.02969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22278;&#38181;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#26469;&#36741;&#21161;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65288;AC-OPF&#65289;&#20248;&#21270;&#20195;&#29702;&#30340;&#21457;&#23637;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#39640;&#36136;&#37327;&#21407;&#22987;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26080;&#27861;&#20026;AC-OPF&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;AC-OPF&#30340;&#19968;&#20010;&#20984;&#26494;&#24347;&#30340;&#20248;&#21270;&#20195;&#29702;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;AC-OPF&#30340;&#20108;&#38454;&#22278;&#38181;&#65288;SOC&#65289;&#26494;&#24347;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20598;&#26550;&#26500;&#65292;&#23884;&#20837;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#30340;&#65288;&#23545;&#20598;&#65289;&#21487;&#34892;&#24615;&#24674;&#22797;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26032;&#26550;&#26500;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30456;&#32467;&#21512;&#65292;&#20943;&#36731;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#38656;&#27714;&#12290;&#23545;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in the development of machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF). Although significant progress has been achieved in predicting high-quality primal solutions, no existing learning-based approach can provide valid dual bounds for AC-OPF. This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF. Namely, the paper considers a second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery, thus providing valid dual bounds. The paper combines this new architecture with a self-supervised learning scheme, which alleviates the need for costly training data generation. Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#20174;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#32957;&#30340;&#21028;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02964</link><description>&lt;p&gt;
&#21512;&#27169;&#24335;&#21270;&#32957;&#30340;&#39034;&#24207;&#21644;&#22270;&#24418;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Co-modeling the Sequential and Graphical Route for Peptide. (arXiv:2310.02964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#20174;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#32957;&#30340;&#21028;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32957;&#26159;&#30001;&#22810;&#20010;&#27688;&#22522;&#37240;&#30340;&#33073;&#27700;&#32553;&#21512;&#24418;&#25104;&#30340;&#12290;&#32957;&#30340;&#20027;&#35201;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#20026;&#27688;&#22522;&#37240;&#24207;&#21015;&#25110;&#30001;&#21407;&#23376;&#21644;&#21270;&#23398;&#38190;&#32452;&#25104;&#30340;&#20998;&#23376;&#22270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#39034;&#24207;&#21644;&#22270;&#24418;&#32957;&#24418;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#36335;&#24452;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21516;&#19968;&#31181;&#32957;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#26041;&#24335;&#19981;&#21516;&#12290;&#23558;&#39034;&#24207;&#21644;&#22270;&#24418;&#27169;&#22411;&#35270;&#20026;&#20174;&#19981;&#21516;&#35282;&#24230;&#36827;&#34892;&#25512;&#29702;&#30340;&#20004;&#20010;&#19987;&#23478;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#34701;&#21512;&#19987;&#23478;&#30693;&#35782;&#65292;&#20016;&#23500;&#23398;&#21040;&#30340;&#34920;&#31034;&#20197;&#25552;&#39640;&#21028;&#21035;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32957;&#21512;&#27169;&#24335;&#21270;&#26041;&#27861;RepCon&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#22686;&#24378;&#20174;&#35299;&#32806;&#30340;&#39034;&#24207;&#21644;&#22270;&#24418;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#34920;&#31034;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peptides are formed by the dehydration condensation of multiple amino acids. The primary structure of a peptide can be represented either as an amino acid sequence or as a molecular graph consisting of atoms and chemical bonds. Previous studies have indicated that deep learning routes specific to sequential and graphical peptide forms exhibit comparable performance on downstream tasks. Despite the fact that these models learn representations of the same modality of peptides, we find that they explain their predictions differently. Considering sequential and graphical models as two experts making inferences from different perspectives, we work on fusing expert knowledge to enrich the learned representations for improving the discriminative performance. To achieve this, we propose a peptide co-modeling method, RepCon, which employs a contrastive learning-based framework to enhance the mutual information of representations from decoupled sequential and graphical end-to-end models. It cons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02956</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#35780;&#20998;&#39044;&#27979;&#65306;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#29992;&#21345;&#30340;&#20351;&#29992;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#20026;&#20102;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#65292;&#24613;&#38656;&#20449;&#29992;&#21345;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#36829;&#32422;&#39044;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#22312;&#26032;&#25552;&#20986;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21253;&#25324;&#20449;&#29992;&#21345;&#20132;&#26131;&#21382;&#21490;&#21644;&#23458;&#25143;&#26723;&#26696;&#65292;&#24182;&#20351;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#12289;XGBoost&#21644;LightGBM&#12290;&#20026;&#20102;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#27491;&#38451;&#24615;&#29575;&#26041;&#38754;&#65292;MLP&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;LightGBM&#21644;XGBoost&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Polish&#31354;&#38388;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#19978;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.02951</link><description>&lt;p&gt;
Fisher-Rao&#26799;&#24230;&#27969;&#22312;Polish&#31354;&#38388;&#20013;&#23545;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces. (arXiv:2310.02951v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Polish&#31354;&#38388;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#19978;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Polish&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#26080;&#38480;&#26102;&#22495;&#30340;&#29109;&#27491;&#21017;&#21270;&#30340;Markov&#20915;&#31574;&#36807;&#31243;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36825;&#20010;&#27969;&#26159;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#38388;&#31867;&#27604;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#33391;&#23450;&#20041;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#20026;&#20102;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#24615;&#21644;&#29109;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#19981;&#36830;&#32493;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#24615;&#33021;&#24046;&#21035;&#24341;&#29702;&#21644;&#26799;&#24230;&#19982;&#38236;&#20687;&#19979;&#38477;&#27969;&#20043;&#38388;&#30340;&#23545;&#20598;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the global convergence of a Fisher-Rao policy gradient flow for infinite-horizon entropy-regularised Markov decision processes with Polish state and action space. The flow is a continuous-time analogue of a policy mirror descent method. We establish the global well-posedness of the gradient flow and demonstrate its exponential convergence to the optimal policy. Moreover, we prove the flow is stable with respect to gradient evaluation, offering insights into the performance of a natural policy gradient flow with log-linear policy parameterisation. To overcome challenges stemming from the lack of the convexity of the objective function and the discontinuity arising from the entropy regulariser, we leverage the performance difference lemma and the duality relationship between the gradient and mirror descent flows.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2310.02949</link><description>&lt;p&gt;
&#38452;&#24433;&#23545;&#40784;&#65306;&#36731;&#26494;&#39072;&#35206;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#21253;&#21547;&#26377;&#23475;&#35821;&#35328;&#30340;&#20363;&#23376;&#65292;&#24314;&#35758;&#35835;&#32773;&#24910;&#37325;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36880;&#28176;&#24320;&#25918;&#37322;&#25918;&#65292;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#27880;&#37322;&#21644;&#35745;&#31639;&#30340;&#26680;&#24515;&#25104;&#26412;&#65292;&#20419;&#36827;&#20102;&#19979;&#28216;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#24615;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23433;&#20840;&#23545;&#40784;&#25514;&#26045;&#65292;&#20197;&#20445;&#25252;&#36825;&#20123;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#20351;&#29992;&#65288;&#20027;&#35201;&#26159;&#30828;&#25552;&#31034;&#25915;&#20987;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#30475;&#20284;&#22362;&#22266;&#30340;&#30420;&#30002;&#32972;&#21518;&#65292;&#21487;&#33021;&#28508;&#20239;&#30528;&#19968;&#20010;&#38452;&#24433;&#12290;&#36890;&#36807;&#20165;&#35843;&#25972;100&#20010;&#24694;&#24847;&#31034;&#20363;&#65292;&#20351;&#29992;1&#20010;GPU&#23567;&#26102;&#65292;&#36825;&#20123;&#23433;&#20840;&#23545;&#40784;&#30340;LLMs&#21487;&#20197;&#36731;&#26494;&#22320;&#34987;&#39072;&#35206;&#20197;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#24418;&#24335;&#19978;&#65292;&#25105;&#20204;&#23558;&#19968;&#31181;&#26032;&#25915;&#20987;&#31216;&#20026;&#38452;&#24433;&#23545;&#40784;&#65306;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#21487;&#20197;&#20351;&#23433;&#20840;&#23545;&#40784;&#27169;&#22411;&#36866;&#24212;&#26377;&#23475;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34987;&#39072;&#35206;&#30340;&#27169;&#22411;&#20173;&#28982;&#20445;&#30041;&#20854;&#23545;&#24120;&#35268;&#26597;&#35810;&#30340;&#36866;&#24403;&#21709;&#24212;&#33021;&#21147;&#12290;&#22312;5&#20010;&#21457;&#34892;&#30340;8&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5
&lt;/p&gt;</description></item><item><title>HappyFeat&#26159;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#26041;&#20415;&#30340;GUI&#21644;&#21442;&#25968;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#20351;&#24471;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02948</link><description>&lt;p&gt;
HappyFeat -- &#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HappyFeat -- An interactive and efficient BCI framework for clinical applications. (arXiv:2310.02948v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02948
&lt;/p&gt;
&lt;p&gt;
HappyFeat&#26159;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#26041;&#20415;&#30340;GUI&#21644;&#21442;&#25968;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#20351;&#24471;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#23558;&#22823;&#33041;&#27963;&#21160;&#36716;&#21270;&#20026;&#21629;&#20196;&#26469;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#31867;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#35757;&#32451;&#38454;&#27573;&#65292;&#21253;&#25324;&#36890;&#36807;&#20351;&#29992;&#35760;&#24405;&#30340;&#20449;&#21495;&#30340;&#29305;&#23450;&#29305;&#24449;&#26469;&#35757;&#32451;&#20998;&#31867;&#31639;&#27861;&#65292;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#22312;&#20020;&#24202;&#32972;&#26223;&#19979;&#65292;&#22914;&#20013;&#39118;&#24247;&#22797;&#65292;&#23545;&#20110;BCI&#30340;&#24615;&#33021;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#22521;&#35757;&#38454;&#27573;&#26377;&#29305;&#23450;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HappyFeat&#65292;&#19968;&#31181;&#36719;&#20214;&#65292;&#22312;&#21333;&#19968;&#20415;&#21033;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#23454;&#39564;&#25110;&#20998;&#26512;&#21442;&#25968;&#30340;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#12290;&#32467;&#26524;&#24037;&#20316;&#27969;&#31243;&#21487;&#20197;&#36731;&#26494;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;BCI&#24615;&#33021;&#12290;&#22522;&#20110;&#21151;&#33021;&#36830;&#36890;&#24615;&#30340;&#26367;&#20195;&#29305;&#24449;&#21487;&#20197;&#19982;&#21151;&#29575;&#35889;&#23494;&#24230;&#30456;&#27604;&#25110;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interface (BCI) systems allow users to perform actions by translating their brain activity into commands. Such systems usually need a training phase, consisting in training a classification algorithm to discriminate between mental states using specific features from the recorded signals. This phase of feature selection and training is crucial for BCI performance and presents specific constraints to be met in a clinical context, such as post-stroke rehabilitation.  In this paper, we present HappyFeat, a software making Motor Imagery (MI) based BCI experiments easier, by gathering all necessary manipulations and analysis in a single convenient GUI and via automation of experiment or analysis parameters. The resulting workflow allows for effortlessly selecting the best features, helping to achieve good BCI performance in time-constrained environments. Alternative features based on Functional Connectivity can be used and compared or combined with Power Spectral Density, allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#30340;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20108;&#36827;&#21046;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23398;&#20064;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.02942</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#22312;&#32447;&#32422;&#26463;&#21152;&#32039;&#65306;&#19968;&#31181;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Constraint Tightening in Stochastic Model Predictive Control: A Regression Approach. (arXiv:2310.02942v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#30340;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20108;&#36827;&#21046;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23398;&#20064;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#27010;&#29575;&#32422;&#26463;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26159;&#25511;&#21046;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#24456;&#23569;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#19981;&#23384;&#22312;&#35299;&#26512;&#35299;&#12290;&#19968;&#31181;&#24120;&#35265;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#26159;&#23558;&#27010;&#29575;&#32422;&#26463;&#37325;&#26032;&#34920;&#36848;&#20026;&#20855;&#26377;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#30340;&#30828;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36873;&#21462;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#24050;&#30693;&#36807;&#31243;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#32422;&#26463;&#36890;&#24120;&#26080;&#27861;&#24471;&#21040;&#20005;&#26684;&#28385;&#36275;&#65292;&#23548;&#33268;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#38381;&#29615;&#30340;&#32422;&#26463;&#21152;&#32039;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20108;&#36827;&#21046;&#22238;&#24402;&#38382;&#39064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving chance-constrained stochastic optimal control problems is a significant challenge in control. This is because no analytical solutions exist for up to a handful of special cases. A common and computationally efficient approach for tackling chance-constrained stochastic optimal control problems consists of reformulating the chance constraints as hard constraints with a constraint-tightening parameter. However, in such approaches, the choice of constraint-tightening parameter remains challenging, and guarantees can mostly be obtained assuming that the process noise distribution is known a priori. Moreover, the chance constraints are often not tightly satisfied, leading to unnecessarily high costs. This work proposes a data-driven approach for learning the constraint-tightening parameters online during control. To this end, we reformulate the choice of constraint-tightening parameter for the closed-loop as a binary regression problem. We then leverage a highly expressive \gls{gp} m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24191;&#20041;&#21487;&#38598;&#20013;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Hoeffding&#19981;&#31561;&#24335;&#65292;&#25299;&#23637;&#20102;&#29616;&#26377;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;Hoeffding&#22411;&#19981;&#31561;&#24335;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26694;&#26550;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20960;&#20010;&#38750;&#28176;&#36817;&#20998;&#26512;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02941</link><description>&lt;p&gt;
Hoeffding&#19981;&#31561;&#24335;&#22312;&#20855;&#26377;&#24191;&#20041;&#21487;&#38598;&#20013;&#26465;&#20214;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hoeffding's Inequality for Markov Chains under Generalized Concentrability Condition. (arXiv:2310.02941v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24191;&#20041;&#21487;&#38598;&#20013;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Hoeffding&#19981;&#31561;&#24335;&#65292;&#25299;&#23637;&#20102;&#29616;&#26377;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;Hoeffding&#22411;&#19981;&#31561;&#24335;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26694;&#26550;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20960;&#20010;&#38750;&#28176;&#36817;&#20998;&#26512;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#36807;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;(IPM)&#23450;&#20041;&#30340;&#24191;&#20041;&#21487;&#38598;&#20013;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Hoeffding&#19981;&#31561;&#24335;&#12290;&#24191;&#20041;&#21487;&#38598;&#20013;&#26465;&#20214;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25554;&#20540;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;Hoeffding&#22411;&#19981;&#31561;&#24335;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#20351;&#24471;Hoeffding&#19981;&#31561;&#24335;&#21487;&#20197;&#24212;&#29992;&#20110;&#20256;&#32479;&#24847;&#20041;&#19978;&#30340;&#38750;&#33258;&#20851;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#38750;&#28176;&#36817;&#20998;&#26512;&#26469;&#35777;&#26126;&#20854;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#65306;(i) &#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;(ii) SGD&#30340;Ployak-Ruppert&#24179;&#22343;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#20197;&#21450;(iii) &#20855;&#26377;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#30340;&#20241;&#24687;&#39532;&#23572;&#21487;&#22827;&#36172;&#21338;&#26426;&#30340;&#26032;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Hoeffding's inequality for Markov chains under the generalized concentrability condition defined via integral probability metric (IPM). The generalized concentrability condition establishes a framework that interpolates and extends the existing hypotheses of Markov chain Hoeffding-type inequalities. The flexibility of our framework allows Hoeffding's inequality to be applied beyond the ergodic Markov chains in the traditional sense. We demonstrate the utility by applying our framework to several non-asymptotic analyses arising from the field of machine learning, including (i) a generalization bound for empirical risk minimization with Markovian samples, (ii) a finite sample guarantee for Ployak-Ruppert averaging of SGD, and (iii) a new regret bound for rested Markovian bandits with general state space.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21693;&#21897;&#30284;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#30340;&#24739;&#32773;&#36229;&#22270;&#32593;&#32476;&#65288;PHGN&#65289;&#12290;&#30740;&#31350;&#36824;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#36827;&#34892;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#19982;GNN&#21644;&#22522;&#20934;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.02931</link><description>&lt;p&gt;
&#21693;&#21897;&#30284;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#30340;&#22270;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph data modelling for outcome prediction in oropharyngeal cancer patients. (arXiv:2310.02931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21693;&#21897;&#30284;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#30340;&#24739;&#32773;&#36229;&#22270;&#32593;&#32476;&#65288;PHGN&#65289;&#12290;&#30740;&#31350;&#36824;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#36827;&#34892;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#19982;GNN&#21644;&#22522;&#20934;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#30142;&#30149;&#20998;&#31867;&#21644;&#32467;&#26524;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#24739;&#32773;&#25968;&#25454;&#19981;&#23481;&#26131;&#20316;&#20026;&#22270;&#30340;&#24418;&#24335;&#33719;&#24471;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#25163;&#21160;&#23450;&#20041;&#24739;&#32773;&#22270;&#65292;&#35201;&#20040;&#22522;&#20110;&#24739;&#32773;&#38388;&#30340;&#37197;&#23545;&#30456;&#20284;&#24615;&#23398;&#20064;&#19968;&#20010;&#28508;&#22312;&#30340;&#22270;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#26041;&#27861;&#36824;&#21033;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#24739;&#32773;&#20043;&#38388;&#30340;&#28508;&#22312;&#39640;&#38454;&#20851;&#32852;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#19968;&#31181;&#24739;&#32773;&#36229;&#22270;&#32593;&#32476;&#65288;PHGN&#65289;&#65292;&#24182;&#22312;&#24402;&#32435;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30340;&#25918;&#23556;&#23398;&#29305;&#24449;&#23545;&#21693;&#21897;&#30284;&#65288;OPC&#65289;&#24739;&#32773;&#36827;&#34892;&#20108;&#20803;&#32467;&#26524;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#36827;&#34892;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#20998;&#26512;&#65292;&#24182;&#19982;GNN&#21644;&#22522;&#20934;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are becoming increasingly popular in the medical domain for the tasks of disease classification and outcome prediction. Since patient data is not readily available as a graph, most existing methods either manually define a patient graph, or learn a latent graph based on pairwise similarities between the patients. There are also hypergraph neural network (HGNN)-based methods that were introduced recently to exploit potential higher order associations between the patients by representing them as a hypergraph. In this work, we propose a patient hypergraph network (PHGN), which has been investigated in an inductive learning setup for binary outcome prediction in oropharyngeal cancer (OPC) patients using computed tomography (CT)-based radiomic features for the first time. Additionally, the proposed model was extended to perform time-to-event analyses, and compared with GNN and baseline linear models.
&lt;/p&gt;</description></item><item><title>OTARI&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#20256;&#36755;&#24418;&#24335;&#65292;&#23427;&#36890;&#36807;&#23545;&#27599;&#20010;&#28857;&#30340;&#36136;&#37327;&#26045;&#21152;&#32422;&#26463;&#26469;&#35299;&#20915;&#20840;&#23616;&#32422;&#26463;&#36896;&#25104;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.02925</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport with Adaptive Regularisation. (arXiv:2310.02925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02925
&lt;/p&gt;
&lt;p&gt;
OTARI&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#20256;&#36755;&#24418;&#24335;&#65292;&#23427;&#36890;&#36807;&#23545;&#27599;&#20010;&#28857;&#30340;&#36136;&#37327;&#26045;&#21152;&#32422;&#26463;&#26469;&#35299;&#20915;&#20840;&#23616;&#32422;&#26463;&#36896;&#25104;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20005;&#26684;&#20984;&#32422;&#26463;&#26469;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30340;&#21407;&#22987;&#24418;&#24335;&#20250;&#22686;&#21152;&#25968;&#20540;&#22797;&#26434;&#24230;&#24182;&#20351;&#20256;&#36755;&#35745;&#21010;&#26356;&#23494;&#38598;&#12290;&#35768;&#22810;&#20844;&#24335;&#23545;&#20256;&#36755;&#35745;&#21010;&#26045;&#21152;&#20840;&#23616;&#32422;&#26463;&#65292;&#20363;&#22914;&#20381;&#36182;&#29109;&#27491;&#21017;&#21270;&#12290;&#30001;&#20110;&#23545;&#20110;&#31163;&#32676;&#28857;&#32780;&#35328;&#25193;&#25955;&#36136;&#37327;&#27604;&#20013;&#24515;&#28857;&#26356;&#26114;&#36149;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#36136;&#37327;&#22312;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#12290;&#23545;&#20110;&#19968;&#20123;&#38656;&#35201;&#27599;&#20010;&#28857;&#26368;&#20302;&#24179;&#28369;&#24615;&#30340;&#24212;&#29992;&#32780;&#35328;&#65292;&#36825;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#65288;OTARI&#65289;&#65292;&#19968;&#31181;&#23545;&#36827;&#20986;&#27599;&#20010;&#28857;&#30340;&#36136;&#37327;&#26045;&#21152;&#32422;&#26463;&#30340;&#26032;&#24418;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularising the primal formulation of optimal transport (OT) with a strictly convex term leads to enhanced numerical complexity and a denser transport plan. Many formulations impose a global constraint on the transport plan, for instance by relying on entropic regularisation. As it is more expensive to diffuse mass for outlier points compared to central ones, this typically results in a significant imbalance in the way mass is spread across the points. This can be detrimental for some applications where a minimum of smoothing is required per point. To remedy this, we introduce OT with Adaptive RegularIsation (OTARI), a new formulation of OT that imposes constraints on the mass going in or/and out of each point. We then showcase the benefits of this approach for domain adaptation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;K-modes&#32858;&#31867;&#26469;&#22686;&#24378;&#38463;&#32946;&#21536;&#38464;&#35786;&#26029;&#20013;&#30340;&#26222;&#25289;&#20811;&#37324;&#33922;&#31867;&#22411;&#21644;Dosha&#37325;&#21472;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;Dosha&#20998;&#31867;&#20026;7&#20010;&#31867;&#21035;&#65292;&#21253;&#25324;&#37325;&#21472;&#30340;Dosha&#31867;&#21035;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02920</link><description>&lt;p&gt;
&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;K-modes&#32858;&#31867;&#22686;&#24378;&#38463;&#32946;&#21536;&#38464;&#35786;&#26029;&#65306;&#23545;&#26222;&#25289;&#20811;&#37324;&#33922;&#31867;&#22411;&#21644;Dosha&#37325;&#21472;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Ayurvedic Diagnosis using Multinomial Naive Bayes and K-modes Clustering: An Investigation into Prakriti Types and Dosha Overlapping. (arXiv:2310.02920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;K-modes&#32858;&#31867;&#26469;&#22686;&#24378;&#38463;&#32946;&#21536;&#38464;&#35786;&#26029;&#20013;&#30340;&#26222;&#25289;&#20811;&#37324;&#33922;&#31867;&#22411;&#21644;Dosha&#37325;&#21472;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;Dosha&#20998;&#31867;&#20026;7&#20010;&#31867;&#21035;&#65292;&#21253;&#25324;&#37325;&#21472;&#30340;Dosha&#31867;&#21035;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20154;&#20307;&#30340;&#26222;&#25289;&#20811;&#37324;&#33922;&#31867;&#22411;&#26159;&#19968;&#31181;&#38271;&#26102;&#38388;&#22833;&#20256;&#30340;&#21307;&#23398;&#23454;&#36341;&#65292;&#26088;&#22312;&#23547;&#25214;&#20154;&#31867;&#26412;&#36136;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#21644;&#35856;&#12290;&#20010;&#20307;&#26377;3&#31181;&#22522;&#26412;&#30340;&#26222;&#25289;&#20811;&#37324;&#33922;&#31867;&#22411;&#65292;&#21487;&#20197;&#23646;&#20110;&#20219;&#20309;Dosha&#12290;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#20915;&#31574;&#26641;&#21644;&#20854;&#20182;&#21508;&#31181;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#36755;&#20986;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#21487;&#20197;&#20511;&#21161;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;K-modes&#32858;&#31867;&#36827;&#34892;&#22686;&#24378;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20165;&#38480;&#20110;3&#20010;&#22522;&#26412;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20986;&#29616;&#37325;&#21472;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;Dosha&#20998;&#31867;&#20026;7&#20010;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;Dosha&#30340;&#37325;&#21472;&#12290;&#36825;&#20123;&#31867;&#21035;&#20998;&#21035;&#26159;VATT-Dosha&#12289;PITT-Dosha&#12289;KAPH-Dosha&#12289;VATT-PITT-Dosha&#12289;PITT-KAPH-Dosha&#12289;KAPH-VATT-Dosha&#21644;VATT-PITT-KAPH-Dosha&#12290;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#21253;&#21547;&#20102;&#25152;&#26377;&#20010;&#20307;&#26465;&#30446;&#30340;&#24179;&#34913;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of Prakriti types for the human body is a long-lost medical practice in finding the harmony between the nature of human beings and their behaviour. There are 3 fundamental Prakriti types of individuals. A person can belong to any Dosha. In the existing models, researchers have made use of SVM, KNN, PCA, Decision Tree, and various other algorithms. The output of these algorithms was quite decent, but it can be enhanced with the help of Multinomial Naive Bayes and K-modes clustering. Most of the researchers have confined themselves to 3 basic classes. This might not be accurate in the real-world scenario, where overlapping might occur. Considering these, we have classified the Doshas into 7 categories, which includes overlapping of Doshas. These are namely, VATT-Dosha, PITT-Dosha, KAPH-Dosha, VATT-PITT-Dosha, PITT-KAPH-Dosha, KAPH-VATT-Dosha, and VATT-PITT-KAPH-Dosha. The data used contains a balanced set of all individual entries on which preprocessing steps of machin
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02919</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#30897;&#22522;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-based Multi-task Learning for Base Editor Outcome Prediction. (arXiv:2310.02919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02919
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#32534;&#36753;&#32467;&#26524;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36951;&#20256;&#30142;&#30149;&#36890;&#24120;&#30001;&#28857;&#31361;&#21464;&#24341;&#36215;&#65292;&#36825;&#20984;&#26174;&#20102;&#23545;&#31934;&#30830;&#22522;&#22240;&#32452;&#32534;&#36753;&#25216;&#26415;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#20854;&#20013;&#65292;&#30897;&#22522;&#32534;&#36753;&#20197;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#26680;&#33527;&#37240;&#27700;&#24179;&#19978;&#36827;&#34892;&#23450;&#21521;&#25913;&#21464;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#20854;&#20020;&#24202;&#24212;&#29992;&#21463;&#21040;&#32534;&#36753;&#25928;&#29575;&#20302;&#21644;&#38750;&#39044;&#26399;&#31361;&#21464;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#23454;&#39564;&#23460;&#36827;&#34892;&#22823;&#37327;&#30340;&#35797;&#38169;&#23454;&#39564;&#12290;&#20026;&#20102;&#21152;&#36895;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20004;&#38454;&#27573;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#32473;&#23450;&#22522;&#22240;&#32452;&#30446;&#26631;&#24207;&#21015;&#30340;&#25152;&#26377;&#21487;&#33021;&#32534;&#36753;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#65292;&#21516;&#26102;&#23398;&#20064;&#22810;&#31181;&#30897;&#22522;&#32534;&#36753;&#22120;&#65288;&#21363;&#21464;&#20307;&#65289;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30897;&#22522;&#32534;&#36753;&#22120;&#21464;&#20307;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#19982;&#23454;&#38469;&#23454;&#39564;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#27169;&#22411;&#25913;&#36827;&#21644;&#21152;&#36895;&#22522;&#22240;&#32534;&#36753;&#35774;&#35745;&#36807;&#31243;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing desig
&lt;/p&gt;</description></item><item><title>ELUQuant&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#28145;&#24230;&#38750;&#24377;&#24615;&#25955;&#23556;&#20013;&#23545;&#20107;&#20214;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#36817;&#20284;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#25552;&#20379;&#35814;&#32454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25551;&#36848;&#12290;&#36825;&#20026;&#20915;&#31574;&#21046;&#23450;&#21644;&#20943;&#23569;&#30495;&#23454;&#19981;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.02913</link><description>&lt;p&gt;
ELUQuant: &#28145;&#24230;&#38750;&#24377;&#24615;&#25955;&#23556;&#20013;&#20107;&#20214;&#32423;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering. (arXiv:2310.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02913
&lt;/p&gt;
&lt;p&gt;
ELUQuant&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#28145;&#24230;&#38750;&#24377;&#24615;&#25955;&#23556;&#20013;&#23545;&#20107;&#20214;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#36817;&#20284;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#25552;&#20379;&#35814;&#32454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25551;&#36848;&#12290;&#36825;&#20026;&#20915;&#31574;&#21046;&#23450;&#21644;&#20943;&#23569;&#30495;&#23454;&#19981;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#24402;&#19968;&#21270;&#27969;&#65288;MNF&#65289;&#26469;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#23545;&#29289;&#29702;&#20107;&#20214;&#32423;&#21035;&#36827;&#34892;&#35814;&#32454;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#24322;&#26041;&#24046;&#30340;&#21807;&#26377;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#20102;&#31934;&#32454;&#30340;&#29289;&#29702;&#27934;&#23519;&#21147;&#12290;&#24212;&#29992;&#20110;&#28145;&#24230;&#38750;&#24377;&#24615;&#25955;&#23556;&#65288;DIS&#65289;&#20107;&#20214;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#25552;&#21462;&#20102;&#21160;&#21147;&#23398;&#21464;&#37327;$x$&#65292;$Q^2$&#21644;$y$&#65292;&#19982;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#22238;&#24402;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#30456;&#21305;&#37197;&#65292;&#20294;&#20855;&#26377;&#20107;&#20214;&#32423;&#21035;UQ&#30340;&#20851;&#38190;&#22686;&#24378;&#12290;&#23545;&#22522;&#20110;HERA&#30340;H1&#25506;&#27979;&#22120;&#36827;&#34892;&#30340;DIS&#27169;&#25311;&#34920;&#26126;&#20102;&#26410;&#26469;EIC&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#30456;&#20851;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#22914;&#20107;&#20214;&#36807;&#28388;&#31561;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#31934;&#32454;&#19981;&#30830;&#23450;&#24615;&#25551;&#36848;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#38750;&#24120;&#23453;&#36149;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#30452;&#25509;&#35775;&#38382;&#22522;&#26412;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;&#30495;&#23454;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a physics-informed Bayesian Neural Network (BNN) with flow approximated posteriors using multiplicative normalizing flows (MNF) for detailed uncertainty quantification (UQ) at the physics event-level. Our method is capable of identifying both heteroskedastic aleatoric and epistemic uncertainties, providing granular physical insights. Applied to Deep Inelastic Scattering (DIS) events, our model effectively extracts the kinematic variables $x$, $Q^2$, and $y$, matching the performance of recent deep learning regression techniques but with the critical enhancement of event-level UQ. This detailed description of the underlying uncertainty proves invaluable for decision-making, especially in tasks like event filtering. It also allows for the reduction of true inaccuracies without directly accessing the ground truth. A thorough DIS simulation using the H1 detector at HERA indicates possible applications for the future EIC. Additionally, this paves the way for related tasks such 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26679;&#26465;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21183;(s-NNP)&#26694;&#26550;&#65292;&#23558;&#31616;&#21333;&#24615;&#30340;s-MEAM&#21407;&#23376;&#38388;&#21183;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;IPs&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#31361;&#30772;&#32463;&#20856;&#21644;ML IPs&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#38190;&#26550;&#26500;&#21464;&#21270;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#26679;&#26465;&#28388;&#27874;&#22120;&#26469;&#32534;&#30721;&#21407;&#23376;&#29615;&#22659;&#65292;&#21487;&#20197;&#20135;&#29983;&#23481;&#26131;&#35299;&#37322;&#30340;&#23884;&#20837;&#23618;&#12290;</title><link>http://arxiv.org/abs/2310.02904</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26465;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;: &#34701;&#21512;&#32463;&#20856;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spline-based neural network interatomic potentials: blending classical and machine learning models. (arXiv:2310.02904v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26679;&#26465;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21183;(s-NNP)&#26694;&#26550;&#65292;&#23558;&#31616;&#21333;&#24615;&#30340;s-MEAM&#21407;&#23376;&#38388;&#21183;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;IPs&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#31361;&#30772;&#32463;&#20856;&#21644;ML IPs&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#38190;&#26550;&#26500;&#21464;&#21270;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#26679;&#26465;&#28388;&#27874;&#22120;&#26469;&#32534;&#30721;&#21407;&#23376;&#29615;&#22659;&#65292;&#21487;&#20197;&#20135;&#29983;&#23481;&#26131;&#35299;&#37322;&#30340;&#23884;&#20837;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#21407;&#23376;&#38388;&#21183;(Interatomic Potentials, IPs)&#22312;&#35757;&#32451;&#26102;&#33021;&#22815;&#36798;&#21040;&#25509;&#36817;&#31532;&#19968;&#21407;&#29702;&#25968;&#25454;&#22266;&#26377;&#22122;&#38899;&#27700;&#24179;&#30340;&#31934;&#30830;&#24230;&#65292;&#20294;&#36824;&#38656;&#35201;&#23637;&#31034;&#23427;&#20204;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#26159;&#21542;&#20005;&#26684;&#24517;&#35201;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;IPs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MLIP&#26694;&#26550;&#65292;&#23427;&#23558;&#22522;&#20110;&#26679;&#26465;&#20989;&#25968;&#30340;MEAM (s-MEAM)&#21407;&#23376;&#38388;&#21183;&#30340;&#31616;&#21333;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;(NN)&#26550;&#26500;&#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#34987;&#31216;&#20026;&#22522;&#20110;&#26679;&#26465;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21183;(s-NNP)&#65292;&#26159;&#20256;&#32479;NNP&#30340;&#31616;&#21270;&#29256;&#26412;&#65292;&#21487;&#20197;&#29992;&#20110;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25551;&#36848;&#22797;&#26434;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#25506;&#32034;&#32463;&#20856;&#21644;ML IPs&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#26550;&#26500;&#21464;&#21270;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#26679;&#26465;&#28388;&#27874;&#22120;&#26469;&#32534;&#30721;&#21407;&#23376;&#29615;&#22659;&#20250;&#20135;&#29983;&#19968;&#20010;&#23481;&#26131;&#35299;&#37322;&#30340;&#23884;&#20837;&#23618;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning (ML) interatomic potentials (IPs) are able to achieve accuracies nearing the level of noise inherent in the first-principles data to which they are trained, it remains to be shown if their increased complexities are strictly necessary for constructing high-quality IPs. In this work, we introduce a new MLIP framework which blends the simplicity of spline-based MEAM (s-MEAM) potentials with the flexibility of a neural network (NN) architecture. The proposed framework, which we call the spline-based neural network potential (s-NNP), is a simplified version of the traditional NNP that can be used to describe complex datasets in a computationally efficient manner. We demonstrate how this framework can be used to probe the boundary between classical and ML IPs, highlighting the benefits of key architectural changes. Furthermore, we show that using spline filters for encoding atomic environments results in a readily interpreted embedding layer which can be coupled with 
&lt;/p&gt;</description></item><item><title>FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.02903</link><description>&lt;p&gt;
FroSSL: &#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02903
&lt;/p&gt;
&lt;p&gt;
FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21487;&#20998;&#31867;&#20026;&#26679;&#26412;&#23545;&#27604;&#12289;&#32500;&#24230;&#23545;&#27604;&#25110;&#38750;&#23545;&#31216;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#23478;&#26063;&#37117;&#26377;&#33258;&#24049;&#30340;&#26041;&#27861;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#12290;&#34429;&#28982;&#32500;&#24230;&#23545;&#27604;&#26041;&#27861;&#25910;&#25947;&#21040;&#19982;&#26679;&#26412;&#23545;&#27604;&#26041;&#27861;&#30456;&#20284;&#30340;&#35299;&#65292;&#20294;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#19968;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#20989;&#25968;FroSSL&#65292;&#23427;&#22312;&#23884;&#20837;&#24402;&#19968;&#21270;&#26041;&#38754;&#26082;&#26159;&#26679;&#26412;&#23545;&#27604;&#21448;&#26159;&#32500;&#24230;&#23545;&#27604;&#12290;FroSSL&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FroSSL&#27604;&#20854;&#20182;&#21508;&#31181;SSL&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26356;&#24555;&#30340;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#23545;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. W
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#22312;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#20026;&#31616;&#21333;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02902</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#25628;&#32034;&#39640;&#20215;&#20540;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Searching for High-Value Molecules Using Reinforcement Learning and Transformers. (arXiv:2310.02902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#22312;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#20026;&#31616;&#21333;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#22270;&#20013;&#30340;&#39640;&#20215;&#20540;&#31574;&#30053;&#26041;&#38754;&#65292;&#20351;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;RL&#38656;&#35201;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#31934;&#24515;&#32467;&#26500;&#21270;&#21644;&#31639;&#27861;&#35774;&#35745;&#25165;&#33021;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#35821;&#27861;&#35774;&#35745;&#21644;&#35757;&#32451;&#31639;&#27861;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;RL&#31574;&#30053;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#36827;&#34892;&#30340;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35813;&#38382;&#39064;&#31354;&#38388;&#20013;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#38416;&#26126;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#23454;&#38469;&#19978;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#26377;&#24110;&#21161;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#36870;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23450;&#20041;&#27491;&#21017;&#21270;&#22120;&#24182;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.02897</link><description>&lt;p&gt;
&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#20013;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#65306;&#36870;&#38382;&#39064;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective. (arXiv:2310.02897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#36870;&#38382;&#39064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23450;&#20041;&#27491;&#21017;&#21270;&#22120;&#24182;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#36807;&#21442;&#25968;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#36864;&#21270;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#26679;&#26412;&#30340;&#24674;&#22797;&#23450;&#20041;&#20026;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#36870;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#38544;&#24335;&#22320;&#23450;&#20041;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#20174;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#12290;&#25105;&#20204;&#23558;&#22797;&#26434;&#30340;&#20248;&#21270;&#20219;&#21153;&#24320;&#21457;&#25104;&#19968;&#20010;&#23454;&#38469;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36845;&#20195;&#22320;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;&#33258;&#32534;&#30721;&#22120;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#35745;&#31639;&#26469;&#20272;&#35745;&#21644;&#22788;&#29702;&#26410;&#30693;&#30340;&#36864;&#21270;&#25805;&#20316;&#31526;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30450;&#30446;&#20462;&#34917;&#65292;&#30446;&#26631;&#26159;&#20174;&#35768;&#22810;&#32570;&#22833;&#30340;&#20687;&#32032;&#20013;&#24674;&#22797;&#35757;&#32451;&#22270;&#20687;&#65292;&#32780;&#36825;&#20123;&#32570;&#22833;&#30340;&#20687;&#32032;&#26159;&#25353;&#29031;&#26410;&#30693;&#30340;&#27169;&#24335;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#22914;&#20840;&#36830;&#25509;&#21644;U-Net&#65288;&#20855;&#26377;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#21644;&#22810;&#26679;&#30340;&#35757;&#32451;&#25439;&#22833;&#20540;&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#33258;&#32534;&#30721;&#22120;&#24674;&#22797;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the recovery of training data from overparameterized autoencoder models. Given a degraded training sample, we define the recovery of the original sample as an inverse problem and formulate it as an optimization task. In our inverse problem, we use the trained autoencoder to implicitly define a regularizer for the particular training dataset that we aim to retrieve from. We develop the intricate optimization task into a practical method that iteratively applies the trained autoencoder and relatively simple computations that estimate and address the unknown degradation operator. We evaluate our method for blind inpainting where the goal is to recover training images from degradation of many missing pixels in an unknown pattern. We examine various deep autoencoder architectures, such as fully connected and U-Net (with various nonlinearities and at diverse train loss values), and show that our method significantly outperforms previous methods for training data recovery from autoen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoLiDE&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;DAG&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.02895</link><description>&lt;p&gt;
CoLiDE: &#20849;&#21516;&#32447;&#24615;&#26377;&#21521;&#26080;&#29615;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoLiDE&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;DAG&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22788;&#29702;&#20174;&#36981;&#24490;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411; (SEM) &#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270; (DAG) &#32467;&#26500;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#21033;&#29992;&#19981;&#21487;&#24494;&#20998;&#12289;&#38750;&#20984;&#30340;&#26377;&#25928;&#24615;&#29305;&#24449;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#21463;&#38480;&#20248;&#21270;&#33539;&#24335;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25506;&#32034;DAG&#31354;&#38388;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22871;&#32034;&#31867;&#22411;&#30340;&#35780;&#20998;&#20989;&#25968;&#26469;&#24341;&#23548;&#36825;&#20010;&#25628;&#32034;&#36807;&#31243;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;$\textit{&#26410;&#30693;}$SEM&#22122;&#22768;&#26041;&#24046;&#22312;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#26102;&#38656;&#36827;&#34892;&#26114;&#36149;&#30340;&#24809;&#32602;&#21442;&#25968;&#37325;&#26032;&#35843;&#25972;&#65292;&#24182;&#19988;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#26377;&#30028;&#21516;&#26041;&#24046;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#31232;&#30095;&#24863;&#30693;&#32447;&#24615;DAG&#30340;&#23398;&#20064;&#65292;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;&#36890;&#36807;&#24179;&#28369;&#30340;&#12289;&#38750;&#20984;&#30340;&#26080;&#29615;&#24809;&#32602;&#39033;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#24471;&#21040;CoLiDE &#65288;&#20849;&#21516;&#32447;&#24615;DAG&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#23567;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#36827;&#28145;&#24230;&#21512;&#22863;&#27169;&#22411;&#26657;&#20934;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#36739;&#20302;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#39640;&#30340;&#21512;&#22863;&#22810;&#26679;&#24615;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02885</link><description>&lt;p&gt;
&#26080;&#20013;&#29983;&#26377;&#65306;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#36827;&#28145;&#24230;&#21512;&#22863;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Something for (almost) nothing: Improving deep ensemble calibration using unlabeled data. (arXiv:2310.02885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#23567;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#36827;&#28145;&#24230;&#21512;&#22863;&#27169;&#22411;&#26657;&#20934;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#36739;&#20302;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#39640;&#30340;&#21512;&#22863;&#22810;&#26679;&#24615;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23567;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#36827;&#28145;&#24230;&#21512;&#22863;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#31616;&#21333;&#65306;&#23545;&#20110;&#27599;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#28857;&#65292;&#22312;&#27599;&#20010;&#21512;&#22863;&#25104;&#21592;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#36827;&#34892;&#25311;&#21512;&#12290;&#25105;&#20204;&#22522;&#20110;PAC-Bayes&#36793;&#30028;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#20445;&#35777;&#22914;&#26524;&#25105;&#20204;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#36827;&#34892;&#36825;&#26679;&#30340;&#26631;&#31614;&#25311;&#21512;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20351;&#29992;&#30495;&#23454;&#26631;&#31614;&#65292;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#21487;&#20197;&#24471;&#21040;&#36739;&#20302;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#36739;&#39640;&#30340;&#21512;&#22863;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#35757;&#32451;&#38598;&#35268;&#27169;&#36739;&#23567;&#21040;&#20013;&#31561;&#35268;&#27169;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#21512;&#22863;&#27169;&#22411;&#27604;&#26631;&#20934;&#30340;&#21512;&#22863;&#27169;&#22411;&#26356;&#20855;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#65292;&#26377;&#26102;&#26174;&#33879;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to improve the calibration of deep ensembles in the small training data regime in the presence of unlabeled data. Our approach is extremely simple to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that if we fit such a labeling on unlabeled data, and the true labels on the training data, we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, our ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#29305;&#27530;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.02877</link><description>&lt;p&gt;
&#26080;&#22343;&#20540;&#22238;&#24402;&#65306;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#19981;&#24688;&#24403;&#26680;
&lt;/p&gt;
&lt;p&gt;
Stationarity without mean reversion: Improper Gaussian process regression and improper kernels. (arXiv:2310.02877v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#29305;&#27530;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#24191;&#27867;&#27969;&#34892;&#12290;GP&#22238;&#24402;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#38745;&#27490;&#21327;&#26041;&#24046;&#20989;&#25968;&#26159;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#65288;&#38750;&#21608;&#26399;&#24615;&#30340;&#65289;&#38745;&#27490;&#21327;&#26041;&#24046;&#20989;&#25968;&#24635;&#26159;&#22343;&#20540;&#22238;&#24402;&#30340;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20110;&#19981;&#36890;&#36807;&#21040;&#22266;&#23450;&#20840;&#23616;&#22343;&#20540;&#20540;&#30340;&#25968;&#25454;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#30149;&#24577;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;&#30340;&#19981;&#24688;&#24403;GP&#20808;&#39564;&#26469;&#23450;&#20041;&#38745;&#27490;&#20294;&#19981;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#26159;&#21487;&#33021;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22823;&#31867;&#21482;&#33021;&#22312;&#36825;&#31181;&#19981;&#24688;&#24403;&#30340;&#33539;&#22260;&#20869;&#23450;&#20041;&#30340;&#19981;&#24688;&#24403;&#26680;&#20989;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#28369;&#34892;&#36208;&#26680;&#65292;&#23427;&#20135;&#29983;&#26080;&#38480;&#24179;&#28369;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#19968;&#31867;&#19981;&#24688;&#24403;&#30340;Matern&#26680;&#65292;&#23427;&#21487;&#20197;&#34987;&#23450;&#20041;&#20026;&#20219;&#24847;&#25972;&#25968;j&#20493;&#21487;&#24494;&#12290;&#25152;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#20197;&#29992;&#35299;&#26512;&#30340;&#26041;&#24335;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GP) regression has gained substantial popularity in machine learning applications. The behavior of a GP regression depends on the choice of covariance function. Stationary covariance functions are favorite in machine learning applications. However, (non-periodic) stationary covariance functions are always mean reverting and can therefore exhibit pathological behavior when applied to data that does not relax to a fixed global mean value. In this paper, we show that it is possible to use improper GP prior with infinite variance to define processes that are stationary but not mean reverting. To this aim, we introduce a large class of improper kernels that can only be defined in this improper regime. Specifically, we introduce the Smooth Walk kernel, which produces infinitely smooth samples, and a family of improper Mat\'ern kernels, which can be defined to be $j$-times differentiable for any integer $j$. The resulting posterior distributions can be computed analyticall
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#22914;&#23396;&#31435;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02874</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#23398;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Methodological Advances in Federated Learning for Healthcare. (arXiv:2310.02874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02874
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#22914;&#23396;&#31435;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#30001;&#20110;&#20262;&#29702;&#12289;&#38544;&#31169;&#25110;&#21518;&#21220;&#38382;&#39064;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#21512;&#24182;&#26469;&#33258;&#22810;&#20010;&#26426;&#26500;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#32780;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#27719;&#38598;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#25361;&#25112;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#23398;&#26469;&#35299;&#20915;&#65292;&#22914;&#39640;&#24230;&#23396;&#31435;&#30340;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;&#32852;&#37030;&#23398;&#20064;&#23545;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#22686;&#21152;&#20102;&#26174;&#33879;&#30340;&#26041;&#27861;&#23398;&#22797;&#26434;&#24615;&#65292;&#38656;&#35201;&#20998;&#24067;&#24335;&#20248;&#21270;&#12289;&#33410;&#28857;&#38388;&#30340;&#36890;&#20449;&#12289;&#27169;&#22411;&#30340;&#32858;&#21512;&#21644;&#27169;&#22411;&#30340;&#37325;&#26032;&#20998;&#21457;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;Scopus&#19978;&#22312;2015&#24180;1&#26376;&#33267;2023&#24180;2&#26376;&#20043;&#38388;&#21457;&#34920;&#30340;&#25152;&#26377;&#25551;&#36848;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#25361;&#25112;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23398;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#23545;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;89&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
For healthcare datasets, it is often not possible to combine data samples from multiple sites due to ethical, privacy or logistical concerns. Federated learning allows for the utilisation of powerful machine learning algorithms without requiring the pooling of data. Healthcare data has many simultaneous challenges which require new methodologies to address, such as highly-siloed data, class imbalance, missing data, distribution shifts and non-standardised variables. Federated learning adds significant methodological complexity to conventional centralised machine learning, requiring distributed optimisation, communication between nodes, aggregation of models and redistribution of models. In this systematic review, we consider all papers on Scopus that were published between January 2015 and February 2023 and which describe new federated learning methodologies for addressing challenges with healthcare data. We performed a detailed review of the 89 papers which fulfilled these criteria. S
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#21644;TabNet&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02870</link><description>&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31283;&#23450;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;: &#24341;&#20837;&#20855;&#26377;&#26032;&#22411;InterpreStability&#25351;&#26631;&#30340;InterpreTabNet
&lt;/p&gt;
&lt;p&gt;
Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric. (arXiv:2310.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02870
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#21644;TabNet&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28145;&#24230;&#25972;&#21512;&#65292;&#23545;&#24378;&#22823;&#27169;&#22411;&#30340;&#36861;&#27714;&#24050;&#32463;&#21152;&#21095;&#12290;&#34429;&#28982;&#22312;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#40657;&#31665;&#12290;&#36825;&#31181;&#19981;&#36879;&#26126;&#24615;&#19981;&#20165;&#20351;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#32473;&#26368;&#32456;&#29992;&#25143;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#32780;&#19988;&#36824;&#38459;&#30861;&#20102;&#27169;&#22411;&#35774;&#35745;&#32773;&#23545;&#20013;&#38388;&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#25913;&#36827;TabNet&#26550;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#35774;&#35745;&#30830;&#20445;&#20102;&#24378;&#22823;&#30340;&#26799;&#24230;&#20256;&#25773;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#35813;&#25351;&#26631;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#25351;&#26631;&#26631;&#24535;&#30528;&#21487;&#35299;&#37322;&#27169;&#22411;&#30740;&#31350;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) integrates deeper into diverse sectors, the quest for powerful models has intensified. While significant strides have been made in boosting model capabilities and their applicability across domains, a glaring challenge persists: many of these state-of-the-art models remain as black boxes. This opacity not only complicates the explanation of model decisions to end-users but also obstructs insights into intermediate processes for model designers. To address these challenges, we introduce InterpreTabNet, a model designed to enhance both classification accuracy and interpretability by leveraging the TabNet architecture with an improved attentive module. This design ensures robust gradient propagation and computational stability. Additionally, we present a novel evaluation metric, InterpreStability, which quantifies the stability of a model's interpretability. The proposed model and metric mark a significant stride forward in explainable models' research, set
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#20013;&#35299;&#20915;&#36991;&#38556;&#35201;&#27714;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#26469;&#36873;&#25321;&#25511;&#21046;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02869</link><description>&lt;p&gt;
&#20351;&#29992;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#19982;&#36991;&#38556;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications. (arXiv:2310.02869v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#20013;&#35299;&#20915;&#36991;&#38556;&#35201;&#27714;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#26469;&#36873;&#25321;&#25511;&#21046;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#26377;&#21161;&#20110;&#35299;&#20915;&#35832;&#22914;&#36991;&#38556;&#38382;&#39064;&#31561;&#30340;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#12290;&#35856;&#27874;CLBF&#21033;&#29992;&#35856;&#27874;&#20989;&#25968;&#28385;&#36275;&#30340;&#26368;&#22823;&#20540;&#21407;&#29702;&#26469;&#32534;&#30721;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#23454;&#39564;&#24320;&#22987;&#26102;&#21021;&#22987;&#21270;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#26679;&#26412;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#12290;&#25511;&#21046;&#36755;&#20837;&#34987;&#36873;&#25321;&#20026;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#12290;&#25968;&#20540;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#36991;&#38556;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#22235;&#20010;&#19981;&#21516;&#31995;&#32479;&#30340;&#24773;&#20917;&#12290;&#35856;&#27874;CLBF&#26174;&#31034;&#20986;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#26174;&#33879;&#38477;&#20302;&#65292;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces harmonic control Lyapunov barrier functions (harmonic CLBF) that aid in constrained control problems such as reach-avoid problems. Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to encode the properties of control Lyapunov barrier functions (CLBFs). As a result, they can be initiated at the start of an experiment rather than trained based on sample trajectories. The control inputs are selected to maximize the inner product of the system dynamics with the steepest descent direction of the harmonic CLBF. Numerical results are presented with four different systems under different reach-avoid environments. Harmonic CLBFs show a significantly low risk of entering unsafe regions and a high probability of entering the goal region.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#36827;&#34892;&#26377;&#38480;&#25968;&#25454;&#27169;&#22411;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20869;&#20135;&#29983;&#31934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#31995;&#32479;&#19988;&#27599;&#20010;&#31995;&#32479;&#30340;&#25968;&#25454;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#23376;&#31354;&#38388;&#20272;&#35745;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.02864</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#36827;&#34892;&#26377;&#38480;&#25968;&#25454;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of Models with Limited Data by Leveraging Shared Structure. (arXiv:2310.02864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#36827;&#34892;&#26377;&#38480;&#25968;&#25454;&#27169;&#22411;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20869;&#20135;&#29983;&#31934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#31995;&#32479;&#19988;&#27599;&#20010;&#31995;&#32479;&#30340;&#25968;&#25454;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#23376;&#31354;&#38388;&#20272;&#35745;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#38598;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#65289;&#36890;&#24120;&#26469;&#33258;&#35768;&#22810;&#20010;&#20307;&#25110;&#31995;&#32479;&#65292;&#20294;&#27599;&#20010;&#21333;&#29420;&#26469;&#28304;&#30340;&#25968;&#25454;&#37117;&#19981;&#36275;&#20197;&#20998;&#21035;&#20272;&#35745;&#20010;&#20307;&#30340;&#39640;&#32500;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31995;&#32479;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#33021;&#21487;&#20197;&#21033;&#29992;&#20854;&#20182;&#31995;&#32479;&#30340;&#25968;&#25454;&#26469;&#24110;&#21161;&#20272;&#35745;&#20010;&#20307;&#21442;&#25968;&#65292;&#21542;&#21017;&#36825;&#20123;&#21442;&#25968;&#21487;&#33021;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#20551;&#35774;&#31995;&#32479;&#20849;&#20139;&#19968;&#20010;&#28508;&#22312;&#30340;&#20302;&#32500;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#24674;&#22797;N&#20010;&#19981;&#21516;&#32447;&#24615;&#31995;&#32479;&#30340;d&#32500;&#21442;&#25968;&#65292;&#21363;&#20351;&#27599;&#20010;&#31995;&#32479;&#21482;&#26377;T&lt;d&#20010;&#35266;&#27979;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#27493;&#31639;&#27861;&#65292;&#20272;&#35745;&#30001;&#31995;&#32479;&#21442;&#25968;&#26500;&#25104;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#65292;&#24182;&#22312;&#23376;&#31354;&#38388;&#20869;&#20135;&#29983;&#31934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#23376;&#31354;&#38388;&#20272;&#35745;&#35823;&#24046;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern data sets, such as those in healthcare and e-commerce, are often derived from many individuals or systems but have insufficient data from each source alone to separately estimate individual, often high-dimensional, model parameters. If there is shared structure among systems however, it may be possible to leverage data from other systems to help estimate individual parameters, which could otherwise be non-identifiable. In this paper, we assume systems share a latent low-dimensional parameter space and propose a method for recovering $d$-dimensional parameters for $N$ different linear systems, even when there are only $T&lt;d$ observations per system. To do so, we develop a three-step algorithm which estimates the low-dimensional subspace spanned by the systems' parameters and produces refined parameter estimates within the subspace. We provide finite sample subspace estimation error guarantees for our proposed method. Finally, we experimentally validate our method on simulations wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20849;&#24418;&#39044;&#27979;&#31639;&#27861;LPCI&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26399;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#21097;&#20313;&#25968;&#25454;&#24314;&#27169;&#20026;&#20998;&#20301;&#25968;&#22266;&#23450;&#25928;&#24212;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#22238;&#24402;&#22120;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#65292;LPCI&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27178;&#25130;&#38754;&#35206;&#30422;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02863</link><description>&lt;p&gt;
&#38271;&#26399;&#25968;&#25454;&#30340;&#20849;&#24418;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictions for Longitudinal Data. (arXiv:2310.02863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20849;&#24418;&#39044;&#27979;&#31639;&#27861;LPCI&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26399;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#21097;&#20313;&#25968;&#25454;&#24314;&#27169;&#20026;&#20998;&#20301;&#25968;&#22266;&#23450;&#25928;&#24212;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#22238;&#24402;&#22120;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#65292;LPCI&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27178;&#25130;&#38754;&#35206;&#30422;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20849;&#24418;&#39044;&#27979;&#31639;&#27861;&#65292;&#31216;&#20026;&#38271;&#26399;&#39044;&#27979;&#20849;&#24418;&#25512;&#26029;&#65288;LPCI&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26399;&#25968;&#25454;&#12290;&#30446;&#21069;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20849;&#24418;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#21464;&#37327;&#35774;&#32622;&#19978;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20110;&#38271;&#26399;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26102;&#32570;&#20047;&#27178;&#25130;&#38754;&#35206;&#30422;&#12290;&#30446;&#21069;&#38271;&#26399;&#25968;&#25454;&#30340;&#26368;&#26032;&#26041;&#27861;&#20381;&#36182;&#20110;&#21019;&#24314;&#26080;&#38480;&#23485;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#20445;&#35777;&#27178;&#25130;&#38754;&#21644;&#28176;&#36817;&#38271;&#26399;&#35206;&#30422;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;LPCI&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#21516;&#26102;&#20445;&#35777;&#32437;&#21521;&#21644;&#27178;&#25130;&#38754;&#35206;&#30422;&#32780;&#26080;&#38656;&#20351;&#29992;&#26080;&#38480;&#23485;&#30340;&#21306;&#38388;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#21097;&#20313;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#20301;&#25968;&#22266;&#23450;&#25928;&#24212;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#22120;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LPCI&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27178;&#25130;&#38754;&#35206;&#30422;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Longitudinal Predictive Conformal Inference (LPCI), a novel distribution-free conformal prediction algorithm for longitudinal data. Current conformal prediction approaches for time series data predominantly focus on the univariate setting, and thus lack cross-sectional coverage when applied individually to each time series in a longitudinal dataset. The current state-of-the-art for longitudinal data relies on creating infinitely-wide prediction intervals to guarantee both cross-sectional and asymptotic longitudinal coverage. The proposed LPCI method addresses this by ensuring that both longitudinal and cross-sectional coverages are guaranteed without resorting to infinitely wide intervals. In our approach, we model the residual data as a quantile fixed-effects regression problem, constructing prediction intervals with a trained quantile regressor. Our extensive experiments demonstrate that LPCI achieves valid cross-sectional coverage and outperforms existing benchmarks in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#36827;&#34892;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#21270;&#23618;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#23569;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.02862</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression. (arXiv:2310.02862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02862
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#36827;&#34892;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#21270;&#23618;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#23569;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#25509;&#35302;&#24335;&#40831;&#36718;&#25925;&#38556;&#35786;&#26029;&#38382;&#39064;&#20013;&#65292;&#32570;&#20047;&#39640;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#27169;&#22411;&#20173;&#28982;&#26159;&#26080;&#32447;&#20256;&#36755;&#40831;&#36718;&#25968;&#25454;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#21464;&#25442;&#22495;&#23618;&#26469;&#21387;&#32553;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#65288;DCST&#65289;&#23618;&#20197;&#26367;&#20195;&#22810;&#23618;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#32447;&#24615;&#23618;&#12290;&#36890;&#36807;&#21033;&#29992;&#21367;&#31215;&#30340;&#20056;&#27861;&#29305;&#24615;&#22312;DCST&#22495;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#21487;&#35757;&#32451;&#30340;&#30828;&#38408;&#20540;&#21270;&#23618;&#26469;&#20943;&#23569;DCST&#23618;&#20013;&#30340;&#20887;&#20313;&#25968;&#25454;&#20197;&#20351;&#29305;&#24449;&#22270;&#31232;&#30095;&#21270;&#12290;&#19982;&#32447;&#24615;&#23618;&#30456;&#27604;&#65292;DCST&#23618;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;DCST&#23618;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24247;&#28037;&#29380;&#26684;&#22823;&#23398;&#65288;Uo...[&#34987;&#25130;&#26029;]
&lt;/p&gt;
&lt;p&gt;
The lack of an efficient compression model remains a challenge for the wireless transmission of gearbox data in non-contact gear fault diagnosis problems. In this paper, we present a signal-adaptive asymmetrical autoencoder with a transform domain layer to compress sensor signals. First, a new discrete cosine Stockwell transform (DCST) layer is introduced to replace linear layers in a multi-layer autoencoder. A trainable filter is implemented in the DCST domain by utilizing the multiplication property of the convolution. A trainable hard-thresholding layer is applied to reduce redundant data in the DCST layer to make the feature map sparse. In comparison to the linear layer, the DCST layer reduces the number of trainable parameters and improves the accuracy of data reconstruction. Second, training the autoencoder with a sparsifying DCST layer only requires a small number of datasets. The proposed method is superior to other autoencoder-based methods on the University of Connecticut (Uo
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.02854</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#23454;&#29616;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#20026;&#23637;&#31034;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33258;&#28982;&#26426;&#20250;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36807;&#20110;&#31616;&#21270;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#36866;&#29992;&#20110;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65307;&#20363;&#22914;&#65292;&#27599;&#20010;&#39046;&#22495;&#37117;&#26469;&#33258;&#19981;&#21516;&#30340;&#21333;&#33410;&#28857;&#23436;&#32654;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#22810;&#39046;&#22495;&#25968;&#25454;&#20013;&#65292;&#24448;&#24448;&#23384;&#22312;&#19968;&#37096;&#20998;&#28508;&#21464;&#37327;&#30340;&#26576;&#20123;&#20998;&#24067;&#23646;&#24615;&#65288;&#20363;&#22914;&#25903;&#25345;&#24230;&#12289;&#26041;&#24046;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65307;&#24403;&#27599;&#20010;&#39046;&#22495;&#26469;&#33258;&#22810;&#33410;&#28857;&#19981;&#23436;&#32654;&#24178;&#39044;&#26102;&#65292;&#36825;&#20010;&#23646;&#24615;&#25104;&#31435;&#12290;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02832</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#36827;&#34892;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#30001;&#20110;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#24178;&#39044;&#35757;&#32451;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#21464;&#25442;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24102;&#22806;&#25968;&#25454;&#65288;BLOOD&#65289;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;BLOOD&#21033;&#29992;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#23618;&#38388;&#34920;&#31034;&#21464;&#25442;&#30456;&#36739;&#20110;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#26356;&#24179;&#28369;&#30340;&#20542;&#21521;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;Transformer&#32593;&#32476;&#20013;&#32463;&#39564;&#35777;&#26126;&#30340;&#19968;&#20010;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BLOOD&#19982;Transformer&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#36164;&#28304;&#38656;&#27714;&#30456;&#24403;&#30340;&#26041;&#27861;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#26102;&#65292;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#20250;&#20445;&#25345;&#20854;&#21407;&#22987;&#30340;&#38160;&#24230;&#65292;&#32780;&#38160;&#24230;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02823</link><description>&lt;p&gt;
&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSL-GFN&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#31574;&#30053;&#26469;&#39034;&#24207;&#29983;&#25104;&#32452;&#21512;&#32467;&#26500;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#12290;&#23427;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#25353;&#27604;&#20363;&#37319;&#26679;&#20855;&#26377;&#30456;&#24212;&#28201;&#24230;&#35843;&#33410;&#30340;&#23545;&#35937;&#30340;&#22870;&#21169;&#12290;&#22312;GFlowNets&#20013;&#65292;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#20195;&#34920;&#20102;&#19968;&#31995;&#21015;&#30001;&#28201;&#24230;&#32034;&#24341;&#30340;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#19982;&#30456;&#24212;&#30340;&#28201;&#24230;&#35843;&#33410;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;&#28201;&#24230;&#26465;&#20214;&#19979;&#30340;GFlowNets&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#26469;&#25511;&#21046;&#23545;GFlowNets&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#28201;&#24230;&#26465;&#20214;&#19979;&#23610;&#24230;&#26631;&#37327;&#21270;&#30340;GFlowNets&#65288;LSL-GFN&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#28201;&#24230;&#26465;&#20214;&#19979;GFlowNets&#30340;&#35757;&#32451;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#20043;&#21069;&#25552;&#20986;&#30340;&#28201;&#24230;&#26465;&#20214;&#26041;&#27861;&#22312;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#24341;&#20837;&#20102;&#25968;&#20540;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#28201;&#24230;&#21487;&#33021;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;: &#23545;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#24863;&#30693;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21046;&#36896;&#19994;&#27491;&#22312;&#25910;&#38598;&#22823;&#37327;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#12290;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479; (SMS) &#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867; (TSC) &#22312;&#35813;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#21046;&#36896;&#19994;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013; TSC &#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312; TSC &#21644;&#21046;&#36896;&#19994;&#25991;&#29486;&#20013;&#25506;&#32034;&#21644;&#32534;&#21046;&#20102;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;92&#20010;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35813;&#21015;&#34920;&#20013;&#36873;&#25321;&#20102;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;36&#20010;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#22312;&#21508;&#31181;&#21046;&#36896;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#32452;&#21253;&#21547;22&#20010;&#21046;&#36896;&#19994;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21046;&#36896;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#21046;&#36896;&#19994;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;G2MILP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.02807</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#29992;&#20110;MILP&#27714;&#35299;&#22120;&#30340;&#28145;&#24230;&#23454;&#20363;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability. (arXiv:2310.02807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;G2MILP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;MILP&#65289;&#65292;&#20986;&#29616;&#20102;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#26524;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#21644;&#26377;&#20559;&#35265;&#30340;&#27714;&#35299;&#22120;&#35780;&#20272;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#31995;&#21015;&#21512;&#25104;MILP&#23454;&#20363;&#29983;&#25104;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#19987;&#23478;&#35774;&#35745;&#30340;&#34920;&#36798;&#24335;&#65292;&#35201;&#20040;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G2MILP&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;G2MILP&#23558;MILP&#23454;&#20363;&#34920;&#31034;&#20026;&#20108;&#20998;&#22270;&#65292;&#24182;&#24212;&#29992;&#36974;&#34109;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#36845;&#20195;&#22320;&#30772;&#22351;&#21644;&#26367;&#25442;&#21407;&#22987;&#22270;&#30340;&#37096;&#20998;&#20197;&#29983;&#25104;&#26032;&#30340;&#23454;&#20363;&#12290;G2MILP&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#26159;&#23427;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, which to the best of our knowledge is the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#31216;&#20026;D-GRW&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.02806</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22303;&#22756;&#20013;&#30340;&#27700;&#27969;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Data-facilitated Numerical Method for Richards Equation to Model Water Flow Dynamics in Soil. (arXiv:2310.02806v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#31216;&#20026;D-GRW&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#21306;&#22303;&#22756;&#28287;&#24230;&#30340;&#30417;&#27979;&#23545;&#20110;&#31934;&#23494;&#20892;&#19994;&#12289;&#26234;&#33021;&#28748;&#28297;&#21644;&#24178;&#26097;&#39044;&#38450;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#36890;&#36807;&#27714;&#35299;Richards&#26041;&#31243;&#36825;&#26679;&#30340;&#27700;&#25991;&#27169;&#22411;&#26469;&#27169;&#25311;&#22303;&#22756;&#30340;&#26102;&#31354;&#27700;&#27969;&#21160;&#21147;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#12290;&#36825;&#31181;&#25968;&#20540;&#35299;&#27861;&#34987;&#31216;&#20026;D-GRW&#65288;Data-facilitated global Random Walk&#65289;&#26041;&#27861;&#65292;&#23427;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#20013;&#21327;&#21516;&#22320;&#25972;&#21512;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#20135;&#29983;&#31934;&#30830;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#19988;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#19977;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;D-GRW&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20934;&#25968;&#20540;&#35299;&#27861;&#21644;&#21830;&#29992;&#36719;&#20214;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root-zone soil moisture monitoring is essential for precision agriculture, smart irrigation, and drought prevention. Modeling the spatiotemporal water flow dynamics in soil is typically achieved by solving a hydrological model, such as the Richards equation which is a highly nonlinear partial differential equation (PDE). In this paper, we present a novel data-facilitated numerical method for solving the mixed-form Richards equation. This numerical method, which we call the D-GRW (Data-facilitated global Random Walk) method, synergistically integrates adaptive linearization scheme, neural networks, and global random walk in a finite volume discretization framework to produce accurate numerical solutions of the Richards equation with guaranteed convergence under reasonable assumptions. Through three illustrative examples, we demonstrate and discuss the superior accuracy and mass conservation performance of our D-GRW method and compare it with benchmark numerical methods and commercial so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#21478;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#22312;&#19968;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#25110;&#34987;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.02804</link><description>&lt;p&gt;
DOMINO: &#22810;&#27493;&#39588;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DOMINO: A Dual-System for Multi-step Visual Language Reasoning. (arXiv:2310.02804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#21478;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#22312;&#19968;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#25110;&#34987;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#20174;&#20449;&#24687;&#23494;&#38598;&#30340;&#22270;&#20687;&#65288;&#22914;&#22270;&#34920;&#25110;&#32472;&#22270;&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#25110;&#25968;&#23383;&#65292;&#24182;&#25191;&#34892;&#36923;&#36753;&#25110;&#31639;&#26415;&#25512;&#29702;&#20197;&#24471;&#20986;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20381;&#38752;&#20197;&#19979;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#35757;&#32451;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#31471;&#21040;&#31471;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#25110;&#32773;&#65288;2&#65289;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#20854;&#20013;&#19968;&#20010;&#23383;&#24149;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#30001;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#25991;&#26412;&#20197;&#25512;&#26029;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#24378;&#36843;&#27169;&#22411;&#29992;&#21333;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21518;&#19968;&#31181;&#26041;&#27861;&#23481;&#26131;&#20135;&#29983;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#19981;&#20934;&#30830;&#25110;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#35753;&#35821;&#35328;&#27169;&#22411;&#28151;&#28102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#22411;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#23427;&#21253;&#21547;&#29992;&#20110;&#35270;&#35273;&#20449;&#24687;&#25552;&#21462;&#30340;&#8220;System-1&#8221;&#27493;&#39588;&#21644;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#30340;&#8220;System-2&#8221;&#27493;&#39588;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#65292;System-2&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#27493;&#39588;&#65292;&#27599;&#20010;&#23376;&#27493;&#39588;&#25351;&#23548;System-1&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language reasoning requires a system to extract text or numbers from information-dense images like charts or plots and perform logical or arithmetic reasoning to arrive at an answer. To tackle this task, existing work relies on either (1) an end-to-end vision-language model trained on a large amount of data, or (2) a two-stage pipeline where a captioning model converts the image into text that is further read by another large language model to deduce the answer. However, the former approach forces the model to answer a complex question with one single step, and the latter approach is prone to inaccurate or distracting information in the converted text that can confuse the language model. In this work, we propose a dual-system for multi-step multimodal reasoning, which consists of a "System-1" step for visual information extraction and a "System-2" step for deliberate reasoning. Given an input, System-2 breaks down the question into atomic sub-steps, each guiding System-1 to extr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02784</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#33410;&#28857;&#65306;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems. (arXiv:2310.02784v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26159;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#20998;&#24067;&#24335;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#26681;&#25454;&#23454;&#38469;&#24773;&#20917;&#22312;&#25968;&#25454;&#20013;&#24515;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#19978;&#36827;&#34892;&#22823;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;14~32%&#30340;GPU&#23567;&#26102;&#29992;&#20110;&#36890;&#20449;&#65292;&#27809;&#26377;&#37325;&#21472;&#35745;&#31639;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23569;&#31561;&#24453;&#36890;&#20449;&#24310;&#36831;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#25351;&#23548;&#24182;&#34892;&#21270;&#21644;&#30828;&#20214;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#31574;&#30053;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPU&#35757;&#32451;&#30828;&#20214;&#19978;&#30340;&#19968;&#22871;&#23454;&#38469;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#22330;&#26223;&#20998;&#21035;&#21487;&#20197;&#25552;&#39640;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#21644;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#21517;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.02782</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#25506;&#32034;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design. (arXiv:2310.02782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#21644;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#21517;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#36827;&#23637;&#26159;&#30001;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#25512;&#21160;&#30340;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#65292;&#24076;&#26395;&#21457;&#29616;&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#12290;&#23613;&#31649;&#20687;&#23398;&#20064;&#31574;&#30053;&#26799;&#24230;&#65288;LPG&#65289;&#36825;&#26679;&#30340;&#31639;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#26159;&#24403;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#26102;&#20173;&#23384;&#22312;&#27867;&#21270;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20803;&#35757;&#32451;&#20998;&#24067;&#30340;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20511;&#37492;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#20803;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#36951;&#25022;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#29615;&#22659;&#35774;&#35745;&#33719;&#24471;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#21644;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#65292;&#20998;&#21035;&#24212;&#29992;&#20110;&#38543;&#26426;&#29615;&#22659;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#12290;&#22312;&#38543;&#26426;&#20219;&#21153;&#20013;&#65292;EFlowNets&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#21452;&#20154;&#28216;&#25103;&#20013;&#65292;AFlowNets&#22312;&#33258;&#25105;&#23545;&#24328;&#20013;&#25214;&#21040;&#20102;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#36229;&#36807;&#20102;AlphaZero&#12290;</title><link>http://arxiv.org/abs/2310.02779</link><description>&lt;p&gt;
&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#39044;&#26399;&#27969;&#32593;&#32476;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Expected flow networks in stochastic environments and two-player zero-sum games. (arXiv:2310.02779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#21644;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#65292;&#20998;&#21035;&#24212;&#29992;&#20110;&#38543;&#26426;&#29615;&#22659;&#21644;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#12290;&#22312;&#38543;&#26426;&#20219;&#21153;&#20013;&#65292;EFlowNets&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#21452;&#20154;&#28216;&#25103;&#20013;&#65292;AFlowNets&#22312;&#33258;&#25105;&#23545;&#24328;&#20013;&#25214;&#21040;&#20102;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#36229;&#36807;&#20102;AlphaZero&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#21305;&#37197;&#32473;&#23450;&#20998;&#24067;&#30340;&#24207;&#21015;&#37319;&#26679;&#27169;&#22411;&#12290;GFlowNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#32467;&#26500;&#23545;&#35937;&#29983;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36805;&#36895;&#37319;&#26679;&#20986;&#22810;&#26679;&#21270;&#19988;&#39640;&#22238;&#25253;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#26399;&#27969;&#32593;&#32476;&#65288;EFlowNets&#65289;&#65292;&#23558;GFlowNets&#25193;&#23637;&#21040;&#38543;&#26426;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;EFlowNets&#22312;&#38543;&#26426;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#35774;&#35745;&#65289;&#20013;&#20248;&#20110;&#20854;&#20182;GFlowNet&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;EFlowNets&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23545;&#25239;&#29615;&#22659;&#20013;&#65292;&#20026;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#25552;&#20986;&#20102;&#23545;&#25239;&#27969;&#32593;&#32476;&#65288;AFlowNets&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AFlowNets&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#22312;Connect-4&#28216;&#25103;&#20013;&#33021;&#25214;&#21040;80%&#20197;&#19978;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#20248;&#20110;AlphaZero&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26377;&#21521;&#22270;&#26469;&#32534;&#30721;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#29992;&#20110;&#30417;&#30563;&#20998;&#31867;&#21644;&#20449;&#21495;&#37325;&#26500;&#65292;&#22312;&#36136;&#37327;&#35782;&#21035;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02774</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#26377;&#21521;&#22270;&#36827;&#34892;&#36136;&#37327;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Time Series as Directed Graphs for Quality Recognition. (arXiv:2310.02774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26377;&#21521;&#22270;&#26469;&#32534;&#30721;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#29992;&#20110;&#30417;&#30563;&#20998;&#31867;&#21644;&#20449;&#21495;&#37325;&#26500;&#65292;&#22312;&#36136;&#37327;&#35782;&#21035;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#27491;&#25104;&#20026;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#30340;&#26680;&#24515;&#65292;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#65292;&#22914;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26377;&#21521;&#22270;&#65292;&#20197;&#20351;&#20854;&#25299;&#25169;&#32467;&#26500;&#32534;&#30721;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#24320;&#22987;&#25506;&#32034;&#22312;&#20854;&#20013;&#24212;&#29992;GNN&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#30417;&#30563;&#20998;&#31867;&#22120;&#65292;&#19968;&#20010;&#26159;&#31867;&#20284;&#20110;&#33258;&#32534;&#30721;&#27169;&#22411;&#30340;&#20449;&#21495;&#37325;&#26500;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#36136;&#37327;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are becoming central in the study of time series, coupled with existing algorithms as Temporal Convolutional Networks and Recurrent Neural Networks. In this paper, we see time series themselves as directed graphs, so that their topology encodes time dependencies and we start to explore the effectiveness of GNNs architectures on them. We develop two distinct Geometric Deep Learning models, a supervised classifier and an autoencoder-like model for signal reconstruction. We apply these models on a quality recognition problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19979;&#30340;&#22522;&#20110;&#26680;&#30340;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#29615;&#22659;&#30417;&#27979;&#21644;&#20256;&#24863;&#22120;&#37325;&#26500;&#31561;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.02767</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#22522;&#20110;&#26680;&#30340;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel-based function learning in dynamic and non stationary environments. (arXiv:2310.02767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19979;&#30340;&#22522;&#20110;&#26680;&#30340;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#29615;&#22659;&#30417;&#27979;&#21644;&#20256;&#24863;&#22120;&#37325;&#26500;&#31561;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#20027;&#39064;&#26159;&#20174;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#20013;&#36827;&#34892;&#20989;&#25968;&#20272;&#35745;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19979;&#30340;&#22522;&#20110;&#26680;&#30340;&#23725;&#22238;&#24402;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#26465;&#20214;&#65292;&#21253;&#25324;&#21487;&#33021;&#26080;&#38480;&#27425;&#21457;&#29983;&#38543;&#26426;&#35843;&#25972;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
One central theme in machine learning is function estimation from sparse and noisy data. An example is supervised learning where the elements of the training set are couples, each containing an input location and an output response. In the last decades, a substantial amount of work has been devoted to design estimators for the unknown function and to study their convergence to the optimal predictor, also characterizing the learning rate. These results typically rely on stationary assumptions where input locations are drawn from a probability distribution that does not change in time. In this work, we consider kernel-based ridge regression and derive convergence conditions under non stationary distributions, addressing also cases where stochastic adaption may happen infinitely often. This includes the important exploration-exploitation problems where e.g. a set of agents/robots has to monitor an environment to reconstruct a sensorial field and their movements rules are continuously upda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2310.02759</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65306;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms. (arXiv:2310.02759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#31456;&#35780;&#20998;&#65288;AES&#65289;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23574;&#31471;&#25216;&#26415;&#65292;&#35780;&#20998;&#25216;&#26415;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#65292;&#21487;&#38752;&#30340;&#24471;&#20998;&#26159;&#22522;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#21464;&#37327;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#36825;&#20123;&#21464;&#37327;&#21487;&#20197;&#26681;&#25454;&#39046;&#22495;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#35745;&#31639;&#20986;&#26469;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#29992;&#25143;&#23545;&#32473;&#23450;&#20027;&#39064;&#30340;&#29702;&#35299;&#65292;&#20998;&#26512;&#26159;&#22522;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20998;&#25351;&#25968;&#65292;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#26368;&#36817;&#23398;&#20064;&#30340;&#20027;&#39064;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32467;&#26524;&#21017;&#23545;&#23398;&#20064;&#20998;&#26512;&#26377;&#25152;&#36129;&#29486;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#34913;&#37327;&#29992;&#25143;&#23545;&#20854;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#12290;&#35813;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;LangChain&#24037;&#20855;&#23545;PDF&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Essay Score (AES) is proven to be one of the cutting-edge technologies. Scoring techniques are used for various purposes. Reliable scores are calculated based on influential variables. Such variables can be computed by different methods based on the domain. The research is concentrated on the user's understanding of a given topic. The analysis is based on a scoring index by using Large Language Models. The user can then compare and contrast the understanding of a topic that they recently learned. The results are then contributed towards learning analytics and progression is made for enhancing the learning ability. In this research, the focus is on summarizing a PDF document and gauging a user's understanding of its content. The process involves utilizing a Langchain tool to summarize the PDF and extract the essential information. By employing this technique, the research aims to determine how well the user comprehends the summarized content.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#22810;&#26679;&#24615;&#24378;&#12289;&#21487;&#25511;&#21046;&#30340;&#36924;&#30495;&#19977;&#32500;&#20154;&#22836;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#12290;&#26041;&#27861;&#21253;&#25324;&#20960;&#20309;&#29983;&#25104;&#22120;&#12289;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#21644;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#12290;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.02753</link><description>&lt;p&gt;
MUNCH: &#24314;&#27169;&#29420;&#29305;&#19988;&#21487;&#25511;&#21046;&#30340;&#22836;&#37096;
&lt;/p&gt;
&lt;p&gt;
MUNCH: Modelling Unique 'N Controllable Heads. (arXiv:2310.02753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#22810;&#26679;&#24615;&#24378;&#12289;&#21487;&#25511;&#21046;&#30340;&#36924;&#30495;&#19977;&#32500;&#20154;&#22836;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#12290;&#26041;&#27861;&#21253;&#25324;&#20960;&#20309;&#29983;&#25104;&#22120;&#12289;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#21644;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#12290;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#33258;&#21160;&#29983;&#25104;&#19977;&#32500;&#20154;&#22836;&#19968;&#30452;&#26159;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#36924;&#30495;&#30340;&#35282;&#33394;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#25511;&#21046;&#26377;&#38480;&#65292;&#24182;&#19988;&#24418;&#29366;&#21644;&#32441;&#29702;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20063;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#25511;&#21046;&#21644;&#36924;&#30495;&#24615;&#26041;&#38754;&#37117;&#20855;&#22791;&#20102;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#65292;&#23545;&#20110;&#28216;&#25103;&#35774;&#35745;&#33402;&#26415;&#23478;&#26469;&#35828;&#36825;&#20123;&#37117;&#26159;&#29702;&#24819;&#30340;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20960;&#20309;&#29983;&#25104;&#22120;&#21487;&#20197;&#35782;&#21035;&#33073;&#32806;&#30340;&#28508;&#22312;&#26041;&#21521;&#24182;&#29983;&#25104;&#26032;&#39062;&#19988;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#23398;&#20064;&#21512;&#25104;&#22810;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#22270;&#65292;&#21253;&#25324;&#21453;&#29031;&#29575;&#12289;&#20809;&#27901;&#24230;&#12289;&#38236;&#38754;&#21453;&#23556;&#21644;&#27861;&#32447;&#26041;&#21521;&#12290;&#23545;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#36755;&#20986;&#30340;&#33402;&#26415;&#23478;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#65292;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#39068;&#33394;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. First, our proposed Geometry Generator identifies disentangled latent directions and generate novel and diverse samples. A Render Map Generator then learns to synthesize multiply high-fidelty physically-based render maps including Albedo, Glossiness, Specular, and Normals. For artists preferring fine-grained control over the output, we introduce a novel Color Transformer Model that allows semantic color control over generated maps. We also introduce quantifiable metrics called Uniqueness and Novelt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#31995;&#32479;&#24615;&#22320;&#36827;&#34892;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.02752</link><description>&lt;p&gt;
&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#65306;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms. (arXiv:2310.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#31995;&#32479;&#24615;&#22320;&#36827;&#34892;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#20570;&#20986;&#23545;&#20154;&#20204;&#29983;&#27963;&#24433;&#21709;&#37325;&#22823;&#30340;&#20915;&#31574;&#65288;&#22914;&#25509;&#21463;&#25110;&#25298;&#32477;&#36151;&#27454;&#12289;&#25307;&#32856;&#20915;&#31574;&#31561;&#65289;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#21040;&#30340;&#20998;&#31867;&#22120;&#38656;&#35201;&#22312;&#19981;&#21516;&#20154;&#32676;&#65288;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#19981;&#21516;&#21464;&#37327;&#20540;&#65289;&#20043;&#38388;&#26082;&#20934;&#30830;&#21448;&#20844;&#24179;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#65292;&#21363;&#36873;&#25321;&#19968;&#20010;&#29305;&#24449;&#23376;&#38598;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#25152;&#20570;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20004;&#31181;&#29992;&#20110;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#30340;&#36951;&#20256;&#31639;&#27861;&#65306;&#65288;a&#65289;&#22522;&#20110;&#24085;&#32047;&#25176;&#20248;&#21183;&#30340;&#36951;&#20256;&#31639;&#27861;&#65307;&#65288;b&#65289;&#22522;&#20110;&#35789;&#20856;&#20248;&#21270;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#20854;&#20013;&#20934;&#30830;&#24615;&#30340;&#26368;&#22823;&#21270;&#20248;&#20808;&#32423;&#39640;&#20110;&#20844;&#24179;&#24615;&#30340;&#26368;&#22823;&#21270;&#12290;&#20004;&#31181;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#36827;&#34892;&#25511;&#21046;&#27604;&#36739;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#27604;&#36739;&#20004;&#31181;&#19981;&#21516;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning classifiers are widely used to make decisions with a major impact on people's lives (e.g. accepting or denying a loan, hiring decisions, etc). In such applications,the learned classifiers need to be both accurate and fair with respect to different groups of people, with different values of variables such as sex and race. This paper focuses on fair feature selection for classification, i.e. methods that select a feature subset aimed at maximising both the accuracy and the fairness of the predictions made by a classifier. More specifically, we compare two recently proposed Genetic Algorithms (GAs) for fair feature selection that are based on two different multi-objective optimisation approaches: (a) a Pareto dominance-based GA; and (b) a lexicographic optimisation-based GA, where maximising accuracy has higher priority than maximising fairness. Both GAs use the same measures of accuracy and fairness, allowing for a controlled comparison. As far as we know, this is the fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHOT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#26799;&#24230;&#20248;&#21270;&#36712;&#36857;&#20013;&#30340;&#28023;&#26862;&#30697;&#38453;&#26469;&#25913;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02751</link><description>&lt;p&gt;
SHOT&#65306;&#25233;&#21046;&#26799;&#24230;&#20248;&#21270;&#36712;&#36857;&#20013;&#30340;&#28023;&#26862;&#30697;&#38453;&#20197;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning. (arXiv:2310.02751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHOT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#26799;&#24230;&#20248;&#21270;&#36712;&#36857;&#20013;&#30340;&#28023;&#26862;&#30697;&#38453;&#26469;&#25913;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20551;&#35774;&#26799;&#24230;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#65288;GBML&#65289;&#22312;&#20869;&#24490;&#29615;&#20013;&#38544;&#24335;&#22320;&#25233;&#21046;&#20102;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#28023;&#26862;&#30697;&#38453;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHOT&#65288;&#25233;&#21046;&#26799;&#24230;&#20248;&#21270;&#36712;&#36857;&#20013;&#30340;&#28023;&#26862;&#30697;&#38453;&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#27169;&#22411;&#21644;&#21442;&#32771;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#25233;&#21046;&#20869;&#24490;&#29615;&#20013;&#30340;&#28023;&#26862;&#30697;&#38453;&#12290;&#23613;&#31649;&#28041;&#21450;&#39640;&#38454;&#39033;&#65292;SHOT&#24182;&#19981;&#26174;&#33879;&#22686;&#21152;&#22522;&#32447;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;SHOT&#23545;GBML&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#21644;&#26550;&#26500;&#37117;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#20351;&#20854;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;GBML&#22522;&#32447;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;SHOT&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#21160;&#21147;&#23398;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#24182;&#35777;&#26126;&#20102;SHOT&#20248;&#20110;&#30456;&#24212;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JunHoo-Lee/SHOT&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we hypothesize that gradient-based meta-learning (GBML) implicitly suppresses the Hessian along the optimization trajectory in the inner loop. Based on this hypothesis, we introduce an algorithm called SHOT (Suppressing the Hessian along the Optimization Trajectory) that minimizes the distance between the parameters of the target and reference models to suppress the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does not increase the computational complexity of the baseline model much. It is agnostic to both the algorithm and architecture used in GBML, making it highly versatile and applicable to any GBML baseline. To validate the effectiveness of SHOT, we conduct empirical tests on standard few-shot learning tasks and qualitatively analyze its dynamics. We confirm our hypothesis empirically and demonstrate that SHOT outperforms the corresponding baseline. Code is available at: https://github.com/JunHoo-Lee/SHOT
&lt;/p&gt;</description></item><item><title>SALSA&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#65288;SALSA&#65289;&#65292;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#23545;&#27604;&#20219;&#21153;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#20043;&#38388;&#30340;&#22270;&#23545;&#22270;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02744</link><description>&lt;p&gt;
SALSA: &#35821;&#20041;&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
SALSA: Semantically-Aware Latent Space Autoencoder. (arXiv:2310.02744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02744
&lt;/p&gt;
&lt;p&gt;
SALSA&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#33258;&#32534;&#30721;&#22120;&#65288;SALSA&#65289;&#65292;&#36890;&#36807;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#23545;&#27604;&#20219;&#21153;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#20043;&#38388;&#30340;&#22270;&#23545;&#22270;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#30740;&#31350;&#20013;&#65292;&#21270;&#23398;&#25968;&#25454;&#36890;&#24120;&#20197;&#31616;&#21270;&#30340;&#20998;&#23376;&#36755;&#20837;&#32447;&#26465;&#36755;&#20837;&#31995;&#32479; (SMILES) &#24207;&#21015;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#26041;&#20415;&#22320;&#23454;&#26045;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20043;&#19968;&#65292;&#21363;&#24207;&#21015;&#21040;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20165;&#20165;&#22312;SMILES&#19978;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26159;&#19981;&#36275;&#20197;&#23398;&#20064;&#21040;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#34920;&#31034;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#35821;&#20041;&#36890;&#36807;&#20998;&#23376;&#20043;&#38388;&#30340;&#32467;&#26500;&#65288;&#22270;&#23545;&#22270;&#65289;&#30456;&#20284;&#24615;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#65292;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#20250;&#23558;&#32467;&#26500;&#30456;&#20284;&#30340;&#20998;&#23376;&#26144;&#23556;&#21040;&#30456;&#36317;&#36739;&#36828;&#30340;&#32534;&#30721;&#65292;&#23548;&#33268;&#19968;&#20010;&#19981;&#19968;&#33268;&#30340;&#28508;&#31354;&#38388;&#65292;&#19981;&#23562;&#37325;&#20998;&#23376;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Semantically-Aware Latent Space Autoencoder (SALSA)&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;&#21464;&#25442;&#22120;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#20043;&#38388;&#30340;&#22270;&#23545;&#22270;&#30456;&#20284;&#24615;&#12290;&#24418;&#24335;&#19978;&#65292;&#23545;&#27604;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
In deep learning for drug discovery, chemical data are often represented as simplified molecular-input line-entry system (SMILES) sequences which allow for straightforward implementation of natural language processing methodologies, one being the sequence-to-sequence autoencoder. However, we observe that training an autoencoder solely on SMILES is insufficient to learn molecular representations that are semantically meaningful, where semantics are defined by the structural (graph-to-graph) similarities between molecules. We demonstrate by example that autoencoders may map structurally similar molecules to distant codes, resulting in an incoherent latent space that does not respect the structural similarities between molecules. To address this shortcoming we propose Semantically-Aware Latent Space Autoencoder (SALSA), a transformer-autoencoder modified with a contrastive task, tailored specifically to learn graph-to-graph similarity between molecules. Formally, the contrastive objective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21644;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#65292;&#23545;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.02743</link><description>&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26377;&#21161;&#20110;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21644;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#65292;&#23545;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20197;&#36981;&#24490;&#25351;&#20196;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#34987;&#29992;&#26469;&#36817;&#20284;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#19981;&#23436;&#32654;&#34920;&#31034;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#12290;Gao&#31561;&#20154;&#22312;&#19968;&#20010;&#20154;&#24037;&#21453;&#39304;&#23454;&#39564;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#29616;&#35937;&#65292;&#20351;&#29992;&#19968;&#20010;&#36739;&#22823;&#30340;&#8220;&#37329;&#26631;&#20934;&#8221;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#30495;&#23454;&#22870;&#21169;&#65288;&#32780;&#19981;&#26159;&#20154;&#31867;&#65289;&#65292;&#24182;&#26174;&#31034;&#36807;&#24230;&#20248;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#20195;&#29702;&#22870;&#21169;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#22914;&#20309;&#12290;&#20351;&#29992;&#31867;&#20284;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22312;&#20351;&#29992;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#26102;&#65292;&#20351;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#65288;&#26368;&#22351;&#24773;&#20917;&#20248;&#21270;&#21644;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#20248;&#21270;&#65289;&#26469;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#19981;&#24179;&#34913;&#24694;&#24847;&#36719;&#20214;&#22270;&#20687;&#20998;&#31867;&#30340;&#20845;&#31181;&#27169;&#22411;&#24615;&#33021;&#65292;&#21457;&#29616;&#31867;&#21035;&#19981;&#24179;&#34913;&#31243;&#24230;&#36234;&#39640;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#36234;&#23569;&#65292;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#20063;&#36234;&#22823;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;ResNet50&#12289;EfficientNetB0&#21644;DenseNet169&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#23545;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02742</link><description>&lt;p&gt;
&#20511;&#21161;&#36801;&#31227;&#23398;&#20064;&#27604;&#36739;&#19981;&#24179;&#34913;&#24694;&#24847;&#36719;&#20214;Byteplot&#22270;&#20687;&#20998;&#31867;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Imbalanced Malware Byteplot Image Classification using Transfer Learning. (arXiv:2310.02742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#19981;&#24179;&#34913;&#24694;&#24847;&#36719;&#20214;&#22270;&#20687;&#20998;&#31867;&#30340;&#20845;&#31181;&#27169;&#22411;&#24615;&#33021;&#65292;&#21457;&#29616;&#31867;&#21035;&#19981;&#24179;&#34913;&#31243;&#24230;&#36234;&#39640;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#36234;&#23569;&#65292;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#20063;&#36234;&#22823;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;ResNet50&#12289;EfficientNetB0&#21644;DenseNet169&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#23545;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31185;&#25216;&#21644;&#20114;&#32852;&#31995;&#32479;&#30340;&#26085;&#30410;&#20381;&#36182;&#65292;&#32593;&#32476;&#23433;&#20840;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#36890;&#36807;&#27604;&#36739;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#26469;&#32531;&#35299;&#32593;&#32476;&#25915;&#20987;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#29305;&#24449;&#25552;&#21462;&#12289;&#35782;&#21035;&#27169;&#24335;&#21644;&#22686;&#24378;&#21160;&#24577;&#20998;&#26512;&#26469;&#25913;&#36827;&#36825;&#20123;&#26816;&#27979;&#22120;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20845;&#31181;&#22810;&#31867;&#21035;&#20998;&#31867;&#27169;&#22411;&#22312;Malimg&#25968;&#25454;&#38598;&#12289;&#28151;&#21512;&#25968;&#25454;&#38598;&#21644;Malevis&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;&#35266;&#23519;&#21040;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#31243;&#24230;&#36234;&#39640;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#36234;&#23569;&#65292;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#20063;&#36234;&#22823;&#12290;&#27492;&#22806;&#65292;&#36824;&#35266;&#23519;&#21040;&#23545;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#65292;ResNet50&#12289;EfficientNetB0&#21644;DenseNet169&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#24179;&#34913;&#30340;&#25968;&#25454;&#12290;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#39640;97%&#30340;&#31934;&#24230;&#65292;&#22312;&#20013;&#31561;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#26368;&#39640;95%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity is a major concern due to the increasing reliance on technology and interconnected systems. Malware detectors help mitigate cyber-attacks by comparing malware signatures. Machine learning can improve these detectors by automating feature extraction, identifying patterns, and enhancing dynamic analysis. In this paper, the performance of six multiclass classification models is compared on the Malimg dataset, Blended dataset, and Malevis dataset to gain insights into the effect of class imbalance on model performance and convergence. It is observed that the more the class imbalance less the number of epochs required for convergence and a high variance across the performance of different models. Moreover, it is also observed that for malware detectors ResNet50, EfficientNetB0, and DenseNet169 can handle imbalanced and balanced data well. A maximum precision of 97% is obtained for the imbalanced dataset, a maximum precision of 95% is obtained on the intermediate imbalance data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20107;&#20214;&#25968;&#25454;&#20998;&#26512;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#30340;&#23398;&#20064;&#36335;&#24452;&#65292;&#36890;&#36807;&#20915;&#31574;&#26641;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21017;&#24314;&#35758;&#65292;&#24182;&#29992;&#20110;&#23398;&#20064;&#35268;&#21010;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#36873;&#35838;&#31243;&#24207;&#21015;&#29305;&#24449;&#23545;&#23398;&#19994;&#34920;&#29616;&#26377;&#36739;&#22909;&#35299;&#37322;&#65292;&#20026;&#21046;&#23450;&#26356;&#36866;&#24212;&#24615;&#30340;&#23398;&#20064;&#35745;&#21010;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.02735</link><description>&lt;p&gt;
&#20174;&#20107;&#20214;&#25968;&#25454;&#20013;&#25552;&#21462;&#35268;&#21017;&#29992;&#20110;&#23398;&#20064;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extracting Rules from Event Data for Study Planning. (arXiv:2310.02735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20107;&#20214;&#25968;&#25454;&#20998;&#26512;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#30340;&#23398;&#20064;&#36335;&#24452;&#65292;&#36890;&#36807;&#20915;&#31574;&#26641;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21017;&#24314;&#35758;&#65292;&#24182;&#29992;&#20110;&#23398;&#20064;&#35268;&#21010;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#36873;&#35838;&#31243;&#24207;&#21015;&#29305;&#24449;&#23545;&#23398;&#19994;&#34920;&#29616;&#26377;&#36739;&#22909;&#35299;&#37322;&#65292;&#20026;&#21046;&#23450;&#26356;&#36866;&#24212;&#24615;&#30340;&#23398;&#20064;&#35745;&#21010;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26657;&#22253;&#31649;&#29702;&#31995;&#32479;&#30340;&#20107;&#20214;&#25968;&#25454;&#26469;&#20998;&#26512;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#30340;&#23398;&#20064;&#36335;&#24452;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#20182;&#20204;&#30340;&#23398;&#20064;&#35268;&#21010;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#20351;&#29992;&#36807;&#31243;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#26469;&#25506;&#32034;&#25152;&#36873;&#35838;&#31243;&#24207;&#21015;&#23545;&#23398;&#19994;&#25104;&#32489;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20197;&#35268;&#21017;&#24418;&#24335;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#35268;&#21010;&#24314;&#35758;&#65292;&#24182;&#23558;&#20854;&#19982;&#25512;&#33616;&#30340;&#23398;&#20064;&#35745;&#21010;&#36827;&#34892;&#27604;&#36739;&#12290;&#35780;&#20272;&#37325;&#28857;&#20851;&#27880;RWTH Aachen&#22823;&#23398;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#22763;&#23398;&#20301;&#35745;&#21010;&#30340;&#23398;&#29983;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35838;&#31243;&#24207;&#21015;&#29305;&#24449;&#26377;&#25928;&#22320;&#35299;&#37322;&#20102;&#23398;&#19994;&#34920;&#29616;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#24314;&#35758;&#20102;&#24320;&#21457;&#26356;&#20855;&#36866;&#24212;&#24615;&#23398;&#20064;&#35745;&#21010;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we examine how event data from campus management systems can be used to analyze the study paths of higher education students. The main goal is to offer valuable guidance for their study planning. We employ process and data mining techniques to explore the impact of sequences of taken courses on academic success. Through the use of decision tree models, we generate data-driven recommendations in the form of rules for study planning and compare them to the recommended study plan. The evaluation focuses on RWTH Aachen University computer science bachelor program students and demonstrates that the proposed course sequence features effectively explain academic performance measures. Furthermore, the findings suggest avenues for developing more adaptable study plans.
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#35748;&#20026;&#27431;&#30431;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#36136;&#37327;&#20445;&#35777;&#26041;&#24335;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25351;&#20986;&#22522;&#20110;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;&#21450;&#20934;&#30830;&#23450;&#20041;&#24212;&#29992;&#26159;&#30830;&#20445;AI&#31995;&#32479;&#21151;&#33021;&#21487;&#20449;&#24230;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2310.02727</link><description>&lt;p&gt;
AI&#31995;&#32479;&#30340;&#21151;&#33021;&#21487;&#20449;&#24230;&#36890;&#36807;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Functional trustworthiness of AI systems by statistically valid testing. (arXiv:2310.02727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02727
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35748;&#20026;&#27431;&#30431;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#36136;&#37327;&#20445;&#35777;&#26041;&#24335;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25351;&#20986;&#22522;&#20110;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;&#21450;&#20934;&#30830;&#23450;&#20041;&#24212;&#29992;&#26159;&#30830;&#20445;AI&#31995;&#32479;&#21151;&#33021;&#21487;&#20449;&#24230;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#20851;&#27880;&#27431;&#27954;&#20844;&#27665;&#30340;&#23433;&#20840;&#12289;&#20581;&#24247;&#21644;&#26435;&#30410;&#38382;&#39064;&#65292;&#22240;&#20026;&#24403;&#21069;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27861;&#26696;&#30340;&#33609;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#31526;&#21512;&#24615;&#35780;&#20272;&#25152;&#38656;&#30340;&#25514;&#26045;&#21644;&#31243;&#24207;&#19981;&#36275;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#24403;&#21069;&#33609;&#26696;&#20197;&#21450;&#22312;CEN/CENELEC&#36827;&#34892;&#30340;&#37197;&#22871;&#26631;&#20934;&#21270;&#24037;&#20316;&#65292;&#37117;&#37319;&#21462;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;AI&#31995;&#32479;&#30340;&#23454;&#38469;&#21151;&#33021;&#20445;&#35777;&#20284;&#20046;&#26159;&#19981;&#20999;&#23454;&#38469;&#19988;&#36807;&#20110;&#22797;&#26434;&#30340;&#12290;&#28982;&#32780;&#65292;&#21046;&#23450;&#19968;&#20010;&#31526;&#21512;&#24615;&#35780;&#20272;&#31243;&#24207;&#65292;&#20351;&#26410;&#32463;&#20805;&#20998;&#35780;&#20272;&#30340;AI&#31995;&#32479;&#20135;&#29983;&#34394;&#20551;&#30340;&#20449;&#20219;&#24187;&#35937;&#65292;&#20805;&#20854;&#37327;&#26159;&#24188;&#31258;&#30340;&#65292;&#20805;&#20854;&#26356;&#31967;&#30340;&#24773;&#20917;&#26159;&#20005;&#37325;&#30095;&#24573;&#30340;&#12290;&#22240;&#27492;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#38169;&#36807;&#20102;&#30830;&#20445;&#36890;&#36807;&#21151;&#33021;&#21487;&#20449;&#24230;&#26469;&#30830;&#20445;&#36136;&#37327;&#21644;&#27491;&#30830;&#20998;&#37197;&#36131;&#20219;&#30340;&#30446;&#30340;&#12290;AI&#20915;&#31574;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#39318;&#20808;&#22312;&#20110;&#23545;&#38543;&#26426;&#36873;&#25321;&#30340;&#26679;&#26412;&#36827;&#34892;&#27491;&#30830;&#30340;&#32479;&#35745;&#27979;&#35797;&#65292;&#24182;&#22312;&#23450;&#20041;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems. We observe that not only the current draft of the EU AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have resorted to the position that real functional guarantees of AI systems supposedly would be unrealistic and too complex anyways. Yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed AI systems is at best naive and at worst grossly negligent. The EU AI Act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.  The trustworthiness of an AI decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#21644;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#34429;&#28982;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20250;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02724</link><description>&lt;p&gt;
&#31070;&#32463;HMM&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65306;&#22312;&#26631;&#31614;&#21644;&#36716;&#31227;&#27010;&#29575;&#19978;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training of a Neural HMM with Label and Transition Probabilities. (arXiv:2310.02724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#21644;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#34429;&#28982;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20250;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#36827;&#34892;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#22312;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#34987;&#26174;&#24335;&#22320;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#24403;&#20195;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20801;&#35768;&#36890;&#36807;&#22312;&#32473;&#23450;&#25299;&#25169;&#32467;&#26500;&#20013;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#31614;&#20998;&#21106;&#36827;&#34892;&#27714;&#21644;&#26469;&#36827;&#34892;&#20174;&#22836;&#35757;&#32451;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23384;&#22312;&#30528;&#26174;&#24335;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#20998;&#21106;&#20043;&#38388;&#30340;&#36716;&#31227;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#38544;&#21547;&#22320;&#32534;&#30721;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#30340;&#31354;&#26631;&#31614;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#22522;&#20110;GPU&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#26631;&#31614;&#21644;&#36716;&#31227;&#27010;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#35782;&#21035;&#32467;&#26524;&#21644;Viterbi&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36716;&#31227;&#27169;&#22411;&#30340;&#35757;&#32451;&#34429;&#28982;&#26080;&#27861;&#25913;&#21892;&#35782;&#21035;&#24615;&#33021;&#65292;&#20294;&#23545;&#40784;&#36136;&#37327;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#29983;&#25104;&#30340;&#23545;&#40784;&#22312;&#26368;&#20808;&#36827;&#30340; Viterbi &#35757;&#32451;&#20013;&#34987;&#35777;&#26126;&#26159;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a novel modeling approach for end-to-end neural network training using hidden Markov models (HMM) where the transition probabilities between hidden states are modeled and learned explicitly. Most contemporary sequence-to-sequence models allow for from-scratch training by summing over all possible label segmentations in a given topology. In our approach there are explicit, learnable probabilities for transitions between segments as opposed to a blank label that implicitly encodes duration statistics. We implement a GPU-based forward-backward algorithm that enables the simultaneous training of label and transition probabilities. We investigate recognition results and additionally Viterbi alignments of our models. We find that while the transition model training does not improve recognition performance, it has a positive impact on the alignment quality. The generated alignments are shown to be viable targets in state-of-the-art Viterbi trainings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;LDTGN&#34920;&#29616;&#20986;&#19982;&#20043;&#21069;&#26041;&#27861;&#21487;&#27604;&#29978;&#33267;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#22359;&#35299;&#32806;&#25552;&#21319;&#26102;&#38388;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Leveraging Temporal Graph Networks Using Module Decoupling. (arXiv:2310.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;LDTGN&#34920;&#29616;&#20986;&#19982;&#20043;&#21069;&#26041;&#27861;&#21487;&#27604;&#29978;&#33267;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25209;&#22788;&#29702;&#26469;&#26367;&#20195;&#36880;&#20010;&#26356;&#26032;&#12290;&#37319;&#29992;&#25209;&#22788;&#29702;&#20351;&#24471;&#36825;&#20123;&#25216;&#26415;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#33021;&#22815;&#22788;&#29702;&#26497;&#24555;&#36895;&#24230;&#30340;&#22270;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25209;&#22788;&#29702;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#26356;&#26032;&#39057;&#29575;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#31574;&#30053;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#20351;&#29992;&#25209;&#22788;&#29702;&#30340;&#21516;&#26102;&#39057;&#32321;&#22320;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#36827;&#34892;&#35299;&#32806;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#36827;&#34892;&#23454;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#39640;&#25928;&#30340;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#27169;&#22411;&#12290;LDTGN&#22312;&#21508;&#31181;&#21160;&#24577;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#22312;&#21534;&#21520;&#37327;&#26174;&#33879;&#39640;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#21487;&#27604;&#25110;&#20855;&#26377;&#26368;&#26032;&#25104;&#26524;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;be&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;20%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern approaches for learning on dynamic graphs have adopted the use of batches instead of applying updates one by one. The use of batches allows these techniques to become helpful in streaming scenarios where updates to graphs are received at extreme speeds. Using batches, however, forces the models to update infrequently, which results in the degradation of their performance. In this work, we suggest a decoupling strategy that enables the models to update frequently while using batches. By decoupling the core modules of temporal graph networks and implementing them using a minimal number of learnable parameters, we have developed the Lightweight Decoupled Temporal Graph Network (LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG was validated on various dynamic graph benchmarks, providing comparable or state-of-the-art results with significantly higher throughput than previous art. Notably, our method outperforms previous approaches by more than 20\% on be
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#31616;&#21333;&#30697;&#38453;&#26041;&#31243;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#35299;&#30340;&#26465;&#20214;&#21644;&#20809;&#35889;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#36890;&#36807;&#24341;&#20837;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19982;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#20808;&#39564;&#26469;&#35299;&#20915;&#20840;&#33394;&#22686;&#24378;&#20013;&#30340;&#29702;&#35770;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02718</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#36870;&#29702;&#35299;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Pan-Sharpening via Generalized Inverse. (arXiv:2310.02718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02718
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#31616;&#21333;&#30697;&#38453;&#26041;&#31243;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#35299;&#30340;&#26465;&#20214;&#21644;&#20809;&#35889;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#36890;&#36807;&#24341;&#20837;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19982;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#20808;&#39564;&#26469;&#35299;&#20915;&#20840;&#33394;&#22686;&#24378;&#20013;&#30340;&#29702;&#35770;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#21033;&#29992;&#20840;&#33394;&#22270;&#20687;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#33719;&#21462;&#20855;&#26377;&#39640;&#31354;&#38388;&#21644;&#39640;&#20809;&#35889;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20248;&#21270;&#26159;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#31616;&#21333;&#30340;&#30697;&#38453;&#26041;&#31243;&#26469;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#35299;&#30340;&#23384;&#22312;&#26465;&#20214;&#20197;&#21450;&#20809;&#35889;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#33719;&#21462;&#31354;&#38388;&#21644;&#20809;&#35889;&#38477;&#37319;&#26679;&#30697;&#38453;&#12290;&#36890;&#36807;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#23545;&#24212;&#20110;&#20004;&#20010;&#20027;&#35201;&#30340;&#20840;&#33394;&#22686;&#24378;&#26041;&#27861;&#65306;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Gram Schmidt&#33258;&#36866;&#24212;(GSA)&#26041;&#27861;&#36981;&#24490;&#20998;&#37327;&#26367;&#20195;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20809;&#35889;&#20989;&#25968;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#20043;&#21069;&#30340;&#27169;&#22411;&#20808;&#39564;&#12290;&#25105;&#20204;&#23545;&#29702;&#35770;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pan-sharpening algorithm utilizes panchromatic image and multispectral image to obtain a high spatial and high spectral image. However, the optimizations of the algorithms are designed with different standards. We adopt the simple matrix equation to describe the Pan-sharpening problem. The solution existence condition and the acquirement of spectral and spatial resolution are discussed. A down-sampling enhancement method was introduced for better acquiring the spatial and spectral down-sample matrices. By the generalized inverse theory, we derived two forms of general inverse matrix formulations that can correspond to the two prominent classes of Pan-sharpening methods, that is, component substitution and multi-resolution analysis methods. Specifically, the Gram Schmidt Adaptive(GSA) was proved to follow the general inverse matrix formulation of component substitution. A model prior to the general inverse matrix of the spectral function was rendered. The theoretical errors are analyzed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02717</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#27599;&#36718;&#20013;&#65292;&#32473;&#23450;&#33218;&#30340;&#29305;&#24449;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#33218;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#29992;&#25143;&#27169;&#22411;&#65292;&#24403;&#36825;&#20010;&#20851;&#38190;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22914;&#20309;&#20026;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#35774;&#35745;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#27169;&#22411;&#20013;&#30340;&#26399;&#26395;&#22870;&#21169;&#21487;&#33021;&#26377;&#20559;&#24046;&#65292;&#19981;&#26159;&#23436;&#32654;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCLUMB&#21644;RSCLUMB&#65288;&#20998;&#21035;&#29992;&#21160;&#24577;&#22270;&#21644;&#38598;&#21512;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
&lt;/p&gt;</description></item><item><title>scHyena&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#26088;&#22312;&#22788;&#29702;&#22823;&#33041;&#20013;&#30340;&#20840;&#38271;scRNA-seq&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02713</link><description>&lt;p&gt;
scHyena: &#22522;&#20110;&#20840;&#38271;&#21333;&#32454;&#32990;RNA-Seq&#30340;&#22823;&#33041;&#20998;&#26512;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain. (arXiv:2310.02713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02713
&lt;/p&gt;
&lt;p&gt;
scHyena&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#26088;&#22312;&#22788;&#29702;&#22823;&#33041;&#20013;&#30340;&#20840;&#38271;scRNA-seq&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;RNA&#27979;&#24207;(scRNA-seq)&#22312;&#25581;&#31034;&#22797;&#26434;&#32452;&#32455;&#20013;&#24494;&#22937;&#30340;&#32454;&#32990;&#22810;&#26679;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#22312;&#22823;&#33041;&#20013;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#22823;&#33041;&#27604;&#20854;&#20182;&#32452;&#32455;&#31867;&#22411;&#26377;&#26356;&#22810;&#31181;&#31867;&#30340;&#32454;&#32990;&#65292;&#20197;&#20415;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#30340;&#22823;&#33041;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#22833;&#20107;&#20214;&#25152;&#20135;&#29983;&#30340;&#22266;&#26377;&#27979;&#37327;&#22122;&#22768;&#20197;&#21450;&#23545;&#22823;&#37327;&#22522;&#22240;&#34920;&#36798;&#20449;&#24687;&#30340;&#26377;&#38480;&#21033;&#29992;&#65292;&#20998;&#26512;scRNA-seq&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;scHyena&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#39640;&#22823;&#33041;&#20013;scRNA-seq&#20998;&#26512;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#21040;&#26368;&#36817;&#30340;Hyena&#31639;&#23376;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#26550;&#26500;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#23427;&#37197;&#22791;&#20102;&#32447;&#24615;&#36866;&#37197;&#22120;&#23618;&#12289;&#36890;&#36807;&#22522;&#22240;&#23884;&#20837;&#23454;&#29616;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#19968;&#20010;&#21452;&#21521;Hyena&#31639;&#23376;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22788;&#29702;&#20840;&#38271;&#30340;scRNA-seq&#25968;&#25454;&#32780;&#19981;&#20002;&#22833;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losi
&lt;/p&gt;</description></item><item><title>ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;</title><link>http://arxiv.org/abs/2310.02712</link><description>&lt;p&gt;
ED-NeRF: &#20351;&#29992;&#28508;&#31354;&#38388; NeRF &#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340; 3D &#22330;&#26223;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02712
&lt;/p&gt;
&lt;p&gt;
ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#24050;&#32463;&#25193;&#23637;&#21040;&#19977;&#32500;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#19977;&#32500;&#23545;&#35937;&#12290;&#36825;&#28436;&#21464;&#25104;&#20102; NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#20801;&#35768;&#23545;&#29616;&#26377;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; NeRF &#32534;&#36753;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20805;&#20998;&#32771;&#34385;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; 3D NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026; ED-NeRF&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#25104;&#21151;&#23884;&#20837;&#21040;&#28508;&#25193;&#25955;&#27169;&#22411; (LDM) &#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32454;&#21270;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#19968;&#20010;&#19981;&#20165;&#26356;&#24555;&#65292;&#32780;&#19988;&#26356;&#36866;&#21512;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#31354;&#38388; NeRF &#32534;&#36753;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02710</link><description>&lt;p&gt;
&#26412;&#22320;&#25628;&#32034;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#23398;&#20064;&#19982;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#31163;&#25955;&#23545;&#35937;&#20998;&#24067;&#30340;&#25674;&#36824;&#37319;&#26679;&#26041;&#27861;&#12290;GFlowNets&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#24191;&#27867;&#26679;&#26412;&#31354;&#38388;&#19978;&#30340;&#36807;&#24230;&#25506;&#32034;&#65292;&#26377;&#26102;&#38590;&#20197;&#19968;&#33268;&#22320;&#29983;&#25104;&#39640;&#22870;&#21169;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#12290;&#36825;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20256;&#32479;&#30340;GFlowNet&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#26041;&#26696;&#21017;&#20351;&#29992;&#27491;&#21521;&#31574;&#30053;&#20174;&#22836;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#22312;&#20960;&#20010;&#29983;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02702</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#24322;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#26679;&#26412;&#34987;&#20998;&#25955;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#34920;&#29616;&#20986;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#26159;&#29420;&#31435;&#21644;&#30456;&#21516;&#30340;&#12290;&#27492;&#22806;&#65292;&#31995;&#32479;&#24322;&#36136;&#24615;&#65292;&#21363;&#23458;&#25143;&#31471;&#35745;&#31639;&#33021;&#21147;&#30340;&#21464;&#21270;&#65292;&#20250;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20559;&#24046;&#12290;&#32479;&#35745;&#21644;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#32508;&#21512;&#25928;&#24212;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#32852;&#37030;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#35880;&#30340;&#35752;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26381;&#21153;&#22120;&#31471;&#20248;&#21270;&#65292;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26381;&#21153;&#22120;&#26356;&#26032;&#26041;&#21521;&#19978;&#33258;&#36866;&#24212;&#22320;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26381;&#21153;&#22120;&#31471;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02698</link><description>&lt;p&gt;
&#25506;&#32034;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#30340;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#23545;&#19968;&#37096;&#20998;&#23458;&#25143;&#36827;&#34892;&#37319;&#26679;&#26469;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;&#26469;&#33258;&#37319;&#26679;&#23458;&#25143;&#30340;&#20449;&#24687;&#24314;&#31435;&#20840;&#23616;&#27169;&#22411;&#30340;&#20840;&#23616;&#20272;&#35745;&#26041;&#24046;&#19982;&#32852;&#37030;&#20248;&#21270;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#8220;&#20813;&#36153;&#8221;&#30340;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#26500;&#24314;&#20102;&#26377;&#21069;&#36884;&#30340;&#37319;&#26679;&#27010;&#29575;&#21644;&#21487;&#38752;&#30340;&#20840;&#23616;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#25429;&#25417;&#20102;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#23567;&#21464;&#20307;&#65292;&#24182;&#30456;&#24212;&#25913;&#36827;&#20102;&#20840;&#23616;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#20248;&#21270;&#20013;&#36981;&#24490;&#23458;&#25143;&#37319;&#26679;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#22312;&#36890;&#20449;&#39044;&#31639;K&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32447;&#24615;&#36895;&#29575;&#19978;&#21319;&#65292;&#20855;&#26377;&#36951;&#25022;&#36793;&#30028;$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{}3}\big)$&#12290;&#32467;&#26524;&#26159;&#65292;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of "free" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#27010;&#29575;&#22359;&#39033;&#20998;&#35299;&#65288;pBTD&#65289;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#38454;&#25968;&#32452;&#30340;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;von-Mises Fisher&#30697;&#38453;&#20998;&#24067;&#26469;&#23454;&#29616;&#27491;&#20132;&#24615;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pBTD&#22312;&#22122;&#22768;&#25968;&#25454;&#21644;&#27169;&#22411;&#39034;&#24207;&#37327;&#21270;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02694</link><description>&lt;p&gt;
&#39640;&#38454;&#25968;&#32452;&#24314;&#27169;&#30340;&#27010;&#29575;&#22359;&#39033;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Block Term Decomposition for the Modelling of Higher-Order Arrays. (arXiv:2310.02694v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#27010;&#29575;&#22359;&#39033;&#20998;&#35299;&#65288;pBTD&#65289;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39640;&#38454;&#25968;&#32452;&#30340;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;von-Mises Fisher&#30697;&#38453;&#20998;&#24067;&#26469;&#23454;&#29616;&#27491;&#20132;&#24615;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pBTD&#22312;&#22122;&#22768;&#25968;&#25454;&#21644;&#27169;&#22411;&#39034;&#24207;&#37327;&#21270;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#34920;&#24449;&#39640;&#38454;&#32467;&#26500;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20998;&#35299;&#21253;&#25324;&#22806;&#31215;&#31209;&#26631;&#20934;&#22810;&#39033;&#24335;&#20998;&#35299;&#65288;CPD&#65289;&#20197;&#21450;&#22810;&#32447;&#24615;&#31209;&#22270;&#23572;&#20811;&#20998;&#35299;&#65292;&#20854;&#20013;&#22359;&#39033;&#20998;&#35299;&#65288;BTD&#65289;&#26159;&#20004;&#31181;&#34920;&#31034;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#20013;&#38388;&#25554;&#20540;&#12290;&#34429;&#28982;CPD&#12289;&#22270;&#23572;&#20811;&#21644;BTD&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20294;&#24050;&#32463;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#24418;&#25104;&#20102;&#27010;&#29575;CPD&#21644;&#22270;&#23572;&#20811;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#27010;&#29575;BTD&#65292;&#23427;&#20351;&#29992;von-Mises Fisher&#30697;&#38453;&#20998;&#24067;&#22312;&#24418;&#25104;BTD&#30340;&#22810;&#32447;&#24615;&#22270;&#23572;&#20811;&#37096;&#20998;&#20013;&#26045;&#21152;&#27491;&#20132;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#25552;&#35758;&#30340;pBTD&#23545;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#20102;&#28436;&#31034;&#21644;&#27169;&#22411;&#39034;&#24207;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#27010;&#29575;BTD&#33021;&#22815;&#37327;&#21270;...
&lt;/p&gt;
&lt;p&gt;
Tensors are ubiquitous in science and engineering and tensor factorization approaches have become important tools for the characterization of higher order structure. Factorizations includes the outer-product rank Canonical Polyadic Decomposition (CPD) as well as the multi-linear rank Tucker decomposition in which the Block-Term Decomposition (BTD) is a structured intermediate interpolating between these two representations. Whereas CPD, Tucker, and BTD have traditionally relied on maximum-likelihood estimation, Bayesian inference has been use to form probabilistic CPD and Tucker. We propose, an efficient variational Bayesian probabilistic BTD, which uses the von-Mises Fisher matrix distribution to impose orthogonality in the multi-linear Tucker parts forming the BTD. On synthetic and two real datasets, we highlight the Bayesian inference procedure and demonstrate using the proposed pBTD on noisy data and for model order quantification. We find that the probabilistic BTD can quantify su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#20026;&#35299;&#20915;&#27668;&#20505;&#27169;&#25311;&#20013;&#38271;&#26399;&#39044;&#27979;&#35823;&#24046;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02691</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators. (arXiv:2310.02691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#28023;&#27915;&#23376;&#21306;&#23610;&#24230;&#31283;&#20581;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#20026;&#35299;&#20915;&#27668;&#20505;&#27169;&#25311;&#20013;&#38271;&#26399;&#39044;&#27979;&#35823;&#24046;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#27169;&#25311;&#20013;&#65292;&#23567;&#23610;&#24230;&#36807;&#31243;&#22609;&#36896;&#20102;&#28023;&#27915;&#21160;&#21147;&#23398;&#65292;&#20294;&#30452;&#25509;&#35299;&#20915;&#36825;&#20123;&#36807;&#31243;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#24120;&#24120;&#20351;&#29992;&#32463;&#39564;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#36129;&#29486;&#65292;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35823;&#24046;&#12290;&#26412;&#25991;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#24320;&#21457;&#20102;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#30340;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39057;&#22495;&#20013;&#36816;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02679</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65306;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#20248;&#21270;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#26469;&#27169;&#25311;&#36825;&#20123;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#36712;&#36857;&#65292;&#23548;&#33268;&#30001;&#20110;&#20351;&#29992;&#23436;&#25972;&#36712;&#36857;&#21644;&#21482;&#22312;&#32456;&#31471;&#26102;&#38388;&#23384;&#22312;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#20351;&#29992;&#32780;&#20135;&#29983;&#32531;&#24930;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23398;&#20064;&#36807;&#31243;&#21487;&#34892;&#22320;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27969;&#20989;&#25968;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#24182;&#20174;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
&lt;/p&gt;</description></item><item><title>PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02676</link><description>&lt;p&gt;
PostRainBench: &#19968;&#31181;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#21644;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02676
&lt;/p&gt;
&lt;p&gt;
PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#31185;&#23398;&#21644;&#31038;&#20250;&#37325;&#35201;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#27169;&#25311;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26377;&#38480;&#65292;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;&#22256;&#38590;&#12290;&#23558;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20043;&#21069;&#36827;&#34892;&#36807;&#21518;&#22788;&#29702;&#30340;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#20301;&#32622;&#30340;&#38477;&#27700;&#25968;&#25454;&#22833;&#34913;&#21644;&#22810;&#20010;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20934;&#30830;&#39044;&#27979;&#22823;&#38632;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PostRainBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#21464;&#37327;NWP&#21518;&#22788;&#29702;&#22522;&#20934;&#65292;&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;NWP&#21518;&#22788;&#29702;&#38477;&#27700;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28192;&#36947;&#27880;&#24847;&#21147;&#27169;&#22411;CAMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02664</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#26032;&#39062;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20856;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#33021;&#29983;&#25104;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#36825;&#34920;&#26126;&#22312;&#29702;&#35770;&#19978;&#20250;&#20986;&#29616;&#35760;&#24518;&#21270;&#30340;&#34892;&#20026;&#65292;&#36825;&#19982;&#29616;&#26377;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27867;&#21270;&#33021;&#21147;&#30456;&#30683;&#30462;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35760;&#24518;&#21270;&#34892;&#20026;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270;(EMM)&#30340;&#23450;&#20041;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#22823;&#25968;&#25454;&#38598;&#19978;&#36817;&#20284;&#20854;&#29702;&#35770;&#26368;&#20248;&#28857;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#24433;&#21709;&#36825;&#20123;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuratio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21305;&#37197;&#37327;&#21270;&#26041;&#26696;&#19982;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#25512;&#36827;&#20102;&#37327;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.02654</link><description>&lt;p&gt;
&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;FPGA&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21305;&#37197;&#37327;&#21270;&#26041;&#26696;&#19982;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#25512;&#36827;&#20102;&#37327;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#22312;QAT&#38454;&#27573;&#21160;&#24577;&#36873;&#25321;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#23558;&#37327;&#21270;&#26041;&#26696;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#21305;&#37197;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#23545;&#35937;&#34987;&#37327;&#21270;&#20026;4&#20301;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#20026;&#25512;&#36827;&#37327;&#21270;&#25216;&#26415;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.
&lt;/p&gt;</description></item><item><title>GPS-AFL&#26159;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#36873;&#25321;&#20559;&#24046;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02651</link><description>&lt;p&gt;
&#22312;&#38656;&#35201;&#26102;&#32856;&#29992;&#65306;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#25307;&#21215;&#29992;&#20110;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning. (arXiv:2310.02651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02651
&lt;/p&gt;
&lt;p&gt;
GPS-AFL&#26159;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#36873;&#25321;&#20559;&#24046;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;DOs&#65289;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#20197;&#21450;&#20182;&#20204;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21160;&#26426;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20197;&#22768;&#35465;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#23545;&#39640;&#22768;&#35465;DOs&#30340;&#28508;&#22312;&#36873;&#25321;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#36739;&#20302;&#22768;&#35465;&#30340;DOs&#34987;&#36807;&#26089;&#22320;&#25490;&#38500;&#22312;&#26410;&#26469;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#36718;&#27425;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#32467;&#26524;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65288;GPS-AFL&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#28608;&#21169;&#26426;&#21046;&#19981;&#21516;&#65292;&#21518;&#32773;&#36890;&#24120;&#35748;&#20026;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#25152;&#26377;DOs&#24517;&#39035;&#19968;&#27425;&#24615;&#36873;&#25321;&#65292;GPS-AFL&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25152;&#38656;&#30340;DOs&#65292;&#38543;&#30528;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#36880;&#28176;&#25581;&#31034;&#26356;&#22810;&#20449;&#24687;&#12290;&#23427;&#30340;&#35774;&#35745;&#26088;&#22312;&#22312;&#25104;&#26412;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of federated Learning (FL) depends on the quantity and quality of the data owners (DOs) as well as their motivation to join FL model training. Reputation-based FL participant selection methods have been proposed. However, they still face the challenges of the cold start problem and potential selection bias towards highly reputable DOs. Such a bias can result in lower reputation DOs being prematurely excluded from future FL training rounds, thereby reducing the diversity of training data and the generalizability of the resulting models. To address these challenges, we propose the Gradual Participant Selection scheme for Auction-based Federated Learning (GPS-AFL). Unlike existing AFL incentive mechanisms which generally assume that all DOs required for an FL task must be selected in one go, GPS-AFL gradually selects the required DOs over multiple rounds of training as more information is revealed through repeated interactions. It is designed to strike a balance between cost s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;(MAABO-MT&#21644;GS-MRM)&#65292;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2310.02633</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;Aitchison-Aitken&#20989;&#25968;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#29190;&#28856;&#20915;&#31574;&#26641;&#30340;&#22810;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization. (arXiv:2310.02633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;(MAABO-MT&#21644;GS-MRM)&#65292;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#20248;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#26681;&#25454;if-then&#35268;&#21017;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#26641;&#26159;&#30001;&#19968;&#20010;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23569;&#30340;&#35268;&#21017;&#23454;&#29616;&#28165;&#26224;&#30340;&#20998;&#31867;&#65292;&#32780;&#26080;&#35770;&#25968;&#25454;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#28508;&#22312;&#35268;&#21017;&#65292;&#37117;&#21482;&#33021;&#25552;&#21462;&#26368;&#23567;&#30340;&#35268;&#21017;&#12290;&#30830;&#23454;&#23384;&#22312;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26500;&#24314;&#22810;&#26869;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#26500;&#24314;&#30340;&#26641;&#30340;&#25968;&#37327;&#20173;&#28982;&#22312;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#65292;&#22240;&#20026;&#29305;&#24449;&#23376;&#38598;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#32452;&#21512;&#24615;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#24403;&#26500;&#24314;&#22810;&#26869;&#26641;&#26102;&#65292;&#20250;&#29983;&#25104;&#35768;&#22810;&#35268;&#21017;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#26159;&#19981;&#21487;&#38752;&#21644;/&#25110;&#38750;&#24120;&#30456;&#20284;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MAABO-MT&#8221;&#21644;&#8220;GS-MRM&#8221;&#31639;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#24182;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees offer the benefit of easy interpretation because they allow the classification of input data based on if--then rules. However, as decision trees are constructed by an algorithm that achieves clear classification with minimum necessary rules, the trees possess the drawback of extracting only minimum rules, even when various latent rules exist in data. Approaches that construct multiple trees using randomly selected feature subsets do exist. However, the number of trees that can be constructed remains at the same scale because the number of feature subsets is a combinatorial explosion. Additionally, when multiple trees are constructed, numerous rules are generated, of which several are untrustworthy and/or highly similar. Therefore, we propose "MAABO-MT" and "GS-MRM" algorithms that strategically construct trees with high estimation performance among all possible trees with small computational complexity and extract only reliable and non-similar rules, respectively. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman VAEs&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;GANs&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#23545;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.02619</link><description>&lt;p&gt;
&#36890;&#36807;Koopman VAEs&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. (arXiv:2310.02619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman VAEs&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;GANs&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#23545;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#24037;&#31243;&#21644;&#31185;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;GANs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24120;&#24120;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#21487;&#33021;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#32780;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#34987;&#35748;&#20026;&#23545;&#36825;&#20123;&#38382;&#39064;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#21364;&#24456;&#23569;&#34987;&#32771;&#34385;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26032;&#22411;&#27169;&#22411;&#20808;&#39564;&#30340;&#29983;&#25104;&#26694;&#26550;Koopman VAE&#65288;KVAE&#65289;&#65292;&#21487;&#20197;&#20026;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;Koopman&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#26469;&#34920;&#31034;&#28508;&#22312;&#26465;&#20214;&#20808;&#39564;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#20004;&#20010;&#26399;&#26395;&#29305;&#24615;&#65306;&#65288;i&#65289;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#21487;&#20197;&#23454;&#29616;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#65307;&#65288;ii&#65289;&#30740;&#31350;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are often unstable during training, and they can suffer from mode collapse. While variational autoencoders (VAEs) are known to be more robust to these issues, they are (surprisingly) less often considered for time series generation. In this work, we introduce Koopman VAE (KVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leverageing spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stablity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#21644;&#25913;&#36827;&#20102;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;&#65292;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#21508;&#32452;&#20214;&#22312;&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#65292;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#20197;&#25913;&#36827;&#26368;&#20248;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;</title><link>http://arxiv.org/abs/2310.02611</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#25913;&#36827;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Improving OT-based Adversarial Networks. (arXiv:2310.02611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#21644;&#25913;&#36827;&#20102;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;&#65292;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#21508;&#32452;&#20214;&#22312;&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#65292;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#20197;&#25913;&#36827;&#26368;&#20248;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#36755;&#36816;&#26041;&#26696;&#65292;&#23427;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#30340;&#21516;&#26102;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#12290;OT&#29702;&#35770;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21021;&#65292;OT&#36317;&#31163;&#34987;&#29992;&#20316;&#35780;&#20272;&#25968;&#25454;&#21644;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#26368;&#36817;&#65292;OT&#20256;&#36755;&#26144;&#23556;&#22312;&#25968;&#25454;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#34987;&#29992;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20123;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#30340;&#23545;&#25239;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#32452;&#20214;&#22312;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#22522;&#20110;OT&#30340;&#27169;&#22411;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#65292;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#28176;&#36827;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29305;&#27530;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;&#22359;&#26469;&#23398;&#20064;&#32570;&#22833;&#30340;&#26102;&#31354;&#36830;&#25509;&#65292;&#23558;&#20854;&#20016;&#23500;&#21518;&#30340;&#22359;&#37051;&#25509;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;</title><link>http://arxiv.org/abs/2310.02606</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Learning adjacency matrix for dynamic graph neural network. (arXiv:2310.02606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29305;&#27530;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;&#22359;&#26469;&#23398;&#20064;&#32570;&#22833;&#30340;&#26102;&#31354;&#36830;&#25509;&#65292;&#23558;&#20854;&#20016;&#23500;&#21518;&#30340;&#22359;&#37051;&#25509;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;[1] &#24341;&#20837;&#20102;&#20351;&#29992;&#22359;&#37051;&#25509;&#30697;&#38453;&#65288;BA&#65289;&#26469;&#34920;&#31034;&#26102;&#31354;&#25968;&#25454;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#20182;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20018;&#32852;&#20102;&#37051;&#25509;&#30697;&#38453;&#65292;&#20197;&#23553;&#35013;&#21333;&#20010;&#22270;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#20294;&#23427;&#24418;&#25104;&#20102;&#19968;&#20010;&#19981;&#36830;&#36890;&#30340;&#22270;&#12290;&#36825;&#20010;&#38480;&#21046;&#22952;&#30861;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#23646;&#20110;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27809;&#26377;&#26102;&#38388;&#38142;&#25509;&#23384;&#22312;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#36825;&#20123;&#32570;&#22833;&#30340;&#26102;&#38388;&#38142;&#25509;&#30340;&#32534;&#30721;&#22120;&#22359;&#12290;&#32534;&#30721;&#22120;&#22359;&#22788;&#29702;BA&#24182;&#39044;&#27979;&#20043;&#21069;&#26410;&#36830;&#25509;&#30340;&#23376;&#22270;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#23500;&#21270;&#30340;&#26102;&#31354;&#22359;&#37051;&#25509;&#30697;&#38453;&#65288;STBAM&#65289;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20010;&#23500;&#21270;&#30340;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;surgVisDom&#21644;C2D2&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#31245;&#39640;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent work, [1] introduced the concept of using a Block Adjacency Matrix (BA) for the representation of spatio-temporal data. While their method successfully concatenated adjacency matrices to encapsulate spatio-temporal relationships in a single graph, it formed a disconnected graph. This limitation hampered the ability of Graph Convolutional Networks (GCNs) to perform message passing across nodes belonging to different time steps, as no temporal links were present. To overcome this challenge, we introduce an encoder block specifically designed to learn these missing temporal links. The encoder block processes the BA and predicts connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to capture the complex spatio-temporal topology of the network. Our evaluations on benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with slightly higher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22788;&#29702;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.02605</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Power Grid Topology Optimization. (arXiv:2310.02605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22788;&#29702;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#20020;&#30528;&#33021;&#28304;&#38656;&#27714;&#22686;&#21152;&#21644;&#39118;&#33021;&#12289;&#22826;&#38451;&#33021;&#31561;&#19981;&#21487;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25361;&#25112;&#65292;&#25805;&#20316;&#30005;&#32593;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#22312;&#31649;&#29702;&#36825;&#20123;&#32593;&#32476;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#24635;&#32447;&#21644;&#32447;&#36335;&#20999;&#25442;&#31561;&#25299;&#25169;&#25805;&#20316;&#65292;&#20294;&#23545;&#20110;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#30340;&#39640;&#25928;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#25193;&#23637;&#34892;&#21160;&#31354;&#38388;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#30005;&#32593;&#22266;&#26377;&#30340;&#20998;&#23618;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MARL&#26694;&#26550;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent challenges in operating power networks arise from increasing energy demands and unpredictable renewable sources like wind and solar. While reinforcement learning (RL) shows promise in managing these networks, through topological actions like bus and line switching, efficiently handling large action spaces as networks grow is crucial. This paper presents a hierarchical multi-agent reinforcement learning (MARL) framework tailored for these expansive action spaces, leveraging the power grid's inherent hierarchical nature. Experimental results indicate the MARL framework's competitive performance with single-agent RL methods. We also compare different RL algorithms for lower-level agents alongside different policies for higher-order agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ViT-ReciproCAM&#65292;&#29992;&#20110;&#35299;&#37322;Vision Transformer&#20013;&#30340;&#39044;&#27979;&#36807;&#31243;&#21644;&#35843;&#35797;&#39044;&#27979;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#20196;&#29260;&#36974;&#32617;&#21644;&#26032;&#30340;&#23618;&#36755;&#20986;&#26469;&#21033;&#29992;&#28608;&#27963;&#30340;&#20196;&#29260;&#19982;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.02588</link><description>&lt;p&gt;
ViT-ReciproCAM: &#19981;&#38656;&#35201;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30340;Vision Transformer&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer. (arXiv:2310.02588v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ViT-ReciproCAM&#65292;&#29992;&#20110;&#35299;&#37322;Vision Transformer&#20013;&#30340;&#39044;&#27979;&#36807;&#31243;&#21644;&#35843;&#35797;&#39044;&#27979;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#20196;&#29260;&#36974;&#32617;&#21644;&#26032;&#30340;&#23618;&#36755;&#20986;&#26469;&#21033;&#29992;&#28608;&#27963;&#30340;&#20196;&#29260;&#19982;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Vision Transformers (ViT)&#22312;&#29702;&#35299;&#39044;&#27979;&#36807;&#31243;&#21644;&#35843;&#35797;&#39044;&#27979;&#38169;&#35823;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;ViT&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#65289;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#21487;&#35270;&#21270;&#35299;&#37322;&#25216;&#26415;&#65292;&#22914;CAM&#12289;Grad-CAM&#12289;Score-CAM&#21644;Recipro-CAM&#65292;&#20294;&#22312;ViT&#19978;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#24403;&#21069;ViT&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#31867;&#19981;&#21487;&#30693;&#30340;Attention-Rollout&#21644;Relevance&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#20381;&#36182;&#26799;&#24230;&#30340;ViT&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#65292;&#31216;&#20026;ViT-ReciproCAM&#65292;&#23427;&#19981;&#38656;&#35201;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#26799;&#24230;&#20449;&#24687;&#12290;ViT-ReciproCAM&#21033;&#29992;&#20196;&#29260;&#36974;&#32617;&#21644;&#20174;&#30446;&#26631;&#23618;&#30340;&#36755;&#20837;&#20135;&#29983;&#30340;&#26032;&#23618;&#36755;&#20986;&#26469;&#21033;&#29992;&#28608;&#27963;&#30340;&#20196;&#29260;&#19982;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24658;&#21151;&#29575;&#24320;&#29615;&#25511;&#21046;&#22120;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#23612;&#40857;&#20154;&#24037;&#32908;&#32905;&#30340;&#31934;&#30830;&#20301;&#32622;&#25511;&#21046;&#12290;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26399;&#26395;&#20301;&#31227;&#36716;&#21270;&#20026;&#25152;&#38656;&#21151;&#29575;&#65292;&#31070;&#32463;&#25511;&#21046;&#22120;&#22312;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#21518;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#28909;&#20154;&#24037;&#32908;&#32905;&#12290;</title><link>http://arxiv.org/abs/2310.02583</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#39640;&#32423;&#28909;&#33268;&#21160;&#22120;&#20013;&#30340;&#31934;&#30830;&#20301;&#32622;&#25511;&#21046;&#21644;&#28909;&#35843;&#33410;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Enabled Precision Position Control and Thermal Regulation in Advanced Thermal Actuators. (arXiv:2310.02583v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24658;&#21151;&#29575;&#24320;&#29615;&#25511;&#21046;&#22120;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#23612;&#40857;&#20154;&#24037;&#32908;&#32905;&#30340;&#31934;&#30830;&#20301;&#32622;&#25511;&#21046;&#12290;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26399;&#26395;&#20301;&#31227;&#36716;&#21270;&#20026;&#25152;&#38656;&#21151;&#29575;&#65292;&#31070;&#32463;&#25511;&#21046;&#22120;&#22312;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#21518;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#28909;&#20154;&#24037;&#32908;&#32905;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23612;&#40857;&#20154;&#24037;&#32908;&#32905;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#33021;&#37327;&#23494;&#24230;&#20960;&#20046;&#26159;&#20154;&#31867;&#32908;&#32905;&#30340;100&#20493;&#65292;&#21151;&#29575;&#23494;&#24230;&#20026;5.3 kW/kg&#65292;&#31867;&#20284;&#20110;&#21943;&#27668;&#21457;&#21160;&#26426;&#30340;&#36755;&#20986;&#65292;&#22240;&#27492;&#29305;&#21035;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#38598;&#25104;&#20256;&#24863;&#22120;&#21644;&#25511;&#21046;&#22120;&#30340;&#24517;&#35201;&#24615;&#23545;&#20854;&#23454;&#38469;&#20351;&#29992;&#36896;&#25104;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24658;&#21151;&#29575;&#24320;&#29615;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#22806;&#37096;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#23612;&#40857;&#20154;&#24037;&#32908;&#32905;&#30340;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#32534;&#30721;&#22120;&#24335;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#20010;&#20174;&#26399;&#26395;&#20301;&#31227;&#36712;&#36857;&#21040;&#25152;&#38656;&#21151;&#29575;&#30340;&#26144;&#23556;&#12290;&#31070;&#32463;&#25511;&#21046;&#22120;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#21435;&#22122;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#28909;&#20154;&#24037;&#32908;&#32905;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#28382;&#21518;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
With their unique combination of characteristics - an energy density almost 100 times that of human muscle, and a power density of 5.3 kW/kg, similar to a jet engine's output - Nylon artificial muscles stand out as particularly apt for robotics applications. However, the necessity of integrating sensors and controllers poses a limitation to their practical usage. Here we report a constant power open-loop controller based on machine learning. We show that we can control the position of a nylon artificial muscle without external sensors. To this end, we construct a mapping from a desired displacement trajectory to a required power using an ensemble encoder-style feed-forward neural network. The neural controller is carefully trained on a physics-based denoised dataset and can be fine-tuned to accommodate various types of thermal artificial muscles, irrespective of the presence or absence of hysteresis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#26041;&#38754;&#24341;&#20837;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.02581</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning. (arXiv:2310.02581v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#26041;&#38754;&#24341;&#20837;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#20195;&#32479;&#35745;&#23398;&#20013;&#22791;&#21463;&#20851;&#27880;&#65292;&#31574;&#30053;&#35780;&#20272;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#19978;&#23545;&#35813;&#20027;&#39064;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#20551;&#35774;&#38543;&#26426;&#22870;&#21169;&#36981;&#24490;&#26631;&#20934;&#20998;&#24067;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#21516;&#26102;&#35299;&#20915;&#20102;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25317;&#25265;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#20854;Bahadur&#34920;&#31034;&#24314;&#31435;&#20102;&#25105;&#20204;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#36807;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#22522;&#20110;&#28176;&#36817;&#20998;&#24067;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#22635;&#34917;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#40065;&#26834;&#32479;&#35745;&#23398;&#21644;&#32479;&#35745;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reinforcement learning has gained prominence in modern statistics, with policy evaluation being a key component. Unlike traditional machine learning literature on this topic, our work places emphasis on statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop an online robust policy evaluation procedure, and establish the limiting distribution of our estimator, based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>AdaMerging&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02575</link><description>&lt;p&gt;
AdaMerging: &#36866;&#24212;&#24615;&#27169;&#22411;&#21512;&#24182;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaMerging: Adaptive Model Merging for Multi-Task Learning. (arXiv:2310.02575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02575
&lt;/p&gt;
&lt;p&gt;
AdaMerging&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21457;&#23637;&#34987;&#31216;&#20026;&#20219;&#21153;&#31639;&#26415;&#65292;&#25581;&#31034;&#20102;&#20960;&#20010;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#21512;&#24182;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#28155;&#21152;&#27169;&#22411;&#24448;&#24448;&#20250;&#23548;&#33268;&#21512;&#24182;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#31181;&#19979;&#38477;&#26159;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#20914;&#31361;&#21644;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#25152;&#33268;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#21512;&#24182;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20351;&#29992;&#20854;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#27169;&#22411;&#21512;&#24182;&#65288;AdaMerging&#65289;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#23398;&#20064;&#27169;&#22411;&#21512;&#24182;&#30340;&#31995;&#25968;&#65292;&#21487;&#20197;&#26159;&#36880;&#20219;&#21153;&#25110;&#36880;&#23618;&#30340;&#26041;&#24335;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#35299;&#37322;&#33976;&#39311;&#65288;KED&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#25945;&#24072;&#30340;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#24182;&#25193;&#23637;&#20102;KED&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02572</link><description>&lt;p&gt;
&#20351;&#29992;&#25945;&#24072;&#35299;&#37322;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Distillation with Teacher's Explanation. (arXiv:2310.02572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#35299;&#37322;&#33976;&#39311;&#65288;KED&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#23398;&#29983;&#20174;&#25945;&#24072;&#30340;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#24182;&#25193;&#23637;&#20102;KED&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#36807;&#19968;&#20010;&#24378;&#22823;&#30340;&#25945;&#24072;&#26469;&#25552;&#39640;&#20302;&#22797;&#26434;&#24230;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#25945;&#24072;&#26159;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#36890;&#36807;&#20854;&#39044;&#27979;&#26469;&#20256;&#25480;&#30693;&#35782;&#32473;&#23398;&#29983;&#12290;&#36825;&#38480;&#21046;&#20102;&#20256;&#36755;&#30693;&#35782;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#35299;&#37322;&#33976;&#39311;&#65288;KED&#65289;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#23398;&#29983;&#19981;&#20165;&#20174;&#25945;&#24072;&#30340;&#39044;&#27979;&#20013;&#23398;&#20064;&#65292;&#36824;&#21487;&#20197;&#20174;&#25945;&#24072;&#30340;&#35299;&#37322;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#33021;&#22815;&#35299;&#37322;&#29305;&#24449;&#32452;&#30340;&#36229;&#29305;&#24449;&#25945;&#24072;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36229;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;KED&#65292;&#20197;&#20943;&#23569;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20801;&#35768;&#19982;&#38544;&#34255;&#34920;&#31034;&#33976;&#39311;&#26041;&#27861;&#30340;&#22686;&#24378;&#65292;&#20197;&#21450;&#20351;&#29992;&#23884;&#21512;&#38598;&#21512;&#22788;&#29702;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;KED&#23398;&#29983;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#36229;&#36234;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) improves the performance of a low-complexity student model with the help of a more powerful teacher. The teacher in KD is a black-box model, imparting knowledge to the student only through its predictions. This limits the amount of transferred knowledge. In this work, we introduce a novel Knowledge Explaining Distillation (KED) framework, which allows the student to learn not only from the teacher's predictions but also from the teacher's explanations. We propose a class of superfeature-explaining teachers that provide explanation over groups of features, along with the corresponding student model. We also present a method for constructing the superfeatures. We then extend KED to reduce complexity in convolutional neural networks, to allow augmentation with hidden-representation distillation methods, and to work with a limited amount of training data using chimeric sets. Our experiments over a variety of datasets show that KED students can substantially outp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#19982;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;GNN&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;32.65%&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#26356;&#22823;&#65292;&#21487;&#20197;&#20316;&#20026;&#31038;&#20250;&#32416;&#27491;&#25514;&#26045;&#38459;&#27490;&#35875;&#35328;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.02568</link><description>&lt;p&gt;
&#20026;&#20102;&#26576;&#20107;&#32780;&#31449;&#31435;&#65292;&#21542;&#21017;&#23601;&#20250;&#20026;&#19968;&#20999;&#22446;&#25481;&#65306;&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks. (arXiv:2310.02568v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02568
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#19982;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;GNN&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;32.65%&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#26356;&#22823;&#65292;&#21487;&#20197;&#20316;&#20026;&#31038;&#20250;&#32416;&#27491;&#25514;&#26045;&#38459;&#27490;&#35875;&#35328;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#27969;&#34892;&#30340;&#35875;&#35328;&#20256;&#25773;&#24050;&#25104;&#20026;&#36843;&#20999;&#30340;&#25361;&#25112;&#65292;&#20294;&#29616;&#26377;&#30340;&#24179;&#21488;&#24178;&#39044;&#25514;&#26045;&#22312;&#36943;&#21046;&#20854;&#20256;&#25773;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#31435;&#22330;&#20027;&#21160;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#30001;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#31435;&#22330;&#21487;&#20197;&#24418;&#25104;&#29420;&#29305;&#30340;&#22238;&#22768;&#23460;&#65292;&#25105;&#20204;&#22312;&#31435;&#22330;&#24863;&#30693;&#30340;GNN&#20013;&#23450;&#21046;&#20102;&#22235;&#20010;&#20449;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#32780;&#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#26435;&#37325;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#27599;&#20010;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#26469;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#31435;&#22330;&#24863;&#30693;&#30340;GNN&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;32.65&#65285;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;&#20808;&#36827;GNN 4.69&#65285;&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#65292;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#39640;&#20110;&#25903;&#25345;&#31435;&#22330;&#65292;&#36825;&#36215;&#21040;&#20102;&#31038;&#20250;&#32416;&#27491;&#20316;&#29992;&#65292;&#38459;&#27490;&#20102;&#35875;&#35328;&#30340;&#20256;&#25773;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pervasive spread of misinformation on social media platforms has become a pressing challenge, existing platform interventions have shown limited success in curbing its dissemination. In this study, we propose a stance-aware graph neural network (stance-aware GNN) that leverages users' stances to proactively predict misinformation spread. As different user stances can form unique echo chambers, we customize four information passing paths in stance-aware GNN, while the trainable attention weights provide explainability by highlighting each structure's importance. Evaluated on a real-world dataset, stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs without user stance by over 4.69%. Furthermore, the attention weights indicate that users' opposition stances have a higher impact on their neighbors' behaviors than supportive ones, which function as social correction to halt misinformation propagation. Overall, our study provides an effective predictive model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;</title><link>http://arxiv.org/abs/2310.02563</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#21512;&#20316;&#20215;&#20540;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Practical, Private Assurance of the Value of Collaboration. (arXiv:2310.02563v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#26041;&#21521;&#24076;&#26395;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#24444;&#27492;&#36879;&#38706;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#21452;&#26041;&#24076;&#26395;&#33021;&#22815;&#24471;&#21040;&#21512;&#20316;&#23558;&#26159;&#23500;&#26377;&#25104;&#26524;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#26041;&#34987;&#25215;&#35834;&#36890;&#36807;&#21512;&#24182;&#26469;&#33258;&#21478;&#19968;&#26041;&#30340;&#25968;&#25454;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#27169;&#22411;&#12290;&#21482;&#26377;&#24403;&#26356;&#26032;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#26102;&#65292;&#21452;&#26041;&#25165;&#24076;&#26395;&#36827;&#19968;&#27493;&#21512;&#20316;&#12290;&#22312;&#30830;&#23450;&#36825;&#19968;&#28857;&#20043;&#21069;&#65292;&#21452;&#26041;&#19981;&#24076;&#26395;&#36879;&#38706;&#20182;&#20204;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Torus&#19978;&#30340;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#65288;TFHE&#65289;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#20854;&#20013;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#29992;&#20110;&#30830;&#20445;&#35745;&#31639;&#19981;&#23436;&#20840;&#22312;&#21152;&#23494;&#39046;&#22495;&#36827;&#34892;&#65292;&#36825;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two parties wish to collaborate on their datasets. However, before they reveal their datasets to each other, the parties want to have the guarantee that the collaboration would be fruitful. We look at this problem from the point of view of machine learning, where one party is promised an improvement on its prediction model by incorporating data from the other party. The parties would only wish to collaborate further if the updated model shows an improvement in accuracy. Before this is ascertained, the two parties would not want to disclose their models and datasets. In this work, we construct an interactive protocol for this problem based on the fully homomorphic encryption scheme over the Torus (TFHE) and label differential privacy, where the underlying machine learning model is a neural network. Label differential privacy is used to ensure that computations are not done entirely in the encrypted domain, which is a significant bottleneck for neural network training according to the cu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32852;&#37030;&#23398;&#20064;&#65288;SemiFL&#65289;&#33539;&#24335;&#65292;&#20197;&#22312;&#22522;&#31449;&#21644;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#38598;&#20013;&#24335;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#28151;&#21512;&#23454;&#29616;&#12290;&#36890;&#36807;&#38598;&#25104;&#31354;&#20013;&#35745;&#31639;&#21644;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#20256;&#36755;&#65292;&#25552;&#39640;&#20102;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02559</link><description>&lt;p&gt;
&#21322;&#32852;&#37030;&#23398;&#20064;&#65306;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#30340;&#25910;&#25947;&#20998;&#26512;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Semi-Federated Learning: Convergence Analysis and Optimization of A Hybrid Learning Framework. (arXiv:2310.02559v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32852;&#37030;&#23398;&#20064;&#65288;SemiFL&#65289;&#33539;&#24335;&#65292;&#20197;&#22312;&#22522;&#31449;&#21644;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#38598;&#20013;&#24335;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#28151;&#21512;&#23454;&#29616;&#12290;&#36890;&#36807;&#38598;&#25104;&#31354;&#20013;&#35745;&#31639;&#21644;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#20256;&#36755;&#65292;&#25552;&#39640;&#20102;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#31449;&#65288;BS&#65289;&#30340;&#32452;&#32455;&#19979;&#65292;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#23454;&#29616;&#20102;&#22810;&#20010;&#35774;&#22791;&#20043;&#38388;&#30340;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;BS&#20165;&#36127;&#36131;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32858;&#21512;&#26412;&#22320;&#26356;&#26032;&#65292;&#36825;&#23548;&#33268;&#20102;BS&#19978;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32852;&#37030;&#23398;&#20064;&#65288;SemiFL&#65289;&#33539;&#24335;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;BS&#21644;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#34892;&#38598;&#20013;&#24335;&#23398;&#20064;&#65288;CL&#65289;&#21644;FL&#30340;&#28151;&#21512;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27599;&#20010;&#35774;&#22791;&#23558;&#26412;&#22320;&#26799;&#24230;&#21644;&#25968;&#25454;&#26679;&#26412;&#21457;&#36865;&#32473;BS&#65292;&#20197;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#21457;&#22120;&#32467;&#26500;&#65292;&#23558;&#31354;&#20013;&#35745;&#31639;&#38598;&#25104;&#21040;&#32858;&#21512;&#21644;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#20256;&#36755;&#20013;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;SemiFL&#30340;&#38381;&#24335;&#26368;&#20248;&#24615;&#24046;&#36317;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#24182;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#20004;&#31181;&#39069;&#22806;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under the organization of the base station (BS), wireless federated learning (FL) enables collaborative model training among multiple devices. However, the BS is merely responsible for aggregating local updates during the training process, which incurs a waste of the computational resource at the BS. To tackle this issue, we propose a semi-federated learning (SemiFL) paradigm to leverage the computing capabilities of both the BS and devices for a hybrid implementation of centralized learning (CL) and FL. Specifically, each device sends both local gradients and data samples to the BS for training a shared global model. To improve communication efficiency over the same time-frequency resources, we integrate over-the-air computation for aggregation and non-orthogonal multiple access for transmission by designing a novel transceiver structure. To gain deep insights, we conduct convergence analysis by deriving a closed-form optimality gap for SemiFL and extend the result to two extra cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02557</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27867;&#21270;&#24615;&#36136;&#28304;&#20110;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#35856;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02557
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21487;&#20197;&#23398;&#20064;&#39640;&#32500;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35757;&#32451;&#38598;&#35760;&#24518;&#21270;&#30340;&#26368;&#26032;&#25253;&#21578;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#32593;&#32476;&#26159;&#21542;&#23398;&#20064;&#20102;&#25968;&#25454;&#30340;&#8220;&#30495;&#23454;&#8221;&#36830;&#32493;&#23494;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#35757;&#32451;&#22312;&#25968;&#25454;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#30340;&#20004;&#20010;&#38477;&#22122;DNN&#23398;&#20064;&#30340;&#20960;&#20046;&#26159;&#30456;&#21516;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#20174;&#32780;&#23398;&#20064;&#20102;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#19988;&#20165;&#38656;&#24456;&#23569;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;/&#25110;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#26377;&#21147;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#38477;&#22122;&#22120;&#22312;&#36866;&#24212;&#20110;&#24213;&#23618;&#22270;&#20687;&#30340;&#22522;&#30784;&#19978;&#25191;&#34892;&#25910;&#32553;&#25805;&#20316;&#12290;&#23545;&#36825;&#20123;&#22522;&#30690;&#30340;&#26816;&#26597;&#25581;&#31034;&#20102;&#27839;&#36718;&#24275;&#21644;&#22343;&#21248;&#22270;&#20687;&#21306;&#22495;&#30340;&#25391;&#33633;&#35856;&#27874;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image region
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30693;&#35782;&#20849;&#33976;&#21512;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#25972;&#20010;&#27744;&#23376;&#21644;&#23481;&#37327;&#36739;&#39640;&#30340;&#37096;&#20998;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21452;&#21521;&#20449;&#24687;&#20132;&#25442;&#21644;&#39046;&#22495;&#36716;&#31227;&#65292;&#25913;&#36827;&#20102;&#32852;&#37030;&#24179;&#22343;&#21270;&#31639;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02549</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#20849;&#33976;&#21512;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning Using Knowledge Codistillation. (arXiv:2310.02549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02549
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#20849;&#33976;&#21512;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22312;&#25972;&#20010;&#27744;&#23376;&#21644;&#23481;&#37327;&#36739;&#39640;&#30340;&#37096;&#20998;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21452;&#21521;&#20449;&#24687;&#20132;&#25442;&#21644;&#39046;&#22495;&#36716;&#31227;&#65292;&#25913;&#36827;&#20102;&#32852;&#37030;&#24179;&#22343;&#21270;&#31639;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24179;&#22343;&#21270;&#21450;&#20854;&#24314;&#31435;&#22312;&#20854;&#19978;&#30340;&#35768;&#22810;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#21464;&#31181;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65306;&#25152;&#26377;&#23458;&#25143;&#31471;&#24517;&#39035;&#20849;&#20139;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#23548;&#33268;&#35768;&#22810;&#23458;&#25143;&#31471;&#19978;&#26410;&#20351;&#29992;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#25972;&#20010;&#27744;&#20869;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#21644;&#22312;&#20855;&#26377;&#26356;&#39640;&#23481;&#37327;&#30340;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#27169;&#22411;&#12290;&#27169;&#22411;&#36890;&#36807;&#30693;&#35782;&#20849;&#33976;&#21512;&#22312;&#26410;&#20849;&#20139;&#21442;&#25968;&#30340;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#21452;&#21521;&#20449;&#24687;&#20132;&#25442;&#65292;&#21033;&#29992;&#19968;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#32852;&#37030;&#24179;&#22343;&#21270;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#39046;&#22495;&#22806;&#25110;&#26377;&#38480;&#39046;&#22495;&#20869;&#30340;&#33976;&#39311;&#25968;&#25454;&#21487;&#29992;&#65292;&#36825;&#31181;&#25216;&#26415;&#20063;&#21487;&#20197;&#24456;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#30693;&#35782;&#20849;&#33976;&#21512;&#22312;&#19981;&#21516;&#27744;&#23376;&#20013;&#24341;&#20837;&#39046;&#22495;&#36716;&#31227;&#26102;&#20801;&#35768;&#27169;&#22411;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Averaging, and many federated learning algorithm variants which build upon it, have a limitation: all clients must share the same model architecture. This results in unused modeling capacity on many clients, which limits model performance. To address this issue, we propose a method that involves training a small model on the entire pool and a larger model on a subset of clients with higher capacity. The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters. We present two variants of our method, which improve upon federated averaging on image classification and language modeling tasks. We show this technique can be useful even if only out-of-domain or limited in-domain distillation data is available. Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#20013;&#24212;&#29992;&#36719;&#25439;&#22833;&#21644;&#31934;&#30830;&#36317;&#31163;&#20989;&#25968;&#30340;&#36793;&#30028;&#26465;&#20214;&#26102;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;PINN&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.02548</link><description>&lt;p&gt;
&#21464;&#31995;&#25968;&#27850;&#26494;&#26041;&#31243;&#20013;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#30830;&#21644;&#36719;&#36793;&#30028;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Exact and soft boundary conditions in Physics-Informed Neural Networks for the Variable Coefficient Poisson equation. (arXiv:2310.02548v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#20013;&#24212;&#29992;&#36719;&#25439;&#22833;&#21644;&#31934;&#30830;&#36317;&#31163;&#20989;&#25968;&#30340;&#36793;&#30028;&#26465;&#20214;&#26102;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;PINN&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#26465;&#20214;&#26159;&#27599;&#20010;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#22312;&#22495;&#36793;&#30028;&#19978;&#23450;&#20041;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#36793;&#30028;&#26465;&#20214;&#32422;&#26463;&#20102;PINN&#35797;&#22270;&#36924;&#36817;&#30340;&#22522;&#26412;&#36793;&#30028;&#20540;&#38382;&#39064;&#65288;BVP&#65289;&#12290;&#22914;&#26524;&#27809;&#26377;&#23427;&#20204;&#65292;&#21807;&#19968;&#30340;PDE&#35299;&#21487;&#33021;&#19981;&#23384;&#22312;&#65292;&#20351;&#29992;PINN&#25214;&#21040;&#36817;&#20284;&#35299;&#23558;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29978;&#33267;&#26159;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;PINN&#20013;&#24212;&#29992;&#22522;&#20110;&#36719;&#25439;&#22833;&#21644;&#22522;&#20110;&#31934;&#30830;&#36317;&#31163;&#20989;&#25968;&#30340;&#36793;&#30028;&#26465;&#20214;&#24378;&#21046;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;&#33879;&#21517;&#30340;&#21464;&#31995;&#25968;&#27850;&#26494;&#26041;&#31243;&#34987;&#29992;&#20316;&#26412;&#24037;&#20316;&#20013;&#25152;&#26377;PINN&#27169;&#22411;&#30340;&#30446;&#26631;PDE&#12290;&#38500;&#20102;&#27604;&#36739;&#36793;&#30028;&#26465;&#20214;&#24378;&#21046;&#26041;&#27861;&#65292;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#36824;&#26159;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;PINN&#30340;&#36164;&#28304;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#27492;&#35780;&#35770;&#19968;&#24182;&#21457;&#24067;&#20102;&#20351;&#29992;Tensorflow&#21518;&#31471;&#30340;Keras&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#20195;&#30721;&#31034;&#20363;&#21644;&#36880;&#27493;&#35828;&#26126;&#22914;&#20309;&#26500;&#24314;&#36719;/&#31934;&#30830;&#36793;&#30028;&#26465;&#20214;PINN&#30340;Python&#31508;&#35760;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boundary conditions (BCs) are a key component in every Physics-Informed Neural Network (PINN). By defining the solution to partial differential equations (PDEs) along domain boundaries, BCs constrain the underlying boundary value problem (BVP) that a PINN tries to approximate. Without them, unique PDE solutions may not exist and finding approximations with PINNs would be a challenging, if not impossible task. This study examines how soft loss-based and exact distance function-based BC imposition approaches differ when applied in PINNs. The well known variable coefficient Poisson equation serves as the target PDE for all PINN models trained in this work. Besides comparing BC imposition approaches, the goal of this work is to also provide resources on how to implement these PINNs in practice. To this end, Keras models with Tensorflow backend as well as a Python notebook with code examples and step-by-step explanations on how to build soft/exact BC PINNs are published alongside this revie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GeoPro&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35774;&#35745;&#34507;&#30333;&#36136;&#30340;&#39592;&#26550;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GeoPro&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#22411;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.02546</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24335;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Joint Design of Protein Sequence and Structure based on Motifs. (arXiv:2310.02546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GeoPro&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35774;&#35745;&#34507;&#30333;&#36136;&#30340;&#39592;&#26550;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GeoPro&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#22411;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21151;&#33021;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoPro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35774;&#35745;&#34507;&#30333;&#36136;&#30340;&#39592;&#26550;&#32467;&#26500;&#21644;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#39592;&#26550;&#32467;&#26500;&#30456;&#20114;&#32422;&#26463;&#65292;&#22240;&#27492;&#32852;&#21512;&#35774;&#35745;&#33021;&#22815;&#36991;&#20813;&#38750;&#25240;&#21472;&#21644;&#38169;&#35823;&#25240;&#21472;&#65292;&#24182;&#20135;&#29983;&#26356;&#22810;&#20855;&#26377;&#25152;&#38656;&#21151;&#33021;&#30340;&#20505;&#36873;&#34507;&#30333;&#36136;&#12290;&#20026;&#27492;&#65292;GeoPro&#21033;&#29992;&#19968;&#20010;&#19977;&#32500;&#39592;&#26550;&#32467;&#26500;&#30340;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#30001;&#19977;&#32500;&#20960;&#20309;&#24341;&#23548;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#35299;&#30721;&#22120;&#12290;&#22312;&#21253;&#25324;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#22312;&#20869;&#30340;&#20004;&#20010;&#20855;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#37329;&#23646;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;GeoPro&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#20123;&#20043;&#21069;&#26410;&#21457;&#29616;&#30340;&#26032;&#22411;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing novel proteins with desired functions is crucial in biology and chemistry. However, most existing work focus on protein sequence design, leaving protein sequence and structure co-design underexplored. In this paper, we propose GeoPro, a method to design protein backbone structure and sequence jointly. Our motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions. To this end, GeoPro is powered by an equivariant encoder for three-dimensional (3D) backbone structure and a protein sequence decoder guided by 3D geometry. Experimental results on two biologically significant metalloprotein datasets, including $\beta$-lactamases and myoglobins, show that our proposed GeoPro outperforms several strong baselines on most metrics. Remarkably, our method discovers novel $\beta$-lactamases and myoglobins which are not present
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#20855;&#26377;&#22270;&#20449;&#24687;&#30340;&#21160;&#24577;&#22270;&#27491;&#21017;&#21270;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#22270;&#23548;&#21521;&#34917;&#20840;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02543</link><description>&lt;p&gt;
&#20855;&#26377;&#22270;&#20449;&#24687;&#30340;&#21487;&#35777;&#26126;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Provable Tensor Completion with Graph Information. (arXiv:2310.02543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#20855;&#26377;&#22270;&#20449;&#24687;&#30340;&#21160;&#24577;&#22270;&#27491;&#21017;&#21270;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#22270;&#23548;&#21521;&#34917;&#20840;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#21464;&#37327;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#26377;&#25928;&#20391;&#38754;&#20449;&#24687;&#65292;&#29992;&#20110;&#21508;&#31181;&#30697;&#38453;/&#24352;&#37327;&#24674;&#22797;&#30456;&#20851;&#24212;&#29992;&#20013;&#30340;&#20934;&#30830;&#25968;&#25454;&#24674;&#22797;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22270;&#20449;&#24687;&#30340;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#30446;&#21069;&#20851;&#20110;&#22270;&#27491;&#21017;&#21270;&#30340;&#24352;&#37327;&#34917;&#20840;&#30340;&#30740;&#31350;&#26356;&#20542;&#21521;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20445;&#35777;&#24615;&#33021;&#30340;&#24674;&#22797;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#22270;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#22312;&#24352;&#37327;&#30456;&#20851;&#22330;&#26223;&#20013;&#23558;&#20854;&#35270;&#20026;&#31867;&#20284;&#30697;&#38453;&#30340;&#38745;&#24577;&#23545;&#35937;&#65292;&#21363;&#20351;&#22270;&#21487;&#33021;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;&#21160;&#24577;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#24314;&#31435;&#20102;&#35299;&#20915;&#21160;&#24577;&#22270;&#27491;&#21017;&#21270;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#27169;&#22411;&#12289;&#29702;&#35770;&#21644;&#31639;&#27861;&#12290;&#23545;&#20110;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21160;&#24577;&#22270;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#22522;&#20110;&#35813;&#34920;&#31034;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#24352;&#37327;&#30340;&#22270;&#23548;&#21521;&#30340;&#34917;&#20840;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs, depicting the interrelations between variables, has been widely used as effective side information for accurate data recovery in various matrix/tensor recovery related applications. In this paper, we study the tensor completion problem with graph information. Current research on graph-regularized tensor completion tends to be task-specific, lacking generality and systematic approaches. Furthermore, a recovery theory to ensure performance remains absent. Moreover, these approaches overlook the dynamic aspects of graphs, treating them as static akin to matrices, even though graphs could exhibit dynamism in tensor-related scenarios. To confront these challenges, we introduce a pioneering framework in this paper that systematically formulates a novel model, theory, and algorithm for solving the dynamic graph regularized tensor completion problem. For the model, we establish a rigorous mathematical representation of the dynamic graph, based on which we derive a new tensor-oriented g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;ReLU&#32593;&#32476;&#22312;XOR&#38598;&#32676;&#25968;&#25454;&#19978;&#20250;&#20135;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#21644;&#29702;&#35299;&#29616;&#35937;&#65292;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#23454;&#29616;&#22122;&#22768;&#26631;&#31614;&#30340;&#23436;&#32654;&#25311;&#21512;&#20294;&#22312;&#27979;&#35797;&#38454;&#27573;&#34920;&#29616;&#38543;&#26426;&#65292;&#22312;&#21518;&#32493;&#38454;&#27573;&#21487;&#20197;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02541</link><description>&lt;p&gt;
&#38024;&#23545;XOR&#38598;&#32676;&#25968;&#25454;&#20013;&#30340;ReLU&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data. (arXiv:2310.02541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;ReLU&#32593;&#32476;&#22312;XOR&#38598;&#32676;&#25968;&#25454;&#19978;&#20250;&#20135;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#21644;&#29702;&#35299;&#29616;&#35937;&#65292;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#23454;&#29616;&#22122;&#22768;&#26631;&#31614;&#30340;&#23436;&#32654;&#25311;&#21512;&#20294;&#22312;&#27979;&#35797;&#38454;&#27573;&#34920;&#29616;&#38543;&#26426;&#65292;&#22312;&#21518;&#32493;&#38454;&#27573;&#21487;&#20197;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23637;&#29616;&#20102;&#35768;&#22810;&#20196;&#20154;&#24778;&#35766;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#21487;&#20197;&#23545;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#23436;&#32654;&#25311;&#21512;&#65292;&#24182;&#19988;&#20173;&#28982;&#33021;&#22815;&#36817;&#20046;&#26368;&#20248;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#34920;&#26126;&#36807;&#25311;&#21512;&#26377;&#26102;&#21487;&#33021;&#26159;&#33391;&#24615;&#30340;&#12290;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#32463;&#21382;&#19968;&#27573;&#32463;&#20856;&#19988;&#26377;&#23475;&#30340;&#36807;&#25311;&#21512;&#26399;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#23454;&#29616;&#23436;&#32654;&#25311;&#21512;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#38543;&#26426;&#65292;&#38543;&#21518;&#36807;&#28193;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;&#27867;&#21270;&#34892;&#20026;&#65288;&#21363;&#8220;&#29702;&#35299;&#8221;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#29616;&#35937;&#22312;&#36890;&#36807;GD&#23545;XOR&#38598;&#32676;&#25968;&#25454;&#19978;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30830;&#23454;&#20250;&#20986;&#29616;&#65292;&#20854;&#20013;&#35757;&#32451;&#26631;&#31614;&#30340;&#19968;&#37096;&#20998;&#20250;&#34987;&#32763;&#36716;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;GD&#30340;&#31532;&#19968;&#27493;&#20043;&#21518;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23454;&#29616;100%&#30340;&#35757;&#32451;&#20934;&#30830;&#24230;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#23436;&#32654;&#25311;&#21512;&#22122;&#22768;&#26631;&#31614;&#65292;&#20294;&#22312;&#27979;&#35797;&#19978;&#34920;&#29616;&#25509;&#36817;&#38543;&#26426;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#32593;&#32476;&#33021;&#22815;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#20173;&#28982;&#25311;&#21512;&#38543;&#26426;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning ("grokking") to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#65292;&#23558;&#20854;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02540</link><description>&lt;p&gt;
Auto-FP:&#33258;&#21160;&#21270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data. (arXiv:2310.02540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#65292;&#23558;&#20854;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65292;&#22312;&#24037;&#19994;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#25935;&#24863;&#65292;&#22240;&#27492;&#29305;&#24449;&#39044;&#22788;&#29702;&#26159;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#33391;&#22909;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#39044;&#22788;&#29702;&#27969;&#31243;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#22312;&#36873;&#25321;&#21738;&#20123;&#39044;&#22788;&#29702;&#22120;&#20197;&#21450;&#20197;&#20160;&#20040;&#39034;&#24207;&#32452;&#21512;&#23427;&#20204;&#26041;&#38754;&#20316;&#20986;&#22256;&#38590;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#12290;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#36739;&#22823;&#65292;&#26292;&#21147;&#35299;&#20915;&#26041;&#26696;&#20195;&#20215;&#22826;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#26377;&#36259;&#22320;&#35266;&#23519;&#21040;Auto-FP&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#38382;&#39064;&#12290;&#36825;&#20010;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21508;&#31181;HPO&#21644;NAS&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#20849;&#36827;&#34892;&#20102;15&#20010;...
&lt;/p&gt;
&lt;p&gt;
Classical machine learning models, such as linear models and tree-based models, are widely used in industry. These models are sensitive to data distribution, thus feature preprocessing, which transforms features from one distribution to another, is a crucial step to ensure good model quality. Manually constructing a feature preprocessing pipeline is challenging because data scientists need to make difficult decisions about which preprocessors to select and in which order to compose them. In this paper, we study how to automate feature preprocessing (Auto-FP) for tabular data. Due to the large search space, a brute-force solution is prohibitively expensive. To address this challenge, we interestingly observe that Auto-FP can be modelled as either a hyperparameter optimization (HPO) or a neural architecture search (NAS) problem. This observation enables us to extend a variety of HPO and NAS algorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation and analysis of 15 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#21644;&#20943;&#36731;&#20102;&#26631;&#31614;&#38169;&#35823;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35757;&#32451;&#36755;&#20837;&#26631;&#31614;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02533</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#20943;&#36731;&#26631;&#31614;&#38169;&#35823;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Quantifying and mitigating the impact of label errors on model disparity metrics. (arXiv:2310.02533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#21644;&#20943;&#36731;&#20102;&#26631;&#31614;&#38169;&#35823;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35757;&#32451;&#36755;&#20837;&#26631;&#31614;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#21462;&#30340;&#26631;&#31614;&#38169;&#35823;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#25552;&#20986;&#20102;&#20943;&#36731;&#26631;&#31614;&#38169;&#35823;&#23545;&#27169;&#22411;&#19979;&#28216;&#20934;&#30830;&#24615;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#38169;&#35823;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20197;&#23454;&#35777;&#26041;&#24335;&#34920;&#24449;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#38169;&#35823;&#23545;&#36825;&#20123;&#24046;&#24322;&#24230;&#37327;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#32676;&#20307;&#26657;&#20934;&#21644;&#20854;&#20182;&#24230;&#37327;&#23545;&#35757;&#32451;&#26102;&#21644;&#27979;&#35797;&#26102;&#30340;&#26631;&#31614;&#38169;&#35823;&#38750;&#24120;&#25935;&#24863;&#65292;&#23588;&#20854;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#12290;&#36825;&#31181;&#24046;&#24322;&#25928;&#24212;&#29978;&#33267;&#36866;&#29992;&#20110;&#20351;&#29992;&#22122;&#22768;&#24863;&#30693;&#31639;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20943;&#36731;&#35757;&#32451;&#26102;&#30340;&#26631;&#31614;&#38169;&#35823;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35757;&#32451;&#36755;&#20837;&#26631;&#31614;&#23545;&#27169;&#22411;&#24046;&#24322;&#24230;&#37327;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#23454;&#35777;&#26041;&#24335;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#19982;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Errors in labels obtained via human annotation adversely affect a model's performance. Existing approaches propose ways to mitigate the effect of label error on a model's downstream accuracy, yet little is known about its impact on a model's disparity metrics. Here we study the effect of label error on a model's disparity metrics. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error -- particularly for minority groups. This disparate effect persists even for models trained with noise-aware algorithms. To mitigate the impact of training-time label error, we present an approach to estimate the influence of a training input's label on a model's group disparity metric. We empirically assess the proposed approach on a variety of datasets and find significant improvement, compared to alternative approaches, in identif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02524</link><description>&lt;p&gt;
&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#19981;&#21464;&#23398;&#20064;&#12289;AUPRC&#26368;&#22823;&#21270;&#21644;&#20803;&#23398;&#20064;&#12290;&#38543;&#30528;&#36825;&#20123;&#24212;&#29992;&#20013;&#23545;&#20351;&#29992;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23545;&#20110;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#20854;&#20013;&#21253;&#25324;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;(FCSG-M)&#12290;&#20026;&#20102;&#36798;&#21040;&#21333;&#26426;&#35774;&#23450;&#19979;&#30340;&#19979;&#30028;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#35774;&#35745;&#20102;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;FL&#20013;MAML&#30340;&#20248;&#21270;&#20998;&#26512;&#30456;&#27604;&#65292;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#32771;&#34385;&#20102;&#26679;&#26412;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02520</link><description>&lt;p&gt;
MedDiffusion: &#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#20854;&#24207;&#21015;&#29305;&#24615;&#65292;&#39640;&#32500;&#24230;&#21644;&#22266;&#26377;&#22122;&#38899;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24433;&#21709;&#23427;&#20204;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#20219;&#21153;&#26080;&#20851;&#35774;&#35745;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;MedDiffusion&#65292;&#26469;&#22686;&#24378;&#39118;&#38505;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.02519</link><description>&lt;p&gt;
&#21442;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#27861;&#29992;&#20110;&#25674;&#38144;&#20248;&#21270;&#20013;&#30446;&#26631;&#20989;&#25968;&#30340;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization. (arXiv:2310.02519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#24471;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#20984;&#25903;&#37197;&#65288;PCM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25674;&#38144;&#20248;&#21270;&#20013;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30001;PCM&#21644;&#38750;&#36127;&#38388;&#38553;&#20989;&#25968;&#20043;&#21644;&#34920;&#31034;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#22312;&#20248;&#21270;&#21464;&#37327;&#19978;&#30001;PCM&#20984;&#20989;&#25968;&#19979;&#30028;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#26159;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;PCM&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#36798;&#21040;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20984;&#20248;&#21270;&#33719;&#21462;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#20316;&#20026;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#21442;&#25968;&#21270;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#23545;&#25968;&#21644;&#25351;&#25968;&#32593;&#32476;&#20316;&#20026;PCM&#12290;&#23545;&#20110;&#38750;&#21442;&#25968;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#36924;&#36817;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized convex minorant (PCM) method is proposed for the approximation of the objective function in amortized optimization. In the proposed method, the objective function approximator is expressed by the sum of a PCM and a nonnegative gap function, where the objective function approximator is bounded from below by the PCM convex in the optimization variable. The proposed objective function approximator is a universal approximator for continuous functions, and the global minimizer of the PCM attains the global minimum of the objective function approximator. Therefore, the global minimizer of the objective function approximator can be obtained by a single convex optimization. As a realization of the proposed method, extended parameterized log-sum-exp network is proposed by utilizing a parameterized log-sum-exp network as the PCM. Numerical simulation is performed for non-parameterized-convex objective function approximation and for learning-based nonlinear model predictive control 
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#20102;&#22522;&#20110;Lipschitz&#30340;&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;...</title><link>http://arxiv.org/abs/2310.02513</link><description>&lt;p&gt;
&#25913;&#36827;&#21487;&#39564;&#35777;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#65306;&#23481;&#37327;&#21644;&#25968;&#25454;&#30340;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
A Recipe for Improved Certifiable Robustness: Capacity and Data. (arXiv:2310.02513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02513
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#20102;&#22522;&#20110;Lipschitz&#30340;&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#21644;&#23454;&#36341;&#37117;&#25903;&#25345;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#31283;&#20581;&#24615;&#35201;&#27714;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22823;&#30340;&#32593;&#32476;&#23481;&#37327;&#21644;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#20005;&#26684;&#30340;Lipschitz&#32422;&#26463;&#19979;&#26377;&#25928;&#22320;&#22686;&#21152;&#23481;&#37327;&#27604;&#30475;&#36215;&#26469;&#26356;&#22256;&#38590;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#20542;&#21521;&#20110;&#20302;&#25311;&#21512;&#32780;&#19981;&#26159;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20027;&#24352;&#23545;&#22522;&#20110;Lipshitz&#30340;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20180;&#32454;&#25506;&#32034;&#19981;&#36275;&#65292;&#36825;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#24615;&#33021;&#25552;&#21319;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;Lipschitz-based&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30830;&#23450;&#24615;&#35748;&#35777;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39564;&#35777;&#31283;&#20581;&#20934;&#30830;&#24615;&#8221;&#65288;VRA&#65289;&#65292;&#24182;&#35206;&#30422;&#19968;&#31995;&#21015;&#25200;&#21160;&#22823;&#23567;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards \emph{underfitting} than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art \emph{verified robust accuracy} (VRA) for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that th
&lt;/p&gt;</description></item><item><title>Ophiuchus&#26159;&#19968;&#20010;&#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#23427;&#33021;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#25805;&#20316;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#32467;&#26500;&#30340;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02508</link><description>&lt;p&gt;
Ophiuchus: &#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02508
&lt;/p&gt;
&lt;p&gt;
Ophiuchus&#26159;&#19968;&#20010;&#36890;&#36807;&#20998;&#23618;&#31895;&#31890;&#21270;SO(3)-&#31561;&#21464;&#33258;&#32534;&#30721;&#22120;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21487;&#25193;&#23637;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#23427;&#33021;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#25805;&#20316;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#32467;&#26500;&#30340;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#28982;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#21407;&#29983;&#24577;&#29366;&#24577;&#26174;&#31034;&#20986;&#37325;&#22797;&#21644;&#20998;&#23618;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#36890;&#24120;&#23616;&#38480;&#20110;&#22312;&#21333;&#19968;&#31934;&#32454;&#21270;&#20998;&#36776;&#29575;&#20869;&#25805;&#20316;&#65292;&#24182;&#19988;&#32570;&#20047;&#25429;&#25417;&#39640;&#32423;&#26500;&#24314;&#27169;&#22359;&#30340;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Ophiuchus&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#23427;&#26159;&#19968;&#20010;SO(3)-&#31561;&#21464;&#31895;&#31890;&#21270;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25805;&#20316;&#26631;&#20934;&#34507;&#30333;&#36136;&#27531;&#22522;&#30340;&#25152;&#26377;&#37325;&#21407;&#23376;&#65292;&#24182;&#21516;&#26102;&#23562;&#37325;&#23427;&#20204;&#30340;&#30456;&#20851;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#24403;&#21069;&#37319;&#29992;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#23616;&#37096;&#21367;&#31215;&#31895;&#21270;&#65292;&#20197;&#22312;&#23545;&#25968;&#32447;&#24615;&#38271;&#24230;&#22797;&#26434;&#24230;&#19979;&#27169;&#25311;&#24207;&#21015;&#27169;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;PDB&#21333;&#20307;&#30340;&#36830;&#32493;&#29255;&#27573;&#23545;Ophiuchus&#36827;&#34892;&#35757;&#32451;&#65292;&#30740;&#31350;&#20854;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#23398;&#20064;&#21040;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#26500;&#35937;&#25554;&#20540;&#20013;&#30340;&#24555;&#36895;&#20351;&#29992;&#65292;&#23558;&#25554;&#20540;&#36712;&#36857;&#19982;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional native states of natural proteins display recurring and hierarchical patterns. Yet, traditional graph-based modeling of protein structures is often limited to operate within a single fine-grained resolution, and lacks hourglass neural architectures to learn those high-level building blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries. Our model departs from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. We train Ophiuchus on contiguous fragments of PDB monomers, investigating its reconstruction capabilities across different compression rates. We examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.02505</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#23558;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#30446;&#26631;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25918;&#22312;&#25193;&#25955;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#26031;&#22122;&#22768;&#21019;&#24314;&#38543;&#26426;&#36712;&#36857;&#65292;&#20351;&#20854;&#36828;&#31163;&#25968;&#25454;&#27969;&#24418;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#36828;&#31163;&#28508;&#22312;&#30446;&#26631;&#29366;&#24577;&#30340;&#36712;&#36857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#20284;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20010;&#31216;&#20026;Merlin&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#36873;&#25321;&#65292;&#29992;&#20110;&#21462;&#20195;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411; - &#32531;&#20914;&#21306;&#20013;&#30340;&#21453;&#21521;&#25773;&#25918;&#65292;&#21453;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22768;&#38899;&#29305;&#36136;&#30340;&#21487;&#35299;&#37322;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24615;&#21035;&#21270;&#30340;PQs&#28155;&#21152;&#21040;&#22768;&#38899;&#24863;&#30693;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#23454;&#29616;&#20102;&#25104;&#24180;&#20154;&#22768;&#38899;&#29305;&#24449;&#30340;&#20013;&#38388;&#23618;&#25277;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#22768;&#38899;&#29305;&#36136;&#26159;&#21487;&#20197;&#34987;&#38750;&#19987;&#19994;&#20154;&#21592;&#24863;&#30693;&#21040;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;PQs&#30340;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#26159;&#21487;&#20197;&#34987;&#21508;&#31181;&#35821;&#38899;&#34920;&#31034;&#39044;&#27979;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.02497</link><description>&lt;p&gt;
&#36890;&#36807;&#24863;&#30693;&#22768;&#38899;&#29305;&#36136;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities. (arXiv:2310.02497v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22768;&#38899;&#29305;&#36136;&#30340;&#21487;&#35299;&#37322;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24615;&#21035;&#21270;&#30340;PQs&#28155;&#21152;&#21040;&#22768;&#38899;&#24863;&#30693;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#23454;&#29616;&#20102;&#25104;&#24180;&#20154;&#22768;&#38899;&#29305;&#24449;&#30340;&#20013;&#38388;&#23618;&#25277;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#22768;&#38899;&#29305;&#36136;&#26159;&#21487;&#20197;&#34987;&#38750;&#19987;&#19994;&#20154;&#21592;&#24863;&#30693;&#21040;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;PQs&#30340;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#26159;&#21487;&#20197;&#34987;&#21508;&#31181;&#35821;&#38899;&#34920;&#31034;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25991;&#26412;&#21644;&#35270;&#35273;&#31561;&#20854;&#20182;&#25968;&#25454;&#24418;&#24335;&#19981;&#21516;&#65292;&#35821;&#38899;&#19981;&#26131;&#35299;&#37322;&#12290;&#34429;&#28982;&#26222;&#36890;&#20154;&#21487;&#20197;&#36890;&#36807;&#24863;&#30693;&#26469;&#25551;&#36848;&#22270;&#20687;&#25110;&#21477;&#23376;&#65292;&#20294;&#23545;&#20110;&#35821;&#38899;&#65292;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#25551;&#36848;&#36890;&#24120;&#20165;&#38480;&#20110;&#39640;&#32423;&#21035;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#22914;&#24615;&#21035;&#25110;&#24180;&#40836;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#22522;&#20110;&#24863;&#30693;&#22768;&#38899;&#29305;&#36136;&#65288;PQs&#65289;&#30340;&#21487;&#35299;&#37322;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#24615;&#21035;&#21270;&#30340;PQs&#28155;&#21152;&#21040;&#20197;&#30149;&#29702;&#20026;&#28966;&#28857;&#30340;&#22768;&#38899;&#32479;&#19968;&#21548;&#35273;&#24863;&#30693;&#35780;&#20272;&#65288;CAPE-V&#65289;&#21327;&#35758;&#20013;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;PQs&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#24180;&#20154;&#22768;&#38899;&#29305;&#24449;&#30340;&#24863;&#30693;&#28508;&#22312;&#31354;&#38388;&#65292;&#23427;&#26159;&#39640;&#32423;&#21035;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20302;&#32423;&#21035;&#22768;&#23398;&#12289;&#29289;&#29702;&#25110;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#20013;&#38388;&#23618;&#25277;&#35937;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;PQs&#26159;&#21487;&#20197;&#36890;&#36807;&#38750;&#19987;&#19994;&#20154;&#21592;&#30340;&#35266;&#23519;&#21548;&#21040;&#30340;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;PQs&#30340;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#26159;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#35821;&#38899;&#34920;&#31034;&#26469;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike other data modalities such as text and vision, speech does not lend itself to easy interpretation. While lay people can understand how to describe an image or sentence via perception, non-expert descriptions of speech often end at high-level demographic information, such as gender or age. In this paper, we propose a possible interpretable representation of speaker identity based on perceptual voice qualities (PQs). By adding gendered PQs to the pathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) protocol, our PQ-based approach provides a perceptual latent space of the character of adult voices that is an intermediary of abstraction between high-level demographics and low-level acoustic, physical, or learned representations. Contrary to prior belief, we demonstrate that these PQs are hearable by ensembles of non-experts, and further demonstrate that the information encoded in a PQ-based representation is predictable by various speech representations.
&lt;/p&gt;</description></item><item><title>DON-LSTM&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#23558;DeepONet&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DON-LSTM&#33021;&#22815;&#22312;&#22810;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#28436;&#21270;&#24314;&#27169;&#26041;&#38754;&#23454;&#29616;&#36739;&#20302;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.02491</link><description>&lt;p&gt;
DON-LSTM: &#29992;DeepONets&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DON-LSTM: Multi-Resolution Learning with DeepONets and Long Short-Term Memory Neural Networks. (arXiv:2310.02491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02491
&lt;/p&gt;
&lt;p&gt;
DON-LSTM&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#23558;DeepONet&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DON-LSTM&#33021;&#22815;&#22312;&#22810;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#28436;&#21270;&#24314;&#27169;&#26041;&#38754;&#23454;&#29616;&#36739;&#20302;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#65288;DeepONets, DONs&#65289;&#22312;&#33021;&#22815;&#35757;&#32451;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#26041;&#38754;&#30456;&#27604;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#27979;&#37327;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#24471;&#65292;&#32780;&#20302;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#26356;&#23481;&#26131;&#33719;&#24471;&#65292;&#36825;&#20010;&#29305;&#24615;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;DeepONets&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#21644;&#20445;&#25345;&#38271;&#24207;&#21015;&#30340;&#30456;&#20851;&#24615;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#21629;&#21517;&#20026;DON-LSTM&#65292;&#23427;&#23558;DeepONet&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26550;&#26500;&#65292;&#25105;&#20204;&#36171;&#20104;&#32593;&#32476;&#26126;&#30830;&#30340;&#26426;&#21046;&#26469;&#21033;&#29992;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#24182;&#25429;&#25417;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#28436;&#21270;&#24314;&#27169;&#26041;&#38754;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#22810;&#20998;&#36776;&#29575;DON-LSTM&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#20302;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep operator networks (DeepONets, DONs) offer a distinct advantage over traditional neural networks in their ability to be trained on multi-resolution data. This property becomes especially relevant in real-world scenarios where high-resolution measurements are difficult to obtain, while low-resolution data is more readily available. Nevertheless, DeepONets alone often struggle to capture and maintain dependencies over long sequences compared to other state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM, which extends the DeepONet with a long short-term memory network (LSTM). Combining these two architectures, we equip the network with explicit mechanisms to leverage multi-resolution data, as well as capture temporal dependencies in long sequences. We test our method on long-time-evolution modeling of multiple non-linear systems and show that the proposed multi-resolution DON-LSTM achieves significantly lower generalization error and requires fewer high-resolut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.02489</link><description>&lt;p&gt;
ResidualTransformer&#65306;&#24102;&#26377;&#26435;&#37325;&#20849;&#20139;&#30340;&#27531;&#24046;&#20302;&#31209;&#23398;&#20064;&#30340;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#21040;&#22987;&#32456;&#24320;&#21551;&#35774;&#22791;&#19978;&#26102;&#65292;&#20869;&#23384;&#38480;&#21046;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#34429;&#28982;&#20351;&#29992;&#36275;&#22815;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#26356;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20351;&#20854;&#36866;&#24212;&#35774;&#22791;&#20869;&#23384;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#20551;&#35774;&#29305;&#27530;&#30340;&#26435;&#37325;&#32452;&#21512;&#21644;&#32467;&#26500;&#65292;&#26469;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21463;ResNet&#21644;&#26368;&#26032;&#30340;LoRA&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Transformer&#23618;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#21253;&#25324;1&#65289;&#19982;&#20854;&#30456;&#37051;&#23618;&#20849;&#20139;&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#21644;2&#65289;&#20165;&#23646;&#20110;&#23427;&#33258;&#24049;&#30340;&#29420;&#29305;&#20302;&#31209;&#32452;&#20214;&#12290;&#20302;&#31209;&#30697;&#38453;&#21482;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28155;&#21152;&#23545;&#35282;&#32447;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#20302;&#31209;&#30697;&#38453;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;10k&#23567;&#26102;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the T
&lt;/p&gt;</description></item><item><title>OCU-Net&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;U-Net&#26550;&#26500;&#65292;&#19987;&#38376;&#29992;&#20110;&#21475;&#33108;&#30284;&#20998;&#21106;&#20219;&#21153;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21644;&#29305;&#24449;&#65292;&#21253;&#25324;&#36890;&#36947;&#21644;&#31354;&#38388;&#27880;&#24847;&#34701;&#21512;&#27169;&#22359;&#12289;&#25380;&#21387;&#28608;&#27963;&#27880;&#24847;&#27169;&#22359;&#12289;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#31561;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02486</link><description>&lt;p&gt;
OCU-Net: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21475;&#33108;&#30284;&#20998;&#21106;&#30340;&#26032;&#22411;U-Net&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
OCU-Net: A Novel U-Net Architecture for Enhanced Oral Cancer Segmentation. (arXiv:2310.02486v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02486
&lt;/p&gt;
&lt;p&gt;
OCU-Net&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;U-Net&#26550;&#26500;&#65292;&#19987;&#38376;&#29992;&#20110;&#21475;&#33108;&#30284;&#20998;&#21106;&#20219;&#21153;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21644;&#29305;&#24449;&#65292;&#21253;&#25324;&#36890;&#36947;&#21644;&#31354;&#38388;&#27880;&#24847;&#34701;&#21512;&#27169;&#22359;&#12289;&#25380;&#21387;&#28608;&#27963;&#27880;&#24847;&#27169;&#22359;&#12289;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#31561;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26816;&#27979;&#21475;&#33108;&#30284;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#21475;&#33108;&#30284;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#30740;&#31350;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;OCU-Net&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;U-Net&#22270;&#20687;&#20998;&#21106;&#26550;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22312;&#34880;&#32418;&#32032;&#19982;&#22122;&#38899;(HE)&#26579;&#33394;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21475;&#33108;&#30284;&#12290;OCU-Net&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#65292;&#22914;&#36890;&#36947;&#21644;&#31354;&#38388;&#27880;&#24847;&#34701;&#21512;(CSAF)&#27169;&#22359;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#35843;HE&#22270;&#20687;&#20013;&#37325;&#35201;&#36890;&#36947;&#21644;&#31354;&#38388;&#21306;&#22495;&#24182;&#25506;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26032;&#39062;&#21019;&#26032;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;OCU-Net&#36824;&#38598;&#25104;&#20102;&#20854;&#20182;&#21019;&#26032;&#32452;&#20214;&#65292;&#22914;&#25380;&#21387;&#28608;&#27963;(SE)&#27880;&#24847;&#27169;&#22359;&#65292;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;(ASPP)&#27169;&#22359;&#65292;&#27531;&#24046;&#22359;&#21644;&#22810;&#23610;&#24230;&#34701;&#21512;&#12290;&#36825;&#20123;&#27169;&#22359;&#30340;&#34701;&#21512;&#22312;&#21475;&#33108;&#30284;&#20998;&#21106;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of oral cancer is crucial for improving patient outcomes. However, the field faces two key challenges: the scarcity of deep learning-based image segmentation research specifically targeting oral cancer and the lack of annotated data. Our study proposes OCU-Net, a pioneering U-Net image segmentation architecture exclusively designed to detect oral cancer in hematoxylin and eosin (H&amp;E) stained image datasets. OCU-Net incorporates advanced deep learning modules, such as the Channel and Spatial Attention Fusion (CSAF) module, a novel and innovative feature that emphasizes important channel and spatial areas in H&amp;E images while exploring contextual information. In addition, OCU-Net integrates other innovative components such as Squeeze-and-Excite (SE) attention module, Atrous Spatial Pyramid Pooling (ASPP) module, residual blocks, and multi-scale fusion. The incorporation of these modules showed superior performance for oral cancer segmentation for two datasets used in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#31867;&#21035;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#31867;&#21035;&#30340;&#25200;&#21160;&#26679;&#26412;&#35270;&#20026;&#21333;&#29420;&#30340;&#31867;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20915;&#31574;&#36793;&#30028;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02480</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#24046;&#24322;&#24615;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Splitting the Difference on Adversarial Training. (arXiv:2310.02480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#31867;&#21035;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#31867;&#21035;&#30340;&#25200;&#21160;&#26679;&#26412;&#35270;&#20026;&#21333;&#29420;&#30340;&#31867;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20915;&#31574;&#36793;&#30028;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#23384;&#22312;&#25351;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#22522;&#26412;&#24369;&#28857;&#12290;&#23545;&#25239;&#35757;&#32451;&#20316;&#20026;&#38024;&#23545;&#27492;&#31867;&#26679;&#26412;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#38656;&#35201;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#36890;&#24120;&#20197;&#33258;&#28982;&#20934;&#30830;&#24615;&#30340;&#38477;&#20302;&#20026;&#20195;&#20215;&#12290;&#22823;&#22810;&#25968;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#28085;&#30422;&#20102;&#24178;&#20928;&#21644;&#25200;&#21160;&#30340;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#20010;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#31867;&#21035;&#30340;&#25200;&#21160;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#21333;&#29420;&#30340;&#38656;&#35201;&#23398;&#20064;&#30340;&#31867;&#21035;&#65292;&#26377;&#25928;&#22320;&#23558;&#27599;&#20010;&#31867;&#21035;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65306;"&#24178;&#20928;"&#21644;"&#23545;&#25239;&#24615;"&#12290;&#36825;&#31181;&#20998;&#21106;&#20351;&#24471;&#38656;&#35201;&#23398;&#20064;&#30340;&#31867;&#21035;&#25968;&#37327;&#32763;&#20493;&#65292;&#20294;&#21516;&#26102;&#22823;&#22823;&#31616;&#21270;&#20102;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#30340;&#21512;&#29702;&#24615;&#35770;&#35777;&#65292;&#20197;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26395;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#26377;&#30410;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes: "clean" and "adversarial." This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#22478;&#24066;&#29305;&#24449;&#23545;&#31354;&#27668;&#27745;&#26579;&#12289;&#22478;&#24066;&#28909;&#23707;&#25928;&#24212;&#21644;&#27946;&#28061;&#28798;&#23475;&#30340;&#26292;&#38706;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#24357;&#34917;&#20102;&#20256;&#32479;&#29615;&#22659;&#19981;&#20844;&#27491;&#35266;&#28857;&#23545;&#22478;&#24066;&#29305;&#24449;&#24433;&#21709;&#30340;&#26377;&#38480;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.02476</link><description>&lt;p&gt;
ML4EJ&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#30721;&#22478;&#24066;&#29305;&#24449;&#22312;&#22609;&#36896;&#29615;&#22659;&#19981;&#20844;&#27491;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
ML4EJ: Decoding the Role of Urban Features in Shaping Environmental Injustice Using Interpretable Machine Learning. (arXiv:2310.02476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#22478;&#24066;&#29305;&#24449;&#23545;&#31354;&#27668;&#27745;&#26579;&#12289;&#22478;&#24066;&#28909;&#23707;&#25928;&#24212;&#21644;&#27946;&#28061;&#28798;&#23475;&#30340;&#26292;&#38706;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#24357;&#34917;&#20102;&#20256;&#32479;&#29615;&#22659;&#19981;&#20844;&#27491;&#35266;&#28857;&#23545;&#22478;&#24066;&#29305;&#24449;&#24433;&#21709;&#30340;&#26377;&#38480;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22609;&#36896;&#29615;&#22659;&#21361;&#23475;&#26292;&#38706;&#21450;&#20854;&#30456;&#20851;&#29615;&#22659;&#19981;&#20844;&#27491;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#23545;&#21046;&#23450;&#20844;&#24179;&#25919;&#31574;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#29615;&#22659;&#19981;&#20844;&#27491;&#35266;&#28857;&#20027;&#35201;&#20851;&#27880;&#31038;&#20250;&#32463;&#27982;&#26041;&#38754;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#24322;&#36136;&#22478;&#24066;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26377;&#38480;&#30340;&#35266;&#28857;&#21487;&#33021;&#38459;&#30861;&#23545;&#29615;&#22659;&#27491;&#20041;&#30340;&#22797;&#26434;&#24615;&#21450;&#20854;&#19982;&#22478;&#24066;&#35774;&#35745;&#29305;&#24449;&#30340;&#20851;&#31995;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#30740;&#31350;&#21508;&#31181;&#22478;&#24066;&#29305;&#24449;&#21450;&#20854;&#38750;&#32447;&#24615;&#20132;&#20114;&#23545;&#19977;&#31181;&#20027;&#35201;&#21361;&#23475;&#65288;&#31354;&#27668;&#27745;&#26579;&#12289;&#22478;&#24066;&#28909;&#23707;&#25928;&#24212;&#21644;&#27946;&#28061;&#28798;&#23475;&#65289;&#30340;&#26292;&#38706;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#20998;&#26512;&#20351;&#29992;&#26469;&#33258;&#32654;&#22269;&#20845;&#20010;&#22823;&#37117;&#20250;&#21439;&#30340;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#12290;&#24615;&#33021;&#29992;&#20110;&#34913;&#37327;&#22478;&#24066;&#29305;&#24449;&#21464;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#22609;&#36896;&#19981;&#20844;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the key factors shaping environmental hazard exposures and their associated environmental injustice issues is vital for formulating equitable policy measures. Traditional perspectives on environmental injustice have primarily focused on the socioeconomic dimensions, often overlooking the influence of heterogeneous urban characteristics. This limited view may obstruct a comprehensive understanding of the complex nature of environmental justice and its relationship with urban design features. To address this gap, this study creates an interpretable machine learning model to examine the effects of various urban features and their non-linear interactions to the exposure disparities of three primary hazards: air pollution, urban heat, and flooding. The analysis trains and tests models with data from six metropolitan counties in the United States using Random Forest and XGBoost. The performance is used to measure the extent to which variations of urban features shape disparitie
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#65292;&#19981;&#38656;&#35201;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26102;&#24207;&#28418;&#31227;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02473</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting-based Efficient Temporal Domain Generalization. (arXiv:2310.02473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02473
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#65292;&#19981;&#38656;&#35201;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26102;&#24207;&#28418;&#31227;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#30456;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#23548;&#33268;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#27867;&#21270;&#33021;&#21147;&#21464;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#26102;&#38388;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#22495;&#25968;&#25454;&#65288;&#21363;&#26410;&#30693;&#30340;&#26410;&#26469;&#26102;&#38388;&#27573;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#21040;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30446;&#26631;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26102;&#24207;&#28418;&#31227;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#26102;&#22495;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#20195;&#30721;&#20179;&#24211;&#23558;&#20844;&#24320;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning traditionally assumes that training and testing data are distributed independently and identically. However, in many real-world settings, the data distribution can shift over time, leading to poor generalization of trained models in future time periods. Our paper presents a novel prompting-based approach to temporal domain generalization that is parameter-efficient, time-efficient, and does not require access to the target domain data (i.e., unseen future time periods) during training. Our method adapts a target pre-trained model to temporal drift by learning global prompts, domain-specific prompts, and drift-aware prompts that capture underlying temporal dynamics. It is compatible across diverse tasks, such as classification, regression, and time series forecasting, and sets a new state-of-the-art benchmark in temporal domain generalization. The code repository will be publicly shared.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;Wasserstein&#24230;&#37327;&#26469;&#30830;&#20445;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#29702;&#35770;&#21644;&#21487;&#24494;&#20998;&#20984;&#35268;&#21010;&#65292;&#23558;&#21452;&#23618;&#38382;&#39064;&#31616;&#21270;&#20026;&#21333;&#23618;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02459</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20998;&#24067;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;: &#22522;&#20110;&#21487;&#24494;&#20998;&#20984;&#35268;&#21010;&#30340;&#21333;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributionally Safe Reinforcement Learning under Model Uncertainty: A Single-Level Approach by Differentiable Convex Programming. (arXiv:2310.02459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;Wasserstein&#24230;&#37327;&#26469;&#30830;&#20445;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#29702;&#35770;&#21644;&#21487;&#24494;&#20998;&#20984;&#35268;&#21010;&#65292;&#23558;&#21452;&#23618;&#38382;&#39064;&#31616;&#21270;&#20026;&#21333;&#23618;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#21095;&#28872;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65288;&#22914;&#20998;&#24067;&#20559;&#31227;&#65289;&#30340;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#65292;&#23433;&#20840;&#20445;&#35777;&#26159;&#19981;&#21487;&#22949;&#21327;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#21592;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#23433;&#20840;&#23398;&#20064;&#20013;&#33258;&#28982;&#20250;&#23548;&#33268;&#19968;&#20010;&#21452;&#23618;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#36739;&#20302;&#23618;&#27425;&#19978;&#22312;&#19981;&#30830;&#23450;&#24615;&#27169;&#31946;&#38598;&#21512;&#20869;&#35780;&#20272;&#65288;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#65289;&#23433;&#20840;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#20998;&#24067;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;Wasserstein&#24230;&#37327;&#26469;&#30830;&#20445;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25805;&#20316;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23545;&#20598;&#29702;&#35770;&#23558;&#36739;&#20302;&#23618;&#27425;&#30340;&#20248;&#21270;&#38382;&#39064;&#20174;&#26080;&#38480;&#32500;&#27010;&#29575;&#31354;&#38388;&#65288;&#29992;&#20110;&#27979;&#37327;&#20998;&#24067;&#20559;&#31227;&#65289;&#36716;&#21270;&#20026;&#26377;&#38480;&#32500;&#21442;&#25968;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20984;&#35268;&#21010;&#65292;&#23558;&#21452;&#23618;&#23433;&#20840;&#23398;&#20064;&#38382;&#39064;&#36827;&#19968;&#27493;&#31616;&#21270;&#20026;&#19968;&#20010;&#21333;&#23618;&#38382;&#39064;&#65292;&#38656;&#35201;&#20004;&#20010;&#39034;&#24207;&#35745;&#31639;&#39640;&#25928;&#27169;&#22359;&#65306;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#26469;&#20445;&#35777;&#23433;&#20840;&#24615;&#32422;&#26463;&#65292;&#19968;&#20010;&#21487;&#24494;&#20998;&#20248;&#21270;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety assurance is uncompromisable for safety-critical environments with the presence of drastic model uncertainties (e.g., distributional shift), especially with humans in the loop. However, incorporating uncertainty in safe learning will naturally lead to a bi-level problem, where at the lower level the (worst-case) safety constraint is evaluated within the uncertainty ambiguity set. In this paper, we present a tractable distributionally safe reinforcement learning framework to enforce safety under a distributional shift measured by a Wasserstein metric. To improve the tractability, we first use duality theory to transform the lower-level optimization from infinite-dimensional probability space where distributional shift is measured, to a finite-dimensional parametric space. Moreover, by differentiable convex programming, the bi-level safe learning problem is further reduced to a single-level one with two sequential computationally efficient modules: a convex quadratic program to gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#19978;&#23398;&#21040;&#30340;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#38169;&#35823;&#30340;&#20351;&#29992;&#26041;&#24335;&#34429;&#28982;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#20294;&#19982;&#27491;&#30830;&#30340;&#36138;&#23146;&#26368;&#22823;&#21270;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#30456;&#27604;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.02456</link><description>&lt;p&gt;
&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#26368;&#20339;&#20248;&#21183;&#65292;&#24182;&#23558;&#20854;&#35823;&#35299;&#20026;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Advantage from Preferences and Mistaking it for Reward. (arXiv:2310.02456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#19978;&#23398;&#21040;&#30340;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#38169;&#35823;&#30340;&#20351;&#29992;&#26041;&#24335;&#34429;&#28982;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#20294;&#19982;&#27491;&#30830;&#30340;&#36138;&#23146;&#26368;&#22823;&#21270;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#30456;&#27604;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#20154;&#31867;&#23545;&#36712;&#36857;&#29255;&#27573;&#23545;&#30340;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#36825;&#22312;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#20165;&#22522;&#20110;&#36825;&#20123;&#29255;&#27573;&#20013;&#31215;&#32047;&#30340;&#22870;&#21169;&#25110;&#20854;&#37096;&#20998;&#22238;&#25253;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23545;&#36825;&#19968;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#24576;&#30097;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#30340;&#26367;&#20195;&#20559;&#22909;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#20551;&#35774;&#20559;&#22909;&#26159;&#22522;&#20110;&#37096;&#20998;&#22238;&#25253;&#32780;&#23454;&#38469;&#19978;&#26469;&#33258;&#36951;&#25022;&#26102;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#23398;&#21040;&#30340;&#20989;&#25968;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;$\hat{A^*_r}$&#30340;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#35299;&#20915;&#20102;&#29305;&#23450;&#30340;&#38519;&#38449;&#65292;&#36825;&#31181;&#38169;&#35823;&#20551;&#35774;&#24182;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#39640;&#24230;&#21464;&#24418;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#31181;&#38169;&#35823;&#20351;&#29992;$\hat{A^*_r}$&#30340;&#26041;&#24335;&#19981;&#22914;&#36866;&#24403;&#19988;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;&#36138;&#23146;&#26368;&#22823;&#21270;$\hat{A^*_r}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\hat{A^*_r}$. From th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#21452;&#38454;&#27573;&#22478;&#24066;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22478;&#24066;&#35268;&#21010;&#26041;&#27861;&#20013;&#24573;&#30053;&#21151;&#33021;&#21306;&#20851;&#31995;&#12289;&#29983;&#25104;&#36807;&#31243;&#19981;&#31283;&#23450;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02453</link><description>&lt;p&gt;
&#21487;&#36861;&#28335;&#22478;&#24066;&#35268;&#21010;&#30340;&#21452;&#38454;&#27573;&#27969;&#24335;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Dual-stage Flows-based Generative Modeling for Traceable Urban Planning. (arXiv:2310.02453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#21452;&#38454;&#27573;&#22478;&#24066;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22478;&#24066;&#35268;&#21010;&#26041;&#27861;&#20013;&#24573;&#30053;&#21151;&#33021;&#21306;&#20851;&#31995;&#12289;&#29983;&#25104;&#36807;&#31243;&#19981;&#31283;&#23450;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#35268;&#21010;&#22312;&#24403;&#20195;&#31038;&#20250;&#30340;&#39640;&#36895;&#22478;&#24066;&#21270;&#36827;&#31243;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#30340;&#20154;&#24037;&#35268;&#21010;&#26041;&#27861;&#22797;&#26434;&#19988;&#32321;&#37325;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#27493;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#33258;&#21160;&#21270;&#35268;&#21010;&#25216;&#26415;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38480;&#21046;&#65306;1&#65289;&#24573;&#30053;&#22478;&#24066;&#21151;&#33021;&#21306;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#21151;&#33021;&#21306;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;2&#65289;&#29983;&#25104;&#36807;&#31243;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#21363;&#21452;&#38454;&#27573;&#22478;&#24066;&#27969;(DSUF)&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#38454;&#27573;&#21033;&#29992;&#21306;&#22495;&#32423;&#22478;&#24066;&#35268;&#21010;&#27969;&#26469;&#29983;&#25104;&#22522;&#20110;&#32473;&#23450;&#37197;&#32622;&#30340;&#22478;&#24066;&#21151;&#33021;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban planning, which aims to design feasible land-use configurations for target areas, has become increasingly essential due to the high-speed urbanization process in the modern era. However, the traditional urban planning conducted by human designers can be a complex and onerous task. Thanks to the advancement of deep learning algorithms, researchers have started to develop automated planning techniques. While these models have exhibited promising results, they still grapple with a couple of unresolved limitations: 1) Ignoring the relationship between urban functional zones and configurations and failing to capture the relationship among different functional zones. 2) Less interpretable and stable generation process. To overcome these limitations, we propose a novel generative framework based on normalizing flows, namely Dual-stage Urban Flows (DSUF) framework. Specifically, the first stage is to utilize zone-level urban planning flows to generate urban functional zones based on give
&lt;/p&gt;</description></item><item><title>Feather&#26159;&#19968;&#31181;&#20248;&#38597;&#30340;DNN&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20855;&#26377;&#39640;&#25928;&#30340;&#31232;&#30095;&#35757;&#32451;&#27169;&#22359;&#21644;&#24378;&#22823;&#30340;&#30452;&#36890;&#36807;&#20272;&#35745;&#22120;&#26680;&#24515;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#31232;&#30095;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22312;ImageNet&#19978;&#20351;&#29992;ResNet-50&#26550;&#26500;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02448</link><description>&lt;p&gt;
Feather:&#19968;&#31181;&#20248;&#38597;&#30340;&#35299;&#20915;DNN&#31232;&#30095;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Feather: An Elegant Solution to Effective DNN Sparsification. (arXiv:2310.02448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02448
&lt;/p&gt;
&lt;p&gt;
Feather&#26159;&#19968;&#31181;&#20248;&#38597;&#30340;DNN&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20855;&#26377;&#39640;&#25928;&#30340;&#31232;&#30095;&#35757;&#32451;&#27169;&#22359;&#21644;&#24378;&#22823;&#30340;&#30452;&#36890;&#36807;&#20272;&#35745;&#22120;&#26680;&#24515;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#31232;&#30095;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22312;ImageNet&#19978;&#20351;&#29992;ResNet-50&#26550;&#26500;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#21098;&#26525;&#21487;&#20197;&#36890;&#36807;&#22810;&#21608;&#26399;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#36807;&#31243;&#26469;&#25191;&#34892;&#65292;&#20294;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#22312;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#21253;&#21547;&#31232;&#30095;&#21270;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Feather&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#31232;&#30095;&#35757;&#32451;&#27169;&#22359;&#65292;&#20854;&#26680;&#24515;&#26159;&#24378;&#22823;&#30340;&#30452;&#36890;&#36807;&#20272;&#35745;&#22120;&#65292;&#37197;&#21512;&#19968;&#20010;&#26032;&#30340;&#38408;&#20540;&#31639;&#23376;&#21644;&#26799;&#24230;&#32553;&#25918;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#31232;&#30095;&#21270;&#24615;&#33021;&#12290;&#22312;CIFAR&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#35777;&#26126;&#20102;Feather&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#32780;&#22312;ImageNet&#19978;&#65292;&#23427;&#20351;&#29992;ResNet-50&#26550;&#26500;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;Top-1&#39564;&#35777;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26356;&#22797;&#26434;&#12289;&#35745;&#31639;&#37327;&#26356;&#22823;&#30340;&#26041;&#27861;&#65292;&#24046;&#36317;&#24456;&#22823;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312; https://git...
&lt;/p&gt;
&lt;p&gt;
Neural Network pruning is an increasingly popular way for producing compact and efficient models, suitable for resource-limited environments, while preserving high performance. While the pruning can be performed using a multi-cycle training and fine-tuning process, the recent trend is to encompass the sparsification process during the standard course of training. To this end, we introduce Feather, an efficient sparse training module utilizing the powerful Straight-Through Estimator as its core, coupled with a new thresholding operator and a gradient scaling technique, enabling robust, out-of-the-box sparsification performance. Feather's effectiveness and adaptability is demonstrated using various architectures on the CIFAR dataset, while on ImageNet it achieves state-of-the-art Top-1 validation accuracy using the ResNet-50 architecture, surpassing existing methods, including more complex and computationally heavy ones, by a considerable margin. Code is publicly available at https://git
&lt;/p&gt;</description></item><item><title>&#19968;&#39033;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#21644;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24179;&#34913;&#32445;&#32422;&#22320;&#38081;&#23548;&#33322;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02447</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#32445;&#32422;&#22320;&#38081;&#23548;&#33322;&#26356;&#23433;&#20840;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
Machine learning assist nyc subway navigation safer and faster. (arXiv:2310.02447v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02447
&lt;/p&gt;
&lt;p&gt;
&#19968;&#39033;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#21644;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24179;&#34913;&#32445;&#32422;&#22320;&#38081;&#23548;&#33322;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#23548;&#33322;&#36719;&#20214;&#22914;&#35895;&#27468;&#22320;&#22270;&#21644;&#33529;&#26524;&#22320;&#22270;&#24448;&#24448;&#32570;&#20047;&#25552;&#20379;&#20197;&#23433;&#20840;&#20026;&#20248;&#20808;&#30340;&#36335;&#32447;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#22987;&#32456;&#26159;&#35768;&#22810;&#20154;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23433;&#20840;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#27491;&#22312;&#35774;&#35745;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#23433;&#20840;&#36335;&#32447;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25512;&#23548;&#23433;&#20840;&#31995;&#25968;&#65292;&#37319;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23558;&#22522;&#20110;&#21508;&#20010;&#22320;&#38081;&#31449;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#65292;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#26368;&#20934;&#30830;&#30340;&#23433;&#20840;&#31995;&#25968;&#20272;&#35745;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#23545;&#19981;&#21516;&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#26681;&#25454;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35780;&#20272;&#23427;&#20204;&#22312;&#21516;&#26102;&#32771;&#34385;&#23433;&#20840;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream navigation software, like Google and Apple Maps, often lacks the ability to provide routes prioritizing safety. However, safety remains a paramount concern for many. Our aim is to strike a balance between safety and efficiency. To achieve this, we're devising an Integer Programming model that takes into account both the shortest path and the safest route. We will harness machine learning to derive safety coefficients, employing methodologies such as generalized linear models, linear regression, and recurrent neural networks. Our evaluation will be based on the Root Mean Square Error (RMSE) across various subway stations, helping us identify the most accurate model for safety coefficient estimation. Furthermore, we'll conduct a comprehensive review of different shortest-path algorithms, assessing them based on time complexity and real-world data to determine their appropriateness in merging both safety and time efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>GenCO&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02442</link><description>&lt;p&gt;
GenCO: &#29992;&#20110;&#20855;&#26377;&#32452;&#21512;&#29305;&#24449;&#30340;&#35774;&#35745;&#38382;&#39064;&#30340;&#29983;&#25104;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature. (arXiv:2310.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02442
&lt;/p&gt;
&lt;p&gt;
GenCO&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GAN&#25110;VAE&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#35937;&#65288;&#22914;&#22270;&#20687;&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#35768;&#22810;&#20256;&#32479;&#19978;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36229;&#36234;&#22270;&#20687;&#29983;&#25104;&#65292;&#22312;&#26356;&#19968;&#33324;&#30340;&#35774;&#35745;&#38382;&#39064;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#35774;&#35745;&#30340;&#22810;&#26679;&#24615;&#21644;&#32422;&#26463;&#30340;&#19968;&#33268;&#24615;&#37117;&#24456;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#35774;&#32622;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#21160;&#30011;&#12289;&#24037;&#19994;&#35774;&#35745;&#12289;&#26448;&#26009;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#36981;&#24490;&#31163;&#25955;/&#32452;&#21512;&#32422;&#26463;&#24182;&#24809;&#32602;&#20219;&#20309;&#20559;&#31163;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#35828;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenCO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#26088;&#22312;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse objects (e.g., images) using generative models (such as GAN or VAE) has achieved impressive results in the recent years, to help solve many design problems that are traditionally done by humans. Going beyond image generation, we aim to find solutions to more general design problems, in which both the diversity of the design and conformity of constraints are important. Such a setting has applications in computer graphics, animation, industrial design, material science, etc, in which we may want the output of the generator to follow discrete/combinatorial constraints and penalize any deviation, which is non-trivial with existing generative models and optimization solvers. To address this, we propose GenCO, a novel framework that conducts end-to-end training of deep generative models integrated with embedded combinatorial solvers, aiming to uncover high-quality solutions aligned with nonlinear objectives. While structurally akin to conventional generative models, GenCO 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#23558;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#27010;&#24565;&#21270;&#20026;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;EMT&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#21457;&#29616;&#21463;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#25581;&#31034;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25581;&#31034;&#21464;&#37327;&#30340;&#26102;&#38388;&#23384;&#20648;&#21644;&#32452;&#21512;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2310.02430</link><description>&lt;p&gt;
&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#26426;&#21046;&#35299;&#37322;&#30340;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks. (arXiv:2310.02430v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02430
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#23558;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#27010;&#24565;&#21270;&#20026;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;EMT&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#21457;&#29616;&#21463;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#25581;&#31034;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25581;&#31034;&#21464;&#37327;&#30340;&#26102;&#38388;&#23384;&#20648;&#21644;&#32452;&#21512;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#22797;&#26434;&#25805;&#20316;&#23545;&#20110;&#25512;&#21160;&#20854;&#33021;&#21147;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#36861;&#27714;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#35828;&#26126;&#20102;RNN&#21487;&#20197;&#34987;&#27010;&#24565;&#21270;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#12290;&#20026;&#20102;&#35777;&#23454;EMT&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;RNN&#30340;&#21464;&#37327;&#32465;&#23450;&#34892;&#20026;&#12290;&#21033;&#29992;EMT&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#25968;&#23398;&#20005;&#35880;&#30340;&#30005;&#36335;&#65292;&#29992;&#20110;&#20419;&#36827;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21464;&#37327;&#32465;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#20174;&#32780;&#34920;&#26126;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23450;&#20041;&#19968;&#20010;&#29305;&#26435;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#22312;&#26102;&#38388;&#20648;&#23384;&#21644;&#32452;&#21512;&#21464;&#37327;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#25104;&#21151;&#25512;&#24191;&#36825;&#20123;&#20219;&#21153;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. 
&lt;/p&gt;</description></item><item><title>EGraFFBench&#23545;&#20845;&#31181;EGraFF&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.02428</link><description>&lt;p&gt;
EGraFFBench: &#29992;&#20110;&#21407;&#23376;&#27169;&#25311;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21147;&#22330;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations. (arXiv:2310.02428v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02428
&lt;/p&gt;
&lt;p&gt;
EGraFFBench&#23545;&#20845;&#31181;EGraFF&#31639;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#21147;&#22330;(EGraFF)&#22312;&#24314;&#27169;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#26032;&#22411;&#26550;&#26500;&#30340;&#24320;&#21457;&#28526;&#65292;&#36825;&#20123;&#26550;&#26500;&#23558;&#31561;&#21464;&#24615;&#30340;&#24402;&#32435;&#20559;&#35265;&#19982;&#22270;&#21464;&#25442;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#26550;&#26500;&#21019;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24314;&#27169;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#23545;&#36825;&#20123;&#20351;&#29992;EGraFF&#36827;&#34892;&#23454;&#38469;&#21407;&#23376;&#27169;&#25311;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#36824;&#32570;&#20047;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#23545;6&#31181;EGraFF&#31639;&#27861;(NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet)&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#23454;&#38469;&#21407;&#23376;&#27169;&#25311;&#20013;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#38500;&#20102;&#23545;&#22522;&#20110;&#22522;&#20934;&#27979;&#35797;&#25991;&#29486;&#30340;&#20843;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#21644;&#20998;&#26512;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs' inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks.
&lt;/p&gt;</description></item><item><title>Delta-AI&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02423</link><description>&lt;p&gt;
Delta-AI: &#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#20013;&#30340;&#23616;&#37096;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Delta-AI: Local objectives for amortized inference in sparse graphical models. (arXiv:2310.02423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02423
&lt;/p&gt;
&lt;p&gt;
Delta-AI&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#22270;&#27169;&#22411;&#30340;&#25674;&#36824;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#21644;&#31163;&#31574;&#30053;&#35757;&#32451;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#30340;&#25674;&#36824;&#25512;&#29702;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Delta-AI&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24403;PGM&#20013;&#30340;&#21464;&#37327;&#37319;&#26679;&#34987;&#35270;&#20026;&#19968;&#20010;&#20195;&#29702;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#24207;&#21015;&#26102;&#65292;PGM&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#23398;&#20064;&#30446;&#26631;&#33021;&#22815;&#36827;&#34892;&#23616;&#37096;&#20449;&#29992;&#20998;&#37197;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23616;&#37096;&#32422;&#26463;&#65292;&#21487;&#20197;&#36716;&#21270;&#20026;&#31867;&#20284;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#20013;&#30340;&#23616;&#37096;&#25439;&#22833;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31163;&#31574;&#30053;&#35757;&#32451;&#65292;&#20294;&#36991;&#20813;&#20102;&#27599;&#20010;&#21442;&#25968;&#26356;&#26032;&#38656;&#35201;&#23454;&#20363;&#21270;&#25152;&#26377;&#38543;&#26426;&#21464;&#37327;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#22823;&#22823;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;Delta-AI&#30446;&#26631;&#19982;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#23398;&#20064;&#37319;&#26679;&#22120;&#20013;&#30340;&#21464;&#37327;&#32473;&#23450;&#20854;&#39532;&#23572;&#21487;&#22827;&#27631;&#23376;&#30340;&#26465;&#20214;&#20998;&#24067;&#30456;&#21305;&#37197;&#65292;&#35813;&#37319;&#26679;&#22120;&#30340;&#32467;&#26500;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#22312;&#30446;&#26631;PGM&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#30340;&#37319;&#26679;&#22120;&#21487;&#20197;&#24674;&#22797;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#36793;&#38469;&#20998;&#24067;&#21644;&#26465;&#20214;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\Delta$-amortized inference ($\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\Delta$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of intere
&lt;/p&gt;</description></item><item><title>OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.02422</link><description>&lt;p&gt;
OneAdapt&#65306;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02422
&lt;/p&gt;
&lt;p&gt;
OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27969;&#23186;&#20307;&#25968;&#25454;&#30340;&#25512;&#26029;&#26041;&#38754;&#24050;&#32463;&#26222;&#21450;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;LiDAR&#25968;&#25454;&#21644;&#38899;&#39057;&#27874;&#24418;&#20013;&#30340;&#25991;&#26412;&#25552;&#21462;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#32593;&#32476;&#24102;&#23485;&#26469;&#25910;&#38598;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#24182;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;GPU&#36164;&#28304;&#26469;&#36816;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#23613;&#31649;&#36890;&#36807;&#20248;&#21270;&#37197;&#32622;&#21442;&#25968;&#65288;&#22914;&#35270;&#39057;&#20998;&#36776;&#29575;&#21644;&#24103;&#29575;&#65289;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23545;&#32593;&#32476;&#24102;&#23485;&#21644;GPU&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#20294;&#30446;&#21069;&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#19977;&#20010;&#35201;&#27714;&#65306;&#65288;i&#65289;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;GPU&#25110;&#24102;&#23485;&#24320;&#38144;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#25968;&#25454;&#23545;&#26368;&#32456;DNN&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20915;&#31574;&#65307;&#65288;iii&#65289;&#38024;&#23545;&#19968;&#31995;&#21015;&#37197;&#32622;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OneAdapt&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#21442;&#25968;&#65292;&#28385;&#36275;&#20102;&#36825;&#20123;&#35201;&#27714;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20805;&#20998;&#21033;&#29992;DNN&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' different
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02421</link><description>&lt;p&gt;
&#23398;&#29983;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#19982;&#20854;&#25945;&#24072;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02421
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#34429;&#28982;&#33021;&#23454;&#29616;&#26080;&#19982;&#20262;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#24102;&#26469;&#37096;&#32626;&#25361;&#25112;&#12290;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#31181;&#23558;&#39640;&#23481;&#37327;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#31616;&#21270;&#30340;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#24378;&#35843;&#20102;&#20854;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#36719;&#26631;&#31614;&#30340;&#23454;&#29992;&#24615;&#21644;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#25104;&#21151;&#33976;&#39311;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#23398;&#29983;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#25945;&#24072;&#30340;&#27700;&#24179;&#20197;&#21450;&#36229;&#21442;&#25968;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#20984;&#26174;&#20102;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#36716;&#25240;&#28857;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedL2P&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;&#31574;&#30053;&#30340;&#20803;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02420</link><description>&lt;p&gt;
FedL2P: &#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedL2P: Federated Learning to Personalize. (arXiv:2310.02420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedL2P&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;&#31574;&#30053;&#30340;&#20803;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#22312;&#24320;&#21457;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#29992;&#20110;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#20010;&#24615;&#21270;&#30340;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#20010;&#24615;&#21270;&#31574;&#30053;&#65292;&#29978;&#33267;&#21487;&#33021;&#26080;&#27861;&#20026;&#25152;&#26377;&#23458;&#25143;&#31471;&#23450;&#20041;&#19968;&#31181;&#26377;&#25928;&#30340;&#36890;&#29992;&#20010;&#24615;&#21270;&#31574;&#30053;&#65306;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26368;&#20248;&#39044;&#27979;&#22120;&#19982;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#31243;&#24230;&#65292;&#21487;&#33021;&#39318;&#36873;&#19981;&#21516;&#30340;&#20010;&#24615;&#21270;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#31574;&#30053;&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#36807;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#32473;&#20986;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#23398;&#20064;&#29575;&#21442;&#25968;&#30340;&#20803;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23398;&#20064;&#36825;&#20123;&#20803;&#32593;&#32476;&#65292;&#25105;&#20204;&#20801;&#35768;&#25972;&#20010;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#21512;&#20316;&#23398;&#20064;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#23450;&#21046;&#20010;&#24615;&#21270;&#31574;&#30053;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;FedL2P&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) research has made progress in developing algorithms for distributed learning of global models, as well as algorithms for local personalization of those common models to the specifics of each client's local data distribution. However, different FL problems may require different personalization strategies, and it may not even be possible to define an effective one-size-fits-all personalization strategy for all clients: depending on how similar each client's optimal predictor is to that of the global model, different personalization strategies may be preferred. In this paper, we consider the federated meta-learning problem of learning personalization strategies. Specifically, we consider meta-nets that induce the batch-norm and learning rate parameters for each client given local data statistics. By learning these meta-nets through FL, we allow the whole FL network to collaborate in learning a customized personalization strategy for each client. Empirical results s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02416</link><description>&lt;p&gt;
&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#30340;&#25216;&#24039;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#36866;&#24212;&#25968;&#25454;&#28418;&#31227;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#24039;&#21644;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#20219;&#24847;&#26410;&#26631;&#35760;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#27599;&#20010;&#20010;&#20307;&#25216;&#26415;&#30340;&#30495;&#27491;&#24433;&#21709;&#24182;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24110;&#21161;&#24041;&#22266;&#31038;&#21306;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25152;&#36873;&#27491;&#20132;TTA&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#23567;&#25209;&#37327;&#24402;&#19968;&#21270;&#12289;&#27969;&#24179;&#34913;&#12289;&#21487;&#38752;&#26679;&#26412;&#36873;&#25321;&#21644;&#32593;&#32476;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#27599;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#24863;&#20852;&#36259;&#30340;&#22330;&#26223;&#19979;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#25152;&#38656;&#35745;&#31639;&#33021;&#21147;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#20043;&#38388;&#20135;&#29983;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#32467;&#21512;&#25216;&#26415;&#26102;&#20135;&#29983;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#33021;&#22815;&#24314;&#31435;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;2&#20301;&#20302;&#20301;&#37327;&#21270;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#38382;&#39064;&#19978;&#30340;&#21387;&#21147;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#20063;&#33021;&#20445;&#25345;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02410</link><description>&lt;p&gt;
&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#65306;&#20302;&#20301;&#37327;&#21270;&#21644;&#40065;&#26834;&#24615;&#30340;&#20114;&#34917;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;2&#20301;&#20302;&#20301;&#37327;&#21270;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#38382;&#39064;&#19978;&#30340;&#21387;&#21147;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#20063;&#33021;&#20445;&#25345;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#36890;&#36807;&#19987;&#23478;&#24182;&#34892;&#24615;&#30340;&#39640;&#25928;&#27169;&#22411;&#25193;&#23637;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#26102;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#26356;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#22686;&#21152;&#30340;&#20869;&#23384;&#24102;&#23485;&#29942;&#39048;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20165;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;&#20110;&#36229;&#20302;&#20301;2&#20301;&#37327;&#21270;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;MoE&#27169;&#22411;&#30340;&#22686;&#22823;&#20869;&#23384;&#21644;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#20301;&#37327;&#21270;&#19982;MoE&#26550;&#26500;&#32467;&#21512;&#65292;&#21363;&#20351;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20063;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#23567;&#20869;&#23384;&#22823;&#23567;&#12290;&#29305;&#21035;&#22320;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#19987;&#23478;&#23618;&#23545;&#37327;&#21270;&#27604;&#20256;&#32479;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#23618;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MoE&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;&#38382;&#39064;&#65292;&#38024;&#23545;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#20013;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2310.02407</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Bug Generation in the era of Large Language Models. (arXiv:2310.02407v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;&#38382;&#39064;&#65292;&#38024;&#23545;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#20013;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65307;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#20462;&#22797;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#30340;&#26041;&#27861;&#12290;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#22797;&#26434;&#30340;&#32570;&#38519;&#65292;&#21363;&#37027;&#20123;&#24456;&#38590;&#36890;&#36807;&#27979;&#35797;&#21644;&#35843;&#35797;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#30340;&#32570;&#38519;&#12290;&#20174;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#19982;&#27491;&#30830;&#30340;&#20195;&#30721;&#22312;&#22810;&#20010;&#20301;&#32622;&#19978;&#26377;&#25152;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;&#32780;&#38590;&#20197;&#26816;&#27979;&#30340;&#32570;&#38519;&#21017;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#21487;&#36798;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#26469;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#21363;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#65292;&#22823;&#22810;&#25968;&#26159;&#19968;&#33268;&#30340;&#65307;&#32570;&#38519;&#29983;&#25104;&#25216;&#26415;&#21487;&#20197;&#23558;&#22810;&#20010;&#35821;&#21477;&#26356;&#25913;&#20026;&#20165;&#22312;&#29305;&#23450;&#36755;&#20837;&#38598;&#21512;&#19979;&#34987;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#26469;&#35828;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#26159;&#30456;&#20114;&#20914;&#31361;&#30340;&#65306;&#19968;&#20010;&#32570;&#38519;&#24212;&#35813;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27491;&#30830;&#20195;&#30721;&#30456;&#20284;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#20197;&#25361;&#25112;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bugs are essential in software engineering; many research studies in the past decades have been proposed to detect, localize, and repair bugs in software systems. Effectiveness evaluation of such techniques requires complex bugs, i.e., those that are hard to detect through testing and hard to repair through debugging. From the classic software engineering point of view, a hard-to-repair bug differs from the correct code in multiple locations, making it hard to localize and repair. Hard-to-detect bugs, on the other hand, manifest themselves under specific test inputs and reachability conditions. These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs, are mostly aligned; a bug generation technique can change multiple statements to be covered only under a specific set of inputs. However, these two objectives are conflicting for learning-based techniques: A bug should have a similar code representation to the correct code in the training data to challenge a bug predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;PCG&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02405</link><description>&lt;p&gt;
PCGPT&#65306;&#21033;&#29992;&#36716;&#25442;&#22120;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PCGPT: Procedural Content Generation via Transformers. (arXiv:2310.02405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;PCG&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;(PCG)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; PCGPT&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#36845;&#20195;&#29983;&#25104;&#28216;&#25103;&#20851;&#21345;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PCG&#26041;&#27861;&#20013;&#37325;&#22797;&#12289;&#21487;&#39044;&#27979;&#25110;&#19981;&#19968;&#33268;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;Transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23545;&#21160;&#20316;&#12289;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#36712;&#36857;&#36827;&#34892;&#24314;&#27169;&#12290;&#35813;&#26041;&#27861;&#22312;Sokoban&#30410;&#26234;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27169;&#22411;&#39044;&#27979;&#25152;&#38656;&#29289;&#21697;&#21450;&#20854;&#30456;&#24212;&#20301;&#32622;&#12290;&#22312;Sokoban&#28216;&#25103;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCGPT&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20197;&#26174;&#33879;&#36739;&#23569;&#30340;&#27493;&#39588;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#28216;&#25103;&#35774;&#35745;&#21644;&#22312;&#32447;&#20869;&#23481;&#29983;&#25104;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the PCGPT framework, an innovative approach to procedural content generation (PCG) using offline reinforcement learning and transformer networks. PCGPT utilizes an autoregressive model based on transformers to generate game levels iteratively, addressing the challenges of traditional PCG methods such as repetitive, predictable, or inconsistent content. The framework models trajectories of actions, states, and rewards, leveraging the transformer's self-attention mechanism to capture temporal dependencies and causal relationships. The approach is evaluated in the Sokoban puzzle game, where the model predicts items that are needed with their corresponding locations. Experimental results on the game Sokoban demonstrate that PCGPT generates more complex and diverse game content. Interestingly, it achieves these results in significantly fewer steps compared to existing methods, showcasing its potential for enhancing game design and online content generation. Our model repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02402</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent. (arXiv:2310.02402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29992;&#20110;&#39034;&#24207;&#27169;&#25311;&#65288;&#22914;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65289;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#65292;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#29702;&#35770;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#26420;&#32032;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;MLMC&#22312;&#29616;&#20195;GPU&#31561;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#22240;&#20026;&#20854;&#24182;&#34892;&#22797;&#26434;&#24615;&#19982;&#26420;&#32032;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#24403;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#33021;&#22815;&#35777;&#26126;&#38477;&#20302;&#24179;&#22343;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#31245;&#24046;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23545;&#20914;&#30340;&#31034;&#20363;&#26469;&#35777;&#26126;&#19982;&#26631;&#20934;MLMC&#22312;SGD&#20013;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the stochastic gradient descent (SGD) for sequential simulations such as the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC) method is known to offer better theoretical computational complexity compared to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on massively parallel computing platforms such as modern GPUs, because of its large parallel complexity which is equivalent to that of the naive Monte Carlo method. To cope with this issue, we propose the delayed MLMC gradient estimator that drastically reduces the parallel complexity of MLMC by recycling previously computed gradient components from earlier steps of SGD. The proposed estimator provably reduces the average parallel complexity per iteration at the cost of a slightly worse per-iteration convergence rate. In our numerical experiments, we use an example of deep hedging to demonstrate the superior parallel complexity of our method compared to the standard MLMC in SGD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#24212;&#23545;&#22312;&#30561;&#30496;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#26512;&#20013;&#30001;&#20110;&#20010;&#20307;&#38388;&#30340;&#39640;&#21464;&#24322;&#24615;&#32780;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#27431;&#20960;&#37324;&#24503;&#23545;&#40784;&#26469;&#35299;&#20915;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#23637;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02398</link><description>&lt;p&gt;
&#20943;&#23569;&#20154;&#21644;&#23567;&#40736;&#33041;&#22806;&#20260; EEG &#25968;&#25454;&#20013;&#31181;&#20869;&#21644;&#31181;&#38388;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#36716;&#31227;&#27431;&#20960;&#37324;&#24503;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reducing Intraspecies and Interspecies Covariate Shift in Traumatic Brain Injury EEG of Humans and Mice Using Transfer Euclidean Alignment. (arXiv:2310.02398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#24212;&#23545;&#22312;&#30561;&#30496;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#26512;&#20013;&#30001;&#20110;&#20010;&#20307;&#38388;&#30340;&#39640;&#21464;&#24322;&#24615;&#32780;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#27431;&#20960;&#37324;&#24503;&#23545;&#40784;&#26469;&#35299;&#20915;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#23637;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30561;&#30496;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#26512;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#20855;&#26377;&#26576;&#20123;&#20248;&#21183;&#65292;&#20294;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#39640;&#21464;&#24322;&#24615;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#26102;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#30456;&#21516;&#20219;&#21153;&#65292;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26410;&#24517;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#39640;&#36136;&#37327;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#20351;&#24471;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#8220;&#36716;&#31227;&#27431;&#20960;&#37324;&#24503;&#23545;&#40784;&#8221;&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#20154;&#31867;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20197;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110; EEGNet &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#27979;&#35797;&#20102;&#36825;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analytics of sleep electroencephalography (EEG) holds certain advantages over other methods in clinical applications, high variability across subjects poses a significant challenge when it comes to deploying machine learning models for classification tasks in the real world. In such instances, machine learning models that exhibit exceptional performance on a specific dataset may not necessarily demonstrate similar proficiency when applied to a distinct dataset for the same task. The scarcity of high-quality biomedical data further compounds this challenge, making it difficult to evaluate the model's generality comprehensively. In this paper, we introduce Transfer Euclidean Alignment - a transfer learning technique to tackle the problem of the dearth of human biomedical data for training deep learning models. We tested the robustness of this transfer learning technique on various rule-based classical machine learning models as well as the EEGNet-based deep learning model by evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.02396</link><description>&lt;p&gt;
&#29992;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#35757;&#32451;&#36741;&#21161;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#26399;&#26395;&#23398;&#20064;&#21487;&#20197;&#37096;&#20998;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#36741;&#21161;&#20219;&#21153;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#21253;&#25324;&#21516;&#26102;&#23398;&#20064;&#65288;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;MTL&#65289;&#21644;&#20381;&#24207;&#23398;&#20064;&#65288;&#39044;&#35757;&#32451;&#21644;&#38543;&#21518;&#24494;&#35843;&#65292;PT+FT&#65289;&#12290;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;&#20004;&#23618;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;MTL&#21644;PT+FT&#30456;&#20851;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#24809;&#32602;&#65292;&#20004;&#32773;&#37117;&#40723;&#21169;&#20219;&#21153;&#20043;&#38388;&#30340;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#31232;&#30095;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20869;&#26680;&#65288;&#25110;&#8220;&#24816;&#24615;&#8221;&#65289;&#29366;&#24577;&#21644;&#29305;&#24449;&#23398;&#20064;&#65288;&#8220;&#20016;&#23500;&#8221;&#65289;&#29366;&#24577;&#20043;&#38388;&#20855;&#26377;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;PT+FT&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#8221;&#34892;&#20026;&#65292;&#35813;&#34892;&#20026;&#26080;&#27861;&#34987;&#20219;&#20309;&#29366;&#24577;&#25152;&#25429;&#25417;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and the feature learning ("rich") regime identified in prior work. Moreover, PT+FT can exhibit a novel "nested feature learning" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02373</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#26377;&#25928;&#25968;&#25454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26080;&#25304;&#26080;&#26463;&#30340;&#25968;&#25454;&#24066;&#22330;&#38656;&#35201;&#22312;&#25968;&#25454;&#25152;&#26377;&#32773;&#21644;&#27169;&#22411;&#25152;&#26377;&#32773;&#26368;&#32456;&#20132;&#26131;&#21069;&#33021;&#22815;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;&#22810;&#26041;&#35745;&#31639;(MPC)&#26469;&#23457;&#26597;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#22522;&#20110;MPC&#30340;Transformer&#27169;&#22411;&#35780;&#20272;&#36807;&#20110;&#32791;&#36153;&#36164;&#28304;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#36873;&#25321;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;(1)&#20351;&#29992;MPC&#36827;&#34892;&#26426;&#23494;&#25968;&#25454;&#36873;&#25321;&#30340;&#24320;&#21019;&#24615;&#27969;&#31243;&#65307;(2)&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#30456;&#20851;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;MLP&#26469;&#22797;&#21046;&#22797;&#26434;&#30340;&#39640;&#32500;&#24230;&#25805;&#20316;&#65307;(3)&#24182;&#21457;&#12289;&#22810;&#38454;&#27573;&#22320;&#23454;&#29616;MPC&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#30452;&#25509;&#22522;&#20110;MPC&#30340;&#35780;&#20272;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02368</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#27979;&#35797;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21019;&#24314;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;&#32500;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#21253;&#25324;&#33258;&#21160;&#21019;&#24314;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#36890;&#24120;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#19981;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#29978;&#33267;&#21253;&#21547;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#65288;&#21453;&#27169;&#24335;&#65289;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#29983;&#25104;&#30340;&#21453;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;LLM&#21487;&#20197;&#29983;&#25104;&#19981;&#33391;&#30340;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#35757;&#32451;&#20102;&#19987;&#38376;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#21033;&#29992;Proximal Policy Optimization &#65288;PPO&#65289;&#26469;&#35757;&#32451;&#36880;&#20010;&#20248;&#21270;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22870;&#21169;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#26029;&#22312;&#20998;&#24067;&#20043;&#38388;&#25554;&#20540;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#21147;&#22330;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#29289;&#29289;&#29702;&#23398;&#20013;&#30340;&#36716;&#24405;&#32452;&#23398;&#65292;&#35299;&#20915;&#20102;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#26102;&#38388;&#25968;&#25454;&#20013;&#25512;&#26029;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02366</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#38543;&#26426;&#21147;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Stochastic force inference via density estimation. (arXiv:2310.02366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#26029;&#22312;&#20998;&#24067;&#20043;&#38388;&#25554;&#20540;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#21147;&#22330;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#29289;&#29289;&#29702;&#23398;&#20013;&#30340;&#36716;&#24405;&#32452;&#23398;&#65292;&#35299;&#20915;&#20102;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#26102;&#38388;&#25968;&#25454;&#20013;&#25512;&#26029;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#29289;&#29702;&#23398;&#20013;&#65292;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#26102;&#38388;&#25968;&#25454;&#20013;&#25512;&#26029;&#21160;&#21147;&#23398;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36716;&#24405;&#32452;&#23398;&#20013;&#65292;&#23558;&#20998;&#23376;&#31243;&#24207;&#19982;&#22122;&#22768;&#20998;&#31163;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24453;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#21487;&#20197;&#22312;&#23569;&#25968;&#26102;&#38388;&#28857;&#37319;&#26679;&#21040;&#36275;&#22815;&#25968;&#37327;&#30340;&#27178;&#25130;&#38754;&#26679;&#26412;&#65292;&#24182;&#20551;&#35774;&#25105;&#20204;&#30340;&#26679;&#26412;&#26159;&#20174;&#28508;&#22312;&#30340;&#25193;&#25955;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#19982;&#28508;&#22312;&#25193;&#25955;&#36807;&#31243;&#30456;&#20851;&#30340;&#27010;&#29575;&#27969;&#65292;&#20197;&#25512;&#26029;&#22312;&#20998;&#24067;&#20043;&#38388;&#25554;&#20540;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#21147;&#22330;&#12290;&#22312;&#32473;&#23450;&#22122;&#22768;&#27169;&#22411;&#30340;&#20808;&#39564;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#27861;&#26469;&#21306;&#20998;&#21147;&#22330;&#19982;&#20869;&#22312;&#22122;&#22768;&#12290;&#36890;&#36807;&#30456;&#20851;&#30340;&#29983;&#29289;&#29289;&#29702;&#23454;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#38750;&#24179;&#31283;&#25968;&#25454;&#20013;&#25552;&#21462;&#38750;&#20445;&#23432;&#21147;&#65292;&#24182;&#19988;&#24403;&#24212;&#29992;&#20110;&#31283;&#24577;&#25968;&#25454;&#26102;&#23398;&#20064;&#24179;&#34913;&#21160;&#21147;&#23398;&#65292;&#32780;&#19988;&#21487;&#20197;&#22788;&#29702;&#21152;&#27861;&#21644;&#20056;&#27861;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring dynamical models from low-resolution temporal data continues to be a significant challenge in biophysics, especially within transcriptomics, where separating molecular programs from noise remains an important open problem. We explore a common scenario in which we have access to an adequate amount of cross-sectional samples at a few time-points, and assume that our samples are generated from a latent diffusion process. We propose an approach that relies on the probability flow associated with an underlying diffusion process to infer an autonomous, nonlinear force field interpolating between the distributions. Given a prior on the noise model, we employ score-matching to differentiate the force field from the intrinsic noise. Using relevant biophysical examples, we demonstrate that our approach can extract non-conservative forces from non-stationary data, that it learns equilibrium dynamics when applied to steady-state data, and that it can do so with both additive and multipli
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#25506;&#31350;&#20102;&#31958;&#23615;&#30149;&#39550;&#39542;&#32773;&#22312;&#24613;&#24615;&#34880;&#31958;&#24773;&#20917;&#19979;&#30340;&#36895;&#24230;&#34892;&#20026;&#27169;&#24335;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#20998;&#24067;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02351</link><description>&lt;p&gt;
&#25506;&#31350;&#33889;&#33796;&#31958;&#31361;&#21457;&#26399;&#38388;&#30340;&#36895;&#24230;&#20559;&#24046;&#27169;&#24335;&#65306;&#19968;&#20010;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Speed Deviation Patterns During Glucose Episodes: A Quantile Regression Approach. (arXiv:2310.02351v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#25506;&#31350;&#20102;&#31958;&#23615;&#30149;&#39550;&#39542;&#32773;&#22312;&#24613;&#24615;&#34880;&#31958;&#24773;&#20917;&#19979;&#30340;&#36895;&#24230;&#34892;&#20026;&#27169;&#24335;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#20998;&#24067;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#31958;&#23615;&#30149;&#30340;&#26085;&#30410;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#31958;&#23615;&#30149;&#23545;&#39550;&#39542;&#31561;&#26085;&#24120;&#21151;&#33021;&#30340;&#24433;&#21709;&#20135;&#29983;&#20102;&#37325;&#35201;&#20852;&#36259;&#12290;&#31958;&#23615;&#30149;&#25511;&#21046;&#30340;&#24182;&#21457;&#30151;&#21253;&#25324;&#20302;&#34880;&#31958;&#21644;&#39640;&#34880;&#31958;&#31361;&#21457;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#39550;&#39542;&#25152;&#38656;&#30340;&#35748;&#30693;&#21644;&#31934;&#31070;&#36816;&#21160;&#21151;&#33021;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#25429;&#33719;&#20998;&#24067;&#27169;&#24335;&#30340;&#20998;&#24067;&#20998;&#26512;&#26041;&#27861;&#65292;&#30830;&#23450;&#31958;&#23615;&#30149;&#39550;&#39542;&#32773;&#22312;&#24613;&#24615;&#34880;&#31958;&#24773;&#20917;&#19979;&#30340;&#36895;&#24230;&#34892;&#20026;&#27169;&#24335;&#65292;&#21644;&#27491;&#24120;&#34880;&#31958;&#30340;&#31958;&#23615;&#30149;&#39550;&#39542;&#32773;&#25110;&#26080;&#31958;&#23615;&#30149;&#30340;&#23545;&#29031;&#39550;&#39542;&#32773;&#30456;&#27604;&#65292;&#20197;&#33258;&#28982;&#39550;&#39542;&#29615;&#22659;&#20026;&#32972;&#26223;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#21160;&#20102;&#20197;&#24448;&#30340;&#35770;&#25991;&#65292;&#21069;&#32773;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#36890;&#36807;&#24179;&#22343;&#36895;&#24230;&#26469;&#25506;&#32034;&#36895;&#24230;&#20559;&#24046;&#27169;&#24335;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the growing prevalence of diabetes, there has been significant interest in determining how diabetes affects instrumental daily functions, like driving. Complication of glucose control in diabetes includes hypoglycemic and hyperglycemic episodes, which may impair cognitive and psychomotor functions needed for safe driving. The goal of this paper was to determine patterns of diabetes speed behavior during acute glucose to drivers with diabetes who were euglycemic or control drivers without diabetes in a naturalistic driving environment. By employing distribution-based analytic methods which capture distribution patterns, our study advances prior literature that has focused on conventional approach of average speed to explore speed deviation patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#65292;&#38024;&#23545;&#22270;&#20687;&#20013;&#30340;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02323</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#20013;$p4m$&#32676;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images. (arXiv:2310.02323v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#65292;&#38024;&#23545;&#22270;&#20687;&#20013;&#30340;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#20302;&#28145;&#24230;&#39640;&#25928;&#27169;&#25311;&#30340;&#37327;&#23376;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#26550;&#26500;&#65292;&#38382;&#39064;&#26080;&#20851;&#30340;&#27169;&#22411;&#36890;&#24120;&#36935;&#21040;&#21487;&#35757;&#32451;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;GQML&#65289;&#65292;&#20351;&#29992;&#20102;&#19982;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#24213;&#23618;&#23545;&#31216;&#24615;&#31561;&#21464;&#30340;QNNs&#12290;GQML&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#27169;&#22411;&#20013;&#26469;&#20026;&#27169;&#22411;&#28155;&#21152;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#22312;&#32422;&#26463;&#25628;&#32034;&#31354;&#38388;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#20248;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\circ$ rotational symmetry. We present the results tested in diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22797;&#26434;&#21322;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#25311;&#38745;&#27490;&#28304;&#30340;&#28608;&#27963;&#24207;&#21015;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20256;&#24863;&#22120;&#25968;&#25454;&#25552;&#21462;&#20010;&#20307;&#28608;&#27963;&#24207;&#21015;&#65292;&#24182;&#23545;&#24037;&#19994;&#36807;&#31243;&#21644;&#21046;&#36896;&#31995;&#32479;&#30340;&#33021;&#28304;&#21487;&#25345;&#32493;&#24615;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.02295</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22797;&#26434;&#21322;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#25311;&#38745;&#27490;&#28304;&#28608;&#27963;&#24207;&#21015;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Complex Semi-Binary Matrix Factorization for Activation Sequence Recovery of Quasi-Stationary Sources. (arXiv:2310.02295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22797;&#26434;&#21322;&#20108;&#36827;&#21046;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#25311;&#38745;&#27490;&#28304;&#30340;&#28608;&#27963;&#24207;&#21015;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20256;&#24863;&#22120;&#25968;&#25454;&#25552;&#21462;&#20010;&#20307;&#28608;&#27963;&#24207;&#21015;&#65292;&#24182;&#23545;&#24037;&#19994;&#36807;&#31243;&#21644;&#21046;&#36896;&#31995;&#32479;&#30340;&#33021;&#28304;&#21487;&#25345;&#32493;&#24615;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;&#21487;&#25345;&#32493;&#12289;&#20855;&#26377;&#38887;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24037;&#19994;&#30340;&#21516;&#26102;&#65292;&#24037;&#19994;5.0&#30340;&#19977;&#20010;&#25903;&#26609;&#38656;&#35201;&#22686;&#21152;&#23545;&#24037;&#19994;&#36807;&#31243;&#21644;&#21046;&#36896;&#31995;&#32479;&#20197;&#21450;&#20854;&#33021;&#28304;&#21487;&#25345;&#32493;&#24615;&#30340;&#29702;&#35299;&#12290;&#29702;&#35299;&#30340;&#19968;&#20010;&#22522;&#26412;&#35201;&#32032;&#26159;&#30693;&#36947;&#31995;&#32479;&#20309;&#26102;&#36816;&#34892;&#65292;&#36825;&#23545;&#23450;&#20301;&#39640;&#33021;&#32791;&#23376;&#31995;&#32479;&#21644;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#24120;&#24120;&#32570;&#20047;&#36825;&#26679;&#30340;&#30693;&#35782;&#12290;&#21487;&#20197;&#36890;&#36807;&#20256;&#24863;&#22120;&#25968;&#25454;&#24674;&#22797;&#28608;&#27963;&#29366;&#24577;&#12290;&#19968;&#20123;&#38750;&#20405;&#20837;&#24335;&#20256;&#24863;&#22120;&#65288;&#21152;&#36895;&#24230;&#35745;&#12289;&#30005;&#27969;&#20256;&#24863;&#22120;&#31561;&#65289;&#33719;&#21462;&#21253;&#21547;&#22810;&#20010;&#25191;&#34892;&#22120;&#20449;&#24687;&#30340;&#28151;&#21512;&#20449;&#21495;&#12290;&#23613;&#31649;&#23427;&#20204;&#30417;&#27979;&#30340;&#31995;&#32479;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#21495;&#22788;&#29702;&#26469;&#25552;&#21462;&#20010;&#20307;&#28608;&#27963;&#24207;&#21015;&#12290;&#20026;&#27492;&#65292;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#21487;&#20197;&#25552;&#21462;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20027;&#35201;&#21160;&#24577;&#12290;&#33879;&#21517;&#30340;&#23383;&#20856;&#23398;&#20064;&#31639;&#27861;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advocating for a sustainable, resilient and human-centric industry, the three pillars of Industry 5.0 call for an increased understanding of industrial processes and manufacturing systems, as well as their energy sustainability. One of the most fundamental elements of comprehension is knowing when the systems are operated, as this is key to locating energy intensive subsystems and operations. Such knowledge is often lacking in practice. Activation statuses can be recovered from sensor data though. Some non-intrusive sensors (accelerometers, current sensors, etc.) acquire mixed signals containing information about multiple actuators at once. Despite their low cost as regards the fleet of systems they monitor, additional signal processing is required to extract the individual activation sequences. To that end, sparse regression techniques can extract leading dynamics in sequential data. Notorious dictionary learning algorithms have proven effective in this regard. This paper considers di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#20013;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#19979;&#30340;&#20351;&#29992;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2310.02286</link><description>&lt;p&gt;
&#26080;&#32593;&#26684;&#21487;&#24494;&#20998;&#32534;&#31243;&#19982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints. (arXiv:2310.02286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#20013;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#19979;&#30340;&#20351;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#21160;&#24494;&#20998;&#24211;&#30340;&#24433;&#21709;&#19979;&#65292;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21464;&#21270;&#12290;&#25105;&#20204;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;(DAL)&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;(PINN)&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;(DP)&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#20351;&#29992;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#36890;&#29992;&#26080;&#32593;&#26684;&#21487;&#24494;&#20998;PDE&#27714;&#35299;&#22120;&#12290;&#22312;&#25289;&#26222;&#25289;&#26031;&#21644;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;DP&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20135;&#29983;&#20102;&#26368;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#21363;&#20351;DAL&#22833;&#36133;&#21644;PINN&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26368;&#20248;&#25511;&#21046;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#25351;&#21335;&#65292;&#24182;&#36827;&#19968;&#27493;&#36830;&#25509;&#20102;&#20182;&#20204;&#19982;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Optimal Control under Partial Differential Equations (PDE) constraints is rapidly changing under the influence of Deep Learning and the accompanying automatic differentiation libraries. Novel techniques like Physics-Informed Neural Networks (PINNs) and Differentiable Programming (DP) are to be contrasted with established numerical schemes like Direct-Adjoint Looping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using a general-purpose mesh-free differentiable PDE solver based on Radial Basis Functions. Under Laplace and Navier-Stokes equations, we found DP to be extremely effective as it produces the most accurate gradients; thriving even when DAL fails and PINNs struggle. Additionally, we provide a detailed benchmark highlighting the limited conditions under which any of those methods can be efficiently used. Our work provides a guide to Optimal Control practitioners and connects them further to the Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#30340;&#20154;&#32676;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02284</link><description>&lt;p&gt;
PASTA: &#24182;&#34892;&#26102;&#31354;&#27880;&#24847;&#21147;&#19982;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#29992;&#20110;&#32454;&#31890;&#24230;&#20154;&#32676;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation gating for fine-grained crowd flow prediction. (arXiv:2310.02284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#30340;&#20154;&#32676;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22478;&#24066;&#20013;&#29289;&#20307;&#65288;&#22914;&#20154;&#31867;&#21644;&#36710;&#36742;&#65289;&#30340;&#36816;&#21160;&#27169;&#24335;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#22478;&#24066;&#35268;&#21010;&#21644;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#32454;&#31890;&#24230;&#22478;&#24066;&#22320;&#22270;&#20013;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#20154;&#32676;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;&#25105;&#20204;&#26041;&#27861;&#20013;&#30340;&#26032;&#39062;&#32452;&#20214;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#12290;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#37319;&#29992;&#31354;&#38388;&#32479;&#35745;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#19981;&#35268;&#21017;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#36127;&#36131;&#22788;&#29702;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#22810;&#20010;&#33539;&#22260;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#21017;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the movement patterns of objects (e.g., humans and vehicles) in a city is essential for many applications, including city planning and management. This paper proposes a method for predicting future city-wide crowd flows by modeling the spatio-temporal patterns of historical crowd flows in fine-grained city-wide maps. We introduce a novel neural network named PArallel Spatio-Temporal Attention with spatial auto-correlation gating (PASTA) that effectively captures the irregular spatio-temporal patterns of fine-grained maps. The novel components in our approach include spatial auto-correlation gating, multi-scale residual block, and temporal attention gating module. The spatial auto-correlation gating employs the concept of spatial statistics to identify irregular spatial regions. The multi-scale residual block is responsible for handling multiple range spatial dependencies in the fine-grained map, and the temporal attention gating filters out irrelevant temporal information
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#25913;&#36827;&#65292;&#21516;&#26102;&#20026;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.02282</link><description>&lt;p&gt;
&#20351;&#29992;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#30340;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#21450;&#20854;&#22312;&#36710;&#36742;&#36712;&#36857;&#36895;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed Prediction using Road Topographical Features. (arXiv:2310.02282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#25913;&#36827;&#65292;&#21516;&#26102;&#20026;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#36890;&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#21482;&#22312;&#29305;&#23450;&#21306;&#22495;&#21487;&#29992;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#34429;&#28982;&#26377;&#30740;&#31350;&#23545;&#36825;&#20123;&#25968;&#25454;&#32473;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22320;&#21306;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#25551;&#36848;&#20840;&#29699;&#20854;&#20182;&#22320;&#21306;&#30340;&#25152;&#26377;&#20132;&#36890;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#12290;&#20026;&#20102;&#39044;&#27979;&#36710;&#36742;&#30340;&#36895;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#19968;&#20010;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#26631;&#20934;&#22238;&#24402;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#20026;&#35774;&#35745;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although traffic is one of the massively collected data, it is often only available for specific regions. One concern is that, although there are studies that give good results for these data, the data from these regions may not be sufficiently representative to describe all the traffic patterns in the rest of the world. In quest of addressing this concern, we propose a speed prediction method that is independent of large historical speed data. To predict a vehicle's speed, we use the trajectory road topographical features to fit a Shared Weight Multilayer Perceptron learning model. Our results show significant improvement, both qualitative and quantitative, over standard regression analysis. Moreover, the proposed framework sheds new light on the way to design new approaches for traffic analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02281</link><description>&lt;p&gt;
&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#30340;&#31471;&#21040;&#31471;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations. (arXiv:2310.02281v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#20114;&#21160;&#36136;&#37327;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#19982;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#19981;&#21516;&#65292;&#23454;&#38469;&#23545;&#35805;&#21457;&#29983;&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#65292;&#24182;&#21463;&#21040;&#24433;&#21709;&#24773;&#24863;&#34920;&#36798;&#30340;&#24773;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#38598;&#65288;CusEmo&#65289;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#23454;&#38469;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#23545;&#24773;&#22659;&#20449;&#24687;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#23558;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30830;&#23450;&#36866;&#24403;&#30340;&#26631;&#31614;&#37319;&#26679;&#29575;&#21644;&#36755;&#20837;&#29255;&#27573;&#38271;&#24230;&#65292;&#20197;&#21450;&#25972;&#21512;&#24773;&#22659;&#20449;&#24687;&#65288;&#23545;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion recognition (SER) in call center conversations has emerged as a valuable tool for assessing the quality of interactions between clients and agents. In contrast to controlled laboratory environments, real-life conversations take place under uncontrolled conditions and are subject to contextual factors that influence the expression of emotions. In this paper, we present our approach to constructing a large-scale reallife dataset (CusEmo) for continuous SER in customer service call center conversations. We adopted the dimensional emotion annotation approach to capture the subtlety, complexity, and continuity of emotions in real-life call center conversations, while annotating contextual information. The study also addresses the challenges encountered during the application of the End-to-End (E2E) SER system to the dataset, including determining the appropriate label sampling rate and input segment length, as well as integrating contextual information (interlocutor's gender 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E-DTWA&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#24182;&#21152;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02280</link><description>&lt;p&gt;
&#19987;&#23478;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expert enhanced dynamic time warping based anomaly detection. (arXiv:2310.02280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E-DTWA&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#24182;&#21152;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24377;&#24615;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#33879;&#21517;&#31639;&#27861;&#12290;&#20854;&#22788;&#29702;&#38750;&#32447;&#24615;&#26102;&#38388;&#25197;&#26354;&#30340;&#33021;&#21147;&#20351;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#20013;&#24456;&#26377;&#24110;&#21161;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#24322;&#24120;&#26816;&#27979;&#65292;&#23427;&#35797;&#22270;&#25581;&#31034;&#20986;&#24847;&#22806;&#30340;&#34892;&#20026;&#32780;&#27809;&#26377;&#38169;&#35823;&#30340;&#26816;&#27979;&#35686;&#25253;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19987;&#23478;&#22686;&#24378;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#24322;&#24120;&#26816;&#27979;&#65288;E-DTWA&#65289;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;DTW&#65292;&#24182;&#22312;&#20854;&#20013;&#21152;&#20837;&#20102;&#19982;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#21253;&#25324;&#39640;&#25928;&#30340;&#26816;&#27979;&#65292;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic time warping (DTW) is a well-known algorithm for time series elastic dissimilarity measure. Its ability to deal with non-linear time distortions makes it helpful in variety of data mining tasks. Such a task is also anomaly detection which attempts to reveal unexpected behaviour without false detection alarms. In this paper, we propose a novel anomaly detection method named Expert enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW with additional enhancements involving human-in-the-loop concept. The main benefits of our approach comprise efficient detection, flexible retraining based on strong consideration of the expert's detection feedback while retaining low computational and space complexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;</title><link>http://arxiv.org/abs/2310.02277</link><description>&lt;p&gt;
"&#22403;&#22334;DNA&#20551;&#35774;&#65306;&#36890;&#36807;&#31232;&#30095;&#24615;&#23545;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#36827;&#34892;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#20998;&#26512;"
&lt;/p&gt;
&lt;p&gt;
Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#23545;"&#22403;&#22334;DNA"&#30340;&#27010;&#24565;&#38271;&#26399;&#20197;&#26469;&#19982;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#30340;&#38750;&#32534;&#30721;&#29255;&#27573;&#30456;&#20851;&#32852;&#65292;&#21344;&#20854;&#32452;&#25104;&#30340;&#22823;&#32422;98%&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#36825;&#20123;&#30475;&#20284;&#26080;&#21151;&#33021;&#30340;DNA&#24207;&#21015;&#22312;&#32454;&#32990;&#36807;&#31243;&#20013;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#19982;&#20154;&#31867;&#22522;&#22240;&#20013;&#35266;&#23519;&#21040;&#30340;&#20887;&#20313;&#24615;&#26377;&#30528;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21253;&#21547;&#20102;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#21453;&#35770;&#26469;&#25361;&#25112;&#36825;&#20010;&#20256;&#32479;&#35266;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#26469;&#29420;&#31435;&#32780;&#20934;&#30830;&#22320;&#37327;&#21270;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20302;&#24133;&#24230;&#26435;&#37325;&#30340;&#32454;&#24494;&#37325;&#35201;&#24615;&#65292;&#20174;&#19979;&#28216;&#20219;&#21153;&#20013;&#24515;&#30340;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#21253;&#21547;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#30340;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26469;&#23398;&#20064;1D&#21644;2D&#39281;&#21644;&#38750;&#32447;&#24615;&#34203;&#23450;&#35860;&#26041;&#31243;&#20013;&#30340;&#23396;&#23376;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#21183;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#33021;&#35299;&#20915;PT&#23545;&#31216;&#21183;&#20989;&#25968;&#30340;&#36870;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#20256;&#25773;&#36317;&#31163;z&#30456;&#20851;&#30340;1D&#21644;2D PT&#23545;&#31216;&#21183;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.02276</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39281;&#21644;&#38750;&#32447;&#24615;&#34203;&#23450;&#35860;&#26041;&#31243;&#20013;&#30340;&#23396;&#23376;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep learning soliton dynamics and complex potentials recognition for 1D and 2D PT-symmetric saturable nonlinear Schr\"odinger equations. (arXiv:2310.02276v1 [nlin.PS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26469;&#23398;&#20064;1D&#21644;2D&#39281;&#21644;&#38750;&#32447;&#24615;&#34203;&#23450;&#35860;&#26041;&#31243;&#20013;&#30340;&#23396;&#23376;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#21183;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#33021;&#35299;&#20915;PT&#23545;&#31216;&#21183;&#20989;&#25968;&#30340;&#36870;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#20256;&#25773;&#36317;&#31163;z&#30456;&#20851;&#30340;1D&#21644;2D PT&#23545;&#31216;&#21183;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#23398;&#20064;&#20809;&#32420;&#20013;&#20855;&#26377;&#20004;&#31181;&#22522;&#26412;&#30340;PT&#23545;&#31216;Scarf-II&#21183;&#21644;&#21608;&#26399;&#21183;&#30340;&#19968;&#32500;&#21644;&#20108;&#32500;&#39281;&#21644;&#38750;&#32447;&#24615;&#34203;&#23450;&#35860;&#26041;&#31243;(SNLSEs)&#30340;&#25968;&#25454;&#39537;&#21160;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#23396;&#23376;&#12290;&#20854;&#27425;&#65292;&#30740;&#31350;&#20102;PT&#23545;&#31216;&#21183;&#20989;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#36870;&#38382;&#39064;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#32500;&#21644;&#20108;&#32500;SNLSEs&#20013;&#30340;&#21183;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;PINNs(mPINNs)&#26041;&#26696;&#65292;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#30452;&#25509;&#35782;&#21035;&#19968;&#32500;&#21644;&#20108;&#32500;SNLSEs&#30340;PT&#21183;&#20989;&#25968;&#12290;&#32780;&#19988;&#36824;&#21033;&#29992;mPINNs&#26041;&#27861;&#30740;&#31350;&#20102;&#20851;&#20110;&#20256;&#25773;&#36317;&#31163;z&#30340;&#19968;&#32500;&#21644;&#20108;&#32500;PT&#23545;&#31216;&#21183;&#30340;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;PINNs&#24212;&#29992;&#20110;SNLSE&#30340;&#24179;&#31283;&#26041;&#31243;&#26469;&#35782;&#21035;&#21183;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#22312;&#19981;&#21516;&#21442;&#25968;&#26465;&#20214;&#19979;&#27604;&#36739;&#20102;&#20004;&#31181;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#20351;&#39044;&#27979;&#30340;PT&#21183;&#33021;&#36798;&#21040;&#30456;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we firstly extend the physics-informed neural networks (PINNs) to learn data-driven stationary and non-stationary solitons of 1D and 2D saturable nonlinear Schr\"odinger equations (SNLSEs) with two fundamental PT-symmetric Scarf-II and periodic potentials in optical fibers. Secondly, the data-driven inverse problems are studied for PT-symmetric potential functions discovery rather than just potential parameters in the 1D and 2D SNLSEs. Particularly, we propose a modified PINNs (mPINNs) scheme to identify directly the PT potential functions of the 1D and 2D SNLSEs by the solution data. And the inverse problems about 1D and 2D PT -symmetric potentials depending on propagation distance z are also investigated using mPINNs method. We also identify the potential functions by the PINNs applied to the stationary equation of the SNLSE. Furthermore, two network structures are compared under different parameter conditions such that the predicted PT potentials can achieve the simil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MuSe-GNN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21333;&#32454;&#32990;&#27979;&#24207;&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#30456;&#20284;&#24615;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;&#36328;&#25968;&#25454;&#22522;&#22240;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#20013;&#25552;&#20379;&#21253;&#21547;&#19981;&#21516;&#32972;&#26223;&#19979;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22522;&#22240;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.02275</link><description>&lt;p&gt;
MuSe-GNN&#65306;&#20174;&#22810;&#27169;&#24577;&#29983;&#29289;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#32479;&#19968;&#30340;&#22522;&#22240;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data. (arXiv:2310.02275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MuSe-GNN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21333;&#32454;&#32990;&#27979;&#24207;&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#30456;&#20284;&#24615;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;&#36328;&#25968;&#25454;&#22522;&#22240;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#20013;&#25552;&#20379;&#21253;&#21547;&#19981;&#21516;&#32972;&#26223;&#19979;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22522;&#22240;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#22240;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#22522;&#22240;&#22312;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#32972;&#26223;&#19979;&#30340;&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#30456;&#20284;&#24615;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MuSe-GNN&#65289;&#30340;&#26032;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21333;&#32454;&#32990;&#27979;&#24207;&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#31034;&#12290;&#21033;&#29992;10&#20010;&#32452;&#32455;&#30340;82&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#19977;&#31181;&#27979;&#24207;&#25216;&#26415;&#21644;&#19977;&#20010;&#29289;&#31181;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#22270;&#32467;&#26500;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#22522;&#22240;&#34920;&#31034;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#21152;&#26435;&#30456;&#20284;&#24615;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;&#36328;&#25968;&#25454;&#22522;&#22240;&#20851;&#31995;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#30830;&#20445;&#25105;&#20204;&#21487;&#20197;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#20013;&#25552;&#20379;&#21253;&#21547;&#19981;&#21516;&#32972;&#26223;&#19979;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22522;&#22240;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering genes with similar functions across diverse biomedical contexts poses a significant challenge in gene representation learning due to data heterogeneity. In this study, we resolve this problem by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. Leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space. Comprehensive benchmarking analysis shows our model's capacity to eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ARRQP&#30340;&#23454;&#26102;QoS&#39044;&#27979;&#26694;&#26550;&#65292;&#37325;&#28857;&#25913;&#21892;&#20102;&#23545;&#25968;&#25454;&#20013;&#24322;&#24120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#25216;&#26415;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2310.02269</link><description>&lt;p&gt;
ARRQP: &#20855;&#26377;&#22270;&#21367;&#31215;&#30340;&#24322;&#24120;&#40065;&#26834;&#23454;&#26102;QoS&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution. (arXiv:2310.02269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ARRQP&#30340;&#23454;&#26102;QoS&#39044;&#27979;&#26694;&#26550;&#65292;&#37325;&#28857;&#25913;&#21892;&#20102;&#23545;&#25968;&#25454;&#20013;&#24322;&#24120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#25216;&#26415;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#38754;&#21521;&#26381;&#21153;&#30340;&#26550;&#26500;&#20013;&#65292;&#30830;&#20445;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#25552;&#21069;&#39044;&#27979;QoS&#20540;&#30340;&#33021;&#21147;&#20351;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21508;&#31181;&#38382;&#39064;&#21644;&#24322;&#24120;&#24773;&#20917;&#65288;&#21253;&#25324;&#24322;&#24120;&#20540;&#12289;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#28784;&#32650;&#23454;&#20363;&#21644;&#20919;&#21551;&#21160;&#22330;&#26223;&#65289;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;QoS&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#29305;&#21035;&#24378;&#35843;&#25913;&#21892;&#23545;&#25968;&#25454;&#20013;&#24322;&#24120;&#30340;&#40065;&#26834;&#24615;&#30340;&#23454;&#26102;QoS&#39044;&#27979;&#26694;&#26550;&#65288;&#31216;&#20026;ARRQP&#65289;&#12290;ARRQP&#21033;&#29992;&#22270;&#21367;&#31215;&#25216;&#26415;&#30340;&#21147;&#37327;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#65292;&#21363;&#20351;&#25968;&#25454;&#26377;&#38480;&#25110;&#31232;&#30095;&#12290;ARRQP&#25972;&#21512;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21327;&#20316;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of modern service-oriented architecture, ensuring Quality of Service (QoS) is of paramount importance. The ability to predict QoS values in advance empowers users to make informed decisions. However, achieving accurate QoS predictions in the presence of various issues and anomalies, including outliers, data sparsity, grey-sheep instances, and cold-start scenarios, remains a challenge. Current state-of-the-art methods often fall short when addressing these issues simultaneously, resulting in performance degradation. In this paper, we introduce a real-time QoS prediction framework (called ARRQP) with a specific emphasis on improving resilience to anomalies in the data. ARRQP utilizes the power of graph convolution techniques to capture intricate relationships and dependencies among users and services, even when the data is limited or sparse. ARRQP integrates both contextual information and collaborative insights, enabling a comprehensive understanding of user-service interac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#23376;&#36890;&#36807;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02094</link><description>&lt;p&gt;
CoNO: &#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoNO: Complex Neural Operator for Continuous Dynamical Systems. (arXiv:2310.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#23376;&#36890;&#36807;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#25193;&#23637;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#26144;&#23556;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#30001;&#24494;&#20998;&#26041;&#31243;&#34920;&#31034;&#30340;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#27969;&#21160;&#25110;&#22266;&#20307;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#23376;&#20173;&#28982;&#20381;&#36182;&#20110;&#23454;&#25968;&#31354;&#38388;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#22797;&#25968;&#31354;&#38388;&#20013;&#36890;&#36807;&#20989;&#25968;&#21464;&#25442;&#21487;&#33021;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#35813;&#31639;&#23376;&#22312;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#22495;&#20013;&#21442;&#25968;&#21270;&#31215;&#20998;&#26680;&#12290;&#21478;&#22806;&#65292;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#28151;&#21472;&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#22797;&#25968;&#20540;&#21644;&#22797;&#20195;&#25968;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12289;&#23545;&#22122;&#22768;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21333;&#19968;&#30340;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#28508;&#22312;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02075</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#30340;&#37327;&#23376;&#36807;&#31243;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#37327;&#23376;&#22522;&#20934;&#27979;&#35797;&#12289;&#23494;&#30721;&#20998;&#26512;&#21644;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#65288;QSQ&#65289;&#27169;&#22411;&#20869;&#30740;&#31350;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#36807;&#31243;&#65288;QPSQs&#65289;&#36827;&#34892;&#32479;&#35745;&#26597;&#35810;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#23450;&#20041;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;QPSQ&#23398;&#20064;&#22120;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#65292;&#24182;&#38468;&#24102;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#24212;&#29992;&#35813;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#65288;CR-QPUFs&#65289;&#30340;&#33030;&#24369;&#24615;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#26397;&#30528;&#28145;&#20837;&#29702;&#35299;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01892</link><description>&lt;p&gt;
FiGURe: &#20351;&#29992;&#36807;&#28388;&#22120;&#22686;&#24378;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#20302;&#36890;&#28388;&#27874;&#22120;&#30340;&#22686;&#24378;&#65292;&#38480;&#21046;&#20102;&#22312;&#38656;&#35201;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#36807;&#28388;&#22120;&#30340;&#22686;&#24378;&#26041;&#27861;&#26469;&#25429;&#25417;&#29305;&#24449;&#39057;&#35889;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#26435;&#37325;&#26159;&#21487;&#33021;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#38656;&#35201;&#39640;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#25968;&#25454;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#22810;&#20010;&#22686;&#24378;&#26041;&#27861;&#26102;&#65292;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#24674;&#22797;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; FiGURe &#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>PharmacoNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#30340;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;&#36807;&#31243;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#20934;&#30830;&#24615;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.00681</link><description>&lt;p&gt;
PharmacoNet: &#21033;&#29992;&#28145;&#24230;&#33647;&#29289;&#35889;&#24314;&#27169;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling. (arXiv:2310.00681v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00681
&lt;/p&gt;
&lt;p&gt;
PharmacoNet&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#30340;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#21152;&#36895;&#22823;&#35268;&#27169;&#34394;&#25311;&#31579;&#36873;&#36807;&#31243;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#20934;&#30830;&#24615;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35775;&#38382;&#21270;&#21512;&#29289;&#24211;&#30340;&#35268;&#27169;&#25193;&#22823;&#21040;&#36229;&#36807;100&#20159;&#20010;&#65292;&#23545;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34394;&#25311;&#31579;&#36873;&#26041;&#27861;&#30340;&#38656;&#27714;&#27491;&#22312;&#20986;&#29616;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;&#39044;&#31579;&#36873;&#26041;&#27861;&#26469;&#24555;&#36895;&#31579;&#36873;&#24211;&#65292;&#20294;&#36866;&#29992;&#20110;&#36890;&#29992;&#34507;&#30333;&#36136;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#65306;&#25361;&#25112;&#26159;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#39044;&#27979;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#32467;&#21512;&#20301;&#23039;&#24182;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#24341;&#20837;PharmacoNet&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20174;&#32467;&#21512;&#20301;&#28857;&#29983;&#25104;&#30340;&#26368;&#20339;&#19977;&#32500;&#33647;&#29289;&#35889;&#25490;&#21015;&#26469;&#35782;&#21035;&#37197;&#20307;&#24212;&#35813;&#20855;&#26377;&#30340;&#31283;&#23450;&#32467;&#21512;&#35201;&#27714;&#12290;&#36890;&#36807;&#31895;&#31890;&#21270;&#22270;&#21305;&#37197;&#65292;&#25105;&#20204;&#22312;&#19968;&#27493;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26114;&#36149;&#30340;&#32467;&#21512;&#20301;&#23039;&#37319;&#26679;&#21644;&#35780;&#20998;&#36807;&#31243;&#12290;PharmacoNet&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#35201;&#24555;&#24471;&#22810;&#65292;&#20294;&#20173;&#20855;&#26377;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#20877;&#21033;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of accessible compound libraries expands to over 10 billion, the need for more efficient structure-based virtual screening methods is emerging. Different pre-screening methods have been developed to rapidly screen the library, but the structure-based methods applicable to general proteins are still lacking: the challenge is to predict the binding pose between proteins and ligands and perform scoring in an extremely short time. We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement which a ligand should have for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we solve the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function. Furthermore, we show the promising re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#31867;&#22411;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#27969;&#20998;&#26512;&#12290;&#20256;&#32479;&#30340;&#31867;&#22411;&#25512;&#26029;&#22312;&#38754;&#23545;&#31243;&#24207;&#35268;&#27169;&#22686;&#38271;&#26102;&#38754;&#20020;&#24615;&#33021;&#25361;&#25112;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32479;&#35745;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#31867;&#22411;&#19978;&#24615;&#33021;&#20173;&#36739;&#24046;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00673</link><description>&lt;p&gt;
&#23398;&#20064;&#31867;&#22411;&#25512;&#26029;&#20197;&#22686;&#24378;&#25968;&#25454;&#27969;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning Type Inference for Enhanced Dataflow Analysis. (arXiv:2310.00673v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#31867;&#22411;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#27969;&#20998;&#26512;&#12290;&#20256;&#32479;&#30340;&#31867;&#22411;&#25512;&#26029;&#22312;&#38754;&#23545;&#31243;&#24207;&#35268;&#27169;&#22686;&#38271;&#26102;&#38754;&#20020;&#24615;&#33021;&#25361;&#25112;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32479;&#35745;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#31867;&#22411;&#19978;&#24615;&#33021;&#20173;&#36739;&#24046;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#20998;&#26512;&#21160;&#24577;&#31867;&#22411;&#20195;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#30475;&#20284;&#24494;&#19981;&#36275;&#36947;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#30830;&#23450;&#36807;&#31243;&#35843;&#29992;&#30340;&#30446;&#26631;&#65292;&#22312;&#27809;&#26377;&#22312;&#32534;&#35793;&#26102;&#30693;&#36947;&#23545;&#35937;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#28176;&#36827;&#24335;&#31867;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#28155;&#21152;&#21040;&#21160;&#24577;&#31867;&#22411;&#35821;&#35328;&#20013;&#65292;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;TypeScript&#65292;&#23427;&#20026;JavaScript&#24341;&#20837;&#20102;&#38745;&#24577;&#31867;&#22411;&#12290;&#28176;&#36827;&#24335;&#31867;&#22411;&#25552;&#39640;&#20102;&#24320;&#21457;&#20154;&#21592;&#39564;&#35777;&#31243;&#24207;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#31283;&#20581;&#12289;&#23433;&#20840;&#21644;&#21487;&#35843;&#35797;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#24456;&#23569;&#30452;&#25509;&#27880;&#37322;&#31867;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#31867;&#22411;&#25512;&#26029;&#22312;&#31243;&#24207;&#35268;&#27169;&#22686;&#38271;&#26102;&#38754;&#20020;&#19982;&#24615;&#33021;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32479;&#35745;&#25216;&#26415;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#25512;&#26029;&#65292;&#20294;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#25972;&#20307;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#36739;&#24046;&#65292;&#32780;&#22312;&#26368;&#24120;&#35265;&#30340;&#20869;&#32622;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#36739;&#22909;&#12290;&#36825;&#26356;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer's ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#20248;&#21270;&#25968;&#25454;&#27969;&#21644;SIMD&#23454;&#29616;&#26469;&#39640;&#25928;&#22320;&#22312;CPU&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#24133;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.00574</link><description>&lt;p&gt;
SIMD&#25968;&#25454;&#27969;&#20849;&#20248;&#21270;&#29992;&#20110;CPU&#19978;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs. (arXiv:2310.00574v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00574
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#20248;&#21270;&#25968;&#25454;&#27969;&#21644;SIMD&#23454;&#29616;&#26469;&#39640;&#25928;&#22320;&#22312;CPU&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#24133;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#22312;CPU&#19978;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#26159;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#27969;&#65288;&#21363;&#35745;&#31639;&#39034;&#24207;&#65289;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#24341;&#23548;&#20998;&#26512;&#21644;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#26469;&#25506;&#32034;&#25968;&#25454;&#37325;&#29992;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#21508;&#31181;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#23454;&#29616;&#20197;&#23454;&#29616;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#36755;&#20986;&#20445;&#25345;&#22312;SIMD&#23492;&#23384;&#22120;&#20013;&#30340;&#25968;&#25454;&#27969;&#21516;&#26102;&#26368;&#22823;&#21270;&#36755;&#20837;&#21644;&#26435;&#37325;&#37325;&#29992;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#19979;&#22987;&#32456;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#30456;&#27604;&#20170;&#22825;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#23454;&#29616;&#65292;8&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#21152;&#36895;&#27604;&#21487;&#36798;3&#20493;&#65292;&#32780;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#21152;&#36895;&#27604;&#21487;&#36798;4.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#26500;&#23545;&#25239;&#30446;&#26631;&#21644;&#24179;&#28369;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#23398;&#20064;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00357</link><description>&lt;p&gt;
&#32467;&#26500;&#23545;&#25239;&#30446;&#26631;&#29992;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structural Adversarial Objectives for Self-Supervised Representation Learning. (arXiv:2310.00357v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#23545;&#25239;&#30446;&#26631;&#21644;&#24179;&#28369;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#23398;&#20064;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39069;&#22806;&#30340;&#32467;&#26500;&#24314;&#27169;&#36131;&#20219;&#26469;&#20351;&#21028;&#21035;&#22120;&#22312;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;&#32467;&#21512;&#23545;&#32593;&#32476;&#26045;&#21152;&#30340;&#26377;&#25928;&#24179;&#28369;&#27491;&#21017;&#21270;&#22120;&#65292;&#36825;&#20123;&#30446;&#26631;&#24341;&#23548;&#21028;&#21035;&#22120;&#23398;&#20064;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22120;&#33021;&#22815;&#20174;&#39046;&#22495;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#40723;&#21169;&#21028;&#21035;&#22120;&#22312;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#23545;&#29305;&#24449;&#36827;&#34892;&#32467;&#26500;&#21270;&#22788;&#29702;&#65306;&#22312;&#31895;&#31890;&#24230;&#19978;&#23545;&#40784;&#20998;&#24067;&#29305;&#24615;&#65288;&#22914;&#22343;&#20540;&#21644;&#26041;&#24046;&#65289;&#65292;&#22312;&#32454;&#31890;&#24230;&#19978;&#23545;&#29305;&#24449;&#36827;&#34892;&#23616;&#37096;&#32858;&#31867;&#12290;&#20316;&#20026;GAN&#26694;&#26550;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#31995;&#32479;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#36825;&#26159;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#12290;&#22312;CIFAR-10/100&#21644;ImageNet&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the framework of generative adversarial networks (GANs), we propose objectives that task the discriminator for self-supervised representation learning via additional structural modeling responsibilities. In combination with an efficient smoothness regularizer imposed on the network, these objectives guide the discriminator to learn to extract informative representations, while maintaining a generator capable of sampling from the domain. Specifically, our objectives encourage the discriminator to structure features at two levels of granularity: aligning distribution characteristics, such as mean and variance, at coarse scales, and grouping features into local clusters at finer scales. Operating as a feature learner within the GAN framework frees our self-supervised system from the reliance on hand-crafted data augmentation schemes that are prevalent across contrastive representation learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments demonstrate that equippi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00270</link><description>&lt;p&gt;
SpatialRank: &#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#19982;NDCG&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#20107;&#20214;&#65289;&#30340;&#39118;&#38505;&#26368;&#39640;&#30340;&#21069;k&#20010;&#22320;&#28857;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#22478;&#24066;&#31649;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#28857;&#20043;&#38388;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#31354;&#38388;&#20013;&#22478;&#24066;&#20107;&#20214;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#20197;&#21450;&#27491;&#30830;&#23545;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#38468;&#36817;&#22320;&#28857;&#36827;&#34892;&#25490;&#21517;&#30340;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#22320;&#28857;&#30340;&#23454;&#38469;&#39118;&#38505;&#24471;&#20998;&#25110;&#20107;&#20214;&#35745;&#25968;&#12290;&#30001;&#20110;&#39044;&#27979;&#38169;&#35823;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#25490;&#21517;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#12290;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#35832;&#22914;&#26631;&#20934;&#21270;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#20043;&#31867;&#30340;&#25351;&#26631;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22320;&#28857;&#20043;&#38388;&#23384;&#22312;&#30340;&#26102;&#31354;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17357</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#31227;&#21160;&#26041;&#26696;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21270;&#35757;&#32451;&#20013;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#28145;&#23618;&#20572;&#28382;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#21463;&#38480;&#35774;&#22791;&#29615;&#22659;&#20013;&#65292;&#36138;&#23146;&#30340;&#36880;&#23618;&#25110;&#36880;&#27169;&#22359;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32469;&#36807;&#31471;&#21040;&#31471;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#27492;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20572;&#28382;&#38382;&#39064;&#65292;&#26089;&#26399;&#23618;&#36807;&#25311;&#21512;&#21644;&#26356;&#28145;&#23618;&#22312;&#19968;&#23450;&#28145;&#24230;&#21518;&#20572;&#27490;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19982;&#20998;&#24067;&#31354;&#38388;&#20013;&#26799;&#24230;&#27969;&#30340;&#26368;&#23567;&#21270;&#31227;&#21160;&#26041;&#27861;&#30456;&#21551;&#21457;&#30340;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;TRGL&#65288;Transport Regularized Greedy Learning&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20250;&#23548;&#33268;&#27169;&#22359;&#21270;&#36138;&#23146;&#26041;&#27861;&#26159;&#35268;&#21017;&#30340;&#65292;&#24182;&#36880;&#27493;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20043;&#21518;&#65292;&#21508;&#31181;&#26550;&#26500;&#65288;&#22914;ResNets&#65292;Transformers&#21644;VGG&#65289;&#30340;&#27169;&#22359;&#21270;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20854;&#20248;&#20110;&#20854;&#20182;&#27169;&#22359;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#29978;&#33267;&#32463;&#24120;&#20248;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#39640;&#36798;60%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.17348</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25191;&#34892;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;ANNs&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#25913;&#21464;&#36755;&#20837;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;ANNs&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23545;&#25239;&#35757;&#32451;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#28155;&#21152;&#20102;&#26679;&#26412;&#29992;&#20110;&#23545;&#25239;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#22686;&#21152;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26159;&#38750;&#24120;&#35745;&#31639;&#28040;&#32791;&#39640;&#30340;&#12290;&#19982;ANNs&#19981;&#21516;&#65292;&#20154;&#31867;&#19981;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;BP&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;BP&#21644;&#8220;Error to Pertu"&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15669</link><description>&lt;p&gt;
&#20851;&#20110;&#35745;&#31639;&#32416;&#32544;&#21450;&#20854;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27450;&#39575;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#65292;&#36890;&#36807;&#32416;&#32544;&#30340;&#27010;&#24565;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#32416;&#32544;&#36827;&#34892;&#20102;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#65292;&#31867;&#20284;&#20110;&#37327;&#23376;&#39046;&#22495;&#20013;&#30340;&#32416;&#32544;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#23545;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.15521</link><description>&lt;p&gt;
&#31232;&#32570;&#22270;&#20687;&#25968;&#25454;&#30340;MLOps&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis. (arXiv:2309.15521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#22312;&#32463;&#21382;&#21069;&#25152;&#26410;&#26377;&#30340;&#27969;&#34892;&#12290;ML&#27169;&#22411;&#30340;&#25805;&#20316;&#21270;&#30001;&#19968;&#32452;&#34987;&#31216;&#20026;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#65288;MLOps&#65289;&#30340;&#27010;&#24565;&#21644;&#26041;&#27861;&#25152;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#19987;&#19994;&#20154;&#21592;&#24448;&#24448;&#26356;&#22810;&#22320;&#20851;&#27880;&#33258;&#21160;&#21270;&#26041;&#38754;&#65292;&#24573;&#35270;MLOps&#30340;&#25345;&#32493;&#37096;&#32626;&#21644;&#30417;&#25511;&#26041;&#38754;&#12290;&#32467;&#26524;&#65292;&#30001;&#20110;&#27010;&#24565;&#28418;&#31227;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#31232;&#32570;&#25968;&#25454;&#26102;&#65292;&#20174;&#29983;&#20135;&#21040;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#21453;&#39304;&#32570;&#20047;&#36830;&#32493;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#20250;&#38543;&#26102;&#38388;&#19981;&#26029;&#24694;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#31232;&#32570;&#25968;&#25454;&#20998;&#26512;&#29615;&#22659;&#20013;&#23436;&#20840;&#24212;&#29992;MLOps&#30340;&#24773;&#20917;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26041;&#27861;&#26469;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#25351;&#32441;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#26681;&#25454;&#25163;&#22836;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#31574;&#30053;&#65307;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Machine Learning (ML) is experiencing tremendous popularity that has never been seen before. The operationalization of ML models is governed by a set of concepts and methods referred to as Machine Learning Operations (MLOps). Nevertheless, researchers, as well as professionals, often focus more on the automation aspect and neglect the continuous deployment and monitoring aspects of MLOps. As a result, there is a lack of continuous learning through the flow of feedback from production to development, causing unexpected model deterioration over time due to concept drifts, particularly when dealing with scarce data. This work explores the complete application of MLOps in the context of scarce data analysis. The paper proposes a new holistic approach to enhance biomedical image analysis. Our method includes: a fingerprinting process that enables selecting the best models, datasets, and model development strategy relative to the image analysis task at hand; an automated model deve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#32479;&#35745;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#25112;&#32988;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;&#24182;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14563</link><description>&lt;p&gt;
&#38754;&#21521;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#32479;&#35745;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24369;&#30417;&#30563;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20102;&#32479;&#35745;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#25112;&#32988;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;&#24182;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22823;&#23567;&#20026;N&#30340;&#26679;&#26412;&#65292;&#36873;&#25321;&#19968;&#20010;&#26356;&#23567;&#30340;&#22823;&#23567;n&lt;N&#30340;&#23376;&#26679;&#26412;&#29992;&#20110;&#32479;&#35745;&#20272;&#35745;&#25110;&#23398;&#20064;&#36890;&#24120;&#26159;&#26377;&#29992;&#30340;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#36873;&#25321;&#27493;&#39588;&#26377;&#21161;&#20110;&#20943;&#23569;&#25968;&#25454;&#26631;&#35760;&#30340;&#35201;&#27714;&#21644;&#23398;&#20064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#32473;&#23450;&#20102;N&#20010;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;{x_i}&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#8220;&#26367;&#20195;&#27169;&#22411;&#8221;&#65292;&#23427;&#21487;&#20197;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#22320;&#39044;&#27979;&#26631;&#31614;y_i&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#19968;&#20010;&#23376;&#26679;&#26412;&#38598;{&#119857;_i}&#65292;&#20854;&#22823;&#23567;&#20026;|G|=n&lt;N&#12290;&#28982;&#21518;&#25105;&#20204;&#20026;&#36825;&#20010;&#38598;&#21512;&#33719;&#21462;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#28151;&#21512;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#28176;&#36817;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#25512;&#23548;&#65292;&#25105;&#20204;&#35777;&#26126;&#65306;(i) &#25968;&#25454;&#36873;&#25321;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20987;&#36133;&#23545;&#25972;&#20010;&#26679;&#26412;&#30340;&#35757;&#32451;&#65307;(ii) &#22312;&#25968;&#25454;&#36873;&#25321;&#26041;&#38754;&#65292;&#26576;&#20123;&#27969;&#34892;&#30340;&#36873;&#25321;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a sample of size $N$, it is often useful to select a subsample of smaller size $n&lt;N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n&lt;N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14509</link><description>&lt;p&gt;
DeepSpeed Ulysses&#65306;&#29992;&#20110;&#35757;&#32451;&#26497;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#30340;&#31995;&#32479;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#25209;&#37327;&#22823;&#23567;&#12289;&#38544;&#34255;&#32500;&#24230;&#12289;&#23618;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#26469;&#25551;&#36848;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21152;&#36895;LLM&#35757;&#32451;&#30340;&#31995;&#32479;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21069;&#19977;&#20010;&#32500;&#24230;&#19978;&#65306;&#25209;&#37327;&#22823;&#23567;&#30340;&#25968;&#25454;&#24182;&#34892;&#21270;&#12289;&#38544;&#34255;&#23610;&#23544;&#30340;&#24352;&#37327;&#24182;&#34892;&#21270;&#20197;&#21450;&#27169;&#22411;&#28145;&#24230;&#25110;&#23618;&#25968;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#21270;&#12290;&#36825;&#20123;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#24182;&#34892;&#24418;&#24335;&#24182;&#19981;&#38024;&#23545;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#37492;&#20110;&#38271;&#24207;&#21015;LLM&#22312;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#19978;&#30340;&#37325;&#35201;&#24615;&#65292;&#24207;&#21015;&#24182;&#34892;&#21270;&#24341;&#36215;&#20102;&#37325;&#26032;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24207;&#21015;&#24182;&#34892;&#21270;&#24037;&#20316;&#21463;&#21040;&#20869;&#23384;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38271;&#24207;&#21015;&#22823;&#27169;&#22411;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#20415;&#25658;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13575</link><description>&lt;p&gt;
&#27010;&#29575;&#26435;&#37325;&#22266;&#23450;&#65306;&#29992;&#20110;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#33021;&#37327;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26041;&#27861;&#24120;&#24120;&#22522;&#20110;&#26435;&#37325;&#20540;&#26412;&#36523;&#36827;&#34892;&#20551;&#35774;&#65292;&#24182;&#24573;&#35270;&#20102;&#26435;&#37325;&#20301;&#32622;&#22312;&#20854;&#20013;&#25198;&#28436;&#30340;&#29420;&#29305;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#26681;&#25454;&#21333;&#20010;&#26435;&#37325;&#30340;&#20301;&#32622;&#29305;&#23450;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26469;&#30830;&#23450;&#21487;&#20197;&#23558;&#21738;&#20123;&#26435;&#37325;&#31227;&#21160;&#21040;&#21738;&#20010;&#32858;&#31867;&#20013;&#24515;&#20197;&#21450;&#31227;&#21160;&#21040;&#20160;&#20040;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21021;&#22987;&#21270;&#35774;&#32622;&#21644;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;-&#27169;&#22411;&#32452;&#21512;&#19979;&#35757;&#32451;BNNs&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#21040;&#30340;&#26435;&#37325;&#20540;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#30340;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13409</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#25968;&#25454;&#37322;&#25918;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#31574;&#30053;&#65292;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#65288;FD&#65289;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#30340;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;FD&#22312;&#20445;&#25345;&#31995;&#21015;&#35760;&#24518;&#30340;&#21516;&#26102;&#31283;&#23450;&#20102;&#23427;&#20197;&#20379;&#24314;&#27169;&#30446;&#30340;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#26469;&#33258;SPY&#25351;&#25968;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26032;&#38395;&#25253;&#36947;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#20010;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;FD&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#37319;&#29992;&#20102;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#26469;&#39564;&#35777;FD&#31995;&#21015;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FD&#30456;&#27604;&#25972;&#25968;&#24046;&#20998;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;/&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;ROCAUC&#65289;&#21644;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#30340;&#35780;&#20272;&#24471;&#21040;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#33258;&#36866;&#24212;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#33258;&#36866;&#24212;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ASRL&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#21644;CBF&#31995;&#25968;&#65292;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#38271;&#26399;&#24615;&#33021;&#12290;&#22312;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#65292;ASRL&#23398;&#20250;&#20102;&#24212;&#23545;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#24182;&#20445;&#25345;&#25104;&#26412;&#36829;&#35268;&#22312;&#25152;&#38656;&#38480;&#21046;&#20043;&#19979;&#12290;</title><link>http://arxiv.org/abs/2309.10657</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#23433;&#20840;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Adaptive Safety for Multi-Agent Systems. (arXiv:2309.10657v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#33258;&#36866;&#24212;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#33258;&#36866;&#24212;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ASRL&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#21644;CBF&#31995;&#25968;&#65292;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#38271;&#26399;&#24615;&#33021;&#12290;&#22312;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#65292;ASRL&#23398;&#20250;&#20102;&#24212;&#23545;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#24182;&#20445;&#25345;&#25104;&#26412;&#36829;&#35268;&#22312;&#25152;&#38656;&#38480;&#21046;&#20043;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#30830;&#20445;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#22312;&#23433;&#20840;&#20445;&#35777;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#24120;&#24120;&#20381;&#36182;&#25163;&#21160;&#35843;&#25972;&#20197;&#24179;&#34913;&#23433;&#20840;&#24615;&#12289;&#21487;&#34892;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#24102;&#26377;CBF&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#23433;&#20840;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBF&#37197;&#32622;&#22914;&#20309;&#28145;&#21051;&#24433;&#21709;&#26032;&#20852;&#34892;&#20026;&#65292;&#20984;&#26174;&#20102;&#23545;CBF&#35774;&#35745;&#36827;&#34892;&#21709;&#24212;&#24335;&#21644;&#21160;&#24577;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ASRL&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#33258;&#36866;&#24212;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23436;&#20840;&#33258;&#21160;&#20248;&#21270;&#31574;&#30053;&#21644;CBF&#31995;&#25968;&#65292;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#38271;&#26399;&#24615;&#33021;&#12290;&#36890;&#36807;&#30452;&#25509;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#36827;&#34892;&#20132;&#20114;&#65292;ASRL&#23398;&#20250;&#20102;&#24212;&#23545;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#24182;&#23558;&#25104;&#26412;&#36829;&#35268;&#20445;&#25345;&#22312;&#25152;&#38656;&#30340;&#38480;&#21046;&#20043;&#19979;&#12290;&#25105;&#20204;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#31454;&#20105;&#20013;&#35780;&#20272;&#20102;ASRL&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety in dynamic multi-agent systems is challenging due to limited information about the other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behavior can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviours and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and a competi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06651</link><description>&lt;p&gt;
ConR: &#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06651
&lt;/p&gt;
&lt;p&gt;
ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#24067;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#24456;&#24120;&#35265;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;&#32422;&#26463;&#65292;&#20197;&#34920;&#31034;&#23569;&#25968;&#31867;&#21035;&#26631;&#31614;&#24182;&#36991;&#20813;&#23545;&#22810;&#25968;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#22823;&#37327;&#30340;&#19981;&#24179;&#34913;&#26041;&#27861;&#22788;&#29702;&#20102;&#20998;&#31867;&#26631;&#31614;&#31354;&#38388;&#65292;&#20294;&#22312;&#36830;&#32493;&#26631;&#31614;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#19978;&#26410;&#33021;&#26377;&#25928;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#36830;&#32493;&#26631;&#31614;&#20043;&#38388;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#32852;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26377;&#25928;&#24314;&#27169;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConR&#65292;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#22810;&#25968;&#37051;&#23621;&#20013;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#25351;&#31034;&#22120;&#65292;ConR&#21306;&#20998;&#20102;&#26631;&#31614;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#26045;&#21152;&#24809;&#32602;&#12290;ConR&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#31574;&#30053;&#20851;&#27880;&#26631;&#31614;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.06256</link><description>&lt;p&gt;
&#19987;&#19994;&#24615;&#19982;&#24191;&#27867;&#24615;&#65306;&#20851;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20855;&#26377;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#25110;&#35843;&#25972;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20351;&#20854;&#33719;&#24471;&#19987;&#19994;&#24615;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#24494;&#35843;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#35206;&#30422;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36861;&#27714;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#19987;&#19994;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#36825;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;(Catastrophic Forgetting, CF)&#30456;&#20851;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;VLMs&#21644;LLMs&#20013;&#30340;&#23384;&#22312;&#12290;&#20363;&#22914;&#65292;&#23545;&#20687;CLIP&#36825;&#26679;&#30340;VLM&#36827;&#34892;&#22312;ImageNet&#19978;&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;Galactica&#36827;&#34892;&#24494;&#35843;&#21017;&#20250;&#23548;&#33268;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04015</link><description>&lt;p&gt;
&#20351;&#29992;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport with Tempered Exponential Measures. (arXiv:2309.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#20013;&#65292;&#20004;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#30456;&#20114;&#23545;&#31435;&#65306;&#65288;i&#65289;&#38750;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#21345;&#25176;&#35834;&#32500;&#22855;&#26041;&#24335;&#8221;&#65292;&#23548;&#33268;&#20102;&#38750;&#24120;&#31232;&#30095;&#30340;&#35268;&#21010;&#65292;&#20294;&#31639;&#27861;&#25928;&#29575;&#36739;&#20302;&#65307;&#65288;ii&#65289;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#36763;&#20811;&#38669;&#24681;-&#24211;&#37117;&#37324;&#26041;&#24335;&#8221;&#65292;&#33719;&#24471;&#20102;&#36817;&#20284;&#32447;&#24615;&#31639;&#27861;&#65292;&#20294;&#26368;&#22823;&#31243;&#24230;&#19978;&#26080;&#27861;&#31232;&#30095;&#35268;&#21010;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21518;&#19968;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#65292;&#21363;&#20855;&#26377;&#38388;&#25509;&#27979;&#24230;&#24402;&#19968;&#21270;&#30340;&#25351;&#25968;&#26063;&#27867;&#21270;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26041;&#20415;&#30340;&#25240;&#20013;&#25928;&#26524;&#65292;&#20855;&#26377;&#38750;&#24120;&#24555;&#30340;&#36817;&#20284;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#38750;&#38190;&#20301;&#21183;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#12290;</title><link>http://arxiv.org/abs/2308.13208</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
Physics-inspired Equivariant Descriptors of Non-bonded Interactions. (arXiv:2308.13208v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13208
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#38750;&#38190;&#20301;&#21183;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#27169;&#25311;&#30340;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#32467;&#26500;&#20960;&#20309;&#30340;&#23616;&#37096;&#25551;&#36848;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#30001;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#39537;&#21160;&#30340;&#25928;&#24212;&#26041;&#38754;&#22256;&#38590;&#37325;&#37325;&#12290;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#21162;&#21147;&#38598;&#20013;&#20110;&#30452;&#25509;&#23558;&#38745;&#30005;&#24341;&#20837;&#65292;&#36825;&#26159;&#26368;&#31361;&#20986;&#30340;&#25928;&#24212;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#26174;&#24335;&#29289;&#29702;&#27169;&#22411;&#30340;&#21151;&#33021;&#24418;&#24335;&#30456;&#20284;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#21253;&#25324;&#20854;&#20182;&#24418;&#24335;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#25110;&#32773;&#39044;&#27979;&#38500;&#20102;&#21407;&#23376;&#38388;&#21183;&#33021;&#20043;&#22806;&#30340;&#24615;&#36136;&#65292;&#38656;&#35201;&#36827;&#34892;&#20020;&#26102;&#20462;&#25913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#36828;&#31243;&#31561;&#21464;&#65288;LODE&#65289;&#26694;&#26550;&#25193;&#23637;&#21040;&#29983;&#25104;&#31867;&#20284;&#20219;&#24847;&#28176;&#36817;&#34892;&#20026;&#30340;&#38750;&#38190;&#20301;&#21183;&#30340;&#21407;&#23376;&#29615;&#22659;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#65292;&#20174;&#28857;&#30005;&#33655;&#38745;&#30005;&#21040;&#33394;&#25955;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;LODE&#24418;&#24335;&#20027;&#20041;&#21487;&#36890;&#36807;&#24191;&#20041;&#22810;&#26497;&#23637;&#24320;&#30452;&#35266;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing machine-learning schemes applied to atomic-scale simulations rely on a local description of the geometry of a structure, and struggle to model effects that are driven by long-range physical interactions. Efforts to overcome these limitations have focused on the direct incorporation of electrostatics, which is the most prominent effect, often relying on architectures that mirror the functional form of explicit physical models. Including other forms of non-bonded interactions, or predicting properties other than the interatomic potential, requires ad hoc modifications. We propose an alternative approach that extends the long-distance equivariant (LODE) framework to generate local descriptors of an atomic environment that resemble non-bonded potentials with arbitrary asymptotic behaviors, ranging from point-charge electrostatics to dispersion forces. We show that the LODE formalism is amenable to a direct physical interpretation in terms of a generalized multipole exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13111</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20302;&#31209;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#39640;&#25928;&#24494;&#35843;&#30340;&#26032;&#33539;&#24335;&#65292;&#20854;&#20013;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#24448;&#24448;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#21487;&#20316;&#20026;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#24182;&#22686;&#24378;&#26657;&#20934;&#33021;&#21147;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Laplace-LoRA&#65292;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#23558;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#24212;&#29992;&#20110;LoRA&#21442;&#25968;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>AQUILA&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#36755;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#23616;&#37096;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#30340;&#20840;&#23616;&#27169;&#22411;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00258</link><description>&lt;p&gt;
AQUILA: &#33258;&#36866;&#24212;&#37327;&#21270;&#25042;&#27719;&#32858;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients. (arXiv:2308.00258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00258
&lt;/p&gt;
&lt;p&gt;
AQUILA&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#36755;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#23616;&#37096;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#30340;&#20840;&#23616;&#27169;&#22411;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#39640;&#36890;&#20449;&#24320;&#38144;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26469;&#33258;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20256;&#36755;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#27861;&#22312;&#27599;&#19968;&#36718;&#35757;&#32451;&#20013;&#37117;&#20551;&#35774;&#35774;&#22791;&#21442;&#19982;&#22343;&#21248;&#65292;&#22312;&#23454;&#36341;&#20013;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36873;&#21462;&#37327;&#21270;&#32423;&#21035;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#32463;&#24120;&#24573;&#35270;&#26412;&#22320;&#35774;&#22791;&#25968;&#25454;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;AQUILA&#65288;&#33258;&#36866;&#24212;&#37327;&#21270;&#25042;&#27719;&#32858;&#26799;&#24230;&#65289;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;AQUILA&#25972;&#21512;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#35774;&#22791;&#36873;&#25321;&#26041;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#35774;&#22791;&#26356;&#26032;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2307.14619</link><description>&lt;p&gt;
&#27169;&#20223;&#22797;&#26434;&#36712;&#36857;&#65306;&#26725;&#25509;&#20302;&#23618;&#31283;&#23450;&#24615;&#19982;&#39640;&#23618;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#38543;&#26426;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#12289;&#28508;&#22312;&#22810;&#27169;&#24577;&#65288;&#21363;&#8220;&#22797;&#26434;&#8221;&#65289;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20302;&#23618;&#25511;&#21046;&#22120;&#65288;&#26080;&#35770;&#26159;&#23398;&#20064;&#30340;&#36824;&#26159;&#38544;&#21547;&#30340;&#65289;&#26469;&#31283;&#23450;&#22260;&#32469;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#65288;a&#65289;&#21512;&#36866;&#30340;&#20302;&#23618;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#65288;b&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#38543;&#26426;&#36830;&#32493;&#24615;&#23646;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24635;&#21464;&#24046;&#36830;&#32493;&#24615;&#8221;&#65289;&#65288;TVC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#29366;&#24577;&#20998;&#24067;&#19978;&#30340;&#34892;&#21160;&#30340;&#27169;&#20223;&#32773;&#20250;&#19982;&#28436;&#31034;&#32773;&#23545;&#25972;&#20010;&#36712;&#36857;&#30340;&#20998;&#24067;&#30456;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#35268;&#21017;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#25216;&#24039;&#30456;&#32467;&#21512;&#65288;&#21363;&#22312;&#25191;&#34892;&#26102;&#28155;&#21152;&#22686;&#24378;&#22122;&#22768;&#65289;&#26469;&#30830;&#20445;TVC&#24182;&#19988;&#26368;&#23567;&#31243;&#24230;&#19978;&#38477;&#20302;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20445;&#35777;&#23454;&#20363;&#21270;&#20026;&#30001;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#23398;&#20064;&#32773;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#28436;&#31034;&#32773;&#30340;&#20998;&#24067;&#65292;&#21017;&#26368;&#32456;&#23436;&#25104;&#36825;&#31181;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
&lt;/p&gt;</description></item><item><title>PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11833</link><description>&lt;p&gt;
PINNsFormer: &#22522;&#20110;Transformer&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks. (arXiv:2307.11833v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11833
&lt;/p&gt;
&lt;p&gt;
PINNsFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#20934;&#30830;&#36924;&#36817;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PINNs&#21644;&#22823;&#22810;&#25968;&#30456;&#20851;&#30740;&#31350;&#37319;&#29992;&#20840;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20316;&#20026;&#26680;&#24515;&#32467;&#26500;&#65292;&#24573;&#30053;&#20102;PDEs&#20013;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#26080;&#27861;&#20934;&#30830;&#36924;&#36817;&#30495;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#21363;PINNsFormer&#65292;&#36890;&#36807;Transformer-based&#27169;&#22411;&#20013;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20934;&#30830;&#36924;&#36817;PDEs&#30340;&#35299;&#12290;PINNsFormer&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#21521;&#37327;&#20197;&#20266;&#24207;&#21015;&#30340;&#24418;&#24335;&#36827;&#34892;&#36817;&#20284;&#39044;&#27979;&#65292;&#36824;&#23558;&#36880;&#28857;&#30340;PINNs&#25439;&#22833;&#25913;&#20026;&#20102;&#39034;&#24207;&#30340;PINNs&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;PINNsFormer&#36824;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#23567;&#27874;&#20989;&#25968;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#20613;&#37324;&#21494;&#20998;&#35299;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PINNsFormer&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to captu
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.02728</link><description>&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#65306;&#26397;&#30528;&#21487;&#34892;&#30340;&#22522;&#20110;&#25480;&#26435;&#30340;&#25216;&#33021;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02728
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#38656;&#35201;&#22823;&#37327;&#30340;&#25216;&#33021;&#12290; &#25480;&#26435; - &#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#22823;&#20114;&#20449;&#24687; - &#20026;&#23398;&#20064;&#22823;&#37327;&#19981;&#21516;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#65292;&#20294;&#20114;&#20449;&#24687;&#24456;&#38590;&#20248;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20998;&#23618;&#25480;&#26435;&#65292;&#36890;&#36807;&#38598;&#25104;&#30446;&#26631;&#26465;&#20214;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#35745;&#31639;&#25480;&#26435;&#26356;&#21152;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#30701;&#26399;&#35270;&#35282;&#19979;&#30340;&#25480;&#26435;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#25351;&#25968;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#25480;&#26435;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#36129;&#29486;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#34434;&#34433;&#23548;&#33322;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#22235;&#32423;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#35206;&#30422;&#38754;&#31215;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.02570</link><description>&lt;p&gt;
&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#35768;&#22810;&#24403;&#21069;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#22120;&#30340;&#32570;&#28857;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#30340;&#36951;&#28431;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#27880;&#24847;&#21147;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;RoBERTa&#26469;&#30830;&#23450;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#23631;&#34109;&#65292;&#20877;&#20351;&#29992;BART&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#37325;&#24314;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;BART&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the named entity omission - the drawback of many current abstractive text summarizers. We suggest a custom pretraining objective to enhance the model's attention on the named entities in a text. At first, the named entity recognition model RoBERTa is trained to determine named entities in the text. After that, this model is used to mask named entities in the text and the BART model is trained to reconstruct them. Next, the BART model is fine-tuned on the summarization task. Our experiments showed that this pretraining approach improves named entity inclusion precision and recall metrics.
&lt;/p&gt;</description></item><item><title>UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00238</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00238
&lt;/p&gt;
&lt;p&gt;
UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24403;&#30446;&#26631;&#25968;&#25454;&#31232;&#32570;&#32780;&#28304;&#25968;&#25454;&#20805;&#36275;&#65292;&#25110;&#32773;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#31227;&#23398;&#20064;&#22312;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;UTrans&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#20302;&#20110;&#20165;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#25968;&#25454;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#25490;&#38500;&#19981;&#21487;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;UTrans&#19982;&#29616;&#26377;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;UTrans&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#32654;&#22269;&#20195;&#38469;&#27969;&#21160;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#25554;&#20540;&#21644;&#22270;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#33410;&#28857;&#20043;&#38388;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.15963</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#23454;&#29616;&#22270;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Graph Interpolation via Fast Fused-Gromovization. (arXiv:2306.15963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#34701;&#21512;Gromov&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#25554;&#20540;&#21644;&#22270;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#33410;&#28857;&#20043;&#38388;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#22312;&#22270;&#32423;&#20998;&#31867;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29420;&#31435;&#22320;&#22686;&#24378;&#22270;&#20449;&#21495;&#31354;&#38388;&#21644;&#22270;&#32467;&#26500;&#31354;&#38388;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#21305;&#37197;&#22270;&#20043;&#38388;&#33410;&#28857;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#32771;&#34385;&#22270;&#32467;&#26500;&#21644;&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;mixup&#31639;&#27861;&#65292;&#31216;&#20026;FGWMixup&#65292;&#23427;&#21033;&#29992;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#24230;&#37327;&#31354;&#38388;&#26469;&#35782;&#21035;&#28304;&#22270;&#30340;&#8220;&#20013;&#28857;&#8221;&#12290;&#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25918;&#26494;&#30340;FGW&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#25910;&#25947;&#36895;&#24230;&#20174;O(t^-1)&#21152;&#36895;&#21040;O(t^-2)&#65292;&#25552;&#39640;&#20102;FGWMixup&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data augmentation has proven to be effective in enhancing the generalizability and robustness of graph neural networks (GNNs) for graph-level classifications. However, existing methods mainly focus on augmenting the graph signal space and the graph structure space independently, overlooking their joint interaction. This paper addresses this limitation by formulating the problem as an optimal transport problem that aims to find an optimal strategy for matching nodes between graphs considering the interactions between graph structures and signals. To tackle this problem, we propose a novel graph mixup algorithm dubbed FGWMixup, which leverages the Fused Gromov-Wasserstein (FGW) metric space to identify a "midpoint" of the source graphs. To improve the scalability of our approach, we introduce a relaxed FGW solver that accelerates FGWMixup by enhancing the convergence rate from $\mathcal{O}(t^{-1})$ to $\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets, utilizin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11313</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#26680;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep graph kernel point processes. (arXiv:2306.11313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#22270;&#20013;&#24322;&#27493;&#20107;&#20214;&#65292;&#21453;&#26144;&#19981;&#21516;&#31867;&#22411;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#19988;&#22270;&#30340;&#22823;&#23567;&#21644;&#25299;&#25169;&#32467;&#26500;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#25581;&#31034;&#20102;&#25429;&#25417;&#22797;&#26434;&#30340;&#20107;&#20214;&#31867;&#21035;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27599;&#20010;&#30446;&#26631;&#20107;&#20214;&#31867;&#22411;&#30340;&#24378;&#24230;&#35745;&#31639;&#20013;&#20351;&#29992;&#20102;&#21253;&#25324;&#25152;&#26377;&#20107;&#20214;&#31867;&#21035;&#22312;&#20869;&#30340;&#26410;&#32463;&#28388;&#27874;&#30340;&#20107;&#20214;&#35760;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#12290;&#23545;&#24212;&#30340;&#26080;&#21521;&#22270;&#20855;&#26377;&#20195;&#34920;&#20107;&#20214;&#31867;&#21035;&#30340;&#33410;&#28857;&#21644;&#34920;&#31034;&#28508;&#22312;&#36129;&#29486;&#20851;&#31995;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#12290;&#26412;&#36136;&#24433;&#21709;&#32467;&#26500;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;-based&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#32858;&#21512;&#36827;&#34892;&#20102;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#20855;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point process models are widely used to analyze asynchronous events occurring within a graph that reflect how different types of events influence one another. Predicting future events' times and types is a crucial task, and the size and topology of the graph add to the challenge of the problem. Recent neural point process models unveil the possibility of capturing intricate inter-event-category dependencies. However, such methods utilize an unfiltered history of events, including all event categories in the intensity computation for each target event type. In this work, we propose a graph point process method where event interactions occur based on a latent graph topology. The corresponding undirected graph has nodes representing event categories and edges indicating potential contribution relationships. We then develop a novel deep graph kernel to characterize the triggering and inhibiting effects between events. The intrinsic influence structures are incorporated via the graph neural
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06991</link><description>&lt;p&gt;
&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast Diffusion Model. (arXiv:2306.06991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20197;&#20854;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#20174;&#38543;&#26426;&#20248;&#21270;&#30340;&#35282;&#24230;&#26174;&#33879;&#21152;&#36895;DMs&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;DMs&#30340;&#25193;&#25955;&#36807;&#31243;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#38543;&#26426;&#26102;&#21464;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#36807;&#31243;&#30456;&#31526;&#21512;&#12290;&#28982;&#21518;&#65292;&#21463;&#21040;&#21160;&#37327;SGD&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#21644;&#39069;&#22806;&#30340;&#21160;&#37327;&#65292;&#20197;&#23454;&#29616;&#27604;SGD&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#65292;&#25105;&#20204;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;DMs&#30340;&#25193;&#25955;&#36807;&#31243;&#20013;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21363;&#20174;&#22522;&#20110;&#21160;&#37327;&#30340;&#25193;&#25955;&#36807;&#31243;&#20013;&#23548;&#20986;&#22122;&#22768;&#25200;&#21160;&#26680;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#26500;&#24314;&#20026;&#19968;&#20010;&#38459;&#23612;&#25391;&#33633;&#31995;&#32479;&#65292;&#20020;&#30028;&#38459;&#23612;&#29366;&#24577;-&#26680;&#35299;&#20915;&#26041;&#26696;-&#36991;&#20813;&#25391;&#33633;&#65292;&#20351;&#25193;&#25955;&#36807;&#31243;&#30340;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FDM&#22312;&#21152;&#36895;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have been adopted across diverse fields with its remarkable abilities in capturing intricate data distributions. In this paper, we propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a stochastic optimization perspective for both faster training and sampling. We first find that the diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem. Then, inspired by momentum SGD that uses both gradient and an extra momentum to achieve faster and more stable convergence than SGD, we integrate momentum into the diffusion process of DMs. This comes with a unique challenge of deriving the noise perturbation kernel from the momentum-based diffusion process. To this end, we frame the process as a Damped Oscillation system whose critically damped state -- the kernel solution -avoids oscillation and yields a faster convergence speed of the diffusion process. Empirical r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05739</link><description>&lt;p&gt;
&#36291;&#36801;&#20110;&#26641;&#31354;&#38388;&#65306;&#36830;&#32493;&#30340;&#26641;&#24418;&#31995;&#32479;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;
&lt;/p&gt;
&lt;p&gt;
Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36827;&#21270;&#31995;&#32479;&#23398;&#29616;&#22312;&#26159;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#38416;&#26126;&#29983;&#21629;&#26089;&#26399;&#25903;&#31995;&#21644;&#20256;&#26579;&#30149;&#30340;&#36215;&#28304;&#21644;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#33021;&#30340;&#26641;&#30340;&#24191;&#38420;&#31354;&#38388;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#31995;&#32479;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#20351;&#26799;&#24230;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#36830;&#32493;&#30340;&#25918;&#26494;&#26041;&#24335;&#20801;&#35768;&#22312;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#20013;&#36328;&#36234;&#26641;&#31354;&#38388;&#65292;&#19988;&#19981;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#30340;&#26080;&#26681;&#26641;&#25512;&#26029;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26641;&#21644;&#26641;&#26681;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#20063;&#24456;&#26377;&#25928;&#65292;&#25105;&#20204;&#22312;&#39052;&#21475;&#21160;&#29289;&#30340;&#31995;&#32479;&#21457;&#32946;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20107;&#23454;&#19978;&#65292;&#20165;&#20855;&#26377;&#36229;&#25351;&#25968;&#20449;&#21495;&#30340;&#23569;&#25968;&#22522;&#22240;&#36890;&#24120;&#36275;&#20197;&#20998;&#36776;&#33034;&#26894;&#21160;&#29289;&#30340;&#20027;&#35201;&#35889;&#31995;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#21152;&#36895;&#21457;&#29616;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03364</link><description>&lt;p&gt;
&#22312;&#21333;&#20301;&#29699;&#19978;&#23398;&#20064;&#34920;&#31034;&#65306;&#24212;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21407;&#29702;&#26469;&#23398;&#20064;&#20998;&#24067;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#38024;&#23545;&#23545;&#31216;&#26041;&#21521;&#25968;&#25454;&#24314;&#31435;&#20102; von Mises-Fisher &#20998;&#24067;&#21644;&#35282;&#39640;&#26031;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34987;&#25512;&#21521;&#22266;&#23450;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#12290;&#36825;&#20351;&#24471;&#23427;&#36866;&#21512;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#65292;&#22240;&#27492;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#19981;&#20877;&#21487;&#29992;&#65292;&#24403;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#21482;&#33021;&#30475;&#19968;&#27425;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36127;&#25968;&#25454;&#25110;&#20219;&#21153;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#30340;&#25209;&#22788;&#29702;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;</title><link>http://arxiv.org/abs/2306.01776</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#22312;&#19977;&#32500;&#28237;&#27969;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Diffusion for 3D Turbulent Flows. (arXiv:2306.01776v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#36991;&#20813;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#27969;&#21160;&#36890;&#24120;&#38590;&#20197;&#39044;&#27979;&#65292;&#20294;&#20108;&#32500;&#21644;&#19977;&#32500;&#30340;&#28237;&#27969;&#27969;&#21160;&#24615;&#36136;&#19981;&#21516;&#12290;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#65292;&#28237;&#27969;&#20250;&#24418;&#25104;&#22823;&#30340;&#12289;&#36830;&#32493;&#30340;&#32467;&#26500;&#65292;&#32780;&#22312;&#19977;&#32500;&#24773;&#20917;&#19979;&#65292;&#26059;&#28065;&#32423;&#32852;&#25104;&#36234;&#26469;&#36234;&#23567;&#30340;&#23610;&#24230;&#65292;&#24418;&#25104;&#35768;&#22810;&#24555;&#36895;&#21464;&#21270;&#30340;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21152;&#21095;&#20102;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#38590;&#20197;&#20351;&#29992;&#22238;&#24402;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#19977;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#28237;&#27969;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#27969;&#22330;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28237;&#27969;&#27969;&#21160;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21482;&#20381;&#38752;&#20960;&#20309;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27604;&#24037;&#19994;&#32423;&#25968;&#20540;&#27714;&#35299;&#22120;&#26356;&#24555;&#22320;&#29983;&#25104;&#28237;&#27969;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent flows are well known to be chaotic and hard to predict; however, their dynamics differ between two and three dimensions. While 2D turbulence tends to form large, coherent structures, in three dimensions vortices cascade to smaller and smaller scales. This cascade creates many fast-changing, small-scale structures and amplifies the unpredictability, making regression-based methods infeasible. We propose the first generative model for forced turbulence in arbitrary 3D geometries and introduce a sample quality metric for turbulent flows based on the Wasserstein distance of the generated velocity-vorticity distribution. In several experiments, we show that our generative diffusion model circumvents the unpredictability of turbulent flows and produces high-quality samples based solely on geometric information. Furthermore, we demonstrate that our model beats an industrial-grade numerical solver in the time to generate a turbulent flow field from scratch by an order of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19685</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21463;&#38543;&#26426;&#21147;&#23398;&#21644;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#20174;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#20013;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#19978;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#32467;&#26524;&#20855;&#26377;&#19982;&#32500;&#25968;&#25968;&#37327;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29992;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992; langevin &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18864</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution. (arXiv:2305.18864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992; langevin &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; langevin &#25110; levy &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#38543;&#26426;&#23398;&#20064;&#21160;&#21147;&#23398;&#36890;&#36807;&#25913;&#21464;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#25110;&#30452;&#25509;&#27880;&#20837;&#22122;&#22768;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#22122;&#22768;&#30340;&#26041;&#24046;&#12290;&#30001;&#20110;&#22122;&#22768;&#26041;&#24046;&#20250;&#24433;&#21709;&#36817;&#20284;&#24615;&#33021;&#65292;&#22312;&#22522;&#20110; SDE &#30340;&#23398;&#20064;&#21644;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#28155;&#21152;&#22122;&#22768;&#30340;&#35774;&#35745;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#20248;&#21270;&#30340;&#38543;&#26426;&#19979;&#38477;&#23398;&#20064;&#26041;&#31243;&#65292;&#37319;&#29992;&#38543;&#26426;&#20998;&#26512;&#30340;&#35270;&#35282;&#65292;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992; langevin SDE &#21160;&#21147;&#23398;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#21487;&#25511;&#22122;&#22768;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545; vanilla &#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise. Since the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation. In this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective. The proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size. Numerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Fur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18171</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65292;&#30001;&#20110;&#22810;&#26679;&#24615;&#21644;&#19981;&#23436;&#32654;&#27880;&#37322;&#23548;&#33268;&#30340;&#22266;&#26377;&#27495;&#20041;&#20351;&#20854;&#21463;&#21040;&#22256;&#25200;&#12290;&#30830;&#23450;&#24615;&#20989;&#25968;&#26080;&#27861;&#36275;&#22815;&#24378;&#22823;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#27010;&#29575;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;&#33945;&#29305;&#21345;&#27931;&#36924;&#36817;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#65292;&#19988;&#22312;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#36328;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#65288;PCME++&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#35299;&#30340;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;PCME++&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20266;&#27491;&#26679;&#26412;&#20197;&#38450;&#27490;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#37319;&#29992;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#27010;&#29575;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCME++&#22312;ITM&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.17212</link><description>&lt;p&gt;
&#26059;&#36716;&#20248;&#21270;&#22120;&#65306;&#31616;&#21333;&#32780;&#24378;&#20581;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational Optimizers: Simple &amp; Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#12289;&#26435;&#37325;&#34928;&#20943;&#12289;&#21021;&#22987;&#21270;&#31561;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#20132;&#20114;&#20316;&#29992;&#21487;&#20197;&#22312;&#23610;&#24230;&#19981;&#21464;&#23618;&#65288;&#22914;&#24402;&#19968;&#21270;&#23618;&#65289;&#20013;&#20135;&#29983;&#29699;&#38754;&#36816;&#21160;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#20854;&#20013;&#26435;&#37325;&#33539;&#25968;&#21644;&#39044;&#26399;&#26059;&#36716;&#26356;&#26032;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#23545;AdamW&#12289;&#24102;&#21160;&#37327;&#30340;SGD&#21644;Lion&#20013;&#30340;&#36825;&#20010;&#24179;&#34913;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#36229;&#21442;&#25968;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#20248;&#21270;&#22120;&#30340;&#26059;&#36716;&#21464;&#20307;&#65288;RVs&#65289;&#65292;&#24378;&#21046;&#39044;&#26399;&#35282;&#24230;&#26356;&#26032;&#22823;&#23567;&#19982;&#25972;&#20010;&#35757;&#32451;&#26399;&#38388;&#30340;&#24179;&#34913;&#20540;&#30456;&#21305;&#37197;&#12290;&#36825;&#31616;&#21270;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#28040;&#38500;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#30340;&#30636;&#24577;&#30456;&#24212;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#20248;&#21270;&#22120;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#21464;&#20307;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#22522;&#32447;&#36229;&#21442;&#25968;&#36827;&#34892;&#26368;&#23569;&#25110;&#19981;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#20010;&#27010;&#24565;&#30340;&#25991;&#26412;&#22330;&#26223;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#30446;&#26631;&#27010;&#24565;&#30340;&#25513;&#30721;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#22330;&#26223;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16311</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#20010;&#27010;&#24565;&#30340;&#22330;&#26223;&#20998;&#35299;&#65306;&#30772;&#35299;&#29616;&#22330;
&lt;/p&gt;
&lt;p&gt;
Break-A-Scene: Extracting Multiple Concepts from a Single Image. (arXiv:2305.16311v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#20010;&#27010;&#24565;&#30340;&#25991;&#26412;&#22330;&#26223;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#30446;&#26631;&#27010;&#24565;&#30340;&#25513;&#30721;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#22330;&#26223;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#30446;&#26631;&#26159;&#24341;&#20837;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#21644;&#23039;&#21183;&#21464;&#21270;&#30340;&#22810;&#20010;&#22270;&#20687;&#20013;&#23398;&#20064;&#21333;&#20010;&#27010;&#24565;&#30340;&#24773;&#20917;&#65292;&#24403;&#24212;&#29992;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#22330;&#26223;&#20998;&#35299;&#30340;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#27010;&#24565;&#30340;&#22330;&#26223;&#30340;&#21333;&#20010;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#21462;&#27599;&#20010;&#27010;&#24565;&#30340;&#29420;&#29305;&#25991;&#26412;&#26631;&#35760;&#65292;&#20174;&#32780;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36755;&#20837;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#25351;&#31034;&#30446;&#26631;&#27010;&#24565;&#30340;&#23384;&#22312;&#30340;&#25513;&#30721;&#12290;&#36825;&#20123;&#25513;&#30721;&#21487;&#20197;&#30001;&#29992;&#25143;&#25552;&#20379;&#65292;&#20063;&#21487;&#20197;&#30001;&#39044;&#35757;&#32451;&#30340;&#20998;&#21106;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23450;&#21046;&#27969;&#31243;&#65292;&#20248;&#21270;&#19968;&#32452;&#19987;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#65288;&#21477;&#26564;&#65289;&#20197;&#21450;&#27169;&#22411;&#26435;&#37325;&#65292;&#20197;&#22312;&#20934;&#30830;&#25429;&#25417;&#27010;&#24565;&#30340;&#21516;&#26102;&#20445;&#25345;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15508</link><description>&lt;p&gt;
&#36890;&#36807;&#20107;&#21518;&#23545;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving selective classification performance of deep neural networks through post-hoc logit normalization and temperature scaling. (arXiv:2305.15508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28508;&#22312;&#38169;&#35823;&#36890;&#36807;&#25918;&#24323;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#38024;&#23545;&#30340;&#26159;&#20248;&#21270;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#35823;&#20998;&#31867;&#26816;&#27979;&#24615;&#33021;&#65292;&#21363;&#36890;&#36807;&#23558;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#20540;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#39044;&#27979;&#26469;&#21306;&#20998;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;$p$-NormSoftmax&#65292;&#36890;&#36807;&#23545;&#25968;&#36827;&#34892;$p$-&#33539;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of selective classification for deep neural networks, where a model is allowed to abstain from low-confidence predictions to avoid potential errors. Specifically, we tackle the problem of optimizing the confidence estimator of a fixed classifier, aiming to enhance its misclassification detection performance, i.e., its ability to discriminate between correct and incorrect predictions by assigning higher confidence values to the correct ones. Previous work has found that different classifiers exhibit varying levels of misclassification detection performance, particularly when using the maximum softmax probability (MSP) as a measure of confidence. However, we argue that these findings are mainly due to a sub-optimal confidence estimator being used for each model. To overcome this issue, we propose a simple and efficient post-hoc confidence estimator, named $p$-NormSoftmax, which consists of transforming the logits through $p$-norm normalization and tempera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14777</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport. (arXiv:2305.14777v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#30740;&#31350;&#19968;&#31181;&#36816;&#36755;&#26144;&#23556;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#21516;&#26102;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;OT&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#21487;&#36861;&#28335;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#25968;&#25454;&#20043;&#38388;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;OT&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#31163;&#32676;&#28857;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#38754;&#20020;&#20248;&#21270;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;OT&#19981;&#21516;&#65292;UOT&#28040;&#38500;&#20102;&#20998;&#24067;&#21305;&#37197;&#30340;&#30828;&#24615;&#32422;&#26463;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31163;&#32676;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#35757;&#32451;&#26399;&#38388;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;UOT&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;CIFAR-10&#21644;CelebA-HQ-256&#19978;&#23454;&#29616;&#20102;&#20998;&#21035;&#20026;2.97&#21644;5.80&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80 on CelebA-HQ-256.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14585</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#25968;&#25454;&#24402;&#23646;&#20219;&#21153;&#65292;&#35299;&#37322;&#22411;AI&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#36890;&#36807;&#35299;&#37322;&#31034;&#20363;&#31574;&#30053;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23558;&#20915;&#31574;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23578;&#26410;&#30456;&#20114;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#24418;&#25104;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#30495;&#27491;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#35777;&#26126;&#20102;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65306;(1)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;(pNTK)&#65292;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#20013;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#26356;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#20026;&#26377;&#25928;&#65307;(2)&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#35268;&#33539;&#21270;pNTK&#21019;&#24314;&#30340;&#24402;&#22240;&#27604;&#36825;&#20123;&#26367;&#20195;&#21697;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.13650</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#40065;&#26834;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#30340;&#23646;&#24615;&#24341;&#23548;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#25506;&#32034;&#20855;&#26377;&#26497;&#24230;&#31232;&#30095;&#30340;&#26377;&#24847;&#20041;&#21306;&#22495;&#30340;&#39640;&#32500;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;&#25628;&#32034;&#27169;&#22411;&#26469;&#36741;&#21161;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#19981;&#24179;&#34913;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;MBO&#26041;&#27861;&#24456;&#38590;&#25110;&#26681;&#26412;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#20854;&#28508;&#22312;&#31354;&#38388;&#30001;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#25353;&#29031;&#36825;&#20123;&#23646;&#24615;&#20540;&#20248;&#20808;&#32771;&#34385;&#26679;&#26412;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MBO&#19982;PGVAE&#31283;&#20581;&#22320;&#21457;&#29616;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35774;&#35745;&#31354;&#38388;&#30340;&#26222;&#36866;&#24615;&#21450;&#20854;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12569</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26159;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Modeling is All You Need for Marked Temporal Point Processes. (arXiv:2305.12569v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#27493;&#20351;&#24471;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#25945;&#27169;&#22411;&#30693;&#36947;&#20309;&#26102;&#29983;&#25104;&#20869;&#23481;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#12289;&#28789;&#27963;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#32500;&#26631;&#35760;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#25429;&#25417;&#28857;&#36807;&#31243;&#30340;&#20998;&#24067;&#32780;&#19981;&#38656;&#26126;&#30830;&#25351;&#23450;&#26465;&#20214;&#24378;&#24230;&#25110;&#27010;&#29575;&#23494;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#20197;&#20107;&#20214;&#21382;&#21490;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#22312;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#19979;&#65292;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21033;&#30410;&#65292;&#21253;&#25324;&#22312;&#23398;&#20064;&#27169;&#22411;&#21644;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#25928;&#29575;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative modeling have made it possible to generate high-quality content from context information, but a key question remains: how to teach models to know when to generate content? To answer this question, this study proposes a novel event generative model that draws its statistical intuition from marked temporal point processes, and offers a clean, flexible, and computationally efficient solution for a wide range of applications involving multi-dimensional marks. We aim to capture the distribution of the point process without explicitly specifying the conditional intensity or probability density. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including exceptional efficiency in learning the model and generating samples, as well as considerable representational power to capture
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#36801;&#31227;&#33021;&#21147;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27492;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11589</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21450;Sim2Real&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer. (arXiv:2305.11589v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#36801;&#31227;&#33021;&#21147;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27492;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#65292;&#36710;&#36742;&#24517;&#39035;&#33021;&#22815;&#25345;&#32493;&#25191;&#34892;&#21508;&#31181;&#39550;&#39542;&#20219;&#21153;&#65292;&#21253;&#25324;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#26159;&#22522;&#26412;&#30340;&#24182;&#19988;&#30740;&#31350;&#24471;&#24456;&#20805;&#20998;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#20219;&#21153;&#19978;&#65292;&#32780;&#36319;&#36710;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#23436;&#25972;&#30340;&#39046;&#23548;-&#36319;&#38543;&#32773;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#32447;&#25511;&#21046;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#26126;&#35757;&#32451;&#30340;DRL&#20195;&#29702;&#30340;Sim2Real&#36716;&#31227;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#20195;&#29702;&#21450;&#20854;Sim2Real&#36716;&#31227;&#33021;&#21147;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve fully autonomous driving, vehicles must be capable of continuously performing various driving tasks, including lane keeping and car following, both of which are fundamental and well-studied driving ones. However, previous studies have mainly focused on individual tasks, and car following tasks have typically relied on complete leader-follower information to attain optimal performance. To address this limitation, we propose a vision-based deep reinforcement learning (DRL) agent that can simultaneously perform lane keeping and car following maneuvers. To evaluate the performance of our DRL agent, we compare it with a baseline controller and use various performance metrics for quantitative analysis. Furthermore, we conduct a real-world evaluation to demonstrate the Sim2Real transfer capability of the trained DRL agent. To the best of our knowledge, our vision-based car following and lane keeping agent with Sim2Real transfer capability is the first of its kind.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11463</link><description>&lt;p&gt;
&#21033;&#29992;Riesz&#26680;&#30340;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#20013;&#65292;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#24230;(MMD)&#27969;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Riesz&#26680;$K(x,y)=-\|x-y\|^r$&#65292;$r \in (0,2)$&#30340;MMD&#27969;&#20855;&#26377;&#26480;&#20986;&#30340;&#24615;&#36136;&#65292;&#21487;&#20801;&#35768;&#20854;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;Riesz&#26680;&#30340;MMD&#19982;&#20854;&#20998;&#21106;&#29256;&#26412;&#30340;MMD&#37325;&#21512;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#19968;&#32500;&#35774;&#32622;&#20013;&#36827;&#34892;MMD&#26799;&#24230;&#30340;&#35745;&#31639;&#12290;&#22312;&#27492;&#22788;&#65292;&#23545;&#20110;$r=1$&#65292;&#21487;&#20197;&#24212;&#29992;&#31616;&#21333;&#30340;&#25490;&#24207;&#31639;&#27861;&#23558;&#20004;&#20010;&#32463;&#39564;&#24230;&#37327;&#30340;&#22797;&#26434;&#24230;&#20174;$O(MN+N^2)$&#38477;&#20302;&#21040;$O((M+N)\log(M+N))$&#65292;&#20854;&#20013;$M$&#21644;$N$&#26159;&#25903;&#25345;&#28857;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;$P$&#20010;&#20999;&#29255;&#26469;&#36817;&#20284;&#20998;&#21106;MMD&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#35823;&#24046;&#20855;&#26377;$O(\sqrt{d/P})$&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#25968;&#25454;&#32500;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;MMD&#26799;&#24230;&#27969;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#29978;&#33267;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2305.08099</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#35299;&#32806;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#26631;&#27880;&#36164;&#28304;&#24773;&#20917;&#19979;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#25216;&#26415;&#22312;&#35828;&#35805;&#20154;&#12289;&#24773;&#24863;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.02299</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#22312;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21305;&#37197;&#20102;&#23494;&#38598;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#65292;&#21516;&#26102;&#20351;&#24471;&#31232;&#30095;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#24471;&#21040;&#30340;&#27169;&#22411;&#39640;&#24230;&#31232;&#30095;&#65292;&#29702;&#35770;&#19978;&#35757;&#32451;&#26356;&#20415;&#23452;&#65292;&#20294;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#21152;&#36895;&#20381;&#28982;&#20855;&#26377;&#20154;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181; DST &#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450; N:M &#31232;&#30095;&#26041;&#27861;&#65288;&#24120;&#25968;&#25159;&#20837;&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#32463;&#36807;&#23545; PyTorch CPU &#23454;&#29616;&#30340;&#31616;&#21333;&#34920;&#31034;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/calgaryml/condensed-sparsity &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26080;&#30417;&#30563;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#26469;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.08319</link><description>&lt;p&gt;
&#32771;&#23519;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;: &#19968;&#39033;&#35843;&#26597;&#21644;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Examining Computational Performance of Unsupervised Concept Drift Detection: A Survey and Beyond. (arXiv:2304.08319v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26080;&#30417;&#30563;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#26469;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#25110;&#23454;&#26102;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#28418;&#31227;&#26816;&#27979;&#22120;&#24517;&#39035;&#28385;&#36275;&#35745;&#31639;&#35201;&#27714;&#25110;&#32422;&#26463;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24320;&#21457;&#28418;&#31227;&#26816;&#27979;&#22120;&#30340;&#37325;&#28857;&#26159;&#26816;&#27979;&#36136;&#37327;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#24615;&#33021;&#65292;&#22914;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#23558;&#35745;&#31639;&#24615;&#33021;&#35270;&#20026;&#27425;&#35201;&#30446;&#26631;&#65292;&#24182;&#27809;&#26377;&#38024;&#23545;&#36825;&#26679;&#30340;&#35780;&#20272;&#25552;&#20986;&#22522;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#26631;&#65292;&#26082;&#32771;&#34385;&#35745;&#31639;&#24615;&#33021;&#21448;&#32771;&#34385;&#26816;&#27979;&#36136;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#38598;&#21253;&#25324;&#30456;&#23545;&#36816;&#34892;&#26102;&#38388;&#24320;&#38144;&#65288;RRO&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#28418;&#31227;&#26816;&#27979;&#22120;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35745;&#31639;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#19987;&#27880;&#20110;&#26080;&#30417;&#30563;&#30340;&#28418;&#31227;&#26816;&#27979;&#22120;&#65292;&#19981;&#21463;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#38480;&#21046;&#12290;&#25105;&#20204;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Concept drift detection is crucial for many AI systems to ensure the system's reliability. These systems often have to deal with large amounts of data or react in real time. Thus, drift detectors must meet computational requirements or constraints with a comprehensive performance evaluation. However, so far, the focus of developing drift detectors is on detection quality, e.g.~accuracy, but not on computational performance, such as running time. We show that the previous works consider computational performance only as a secondary objective and do not have a benchmark for such evaluation. Hence, we propose a set of metrics that considers both, computational performance and detection quality. Among others, our set of metrics includes the Relative Runtime Overhead RRO to evaluate a drift detector's computational impact on an AI system. This work focuses on unsupervised drift detectors, not being restricted to the availability of labeled data. We measure the computational performance base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17841</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#30456;&#20851;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#20027;&#35201; bottleneck&#12290;&#32780;&#19988;&#65292;&#20026;&#20102;&#35299;&#20915;&#37326;&#24515;&#21187;&#21187;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#38468;&#24102;&#24102;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#20197;&#20415;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25163;&#21160;&#26631;&#35760;&#20855;&#26377;&#39640;&#36136;&#37327;&#26631;&#31614;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24448;&#24448;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#29942;&#39048;&#12290;&#24369;&#30417;&#30563;&#23398;&#20064; (WSL) &#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#26681;&#25454;&#21551;&#21457;&#24335;&#12289;&#36828;&#31243;&#30417;&#35270;&#21644;&#30693;&#35782;&#24211;&#26469;&#36171;&#20104;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#32422;&#26631;&#31614; (&#20266;&#26631;&#31614;) &#30340;&#33258;&#21160;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#25105;&#20204;&#24212;&#29992;&#27010;&#29575;&#29983;&#25104;&#38544;&#21464;&#37327;&#27169;&#22411; (PLVMs)&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#34920;&#31034;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PLVMs &#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#22810;&#25165;&#22810;&#33402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.17708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#20013;&#30340;&#25925;&#38556;&#21644;&#39118;&#38505;&#20998;&#26512;&#65306;&#20197;ONNX&#29983;&#24577;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem. (arXiv:2303.17708v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#24320;&#21457;&#65292;&#20248;&#21270;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#26694;&#26550;&#20013;&#20351;&#29992;&#21644;&#37325;&#26032;&#20351;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#36816;&#34892;&#26102;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#22312;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#24037;&#31243;&#24072;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#23558;&#27169;&#22411;&#20174;&#26694;&#26550;&#31227;&#21160;&#21040;&#36816;&#34892;&#26102;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36716;&#25442;&#22120;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#24182;&#30772;&#22351;&#37096;&#32626;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#39057;&#29575;&#21644;&#25925;&#38556;&#27169;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;ONNX (Open Neural Network eXchange)&#30456;&#20851;&#30340;&#27169;&#22411;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ONNX&#36716;&#25442;&#22120;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;DL&#26694;&#26550;PyTorch&#21644;TensorFlow&#20013;&#30340;&#36807;&#21435;&#25925;&#38556;&#12290;&#36824;&#25253;&#21578;&#20102;&#25925;&#38556;&#65288;N=200&#20010;&#38382;&#39064;&#65289;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36716;&#25442;8,797&#20010;&#27169;&#22411;&#65288;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#23454;&#20363;&#65289;&#26469;&#35780;&#20272;&#24403;&#20170;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers develop, fine-tune, and deploy deep learning (DL) models. They use and re-use models in a variety of development frameworks and deploy them on a range of runtime environments. In this diverse ecosystem, engineers use DL model converters to move models from frameworks to runtime environments. However, errors in converters can compromise model quality and disrupt deployment. The failure frequency and failure modes of DL model converters are unknown.  In this paper, we conduct the first failure analysis on DL model converters. Specifically, we characterize failures in model converters associated with ONNX (Open Neural Network eXchange). We analyze past failures in the ONNX converters in two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and locations of failures (for N=200 issues), and trends over time are also reported. We also evaluate present-day failures by converting 8,797 models, both real-world and synthetically generated instances. The consis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12959</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21435;&#32416;&#32544;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#21435;&#32416;&#32544;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26435;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20043;&#21069;&#20165;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#36880;&#27493;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#65292;&#20197;&#20174;&#21435;&#32416;&#32544;&#21040;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#29942;&#39048;&#20250;&#22833;&#21435;&#21435;&#32416;&#32544;&#30340;&#32422;&#26463;&#65292;&#23548;&#33268;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#20248;&#21270;&#19981;&#21516;&#23618;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#31216;&#20026;DeVAE&#12290;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#19981;&#21516;&#28508;&#22312;&#31354;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;DeVAE &#24179;&#34913;&#20102;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#12290;&#30001;&#20110;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;DeVAE &#20801;&#35768;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20197;&#22312;&#20445;&#25345;&#21435;&#32416;&#32544;&#32422;&#26463;&#30340;&#21516;&#26102;&#20248;&#21270;&#37325;&#26500;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#26694;&#26550;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#65292;&#21033;&#29992;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#19979;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#65292;&#32467;&#35770;&#33719;&#24471;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.10358</link><description>&lt;p&gt;
&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65306;&#31070;&#32463;&#29983;&#23384;&#22238;&#24402;&#20013;&#36229;&#27604;&#20363;&#21361;&#38505;&#20551;&#35774;&#30340;&#31361;&#30772;
&lt;/p&gt;
&lt;p&gt;
Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions. (arXiv:2303.10358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#26694;&#26550;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#65292;&#21033;&#29992;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#19979;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#65292;&#32467;&#35770;&#33719;&#24471;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#28789;&#27963;&#30340;&#31070;&#32463;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#12290;NFM&#26694;&#26550;&#21033;&#29992;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#31070;&#32463;&#32467;&#26500;&#30340;&#24378;&#22823;&#36924;&#36817;&#33021;&#21147;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#26694;&#26550;&#19979;&#25512;&#23548;&#20986;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#65292;&#23427;&#20204;&#20998;&#21035;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#20801;&#35768;&#22312;&#20284;&#28982;&#30446;&#26631;&#19979;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#29702;&#35770;&#19978;&#65292;&#23545;&#20110;&#20004;&#20010;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#20854;&#25910;&#25947;&#36895;&#29575;&#65292;&#24314;&#31435;&#20102;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#38750;&#21442;&#25968;&#32452;&#20214;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21512;&#25104;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#38472;&#36848;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis to capture unobserved heterogeneity among individuals, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over $6$ benchmark datasets of different scales, showing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06419</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#20013;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new machine learning approach, recasting machine learning from explanations (MLX) as an adversarial robustness problem, which specifies a lower dimensional manifold from which perturbations can be drawn based on human-provided annotations, and shows improved performance over prior MLX methods on both synthetic and real-world benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#20851;&#27599;&#20010;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;MLX&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#24378;&#22823;&#30340;&#21442;&#25968;&#27491;&#21017;&#21270;&#26469;&#23545;&#40784;&#27169;&#22411;&#21644;&#20154;&#31867;&#35299;&#37322;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;MLX&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#20154;&#31867;&#35299;&#37322;&#25351;&#23450;&#20102;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#21487;&#20197;&#20174;&#20013;&#32472;&#21046;&#25200;&#21160;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20943;&#36731;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20808;&#21069;MLX&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#19982;&#26089;&#26399;&#30340;MLX&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from explanations (MLX) is an approach to learning that uses human-provided annotations of relevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely heavily on a specific model interpretation approach and require strong parameter regularization to align model and human explanations, leading to sub-optimal performance. We recast MLX as an adversarial robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong parameter regularization. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#36755;&#20837;&#20998;&#24067;&#21644;&#26435;&#37325;&#30697;&#38453;&#30340;&#20551;&#35774;&#23545;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#26031;&#36755;&#20837;&#20998;&#24067;&#19979;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#65292;&#21363;&#20351;&#26435;&#37325;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#20063;&#38754;&#20020;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2302.07426</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;: &#20809;&#28369;&#24615;&#21644;&#36864;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy. (arXiv:2302.07426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#36755;&#20837;&#20998;&#24067;&#21644;&#26435;&#37325;&#30697;&#38453;&#30340;&#20551;&#35774;&#23545;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#26031;&#36755;&#20837;&#20998;&#24067;&#19979;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#65292;&#21363;&#20351;&#26435;&#37325;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#20063;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#21487;&#20197;&#34987;&#26377;&#25928;&#23398;&#20064;&#26159;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24050;&#26377;&#30340;&#38590;&#24230;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#36755;&#20837;&#20998;&#24067;&#21644;&#32593;&#32476;&#26435;&#37325;&#37117;&#38656;&#35201;&#20570;&#20986;&#19968;&#23450;&#30340;&#20551;&#35774;&#25165;&#33021;&#24471;&#21040;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#20551;&#35774;&#36755;&#20837;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#19988;&#26435;&#37325;&#30697;&#38453;&#38750;&#36864;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#20551;&#35774;&#26159;&#21542;&#36866;&#29992;&#20110;&#23398;&#20064;&#26356;&#28145;&#30340;&#32593;&#32476;&#65292;&#24182;&#32473;&#20986;&#20102;&#21542;&#23450;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20809;&#28369;&#20998;&#26512;&#26694;&#26550;&#19979;&#65292;&#21363;&#22312;&#32593;&#32476;&#21442;&#25968;&#20013;&#21152;&#20837;&#38543;&#26426;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#22312;&#39640;&#26031;&#36755;&#20837;&#20998;&#24067;&#19979;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#21363;&#20351;&#26435;&#37325;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#65292;&#23398;&#20064;&#28145;&#24230;&#20026;3&#30340;ReLU&#32593;&#32476;&#22312;&#39640;&#26031;&#20998;&#24067;&#19979;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20809;&#28369;&#20998;&#26512;&#26694;&#26550;&#19979;&#23398;&#20064;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding when neural networks can be learned efficiently is a fundamental question in learning theory. Existing hardness results suggest that assumptions on both the input distribution and the network's weights are necessary for obtaining efficient algorithms. Moreover, it was previously shown that depth-$2$ networks can be efficiently learned under the assumptions that the input distribution is Gaussian, and the weight matrix is non-degenerate. In this work, we study whether such assumptions may suffice for learning deeper networks and prove negative results. We show that learning depth-$3$ ReLU networks under the Gaussian input distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. It implies that learning depth-$3$ ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Moreover, we consider depth-$2$ networks, and show hardness of learning in the smoothed-analysis fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05624</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#30340;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to generate datasets with XAI ground truth to evaluate image models. (arXiv:2302.05624v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#38656;&#27714;&#25512;&#21160;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26032;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29702;&#35770;&#19978;&#30830;&#23450;AI&#20915;&#31574;&#30340;&#21407;&#22240;&#12290;XAI&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#39564;&#35777;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#65292;&#32771;&#34385;&#21040;&#32570;&#20047;&#22522;&#20934;&#65288;GT&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24102;&#26377;GT&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#30340;GT&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#35777;&#23454;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased usage of artificial intelligence (AI), it is imperative to understand how these models work internally. These needs have led to the development of a new field called eXplainable artificial intelligence (XAI). This field consists of on a set of techniques that allows us to theoretically determine the cause of the AI decisions. One main issue of XAI is how to verify the works on this field, taking into consideration the lack of ground truth (GT). In this study, we propose a new method to generate datasets with GT. We conducted a set of experiments that compared our GT with real model explanations and obtained excellent results confirming that our proposed method is correct.
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#26469;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#26469;&#21019;&#24314;&#38598;&#21512;&#65292;&#36816;&#29992;&#20110;&#21508;&#31181;&#23454;&#39564;&#24182;&#21487;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2302.01312</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#38598;&#21512;&#29992;&#20110;&#20016;&#23500;&#30340;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling. (arXiv:2302.01312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#26469;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#26469;&#21019;&#24314;&#38598;&#21512;&#65292;&#36816;&#29992;&#20110;&#21508;&#31181;&#23454;&#39564;&#24182;&#21487;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21487;&#38752;&#22320;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25429;&#25417;&#22797;&#26434;Aleatoric&#20998;&#24067;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#65292;&#36825;&#26159;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#38598;&#21512;&#26159;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#38598;&#21512;&#21019;&#24314;&#30340;&#65292;&#27604;&#21019;&#24314;&#21333;&#29420;&#30340;NF&#27169;&#22411;&#26356;&#21152;&#32463;&#27982;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;NF&#30340;&#29420;&#29305;&#32467;&#26500;&#8212;&#8212;&#22522;&#30784;&#20998;&#24067;&#8212;&#8212;&#26469;&#20272;&#35745;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#25512;&#23548;&#20986;&#26080;&#20559;&#30340;&#24494;&#20998;&#29109;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#20013;&#65306;1D&#27491;&#24358;&#25968;&#25454;&#65292;2D&#26377;&#39118;&#26684;&#32593;&#26684;&#19990;&#30028;&#65288;$\it{Wet Chicken}$&#65289;&#65292;$\it{Pendulum}$&#21644;$\it{Hopper}$&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#27169;&#22411;&#22312;&#27979;&#37327;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world ($\it{Wet Chicken}$), $\it{Pendulum}$, and $\it{Hopper}$. In these experiments, we setup an active learning framework and evaluate each model's capability at measuring aleatoric and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#65292;&#36890;&#36807;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;AltUp&#26041;&#27861;&#22312;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.13310</link><description>&lt;p&gt;
&#39640;&#25928;Transformer&#30340;&#20132;&#26367;&#26356;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternating Updates for Efficient Transformers. (arXiv:2301.13310v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#65292;&#36890;&#36807;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;AltUp&#26041;&#27861;&#22312;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22686;&#21152;&#28145;&#24230;Transformer&#32593;&#32476;&#30340;&#35268;&#27169;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35268;&#27169;&#30340;&#22686;&#21152;&#24448;&#24448;&#20250;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#22823;&#24133;&#22686;&#21152;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#12290;AltUp&#36890;&#36807;&#22312;&#27599;&#19968;&#23618;&#20013;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20165;&#22312;&#24310;&#36831;&#19978;&#24494;&#19981;&#36275;&#36947;&#30340;&#24773;&#20917;&#19979;&#25193;&#22823;&#20102;&#23398;&#20064;&#34920;&#31034;&#65292;&#21363;&#26631;&#35760;&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;AltUp&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#20854;&#22312;&#24207;&#21015;&#32500;&#24230;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;AltUp&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#26356;&#39640;&#23481;&#37327;&#30340;&#39640;&#25928;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;Transformer&#27169;&#22411;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;AltUp&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#35299;&#37322;&#27010;&#24565;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#31616;&#27905;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#20415;&#20110;&#38750;&#19987;&#23478;&#29702;&#35299;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.05109</link><description>&lt;p&gt;
&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#35299;&#37322;$\mathcal{ELH}$&#27010;&#24565;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual Reasoning. (arXiv:2301.05109v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#35299;&#37322;&#27010;&#24565;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#31616;&#27905;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#20415;&#20110;&#38750;&#19987;&#23478;&#29702;&#35299;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20449;&#24687;&#31649;&#29702;&#65292;&#33021;&#22815;&#25903;&#25345;&#39640;&#24433;&#21709;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#23427;&#20204;&#20063;&#20316;&#20026;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#21307;&#30103;&#35786;&#26029;&#21644;&#20449;&#29992;&#35780;&#20998;&#12290;&#30001;&#20110;&#21463;&#21040;&#36825;&#20123;&#20915;&#31574;&#24433;&#21709;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#24076;&#26395;&#20102;&#35299;&#20182;&#20204;&#30340;&#24773;&#20917;&#24182;&#39564;&#35777;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#37322;&#26041;&#27861;&#12290;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#27010;&#24565;&#26469;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#31181;&#22266;&#26377;&#36879;&#26126;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#21475;&#22836;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#27010;&#24565;&#20063;&#20250;&#21464;&#24471;&#20887;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#19987;&#23478;&#32780;&#35328;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#20351;&#29992;&#21453;&#20107;&#23454;&#26469;&#22238;&#31572;&#38382;&#39064;&#65306;&#8220;&#20026;&#20102;&#24471;&#21040;&#19981;&#21516;&#30340;&#20998;&#31867;&#65292;&#29305;&#24449;&#20540;&#24212;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#36890;&#36807;&#20851;&#27880;&#26368;&#23567;&#30340;&#29305;&#24449;&#21464;&#21270;&#65292;&#35299;&#37322;&#21464;&#24471;&#30701;&#23567;&#12289;&#26131;&#20110;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21464;&#21270;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#30340;&#26126;&#30830;&#34892;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge bases are widely used for information management, enabling high-impact applications such as web search, question answering, and natural language processing. They also serve as the backbone for automatic decision systems, e.g., for medical diagnostics and credit scoring. As stakeholders affected by these decisions would like to understand their situation and verify how fair the decisions are, a number of explanation approaches have been proposed. An intrinsically transparent way to do classification is by using concepts in description logics. However, these concepts can become long and difficult to fathom for non-experts, even when verbalized. One solution is to employ counterfactuals to answer the question, ``How must feature values be changed to obtain a different classification?'' By focusing on the minimal feature changes, the explanations are short, human-friendly, and provide a clear path of action regarding the change in prediction. While previous work investigated coun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2301.03403</link><description>&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#32508;&#21512;&#22238;&#39038;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24341;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#25105;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#20851;&#20110;&#27599;&#20010;&#25105;&#20204;&#24819;&#28085;&#30422;&#30340;&#20027;&#39064;&#30340;&#19968;&#20123;&#27969;&#34892;&#21644;&#33879;&#21517;&#30340;&#35770;&#25991;&#24320;&#22987;&#65292;&#28982;&#21518;&#25105;&#20204;&#36861;&#36394;&#20102;&#8220;&#21521;&#21518;&#24341;&#29992;&#8221;&#65288;&#34987;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#24341;&#29992;&#30340;&#35770;&#25991;&#65289;&#21644;&#8220;&#21521;&#21069;&#24341;&#29992;&#8221;&#65288;&#24341;&#29992;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#30340;&#36739;&#26032;&#35770;&#25991;&#65289;&#12290;&#20026;&#20102;&#32452;&#32455;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#22522;&#20110;&#19981;&#21516;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#38500;&#20102;&#20171;&#32461;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#21487;&#29992;&#20110;&#25688;&#35201;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22238;&#39038;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#37329;&#26631;&#20934;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a literature review about Automatic Text Summarization (ATS) systems. We consider a citation-based approach. We start with some popular and well-known papers that we have in hand about each topic we want to cover and we have tracked the "backward citations" (papers that are cited by the set of papers we knew beforehand) and the "forward citations" (newer papers that cite the set of papers we knew beforehand). In order to organize the different methods, we present the diverse approaches to ATS guided by the mechanisms they use to generate a summary. Besides presenting the methods, we also present an extensive review of the datasets available for summarization tasks and the methods used to evaluate the quality of the summaries. Finally, we present an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.13976</link><description>&lt;p&gt;
&#29992;&#24341;&#23548;&#24819;&#35937;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Expanding Small-Scale Datasets with Guided Imagination. (arXiv:2211.13976v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21151;&#25928;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#36890;&#24120;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#39033;&#21517;&#20026;&#25968;&#25454;&#38598;&#25193;&#20805;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#25193;&#20805;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#23574;&#31471;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#20174;&#36755;&#20837;&#30340;&#31181;&#23376;&#25968;&#25454;&#20013;&#8220;&#24819;&#35937;&#8221;&#24182;&#21019;&#24314;&#20449;&#24687;&#20016;&#23500;&#30340;&#26032;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GIF&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#30340;&#28508;&#22312;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#30340;&#24819;&#35937;&#65292;&#20174;&#32780;&#21019;&#24314;&#20855;&#26377;&#26032;&#20869;&#23481;&#30340;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#24341;&#23548;&#24819;&#35937;&#26397;&#30528;&#21019;&#24314;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#20449;&#24687;&#20016;&#23500;&#26679;&#26412;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#26631;&#20934;&#65292;&#21363;&#31867;&#21035;&#20445;&#25345;&#20449;&#24687;&#25552;&#21319;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20419;&#36827;&#12290;&#36825;&#20123;&#26631;&#20934;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time-consuming. To address this issue, we explore a new task, termed dataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boosting and sample diversity promotion. These criteria are verified to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#24212;&#29992;&#20110;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#26799;&#24230;&#26356;&#26032;&#30340;&#24191;&#21578;&#25968;&#25454;&#20013;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.11896</link><description>&lt;p&gt;
&#20351;&#29992;DP-SGD&#30340;&#31169;&#26377;&#24191;&#21578;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Private Ad Modeling with DP-SGD. (arXiv:2211.11896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#24212;&#29992;&#20110;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#26799;&#24230;&#26356;&#26032;&#30340;&#24191;&#21578;&#25968;&#25454;&#20013;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#12290;&#23613;&#31649;&#35813;&#31639;&#27861;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#24050;&#32463;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#21578;&#25968;&#25454;&#65292;&#32780;&#24191;&#21578;&#25968;&#25454;&#22240;&#20854;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31232;&#30095;&#30340;&#26799;&#24230;&#26356;&#26032;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26412;&#30740;&#31350;&#25105;&#20204;&#23558;DP-SGD&#24212;&#29992;&#20110;&#22810;&#20010;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#39044;&#27979;&#28857;&#20987;&#29575;&#12289;&#36716;&#21270;&#29575;&#21644;&#36716;&#21270;&#20107;&#20214;&#25968;&#37327;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23454;&#35777;&#20102;DP-SGD&#21487;&#20197;&#20026;&#24191;&#21578;&#24314;&#27169;&#20219;&#21153;&#25552;&#20379;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known algorithm in privacy-preserving ML is differentially private stochastic gradient descent (DP-SGD). While this algorithm has been evaluated on text and image data, it has not been previously applied to ads data, which are notorious for their high class imbalance and sparse gradient updates. In this work we apply DP-SGD to several ad modeling tasks including predicting click-through rates, conversion rates, and number of conversion events, and evaluate their privacy-utility trade-off on real-world datasets. Our work is the first to empirically demonstrate that DP-SGD can provide both privacy and utility for ad modeling tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#21516;&#27493;&#36890;&#20449;&#20551;&#35774;&#21644;&#21435;&#38500;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#24615;&#20551;&#35774;&#26469;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.01176</link><description>&lt;p&gt;
PersA-FL&#65306;&#20010;&#24615;&#21270;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#21516;&#27493;&#36890;&#20449;&#20551;&#35774;&#21644;&#21435;&#38500;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#24615;&#20551;&#35774;&#26469;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#24076;&#26395;&#33719;&#24471;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#33021;&#22815;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#20010;&#24615;&#21270;&#26694;&#26550;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#21644;&#65288;ii&#65289;Moreau&#21253;&#32476;&#65288;ME&#65289;&#12290;MAML&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#36866;&#24212;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#32780;ME&#36890;&#36807;&#38544;&#24335;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#36890;&#36807;&#35268;&#33539;&#21270;&#25439;&#22833;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;&#26377;&#30028;&#28382;&#21518;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#32479;&#19968;&#35777;&#26126;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MAML&#21644;ME&#20010;&#24615;&#21270;&#26694;&#26550;&#12290;&#38024;&#23545;&#24179;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25152;&#30740;&#31350;&#30340;&#20989;&#25968;&#31867;&#65292;&#21435;&#38500;&#20102;&#26799;&#24230;&#33539;&#25968;&#30340;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2209.14827</link><description>&lt;p&gt;
&#35770;AdaGrad&#22312;$\R^{d}$&#19978;&#30340;&#25910;&#25947;&#24615;&#65306;&#36229;&#36234;&#20984;&#24615;&#12289;&#38750;&#28176;&#36817;&#36895;&#29575;&#21644;&#21152;&#36895;&#65288;arXiv&#65306;2209.14827v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#23637;&#31034;&#20102;AdaGrad&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#26126;&#30830;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#30340;AdaGrad&#21644;&#20854;&#20182;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#20998;&#26512;&#36890;&#24120;&#26159;&#38024;&#23545;&#20855;&#26377;&#26377;&#30028;&#23450;&#20041;&#22495;&#30452;&#24452;&#30340;&#20989;&#25968;&#12290;&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20445;&#35777;&#20102;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#24658;&#23450;&#22240;&#23376;&#65292;&#36825;&#36866;&#29992;&#20110;&#25972;&#20010;&#20989;&#25968;&#31867;&#12290;&#27492;&#22806;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21482;&#20998;&#26512;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;&#26412;&#30340;AdaGrad&#65292;&#19982;&#36890;&#24120;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#29256;&#26412;&#19981;&#21516;&#65292;&#22312;&#36825;&#20010;&#22238;&#24402;&#20013;&#19981;&#20351;&#29992;&#26368;&#26032;&#30340;&#26799;&#24230;&#26469;&#26356;&#26032;&#27493;&#24133;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#24182;&#22312;&#24179;&#28369;&#20984;&#20989;&#25968;&#30340;&#26631;&#20934;&#24773;&#20917;&#19979;&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;quasar&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#28145;&#20837;&#29702;&#35299;AdaGrad&#21450;&#20854;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#25216;&#26415;&#65292;&#26126;&#30830;&#22320;&#38480;&#23450;&#20102;vanilla AdaGrad&#22312;&#26080;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#26080;&#35770;&#26159;&#30830;&#23450;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AdaGrad&#21464;&#31181;&#65292;&#25105;&#20204;&#21487;&#20197;&#23637;&#31034;l&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#36807;&#31243;&#23478;&#26063;&#26088;&#22312;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20803;&#23398;&#20064;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24102;&#26469;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.00517</link><description>&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;&#23478;&#26063;&#65306;&#35843;&#26597;&#12289;&#24212;&#29992;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
The Neural Process Family: Survey, Applications and Perspectives. (arXiv:2209.00517v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00517
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;&#23478;&#26063;&#26088;&#22312;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#28857;&#65292;&#23454;&#29616;&#20803;&#23398;&#20064;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24102;&#26469;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#33021;&#21147;&#65292;&#20294;&#22312;&#23398;&#20064;&#20803;&#34920;&#31034;&#21644;&#25512;&#29702;&#20854;&#39044;&#27979;&#20013;&#30340;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39640;&#26031;&#36807;&#31243;&#37319;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#26696;&#26469;&#20272;&#35745;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#21463;&#21040;&#20854;&#25928;&#29575;&#21644;&#36924;&#36817;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#36807;&#31243;&#23478;&#26063;&#65288;NPF&#65289;&#24847;&#22312;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20803;&#23398;&#20064;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#31181;&#28508;&#21147;&#24050;&#32463;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#27963;&#21160;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#19968;&#39033;&#20840;&#38754;&#30340;NPF&#27169;&#22411;&#35843;&#26597;&#65292;&#20197;&#32452;&#32455;&#21644;&#20851;&#32852;&#23427;&#20204;&#30340;&#21160;&#26426;&#12289;&#26041;&#27861;&#21644;&#23454;&#39564;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#28145;&#20837;&#25506;&#35752;&#20851;&#20110;&#23478;&#26063;&#25104;&#21592;&#30340;&#24418;&#24335;&#21270;&#12289;&#30740;&#31350;&#20027;&#39064;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30528;&#37325;&#38416;&#36848;&#23427;&#20204;&#22312;&#24102;&#26469;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20123;&#26368;&#26032;&#36827;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard approaches to neural network implementation yield powerful function approximation capabilities but are limited in their abilities to learn meta representations and reason probabilistic uncertainties in their predictions. Gaussian processes, on the other hand, adopt the Bayesian learning scheme to estimate such uncertainties but are constrained by their efficiency and approximation capacity. The Neural Processes Family (NPF) intends to offer the best of both worlds by leveraging neural networks for meta-learning predictive uncertainties. Such potential has brought substantial research activity to the family in recent years. Therefore, a comprehensive survey of NPF models is needed to organize and relate their motivation, methodology, and experiments. This paper intends to address this gap while digging deeper into the formulation, research themes, and applications concerning the family members. We shed light on their potential to bring several recent advances in other deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#36807;&#31243;&#36716;&#25442;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#20284;&#28982;&#27604;&#26469;&#23450;&#20301;&#36807;&#31243;&#20013;&#30340;&#21464;&#28857;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.10317</link><description>&lt;p&gt;
&#28508;&#22312;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Latent Neural Stochastic Differential Equations for Change Point Detection. (arXiv:2208.10317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#36807;&#31243;&#36716;&#25442;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#20284;&#28982;&#27604;&#26469;&#23450;&#20301;&#36807;&#31243;&#20013;&#30340;&#21464;&#28857;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#35835;&#25968;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#33258;&#21160;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#29992;&#20110;&#23450;&#20301;&#36807;&#31243;&#30340;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#20013;&#30340;&#31361;&#21464;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#26032;&#22411;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#23558;&#36807;&#31243;&#38750;&#32447;&#24615;&#22320;&#36716;&#25442;&#20026;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20272;&#35745;&#25551;&#36848;&#20854;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;SDE&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#20284;&#28982;&#27604;&#26469;&#25214;&#21040;&#36807;&#31243;&#30340;&#21464;&#28857;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#22823;&#22810;&#25968;&#23454;&#39564;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated analysis of complex systems based on multiple readouts remains a challenge. Change point detection algorithms are aimed to locating abrupt changes in the time series behaviour of a process. In this paper, we present a novel change point detection algorithm based on Latent Neural Stochastic Differential Equations (SDE). Our method learns a non-linear deep learning transformation of the process into a latent space and estimates a SDE that describes its evolution over time. The algorithm uses the likelihood ratio of the learned stochastic processes in different timestamps to find change points of the process. We demonstrate the detection capabilities and performance of our algorithm on synthetic and real-world datasets. The proposed method outperforms the state-of-the-art algorithms on the majority of our experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27979;&#35797;&#39537;&#21160;&#20195;&#30721;&#29983;&#25104;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#30340;&#27979;&#35797;&#24418;&#24335;&#21270;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#25490;&#21517;&#20195;&#30721;&#24314;&#35758;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#20195;&#30721;&#24314;&#35758;&#38598;&#12290;</title><link>http://arxiv.org/abs/2208.05950</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#35797;&#39537;&#21160;&#30340;&#29992;&#25143;&#24847;&#22270;&#35268;&#33539;&#21270;&#36827;&#34892;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Interactive Code Generation via Test-Driven User-Intent Formalization. (arXiv:2208.05950v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27979;&#35797;&#39537;&#21160;&#20195;&#30721;&#29983;&#25104;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#30340;&#27979;&#35797;&#24418;&#24335;&#21270;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#25490;&#21517;&#20195;&#30721;&#24314;&#35758;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#20195;&#30721;&#24314;&#35758;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#36890;&#36807;&#20174;&#38750;&#27491;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#24847;&#22270;&#29983;&#25104;&#33258;&#28982;&#20195;&#30721;&#26469;&#33258;&#21160;&#21270;&#32534;&#30721;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#29992;&#25143;&#19981;&#33021;&#20445;&#35777;&#29983;&#25104;&#30340;&#20195;&#30721;&#24314;&#35758;&#27491;&#30830;&#22320;&#28385;&#36275;&#20854;&#25552;&#20379;&#30340;&#24847;&#22270;&#12290;&#20107;&#23454;&#19978;&#65292;&#24456;&#38590;&#23450;&#20041;&#27491;&#30830;&#24615;&#30340;&#27010;&#24565;&#65292;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#21487;&#33021;&#23384;&#22312;&#27495;&#20041;&#65292;&#24182;&#19988;&#32570;&#20047;&#24418;&#24335;&#21270;&#35821;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20132;&#20114;&#24335;&#27979;&#35797;&#39537;&#21160;&#20195;&#30721;&#29983;&#25104;&#8221;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23427;&#21033;&#29992;&#36731;&#37327;&#32423;&#29992;&#25143;&#21453;&#39304;&#26469;&#65288;a&#65289;&#20351;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#26469;&#24418;&#24335;&#21270;&#29992;&#25143;&#24847;&#22270;&#65292;&#36825;&#23545;&#20110;&#35843;&#35797;&#38750;&#24120;&#26377;&#29992;&#65292;&#20197;&#21450;&#65288;b&#65289;&#36890;&#36807;&#20462;&#21098;&#21644;&#25490;&#21517;&#20505;&#36873;&#20195;&#30721;&#24314;&#35758;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#20195;&#30721;&#24314;&#35758;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#25277;&#35937;&#31639;&#27861;&#21644;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#29616;TiCoder&#12290;&#25105;&#20204;&#22312;\emph {MBPP}&#21644;\emph {HumanEval}&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#23545;TiCoder&#36827;&#34892;&#20102;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, when interacting with LLMs, users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.  In this paper, we propose the workflow of {\it interactive test-driven code generation}, which leverages lightweight user feedback to (a) formalize the user intent using generated tests that can be useful for debugging, and (b) produce an improved set of code suggestions by pruning and ranking candidate code suggestions. We describe a language-agnostic abstract algorithm and a concrete implementation TiCoder. We perform an automated evaluation of TiCoder on the \emph{MBPP} and \emph{HumanEval} code generation benchmarks. Our results are promising with using 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#36827;&#34892;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20197;&#21450;&#26377;&#20851;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2207.10574</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#20998;&#26512;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10574
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#36827;&#34892;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20197;&#21450;&#26377;&#20851;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#35821;&#35328;&#27807;&#36890;&#20316;&#20026;&#31038;&#20250;&#21644;&#24515;&#29702;&#29616;&#35937;&#27979;&#37327;&#30340;&#35777;&#25454;&#65292;&#33258;&#21160;&#21270;&#30340;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#23545;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#26816;&#27979;&#19982;&#31038;&#20132;&#29305;&#24449;&#65288;&#22914;&#39046;&#23548;&#21147;&#12289;&#25903;&#37197;&#21147;&#12289;&#20010;&#24615;&#29305;&#24449;&#65289;&#12289;&#31038;&#20132;&#35282;&#33394;/&#20851;&#31995;&#21644;&#20114;&#21160;&#21160;&#24577;&#65288;&#22914;&#22242;&#38431;&#20957;&#32858;&#21147;&#12289;&#21442;&#19982;&#24230;&#12289;&#34701;&#27965;&#24230;&#65289;&#30456;&#20851;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#23548;&#33268;&#26377;&#25928;&#24615;&#33021;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#22312;&#28041;&#21450;&#26368;&#24191;&#27867;&#30340;&#31038;&#20250;&#29616;&#35937;&#21644;&#20114;&#21160;&#22330;&#26223;&#65288;&#33258;&#30001;&#23545;&#35805;&#12289;&#20250;&#35758;&#12289;&#20108;&#20803;&#32452;&#21644;&#20154;&#32676;&#65289;&#26041;&#38754;&#19982;&#20854;&#21516;&#31867;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24635;&#32467;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#23454;&#26045;&#12289;&#25968;&#25454;&#38598;&#31574;&#21010;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#19968;&#20123;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#26159;&#20849;&#21516; ...
&lt;/p&gt;
&lt;p&gt;
Automated co-located human-human interaction analysis has been addressed by the use of nonverbal communication as measurable evidence of social and psychological phenomena. We survey the computing studies (since 2010) detecting phenomena related to social traits (e.g., leadership, dominance, personality traits), social roles/relations, and interaction dynamics (e.g., group cohesion, engagement, rapport). Our target is to identify the nonverbal cues and computational methodologies resulting in effective performance. This survey differs from its counterparts by involving the widest spectrum of social phenomena and interaction settings (free-standing conversations, meetings, dyads, and crowds). We also present a comprehensive summary of the related datasets and outline future research directions which are regarding the implementation of artificial intelligence, dataset curation, and privacy-preserving interaction analysis. Some major observations are: the most often used nonverbal cue, co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#20013;&#20013;&#38388;&#22495;&#26377;&#38480;&#19988;&#36317;&#31163;&#36739;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;</title><link>http://arxiv.org/abs/2206.11492</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#20934;&#21270;&#27969;&#36827;&#34892;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Gradual Domain Adaptation via Normalizing Flows. (arXiv:2206.11492v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#20013;&#20013;&#38388;&#22495;&#26377;&#38480;&#19988;&#36317;&#31163;&#36739;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#26102;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#25928;&#26524;&#19981;&#20339;&#12290;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#28041;&#21450;&#21033;&#29992;&#36880;&#28176;&#20174;&#28304;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#30340;&#20013;&#38388;&#22495;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20551;&#35774;&#20013;&#38388;&#22495;&#30340;&#25968;&#37327;&#36739;&#22823;&#19988;&#30456;&#37051;&#22495;&#20043;&#38388;&#30340;&#36317;&#31163;&#36739;&#23567;&#65292;&#22240;&#27492;&#65292;&#28041;&#21450;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#36880;&#28176;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36880;&#28176;&#33258;&#25105;&#35757;&#32451;&#23558;&#22833;&#36133;&#65292;&#22240;&#20026;&#20013;&#38388;&#22495;&#30340;&#25968;&#37327;&#26377;&#38480;&#19988;&#30456;&#37051;&#22495;&#20043;&#38388;&#30340;&#36317;&#31163;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#28304;&#22495;&#21040;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard domain adaptation methods do not work well when a large gap exists between the source and target domains. Gradual domain adaptation is one of the approaches used to address the problem. It involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. In previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled datasets, is applicable. In practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. We propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. The proposed method learns a transformation from the distribution of the target domain to the Gaussian mixture distribution via the source domain. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21098;&#36753;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#35843;&#25972;&#21098;&#36753;&#38408;&#20540;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.07136</link><description>&lt;p&gt;
&#33258;&#21160;&#21098;&#36753;&#65306;&#26356;&#31616;&#21333;&#21644;&#26356;&#24378;&#22823;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21098;&#36753;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#35843;&#25972;&#21098;&#36753;&#38408;&#20540;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#26679;&#26412;&#26799;&#24230;&#21098;&#36753;&#26159;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#30340;&#20851;&#38190;&#31639;&#27861;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#21098;&#36753;&#38408;&#20540;R&#30340;&#36873;&#25321;&#23545;&#20110;&#22312;DP&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#31216;&#20026;&#33258;&#21160;&#21098;&#36753;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#20219;&#20309;DP&#20248;&#21270;&#22120;&#65288;&#21253;&#25324;DP-SGD&#12289;DP-Adam&#12289;DP-LAMB&#31561;&#65289;&#35843;&#25972;R&#30340;&#38656;&#35201;&#12290;&#33258;&#21160;&#21464;&#37327;&#19982;&#29616;&#26377;&#30340;DP&#20248;&#21270;&#22120;&#19968;&#26679;&#20855;&#26377;&#38544;&#31169;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#19981;&#38656;&#35201;DP&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#27492;&#20351;&#24471;DP&#35757;&#32451;&#20687;&#26631;&#20934;&#30340;&#38750;&#38544;&#31169;&#35757;&#32451;&#19968;&#26679;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#23545;&#33258;&#21160;DP-SGD&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#34920;&#26126;&#22312;&#26679;&#26412;&#26799;&#24230;&#30340;&#23545;&#31216;&#24615;&#22122;&#22768;&#20551;&#35774;&#19979;&#65288;&#22312;&#38750;DP&#25991;&#29486;&#20013;&#24120;&#29992;&#65289;&#65292;&#23427;&#33021;&#22815;&#20139;&#21463;&#19982;&#26631;&#20934;SGD&#30456;&#21516;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold R, however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune R for any DP optimizers, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision tasks that automatic clipping outperforms o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#30340;&#20851;&#31995;&#65292;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.03851</link><description>&lt;p&gt;
&#22312;&#26080;&#20559;&#25512;&#33616;&#20013;&#37325;&#26032;&#32771;&#34385;&#23398;&#20064;&#30446;&#26631;&#65306;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective. (arXiv:2206.03851v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#30340;&#20851;&#31995;&#65292;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#35299;&#37322;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#21521;&#21453;&#39304;&#20013;&#23398;&#20064;&#26080;&#20559;&#31639;&#27861;&#36827;&#34892;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#24067;&#36716;&#31227;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#22312;&#26080;&#20559;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#21508;&#31181;&#25216;&#26415;&#22914;&#37325;&#26032;&#21152;&#26435;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#37096;&#20998;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#26368;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#26080;&#20559;&#23398;&#20064;&#30446;&#26631;&#20026;&#20309;&#36866;&#29992;&#20110;&#26080;&#20559;&#25512;&#33616;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26080;&#20559;&#25512;&#33616;&#19982;&#20998;&#24067;&#36716;&#31227;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#26174;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#20559;&#23398;&#20064;&#30446;&#26631;&#38544;&#21547;&#22320;&#23558;&#26377;&#20559;&#30340;&#35757;&#32451;&#20998;&#24067;&#19982;&#26080;&#20559;&#30340;&#27979;&#35797;&#20998;&#24067;&#23545;&#40784;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#31995;&#65292;&#25105;&#20204;&#38024;&#23545;&#29616;&#26377;&#30340;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#21457;&#23637;&#20102;&#20004;&#20010;&#27867;&#21270;&#30028;&#38480;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#25311;&#29275;&#39039;&#31639;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25928;&#29575;&#65292;&#20998;&#26512;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#21644;&#23545;&#31216;&#31209;&#19968;SR1&#20004;&#31181;&#26356;&#26032;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#32773;&#30340;&#20248;&#21155;&#65292;&#25506;&#35752;&#20102;SR1&#31639;&#27861;&#22312;&#22788;&#29702;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30149;&#24577;&#38797;&#28857;&#26102;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.09121</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#25311;&#29275;&#39039;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the efficiency of Stochastic Quasi-Newton Methods for Deep Learning. (arXiv:2205.09121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#25311;&#29275;&#39039;&#31639;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25928;&#29575;&#65292;&#20998;&#26512;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#21644;&#23545;&#31216;&#31209;&#19968;SR1&#20004;&#31181;&#26356;&#26032;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#32773;&#30340;&#20248;&#21155;&#65292;&#25506;&#35752;&#20102;SR1&#31639;&#27861;&#22312;&#22788;&#29702;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30149;&#24577;&#38797;&#28857;&#26102;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#19968;&#38454;&#26041;&#27861;&#38750;&#24120;&#27969;&#34892;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#26126;&#26174;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#32570;&#28857;&#65292;&#26368;&#36817;&#19968;&#30452;&#26377;&#20852;&#36259;&#24212;&#29992;&#22522;&#20110;&#25311;&#29275;&#39039;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26500;&#24314;Hessian&#30697;&#38453;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#38543;&#26426;&#25311;&#29275;&#39039;&#31639;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#25311;&#29275;&#39039;&#26356;&#26032;&#31639;&#27861;&#65292;&#21363;&#26377;&#38480;&#20869;&#23384;Broyden-Fletcher-Goldfarb-Shanno&#65288;BFGS&#65289;&#21644;&#23545;&#31216;&#31209;&#19968;&#65288;SR1&#65289;&#31639;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#20851;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#30495;&#23454;&#24615;&#33021;&#30340;&#31354;&#30333;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;BFGS&#31639;&#27861;&#36824;&#26159;&#26356;&#24265;&#20215;&#30340;SR1&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#26159;&#21542;&#24471;&#21040;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#65292;SR1&#31639;&#27861;&#20801;&#35768;&#19981;&#23450;Hessian&#30697;&#38453;&#36817;&#20284;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36991;&#24320;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#30149;&#24577;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While first-order methods are popular for solving optimization problems that arise in large-scale deep learning problems, they come with some acute deficiencies. To diminish such shortcomings, there has been recent interest in applying second-order methods such as quasi-Newton based methods which construct Hessians approximations using only gradient information. The main focus of our work is to study the behaviour of stochastic quasi-Newton algorithms for training deep neural networks. We have analyzed the performance of two well-known quasi-Newton updates, the limited memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the Symmetric Rank One (SR1). This study fills a gap concerning the real performance of both updates and analyzes whether more efficient training is obtained when using the more robust BFGS update or the cheaper SR1 formula which allows for indefinite Hessian approximations and thus can potentially help to better navigate the pathological saddle points present in the non
&lt;/p&gt;</description></item><item><title>GFlowNets&#20351;&#29992;&#36712;&#36857;&#24179;&#34913;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#23398;&#20064;&#30446;&#26631;&#20013;&#20449;&#29992;&#20256;&#25773;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25910;&#25947;&#24615;&#12289;&#29983;&#25104;&#26679;&#26412;&#22810;&#26679;&#24615;&#20197;&#21450;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2201.13259</link><description>&lt;p&gt;
&#36712;&#36857;&#24179;&#34913;&#65306;&#25913;&#36827;&#20102;GFlowNets&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Trajectory balance: Improved credit assignment in GFlowNets. (arXiv:2201.13259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13259
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#20351;&#29992;&#36712;&#36857;&#24179;&#34913;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#23398;&#20064;&#30446;&#26631;&#20013;&#20449;&#29992;&#20256;&#25773;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25910;&#25947;&#24615;&#12289;&#29983;&#25104;&#26679;&#26412;&#22810;&#26679;&#24615;&#20197;&#21450;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#21160;&#20316;&#24207;&#21015;&#29983;&#25104;&#32452;&#21512;&#23545;&#35937;&#65288;&#22914;&#22270;&#24418;&#25110;&#23383;&#31526;&#20018;&#65289;&#30340;&#38543;&#26426;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35768;&#22810;&#21487;&#33021;&#30340;&#21160;&#20316;&#24207;&#21015;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#25552;&#20986;&#30340;GFlowNets&#23398;&#20064;&#30446;&#26631;&#65292;&#21363;&#27969;&#21305;&#37197;&#21644;&#35814;&#32454;&#24179;&#34913;&#65292;&#31867;&#20284;&#20110;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65292;&#23481;&#26131;&#22312;&#38271;&#30340;&#21160;&#20316;&#24207;&#21015;&#20013;&#20986;&#29616;&#20449;&#29992;&#20256;&#25773;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21363;&#36712;&#36857;&#24179;&#34913;&#65292;&#20316;&#20026;&#20808;&#21069;&#20351;&#29992;&#30446;&#26631;&#30340;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36712;&#36857;&#24179;&#34913;&#30446;&#26631;&#30340;&#20219;&#20309;&#20840;&#23616;&#26497;&#23567;&#20540;&#21487;&#20197;&#23450;&#20041;&#19968;&#20010;&#20174;&#30446;&#26631;&#20998;&#24067;&#31934;&#30830;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#36712;&#36857;&#24179;&#34913;&#30446;&#26631;&#23545;&#20110;GFlowNet&#25910;&#25947;&#24615;&#12289;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#23545;&#38271;&#21160;&#20316;&#24207;&#21015;&#21644;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item><item><title>AKE-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#22312;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#20132;&#25442;&#36890;&#36947;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2106.05455</link><description>&lt;p&gt;
AKE-GNN: &#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#27969;&#30340;&#26377;&#25928;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange. (arXiv:2106.05455v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05455
&lt;/p&gt;
&lt;p&gt;
AKE-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#22312;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#20132;&#25442;&#36890;&#36947;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#25366;&#25496;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#23398;&#21040;&#30340;&#26435;&#37325;(&#36890;&#36947;)&#26159;&#39640;&#24230;&#20887;&#20313;&#30340;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#38480;&#21046;&#20102;GNN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#20026;&#20102;&#25928;&#29575;&#32771;&#34385;&#32780;&#31227;&#38500;&#36825;&#20123;&#20887;&#20313;&#36890;&#36947;&#65292;&#32780;&#26159;&#35797;&#22270;&#37325;&#26032;&#28608;&#27963;&#23427;&#20204;&#65292;&#20197;&#25193;&#22823;GNN&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AKE-GNN&#30340;&#26032;&#22411;GNN&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23545;&#22270;&#22686;&#24378;&#29983;&#25104;&#30340;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#36827;&#34892;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;AKE-GNN&#39318;&#20808;&#35757;&#32451;&#22810;&#20010;GNN&#65292;&#27599;&#20010;&#23545;&#24212;&#19968;&#20010;&#22270;&#35270;&#22270;&#65292;&#20197;&#33719;&#24471;&#20449;&#24687;&#36890;&#36947;&#12290;&#28982;&#21518;&#65292;AKE-GNN&#36845;&#20195;&#22320;&#20197;&#36880;&#23618;&#26041;&#24335;&#22312;&#19968;&#20010;GNN&#30340;&#26435;&#37325;&#21442;&#25968;&#30697;&#38453;&#20013;&#36827;&#34892;&#20887;&#20313;&#36890;&#36947;&#19982;&#21478;&#19968;&#20010;GNN&#30340;&#20449;&#24687;&#36890;&#36947;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have already been widely used in various graph mining tasks. However, recent works reveal that the learned weights (channels) in well-trained GNNs are highly redundant, which inevitably limits the performance of GNNs. Instead of removing these redundant channels for efficiency consideration, we aim to reactivate them to enlarge the representation capacity of GNNs for effective graph learning. In this paper, we propose to substitute these redundant channels with other informative channels to achieve this goal. We introduce a novel GNN learning framework named AKE-GNN, which performs the Adaptive Knowledge Exchange strategy among multiple graph views generated by graph augmentations. AKE-GNN first trains multiple GNNs each corresponding to one graph view to obtain informative channels. Then, AKE-GNN iteratively exchanges redundant channels in the weight parameter matrix of one GNN with informative channels of another GNN in a layer-wise manner. Additionally, 
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;LQR&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22825;&#30495;&#30340;&#25506;&#32034;&#26159;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#23567;&#36951;&#25022;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#20110;&#35299;&#20915;&#22312;&#32447;&#33258;&#36866;&#24212;&#25511;&#21046;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2001.09576</link><description>&lt;p&gt;
Naive Exploration is Optimal for Online LQR&#65288;&#22312;&#32447;LQR&#38382;&#39064;&#20013;&#65292;&#22825;&#30495;&#30340;&#25506;&#32034;&#26159;&#26368;&#20248;&#30340;&#65289;
&lt;/p&gt;
&lt;p&gt;
Naive Exploration is Optimal for Online LQR. (arXiv:2001.09576v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.09576
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;LQR&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22825;&#30495;&#30340;&#25506;&#32034;&#26159;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#23567;&#36951;&#25022;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#20110;&#35299;&#20915;&#22312;&#32447;&#33258;&#36866;&#24212;&#25511;&#21046;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#22312;&#32447;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30495;&#23454;&#31995;&#32479;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26032;&#30340;&#19978;&#19979;&#30028;&#65292;&#34920;&#26126;&#26368;&#20248;&#36951;&#25022;&#30340;&#23610;&#24230;&#19982;$\widetilde{\Theta}({\sqrt{d_{\mathbf{u}}^2 d_{\mathbf{x}} T}})$&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$d_{\mathbf{u}}$&#26159;&#36755;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;$d_{\mathbf{x}}$&#26159;&#31995;&#32479;&#29366;&#24577;&#30340;&#32500;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19979;&#30028;&#25490;&#38500;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;$\mathrm{poly}(\log{}T)$&#36951;&#25022;&#31639;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#38382;&#39064;&#26126;&#26174;&#30340;&#24378;&#20984;&#24615;&#24341;&#20986;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#30340;&#19978;&#30028;&#36890;&#36807;&#19968;&#31181;&#31616;&#21333;&#30340;$\textit{{&#30830;&#23450;&#24615;&#31561;&#20215;&#25511;&#21046;}}$&#30340;&#21464;&#20307;&#24471;&#21040;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26681;&#25454;&#23545;&#31995;&#32479;&#30340;&#20272;&#35745;&#36873;&#25321;&#25511;&#21046;&#36755;&#20837;&#65292;&#24182;&#27880;&#20837;&#25506;&#32034;&#24615;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;$\sqrt{T}$&#30340;&#36951;&#25022;(Raina et al. 2019)&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#23398;&#20064;&#32773;&#19981;&#26029;&#25913;&#36827;&#20182;&#20204;&#30340;&#20272;&#35745;&#65292;&#25214;&#21040;&#20102;&#31995;&#32479;&#30340;&#30495;&#23454;&#21442;&#25968;&#65292;&#21017;&#21487;&#20197;&#23454;&#29616;&#38646;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online adaptive control of the linear quadratic regulator, where the true system parameters are unknown. We prove new upper and lower bounds demonstrating that the optimal regret scales as $\widetilde{\Theta}({\sqrt{d_{\mathbf{u}}^2 d_{\mathbf{x}} T}})$, where $T$ is the number of time steps, $d_{\mathbf{u}}$ is the dimension of the input space, and $d_{\mathbf{x}}$ is the dimension of the system state. Notably, our lower bounds rule out the possibility of a $\mathrm{poly}(\log{}T)$-regret algorithm, which had been conjectured due to the apparent strong convexity of the problem. Our upper bound is attained by a simple variant of $\textit{{certainty equivalent control}}$, where the learner selects control inputs according to the optimal controller for their estimate of the system while injecting exploratory random noise. While this approach was shown to achieve $\sqrt{T}$-regret by (Mania et al. 2019), we show that if the learner continually refines their esti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32500;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#31181;&#32593;&#32476;&#65292;L2&#27491;&#21017;&#21270;&#22238;&#24402;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23545;&#24212;&#20110;&#23545;&#20272;&#35745;&#30340;&#20108;&#38454;&#23548;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26089;&#20572;&#27490;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24179;&#28369;&#26679;&#26465;&#22238;&#24402;&#20043;&#38388;&#30340;&#26032;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/1911.02903</link><description>&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;ReLU&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#21051;&#30011;&#23398;&#20064;&#20989;&#25968; - &#31532;&#19968;&#37096;&#20998;&#65306;&#38543;&#26426;&#31532;&#19968;&#23618;&#30340;&#20004;&#23618;&#19968;&#32500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
How Implicit Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part I: the 1-D Case of Two Layers with Random First Layer. (arXiv:1911.02903v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.02903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32500;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#31181;&#32593;&#32476;&#65292;L2&#27491;&#21017;&#21270;&#22238;&#24402;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23545;&#24212;&#20110;&#23545;&#20272;&#35745;&#30340;&#20108;&#38454;&#23548;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26089;&#20572;&#27490;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24179;&#28369;&#26679;&#26465;&#22238;&#24402;&#20043;&#38388;&#30340;&#26032;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32500;&#65288;&#27973;&#23618;&#65289;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#38543;&#26426;&#36873;&#25321;&#30340;&#65292;&#21482;&#26377;&#32456;&#31471;&#23618;&#36827;&#34892;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#31181;&#32593;&#32476;&#65292;L2&#27491;&#21017;&#21270;&#22238;&#24402;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23545;&#24212;&#20110;&#23545;&#20272;&#35745;&#30340;&#20108;&#38454;&#23548;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#20110;&#30456;&#24403;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23545;&#20110;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#22312;&#38544;&#34255;&#33410;&#28857;&#25968;&#36235;&#21521;&#26080;&#31351;&#26102;&#25910;&#25947;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#24179;&#28369;&#26679;&#26465;&#25554;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26089;&#20572;&#27490;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;&#27809;&#26377;&#23545;&#26435;&#37325;&#36827;&#34892;&#26174;&#24335;&#27491;&#21017;&#21270;&#65289;&#19982;&#24179;&#28369;&#26679;&#26465;&#22238;&#24402;&#20043;&#38388;&#30340;&#26032;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider one dimensional (shallow) ReLU neural networks in which weights are chosen randomly and only the terminal layer is trained. First, we mathematically show that for such networks L2-regularized regression corresponds in function space to regularizing the estimate's second derivative for fairly general loss functionals. For least squares regression, we show that the trained network converges to the smooth spline interpolation of the training data as the number of hidden nodes tends to infinity. Moreover, we derive a novel correspondence between the early stopped gradient descent (without any explicit regularization of the weights) and the smoothing spline regression.
&lt;/p&gt;</description></item></channel></rss>