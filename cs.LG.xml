<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15053</link><description>&lt;p&gt;
&#20851;&#20110;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#30340;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65306;(1) &#36890;&#36807;(&#27169;&#25311;)&#22312;&#32447;&#23454;&#39564;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#37329;&#26631;&#20934;&#65292;&#25110;&#32773;(2) &#36890;&#36807;&#19968;&#20123;&#31163;&#32447;&#35780;&#20272;&#31243;&#24207;&#65292;&#30446;&#26631;&#26159;&#36817;&#20284;&#22312;&#32447;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;&#25991;&#29486;&#20013;&#37319;&#29992;&#20102;&#20960;&#31181;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#65292;&#21463;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#25490;&#21517;&#25351;&#26631;&#30340;&#21551;&#21457;&#12290;(Normalised) Discounted Cumulative Gain (nDCG)&#26159;&#20854;&#20013;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#24456;&#22810;&#24180;&#37324;&#65292;&#26356;&#39640;&#30340;(n)DCG&#20540;&#34987;&#29992;&#26469;&#23637;&#31034;&#26032;&#26041;&#27861;&#22312;Top-n&#25512;&#33616;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#35270;&#65292;&#24182;&#30740;&#31350;&#20102;&#25105;&#20204;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#19978;&#27491;&#24335;&#25552;&#20986;&#20102;DCG&#34987;&#35748;&#20026;&#26159;&#22312;&#32447;&#22870;&#21169;&#30340;&#26080;&#20559;&#20272;&#35745;&#30340;&#20551;&#35774;&#65292;&#24182;&#32473;&#20986;&#20102;&#36825;&#20010;&#25351;&#26631;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.  Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#38463;&#25289;&#20271;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer Transducer&#21644;&#26631;&#20934;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#21270;&#21644;&#35821;&#35328;&#35268;&#21017;&#19981;&#32771;&#34385;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.15045</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38463;&#25289;&#20271;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#38463;&#25289;&#20271;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer Transducer&#21644;&#26631;&#20934;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#21270;&#21644;&#35821;&#35328;&#35268;&#21017;&#19981;&#32771;&#34385;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#35782;&#21035;&#26159;&#27169;&#24335;&#35782;&#21035;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#39046;&#22495;&#24191;&#27867;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#35782;&#21035;&#31163;&#32447;&#38463;&#25289;&#20271;&#25163;&#20889;&#25991;&#26412;&#30340;&#29305;&#23450;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24314;&#27169;&#65292;&#20351;&#29992;&#32852;&#32467;&#26102;&#24207;&#20998;&#31867;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24207;&#21015;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#24182;&#34892;&#21270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#32771;&#34385;&#35821;&#35328;&#35268;&#21017;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#21518;&#22788;&#29702;&#38454;&#27573;&#20351;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26367;&#20195;&#26550;&#26500;&#65292;&#21363;Transformer Transducer&#21644;&#26631;&#20934;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;Transformer&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and spee
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.15043</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36890;&#29992;&#21644;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#24341;&#36215;&#21453;&#24863;&#30340;&#20869;&#23481;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#29983;&#25104;&#12290;&#23613;&#31649;&#22312;&#35268;&#36991;&#36825;&#20123;&#25514;&#26045;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#25152;&#35859;&#30340;&#23545;LLMs&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#20154;&#20026;&#30340;&#24039;&#24605;&#65292;&#23454;&#38469;&#19978;&#24182;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#21518;&#32512;&#65292;&#24403;&#38468;&#21152;&#21040;&#21508;&#31181;&#26597;&#35810;&#19978;&#65292;&#20379;LLM&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#26102;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#20135;&#29983;&#32943;&#23450;&#22238;&#31572;&#65288;&#32780;&#19981;&#26159;&#25298;&#32477;&#22238;&#31572;&#65289;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#19988;&#22312;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15034</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#21152;&#36895;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15034
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#31639;&#22120;&#30340;&#20195;&#29702;&#26144;&#23556;&#12290;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#28857;&#65292;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#37117;&#26159;&#37325;&#35201;&#29942;&#39048;&#12290;&#34429;&#28982;&#23545;&#20110;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26377;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20294;&#37027;&#20123;&#21482;&#36866;&#29992;&#20110;&#26377;&#38480;&#32500;&#24230;&#19978;&#30340;&#23454;&#20540;&#25968;&#25454;&#31867;&#22411;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22312;&#22797;&#20540;&#65288;&#20613;&#37324;&#21494;&#65289;&#22495;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#37325;&#35201;&#25805;&#20316;&#30340;FNO&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#26412;&#36523;&#23601;&#26159;&#19968;&#27425;&#36817;&#20284;&#65288;&#30001;&#20110;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#23384;&#22312;&#65289;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#20197;&#23436;&#20840;&#31934;&#24230;&#25191;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;i&#65289;&#23545;&#20351;&#29992;&#20840;&#31934;&#24230;&#21644;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;FNO&#36827;&#34892;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#21078;&#26512;&#65292;&#65288;ii&#65289;&#23545;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26694;&#26550;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35265;&#30340;&#30772;&#22351;&#65292;&#24182;&#23454;&#29616;&#29305;&#24449;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15019</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22270;&#21464;&#25442;&#22120;&#29992;&#20110;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26694;&#26550;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35265;&#30340;&#30772;&#22351;&#65292;&#24182;&#23454;&#29616;&#29305;&#24449;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#20266;&#36896;&#29289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#35757;&#32451;&#21644;&#27979;&#35797;&#22312;&#20869;&#37096;&#20998;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26410;&#30693;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31995;&#32479;&#24517;&#39035;&#23545;&#20266;&#36896;&#31867;&#22411;&#12289;&#22806;&#35266;&#21644;&#36136;&#37327;&#20445;&#25345;&#20844;&#27491;&#65292;&#20197;&#30830;&#20445;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#23581;&#35797;&#25552;&#39640;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#27979;&#35797;&#24120;&#35265;&#30340;&#21518;&#22788;&#29702;&#25200;&#21160;&#26102;&#65292;&#22914;&#35270;&#39057;&#21387;&#32553;&#25110;&#27169;&#31946;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25215;&#21463;&#24120;&#35265;&#30340;&#30772;&#22351;&#65292;&#24182;&#23454;&#29616;&#29305;&#24449;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2307.15017</link><description>&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#37319;&#26679;&#21311;&#21517;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#31169;&#26377;&#32479;&#35745;&#21327;&#35758;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38544;&#31169;&#36134;&#21153;&#65292;&#25509;&#36817;&#20110;&#38598;&#20013;&#35774;&#32622;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#35813;&#21407;&#35821;&#30340;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#23433;&#20840;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.15016</link><description>&lt;p&gt;
Google Bard&#30340;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#22914;&#20309;&#65311;&#24320;&#25918;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19982;OpenAI&#30340;ChatGPT&#25104;&#20026;&#20102;&#24378;&#22823;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Bard&#26368;&#36817;&#24050;&#32463;&#26356;&#26032;&#65292;&#21487;&#20197;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#22788;&#29702;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#37492;&#20110;Bard&#22312;&#22788;&#29702;&#25991;&#26412;&#36755;&#20837;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#30001;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25506;&#32034;&#26377;&#28508;&#21147;&#25581;&#31034;Bard&#21644;&#20854;&#20182;&#21363;&#23558;&#21457;&#24067;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#20934;&#30830;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#22797;&#26434;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#26102;&#30340;&#26032;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;15&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#21253;&#25324;&#24120;&#35268;&#12289;&#20266;&#35013;&#12289;&#21307;&#23398;&#12289;&#27700;&#19979;&#21644;&#36965;&#24863;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;Bard&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;Bard&#22312;&#36825;&#20123;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#31361;&#26174;&#20102;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#21512;&#25104;&#27963;&#24615;&#24494;&#31890;&#31995;&#32479;&#36827;&#34892;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#21644;&#33258;&#32806;&#21512;&#30340;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#22122;&#22768;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.15010</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#27963;&#24615;&#31890;&#23376;&#36827;&#34892;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Harnessing Synthetic Active Particles for Physical Reservoir Computing. (arXiv:2307.15010v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#21512;&#25104;&#27963;&#24615;&#24494;&#31890;&#31995;&#32479;&#36827;&#34892;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#21644;&#33258;&#32806;&#21512;&#30340;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#22122;&#22768;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#22788;&#29702;&#26159;&#29983;&#29289;&#31995;&#32479;&#19981;&#21487;&#25110;&#32570;&#30340;&#29305;&#24615;&#65292;&#30001;&#20855;&#26377;&#24040;&#22823;&#22797;&#26434;&#24615;&#30340;&#27963;&#21160;&#36807;&#31243;&#32593;&#32476;&#23454;&#29616;&#12290;&#36825;&#20123;&#32593;&#32476;&#28608;&#21457;&#20102;&#35768;&#22810;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#21464;&#20307;&#20043;&#19968;&#65292;&#21363;&#20648;&#23618;&#35745;&#31639;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#34928;&#33853;&#35760;&#24518;&#30340;&#33410;&#28857;&#32593;&#32476;&#20013;&#23454;&#29616;&#20648;&#23618;&#35745;&#31639;&#65292;&#21487;&#20197;&#36827;&#34892;&#35745;&#31639;&#21644;&#22797;&#26434;&#39044;&#27979;&#12290;&#20648;&#23618;&#21487;&#22312;&#35745;&#31639;&#26426;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#20063;&#21487;&#20197;&#22312;&#38750;&#20256;&#32479;&#30340;&#29289;&#29702;&#22522;&#36136;&#19978;&#23454;&#29616;&#65292;&#22914;&#26426;&#26800;&#25391;&#23376;&#12289;&#33258;&#26059;&#25110;&#32454;&#33740;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#33258;&#32452;&#32455;&#30340;&#21512;&#25104;&#27963;&#24615;&#24494;&#31890;&#31995;&#32479;&#36827;&#34892;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#65292;&#35813;&#31995;&#32479;&#30001;&#27963;&#24615;&#21644;&#34987;&#21160;&#32452;&#25104;&#30340;&#25104;&#20998;&#24418;&#25104;&#22266;&#26377;&#30340;&#22122;&#22768;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21333;&#20803;&#12290;&#21333;&#20803;&#30340;&#33258;&#32452;&#32455;&#21644;&#21160;&#21147;&#21709;&#24212;&#26159;&#36890;&#36807;&#24494;&#28216;&#21160;&#22120;&#23545;&#34987;&#21160;&#30446;&#26631;&#30340;&#24310;&#36831;&#25512;&#36827;&#26469;&#23454;&#29616;&#30340;&#12290;&#36890;&#36807;&#24310;&#36831;&#21709;&#24212;&#30340;&#33258;&#32806;&#21512;&#65292;&#36825;&#26679;&#30340;&#21333;&#20803;&#20648;&#23618;&#21487;&#20197;&#25191;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#23613;&#31649;&#23384;&#22312;&#24378;&#28872;&#30340;&#22122;&#22768;&#21644;&#38750;&#32447;&#24615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The processing of information is an indispensable property of living systems realized by networks of active processes with enormous complexity. They have inspired many variants of modern machine learning one of them being reservoir computing, in which stimulating a network of nodes with fading memory enables computations and complex predictions. Reservoirs are implemented on computer hardware, but also on unconventional physical substrates such as mechanical oscillators, spins, or bacteria often summarized as physical reservoir computing. Here we demonstrate physical reservoir computing with a synthetic active microparticle system that self-organizes from an active and passive component into inherently noisy nonlinear dynamical units. The self-organization and dynamical response of the unit is the result of a delayed propulsion of the microswimmer to a passive target. A reservoir of such units with a self-coupling via the delayed response can perform predictive tasks despite the strong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#33021;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#35780;&#20272;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#25104;&#21151;&#30772;&#35299;&#20102;AI-Guardian&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#36807;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#23454;&#26045;&#25915;&#20987;&#31639;&#27861;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15008</link><description>&lt;p&gt;
&#36890;&#36807;LLM&#36741;&#21161;&#65292;&#23545;AI-Guardian&#30340;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#33021;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#35780;&#20272;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#25104;&#21151;&#30772;&#35299;&#20102;AI-Guardian&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#36807;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#23454;&#26045;&#25915;&#20987;&#31639;&#27861;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#33021;&#22815;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GPT-4&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#36741;&#21161;&#36827;&#34892;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#20197;AI-Guardian&#20026;&#26696;&#20363;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20010;&#26368;&#36817;&#22312;IEEE S&amp;P 2023&#19978;&#21457;&#34920;&#30340;&#38024;&#23545;&#23545;&#25239;&#26679;&#26412;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#30772;&#35299;&#20102;&#36825;&#20010;&#38450;&#24481;&#26426;&#21046;&#65306;&#19982;&#26410;&#38450;&#24481;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#24182;&#27809;&#26377;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24182;&#27809;&#26377;&#32534;&#20889;&#25915;&#20987;&#35813;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#32780;&#26159;&#25351;&#31034;&#21644;&#24341;&#23548;GPT-4&#25353;&#29031;&#25105;&#20204;&#30340;&#25351;&#20196;&#23454;&#26045;&#25152;&#26377;&#25915;&#20987;&#31639;&#27861;&#12290;&#36825;&#20010;&#36807;&#31243;&#20986;&#22855;&#22320;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#26377;&#26102;&#20505;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#26126;&#30830;&#30340;&#25351;&#20196;&#19979;&#20135;&#29983;&#30340;&#20195;&#30721;&#27604;&#26412;&#25991;&#20316;&#32773;&#36824;&#35201;&#24555;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#65288;1&#65289;&#35780;&#20272;&#20013;&#20986;&#29616;&#30340;&#35686;&#31034;&#20449;&#21495;&#34920;&#26126;AI-Guardian&#23558;&#34987;&#25915;&#30772;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25105;&#20204;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#25915;&#20987;&#26041;&#26696;&#26102;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&amp;P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.  We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15007</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#29305;&#24449;&#24402;&#22240;&#65306;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#19968;&#30452;&#24378;&#35843;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#27010;&#36848;&#20102;&#20004;&#31181;&#24191;&#27867;&#30340;&#31574;&#30053;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65307;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#24544;&#23454;&#65292;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#25105;&#20204;&#26080;&#27861;&#39564;&#35777;&#23427;&#20204;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#36807;&#23558;&#35299;&#37322;&#24615;&#20449;&#24687;&#26126;&#30830;&#32534;&#30721;&#21040;&#27169;&#22411;&#26550;&#26500;&#20013;&#26469;&#35268;&#36991;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35299;&#37322;&#33258;&#28982;&#24544;&#23454;&#19988;&#21487;&#39564;&#35777;&#65292;&#20294;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#20204;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t
&lt;/p&gt;</description></item><item><title>Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14993</link><description>&lt;p&gt;
Thinker: &#23398;&#20064;&#35268;&#21010;&#21644;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14993
&lt;/p&gt;
&lt;p&gt;
Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Thinker&#31639;&#27861;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#19982;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24182;&#21033;&#29992;&#20854;&#12290; Thinker&#31639;&#27861;&#36890;&#36807;&#32473;&#29615;&#22659;&#28155;&#21152;&#19990;&#30028;&#27169;&#22411;&#26469;&#25913;&#21464;&#29615;&#22659;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#19982;&#19990;&#30028;&#27169;&#22411;&#20132;&#20114;&#30340;&#26032;&#21160;&#20316;&#12290;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#22312;&#36873;&#25321;&#26368;&#32456;&#30340;&#29615;&#22659;&#21160;&#20316;&#20043;&#21069;&#21521;&#19990;&#30028;&#27169;&#22411;&#25552;&#20986;&#22791;&#36873;&#35745;&#21010;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#35268;&#21010;&#26469;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20195;&#29702;&#30340;&#35745;&#21010;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;Thinker&#31639;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;Thinker&#31639;&#27861;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#24050;&#32463;&#23398;&#21040;&#20102;&#20248;&#31168;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#20013;&#38388;&#20540;&#24182;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.14988</link><description>&lt;p&gt;
&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#20013;&#38388;&#20540;&#24182;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#29992;&#25143;&#36755;&#20837;&#65289;&#26102;&#24120;&#38754;&#20020;&#30528;&#39640;&#25928;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#35745;&#31639;&#26469;&#36866;&#24212;&#36755;&#20837;&#21464;&#21270;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#26469;&#31163;&#25955;&#21270;&#32593;&#32476;&#20013;&#30340;&#20013;&#38388;&#20540;&#65292;&#24182;&#36807;&#28388;&#22122;&#22768;&#21644;&#19981;&#24517;&#35201;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#20462;&#25913;&#65292;&#20174;&#32780;&#20419;&#36827;&#20540;&#30340;&#37325;&#29992;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Transformer&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#37327;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14971</link><description>&lt;p&gt;
Take-A-Photo: &#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#28857;&#20113;&#27169;&#22411;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MAE&#24102;&#39046;&#19979;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#21319;&#22522;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;Transformer&#20026;&#22522;&#30784;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#28857;&#20113;&#30340;&#26080;&#24207;&#24615;&#38480;&#21046;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#28857;&#20113;&#27169;&#22411;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#19981;&#21516;&#30340;&#23039;&#21183;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#20854;&#28857;&#20113;&#23545;&#24212;&#29289;&#65292;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#20174;&#32780;&#24110;&#21161;3D&#32972;&#39592;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#26377;&#25928;&#22320;&#25552;&#21319;...
&lt;/p&gt;
&lt;p&gt;
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#31232;&#30095;&#25512;&#29702;&#31639;&#27861;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#33258;&#32452;&#32455;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#35299;&#37322;&#23439;&#35266;&#27169;&#24335;&#24418;&#25104;&#30340;&#23616;&#37096;&#20027;&#23548;&#21147;&#24179;&#34913;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#25773;&#24102;&#26159;&#30001;&#23494;&#24230;&#26799;&#24230;&#39537;&#21160;&#30340;&#23616;&#37096;&#23545;&#40784;&#30456;&#20114;&#20316;&#29992;&#24418;&#25104;&#30340;&#65292;&#32780;&#31283;&#24577;&#26143;&#29615;&#26159;&#30001;&#25197;&#26354;&#24341;&#36215;&#30340;&#36127;&#21387;&#32553;&#24615;&#26426;&#21046;&#22609;&#36896;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.14970</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#32452;&#32455;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#23616;&#37096;&#20027;&#23548;&#21147;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Learning locally dominant force balances in active particle systems. (arXiv:2307.14970v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14970
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#31232;&#30095;&#25512;&#29702;&#31639;&#27861;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#33258;&#32452;&#32455;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#35299;&#37322;&#23439;&#35266;&#27169;&#24335;&#24418;&#25104;&#30340;&#23616;&#37096;&#20027;&#23548;&#21147;&#24179;&#34913;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#25773;&#24102;&#26159;&#30001;&#23494;&#24230;&#26799;&#24230;&#39537;&#21160;&#30340;&#23616;&#37096;&#23545;&#40784;&#30456;&#20114;&#20316;&#29992;&#24418;&#25104;&#30340;&#65292;&#32780;&#31283;&#24577;&#26143;&#29615;&#26159;&#30001;&#25197;&#26354;&#24341;&#36215;&#30340;&#36127;&#21387;&#32553;&#24615;&#26426;&#21046;&#22609;&#36896;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#31232;&#30095;&#25512;&#29702;&#31639;&#27861;&#30340;&#32452;&#21512;&#26469;&#23398;&#20064;&#35299;&#37322;&#33258;&#32452;&#32455;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#23439;&#35266;&#27169;&#24335;&#24418;&#25104;&#30340;&#23616;&#37096;&#20027;&#23548;&#21147;&#24179;&#34913;&#12290;&#33258;&#32452;&#32455;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#23439;&#35266;&#27169;&#24335;&#26469;&#33258;&#20110;&#33258;&#39537;&#21160;&#31890;&#23376;&#20043;&#38388;&#30340;&#24494;&#35266;&#30456;&#20114;&#20316;&#29992;&#65292;&#26159;&#19968;&#31181;&#24191;&#27867;&#35266;&#23519;&#21040;&#30340;&#33258;&#28982;&#29616;&#35937;&#12290;&#23613;&#31649;&#27969;&#20307;&#21160;&#21147;&#23398;&#29702;&#35770;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#30340;&#29289;&#29702;&#22522;&#30784;&#65292;&#20294;&#26159;&#22312;&#27963;&#21160;&#31890;&#23376;&#31995;&#32479;&#20013;&#65292;&#35782;&#21035;&#20986;&#22609;&#36896;&#12289;&#35843;&#33410;&#21644;&#32500;&#25345;&#33258;&#32452;&#32455;&#32467;&#26500;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#36275;&#22815;&#38598;&#21512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#32463;&#20856;&#30340;&#33258;&#39537;&#21160;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23427;&#20135;&#29983;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#27169;&#24335;&#65292;&#22914;&#26143;&#29615;&#21644;&#31227;&#21160;&#23494;&#24230;&#24102;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#26174;&#31034;&#65292;&#20256;&#25773;&#24102;&#26159;&#30001;&#23494;&#24230;&#26799;&#24230;&#39537;&#21160;&#30340;&#23616;&#37096;&#23545;&#40784;&#30456;&#20114;&#20316;&#29992;&#24418;&#25104;&#30340;&#65292;&#32780;&#31283;&#24577;&#26143;&#29615;&#26159;&#30001;&#19968;&#31181;&#25197;&#26354;&#24341;&#36215;&#30340;&#36127;&#21387;&#32553;&#24615;&#26426;&#21046;&#22609;&#36896;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a combination of unsupervised clustering and sparsity-promoting inference algorithms to learn locally dominant force balances that explain macroscopic pattern formation in self-organized active particle systems. The self-organized emergence of macroscopic patterns from microscopic interactions between self-propelled particles can be widely observed nature. Although hydrodynamic theories help us better understand the physical basis of this phenomenon, identifying a sufficient set of local interactions that shape, regulate, and sustain self-organized structures in active particle systems remains challenging. We investigate a classic hydrodynamic model of self-propelled particles that produces a wide variety of patterns, like asters and moving density bands. Our data-driven analysis shows that propagating bands are formed by local alignment interactions driven by density gradients, while steady-state asters are shaped by a mechanism of splay-induced negative compressibility arising
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#23454;&#29616;&#32852;&#37030;&#27169;&#22411;&#32858;&#21512;&#12290;&#26681;&#25454;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#30417;&#30563;&#36741;&#21161;&#32593;&#32476;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#20351;&#29992;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;&#19968;&#33268;&#30340;&#21457;&#25955;&#27979;&#37327;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#24341;&#23548;&#20840;&#23616;&#27169;&#22411;&#20248;&#21270;&#30340;&#21160;&#24577;&#24179;&#34913;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#24230;&#40065;&#26834;&#21644;&#26080;&#20559;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14959</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#23454;&#29616;&#32852;&#37030;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#23454;&#29616;&#32852;&#37030;&#27169;&#22411;&#32858;&#21512;&#12290;&#26681;&#25454;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#30417;&#30563;&#36741;&#21161;&#32593;&#32476;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#20351;&#29992;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;&#19968;&#33268;&#30340;&#21457;&#25955;&#27979;&#37327;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#24341;&#23548;&#20840;&#23616;&#27169;&#22411;&#20248;&#21270;&#30340;&#21160;&#24577;&#24179;&#34913;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#24230;&#40065;&#26834;&#21644;&#26080;&#20559;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#22788;&#29702;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30382;&#32932;&#30149;&#21464;&#21644;&#32963;&#32928;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#26041;&#27861;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#30001;&#20110;&#19981;&#21516;&#20154;&#32676;&#12289;&#21457;&#29616;&#21644;&#25195;&#25551;&#20202;&#23548;&#33268;&#30340;&#21307;&#23398;&#22270;&#20687;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#30417;&#30563;&#36741;&#21161;&#32593;&#32476;&#30740;&#31350;&#20102;&#23458;&#25143;&#38388;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#23616;&#37096;&#20351;&#29992;&#20849;&#20139;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;MoCo-V2&#65289;&#21487;&#20197;&#24471;&#21040;&#19968;&#33268;&#30340;&#21457;&#25955;&#27979;&#37327;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#30417;&#30563;&#20808;&#39564;&#65288;MAS&#65289;&#24341;&#23548;&#20840;&#23616;&#27169;&#22411;&#20248;&#21270;&#30340;&#21160;&#24577;&#24179;&#34913;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#12290;Fed-MAS&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#23454;&#29616;&#23545;&#39640;&#24230;&#40065;&#26834;&#21644;&#26080;&#20559;&#20840;&#23616;&#27169;&#22411;&#30340;&#26377;&#25928;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \url{https://github.com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#32593;&#32476;&#23481;&#38169;&#21644;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#31038;&#20132;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#36890;&#20449;&#25925;&#38556;&#21644;&#25932;&#23545;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#20449;&#24687;&#34701;&#21512;&#35268;&#21017;&#21644;&#20855;&#26377;&#21487;&#35777;&#26126;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14952</link><description>&lt;p&gt;
&#32593;&#32476;&#23481;&#38169;&#21644;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#31038;&#20132;&#23398;&#20064;&#36890;&#36807;&#21327;&#20316;&#24335;&#20998;&#23618;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning. (arXiv:2307.14952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#32593;&#32476;&#23481;&#38169;&#21644;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#31038;&#20132;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#36890;&#20449;&#25925;&#38556;&#21644;&#25932;&#23545;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#20449;&#24687;&#34701;&#21512;&#35268;&#21017;&#21644;&#20855;&#26377;&#21487;&#35777;&#26126;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#29616;&#26377;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#24320;&#22987;&#33853;&#21518;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#65292;&#22914;&#65288;1&#65289;&#20449;&#24687;&#20256;&#25773;&#32531;&#24930;&#65292;&#65288;2&#65289;&#32593;&#32476;&#36890;&#20449;&#25925;&#38556;&#21644;&#65288;3&#65289;&#22806;&#37096;&#25932;&#23545;&#25915;&#20987;&#12290;&#26412;&#25991;&#20851;&#27880;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;&#23481;&#26131;&#21463;&#21040;&#36890;&#20449;&#25925;&#38556;&#21644;&#25932;&#23545;&#25915;&#20987;&#30340;&#32593;&#32476;&#19978;&#30340;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#32593;&#32476;&#36890;&#20449;&#26041;&#38754;&#65292;&#32771;&#34385;&#21253;&#20002;&#22833;&#30340;&#38142;&#36335;&#25925;&#38556;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#40065;&#26834;&#30340;push-sum&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#39057;&#32321;&#30340;&#21253;&#20002;&#22833;&#38142;&#36335;&#25925;&#38556;&#24773;&#20917;&#19979;&#23454;&#29616;&#24179;&#22343;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20026;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#20219;&#24847;&#36873;&#25321;&#30340;&#32593;&#32476;&#20195;&#34920;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#31181;&#31232;&#30095;&#20449;&#24687;&#34701;&#21512;&#35268;&#21017;&#12290;&#28982;&#21518;&#65292;&#23558;&#19968;&#33268;&#24615;&#26356;&#26032;&#27493;&#39588;&#19982;&#20197;KL&#25955;&#24230;&#20316;&#20026;&#37051;&#36817;&#20989;&#25968;&#30340;&#21452;&#22343;&#20540;&#26356;&#26032;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#26126;&#25910;&#25947;&#24615;&#30340;&#23481;&#38169;&#38750;&#36125;&#21494;&#26031;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.  We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence gu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14940</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#26041;&#27861;&#29992;&#20110;&#23558;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(Neural ODEs)&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#33258;&#28982;&#31995;&#32479;&#30340;&#36830;&#32493;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#32780;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#65292;&#27169;&#22411;&#24517;&#39035;&#36981;&#24490;&#31649;&#29702;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#35268;&#24459;&#25110;&#23450;&#24459;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#20989;&#25968;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#24809;&#32602;&#21442;&#25968;&#12290;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#26377;&#21161;&#20110;&#22686;&#21152;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#19977;&#20010;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;(&#20154;&#21475;&#22686;&#38271;&#65292;&#21270;&#23398;&#21453;&#24212;&#28436;&#21270;&#21644;&#38459;&#23612;&#35856;&#25391;&#36816;&#21160;)&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#22312;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#24809;&#32602;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#21644;&#21407;&#22987;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14938</link><description>&lt;p&gt;
&#39640;&#25928;&#20114;&#21160;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#29615;&#30340;&#21306;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24320;&#29615;&#31995;&#32479;&#30340;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#38597;&#21487;&#27604;&#36793;&#30028;&#30340;&#24320;&#29615;&#21160;&#21147;&#23398;&#21253;&#21547;&#20989;&#25968;&#30340;&#26032;&#31867;&#21035;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#23545;&#20110;&#20219;&#24847;&#21160;&#21147;&#31995;&#32479;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#20989;&#25968;&#26500;&#24314;&#19968;&#20010;&#29366;&#24577;&#25968;&#26159;&#21407;&#31995;&#32479;&#20004;&#20493;&#30340;&#23884;&#20837;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#23884;&#20837;&#31995;&#32479;&#30340;&#21333;&#20010;&#36712;&#36857;&#21487;&#20197;&#25552;&#20379;&#21487;&#36798;&#38598;&#30340;&#36229;&#30697;&#24418;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#30340;&#38381;&#29615;&#23884;&#20837;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a computationally efficient framework for interval reachability of neural network controlled systems. Our approach builds upon inclusion functions for the neural network controller and the open-loop system. We observe that many state-of-the-art neural network verifiers can produce inclusion functions for neural networks. We introduce and analyze a new class of inclusion functions for the open-loop dynamics based on bounds of the function Jacobian that is particularly suitable for capturing the interactions between systems and neural network controllers. Next, for any dynamical system, we use inclusion functions to construct an embedding system with twice the number of states as the original system. We show that a single trajectory of this embedding system provides hyper-rectangular over-approximations of reachable sets. We then propose two approaches for constructing a closed-loop embedding system for a neural network controlled dynamical system that accounts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;</title><link>http://arxiv.org/abs/2307.14936</link><description>&lt;p&gt;
PanGu-Coder2&#65306;&#21033;&#29992;&#25490;&#21517;&#21453;&#39304;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#27599;&#21608;&#37117;&#26377;&#26032;&#30340;&#24378;&#22823;&#27169;&#22411;&#21457;&#24067;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#35757;&#32451;&#30340;Code LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#12289;&#25351;&#20196;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RRTF&#65288;Rank Responses to align Test&amp;Teacher Feedback&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#25552;&#21319;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PanGu-Coder2&#65292;&#22312;OpenAI HumanEval&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;62.20%&#30340;&#19968;&#32423;&#36890;&#36807;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;CoderEval&#21644;LeetCode&#22522;&#20934;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PanGu-Coder2&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
&lt;/p&gt;</description></item><item><title>Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.14935</link><description>&lt;p&gt;
&#20351;&#29992;Desbordante&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65306;&#19968;&#39033;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14935
&lt;/p&gt;
&lt;p&gt;
Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#26159;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#34892;&#19994;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#65292;&#21253;&#25324;&#20989;&#25968;&#20381;&#36182;&#12289;&#25968;&#25454;&#32422;&#26463;&#12289;&#20851;&#32852;&#35268;&#21017;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#22797;&#26434;&#32479;&#35745;&#30340;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#31995;&#32479;&#22312;&#19982;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#30340;&#24037;&#20855;&#36827;&#34892;&#36866;&#24403;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#22312;&#34892;&#19994;&#20013;&#23545;&#36825;&#20123;&#24037;&#20855;&#37319;&#29992;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#31995;&#32479;&#24182;&#19981;&#32771;&#34385;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#19981;&#26088;&#22312;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#20160;&#20040;&#25214;&#19981;&#21040;&#32473;&#23450;&#30340;&#27169;&#24335;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#20102;&#35299;&#29305;&#23450;&#27169;&#24335;&#32570;&#22833;&#30340;&#26681;&#26412;&#21407;&#22240;&#23545;&#22522;&#20110;&#25968;&#25454;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#24335;&#23454;&#38469;&#19978;&#26159;&#28040;&#22833;&#22312;&#31354;&#20013;&#20013;&#65306;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#30456;&#23545;&#26377;&#38480;&#65292;&#24456;&#23569;&#34987;&#24191;&#22823;&#20844;&#20247;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20998;&#21035;&#29983;&#25104;&#38899;&#20048;&#22270;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25351;&#23450;&#20048;&#22120;&#26469;&#26465;&#20214;&#29983;&#25104;&#38899;&#20048;&#65292;&#20026;&#38899;&#20048;&#20849;&#21019;&#30340;&#20154;&#26426;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14928</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graph-based Polyphonic Multitrack Music Generation. (arXiv:2307.14928v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20998;&#21035;&#29983;&#25104;&#38899;&#20048;&#22270;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25351;&#23450;&#20048;&#22120;&#26469;&#26465;&#20214;&#29983;&#25104;&#38899;&#20048;&#65292;&#20026;&#38899;&#20048;&#20849;&#21019;&#30340;&#20154;&#26426;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#22810;&#38899;&#36712;&#30340;&#31526;&#21495;&#38899;&#20048;&#65292;&#20854;&#20013;&#38899;&#31526;&#12289;&#21644;&#24358;&#21644;&#25972;&#20010;&#29255;&#27573;&#21487;&#20197;&#25353;&#29031;&#38899;&#35843;&#21644;&#33410;&#22863;&#30340;&#20851;&#31995;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#22312;&#38899;&#20048;&#29983;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#32570;&#20047;&#32771;&#34385;&#22270;&#34920;&#31034;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#31034;&#21644;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#33258;&#21160;&#32534;&#30721;&#22120;&#20998;&#21035;&#29983;&#25104;&#38899;&#20048;&#22270;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#20854;&#23618;&#27425;&#32467;&#26500;&#19982;&#38899;&#20048;&#30340;&#32467;&#26500;&#20808;&#39564;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23558;&#38899;&#20048;&#22270;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#20998;&#31163;&#65292;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#29305;&#23450;&#26102;&#38388;&#25773;&#25918;&#30340;&#20048;&#22120;&#26469;&#26465;&#20214;&#29983;&#25104;&#65292;&#20026;&#38899;&#20048;&#20849;&#21019;&#30340;&#20154;&#26426;&#20114;&#21160;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#29616;&#26377;&#30340;MIDI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;...
&lt;/p&gt;
&lt;p&gt;
Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20004;&#20010;HPC&#31995;&#32479;&#19978;&#36827;&#34892;&#26448;&#26009;&#20998;&#21106;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;Vulcanite&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#20855;&#26377;&#26356;&#24555;&#30340;&#27169;&#22411;&#26102;&#38388;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#65292;&#32780;Onyx&#30340;&#27169;&#22411;&#26102;&#38388;&#22312;&#25152;&#26377;&#27979;&#35797;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.14921</link><description>&lt;p&gt;
&#22312;&#20004;&#20010;HPC&#31995;&#32479;&#19978;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26448;&#26009;&#20998;&#21106;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems. (arXiv:2307.14921v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20004;&#20010;HPC&#31995;&#32479;&#19978;&#36827;&#34892;&#26448;&#26009;&#20998;&#21106;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;Vulcanite&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#20013;&#20855;&#26377;&#26356;&#24555;&#30340;&#27169;&#22411;&#26102;&#38388;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#65292;&#32780;Onyx&#30340;&#27169;&#22411;&#26102;&#38388;&#22312;&#25152;&#26377;&#27979;&#35797;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HPC&#31995;&#32479;&#30340;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#25552;&#20379;&#20449;&#24687;&#65292;&#20197;&#22686;&#21152;&#24615;&#33021;&#24182;&#25913;&#21892;&#31649;&#29702;&#36825;&#20123;&#31995;&#32479;&#30340;&#20316;&#19994;&#35843;&#24230;&#31243;&#24207;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;GPU&#21152;&#36895;&#33410;&#28857;&#19978;&#36827;&#34892;&#26448;&#26009;&#20998;&#21106;&#20998;&#26512;&#24182;&#25910;&#38598;&#24615;&#33021;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;&#32463;&#36807;MMdnn&#24037;&#20855;&#21253;&#23558;Caffe&#36716;&#25442;&#20026;PyTorch&#30340;ML&#27169;&#22411;&#21644;MINC-2500&#25968;&#25454;&#38598;&#12290;&#22312;&#20004;&#20010;ERDC DSRC&#31995;&#32479;Onyx&#21644;Vulcanite&#19978;&#25910;&#38598;&#20102;&#24615;&#33021;&#25968;&#25454;&#12290;&#25968;&#25454;&#26174;&#31034;&#65292;&#23613;&#31649;Vulcanite&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#26356;&#24555;&#30340;&#27169;&#22411;&#26102;&#38388;&#65292;&#20294;&#23427;&#20063;&#26356;&#23481;&#26131;&#21463;&#21040;&#19968;&#20123;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#24615;&#33021;&#20302;&#20110;Onyx&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Onyx&#30340;&#27169;&#22411;&#26102;&#38388;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#28982;&#25903;&#25345;&#24615;&#29289;&#20214;&#65288;NSA&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;&#35270;&#35273;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20197;&#25269;&#24481;&#33258;&#28982;&#30772;&#22351;&#12290;NSAs&#26159;&#36890;&#36807;&#20351;&#29992;DC-GAN&#36827;&#34892;&#29289;&#20214;&#35757;&#32451;&#29983;&#25104;&#30340;&#33258;&#28982;&#22806;&#35266;&#29289;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.14917</link><description>&lt;p&gt;
NSA: &#33258;&#28982;&#25903;&#25345;&#24615;&#29289;&#20214;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#28982;&#25903;&#25345;&#24615;&#29289;&#20214;&#65288;NSA&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;&#35270;&#35273;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20197;&#25269;&#24481;&#33258;&#28982;&#30772;&#22351;&#12290;NSAs&#26159;&#36890;&#36807;&#20351;&#29992;DC-GAN&#36827;&#34892;&#29289;&#20214;&#35757;&#32451;&#29983;&#25104;&#30340;&#33258;&#28982;&#22806;&#35266;&#29289;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#33258;&#28982;&#21644;&#21512;&#25104;&#29289;&#29702;&#30772;&#22351;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#30772;&#22351;&#36890;&#24120;&#20986;&#29616;&#24847;&#22806;&#24182;&#25913;&#21464;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#30772;&#22351;&#65288;&#20363;&#22914;&#38634;&#12289;&#38654;&#12289;&#23576;&#22303;&#65289;&#23545;&#20110;&#35270;&#35273;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#26080;&#22788;&#19981;&#22312;&#30340;&#23041;&#32961;&#65292;&#21516;&#26679;&#24212;&#35813;&#34987;&#35270;&#20026;&#37325;&#35201;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26377;&#36259;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35757;&#32451;&#25239;&#20987;&#33258;&#28982;&#30772;&#22351;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#36825;&#20123;&#20316;&#21697;&#35201;&#20040;&#21033;&#29992;&#22270;&#20687;&#22686;&#24378;&#65292;&#36825;&#20250;&#22686;&#21152;&#27169;&#22411;&#22521;&#35757;&#30340;&#25104;&#26412;&#65292;&#35201;&#20040;&#22312;&#22330;&#26223;&#20013;&#25918;&#32622;&#21487;&#30097;&#30340;&#34917;&#19969;&#26469;&#35774;&#35745;&#38750;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#25903;&#25345;&#24615;&#29289;&#20214;&#65288;NSA&#65289;&#30340;&#27010;&#24565;&#26469;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#12290;NSAs&#22312;&#27169;&#22411;&#21442;&#25968;&#19981;&#21487;&#35775;&#38382;&#19988;&#22312;&#22330;&#26223;&#20013;&#28155;&#21152;&#29289;&#20214;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#30410;&#22788;&#12290;NSAs&#26159;&#36890;&#36807;&#20351;&#29992;DC-GAN&#36827;&#34892;&#29289;&#20214;&#35757;&#32451;&#29983;&#25104;&#30340;&#33258;&#28982;&#22806;&#35266;&#29289;&#20214;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35270;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14906</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRON&#65292;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#30340;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#12290;&#21463;&#21040;SASRec&#21644;GRU4Rec+&#31561;&#29616;&#26377;&#27169;&#22411;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;TRON&#38598;&#25104;&#20102;top-k&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#20854;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#22312;&#30456;&#20851;&#30340;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;TRON&#22312;&#20445;&#25345;&#19982;SASRec&#31867;&#20284;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#19968;&#39033;&#23454;&#26102;&#30340;A/B&#27979;&#35797;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;SASRec&#65292;TRON&#30340;&#28857;&#20987;&#29575;&#22686;&#21152;&#20102;18.14%&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
&lt;/p&gt;</description></item><item><title>CodeLens&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#26041;&#27861;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#21644;&#25506;&#32034;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2307.14902</link><description>&lt;p&gt;
CodeLens: &#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14902
&lt;/p&gt;
&lt;p&gt;
CodeLens&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#26041;&#27861;&#21644;&#32534;&#31243;&#35821;&#35328;&#65292;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#21644;&#25506;&#32034;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28304;&#20195;&#30721;&#20197;&#36890;&#29992;&#36755;&#20837;&#26684;&#24335;&#34920;&#31034;&#23545;&#20110;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#21487;&#35270;&#21270;&#20195;&#30721;&#34920;&#31034;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#20154;&#31867;&#19987;&#23478;&#30452;&#35266;&#22320;&#29702;&#35299;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19968;&#31181;&#36890;&#29992;&#24037;&#20855;&#33021;&#22815;&#21516;&#26102;&#21487;&#35270;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#30721;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeLens&#30340;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#20132;&#20114;&#29615;&#22659;&#65292;&#25903;&#25345;&#21508;&#31181;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#29702;&#35299;&#21644;&#25506;&#32034;&#23427;&#20204;&#12290;CodeLens&#34987;&#35774;&#35745;&#25104;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#22914;Java&#12289;Python&#21644;JavaScript&#65292;&#20197;&#21450;&#22235;&#31181;&#20195;&#30721;&#34920;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#26631;&#35760;&#24207;&#21015;&#12289;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25968;&#25454;&#27969;&#22270;&#65288;DFG&#65289;&#21644;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;CodeLens&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#24555;&#36895;&#21487;&#35270;&#21270;&#29305;&#23450;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#24182;&#33719;&#21462;&#20195;&#30721;&#27169;&#22411;&#30340;&#34920;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The W
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24178;&#29157;&#23545;&#27969;&#36793;&#30028;&#23618;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#28151;&#21512;&#23618;&#29702;&#35770;&#20013;&#30340;&#33258;&#30456;&#20284;&#23618;&#29983;&#38271;&#29289;&#29702;&#23398;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21512;&#25104;&#28237;&#27969;&#22330;&#30340;&#32479;&#35745;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14857</link><description>&lt;p&gt;
&#24178;&#29157;&#22823;&#27668;&#36793;&#30028;&#23618;&#30340;&#29983;&#25104;&#23545;&#27969;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generative convective parametrization of dry atmospheric boundary layer. (arXiv:2307.14857v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24178;&#29157;&#23545;&#27969;&#36793;&#30028;&#23618;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#28151;&#21512;&#23618;&#29702;&#35770;&#20013;&#30340;&#33258;&#30456;&#20284;&#23618;&#29983;&#38271;&#29289;&#29702;&#23398;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21512;&#25104;&#28237;&#27969;&#22330;&#30340;&#32479;&#35745;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#21442;&#25968;&#21270;&#22312;&#20844;&#37324;&#23610;&#24230;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#20013;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;&#22312;&#23545;&#27969;&#36793;&#30028;&#23618;&#20013;&#65292;&#24179;&#22343;&#22402;&#30452;&#26799;&#24230;&#30340;&#20445;&#23432;&#24615;&#36136;&#65292;&#22914;&#28508;&#22312;&#28201;&#24230;&#21644;&#28287;&#24230;&#65292;&#36817;&#20284;&#20026;&#38646;&#65292;&#26631;&#20934;&#30340;&#27169;&#22411;&#23558;&#28237;&#27969;&#36890;&#37327;&#19982;&#24179;&#22343;&#22402;&#30452;&#26799;&#24230;&#20043;&#38388;&#30340;&#28065;&#25193;&#25955;&#31995;&#25968;&#24310;&#23637;&#20026;&#29992;&#36136;&#37327;&#36890;&#37327;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#65292; &#29992;&#20110;&#22823;&#27668;&#36793;&#30028;&#23618;&#20013;&#30340;&#36890;&#24120;&#38750;&#23545;&#31216;&#30340;&#19978;&#21319;&#27668;&#27969;&#21644;&#19979;&#27785;&#27668;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#24178;&#29157;&#23545;&#27969;&#36793;&#30028;&#23618;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;Deardorff&#30340;&#32463;&#20856;&#28151;&#21512;&#23618;&#29702;&#35770;&#20013;&#30340;&#33258;&#30456;&#20284;&#23618;&#29983;&#38271;&#29289;&#29702;&#23398;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#36793;&#30028;&#23618;&#20869;&#19981;&#21516;&#39640;&#24230;&#20135;&#29983;&#30340;&#21512;&#25104;&#28237;&#27969;&#22330;&#30340;&#32479;&#35745;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulence parametrizations will remain a necessary building block in kilometer-scale Earth system models. In convective boundary layers, where the mean vertical gradients of conserved properties such as potential temperature and moisture are approximately zero, the standard ansatz which relates turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be extended by mass flux parametrizations for the typically asymmetric up- and downdrafts in the atmospheric boundary layer. In this work, we present a parametrization for a dry convective boundary layer based on a generative adversarial network. The model incorporates the physics of self-similar layer growth following from the classical mixed layer theory by Deardorff. This enhances the training data base of the generative machine learning algorithm and thus significantly improves the predicted statistics of the synthetically generated turbulence fields at different heights inside the boundary layer. The algorithm train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#30340;&#20027;&#35201;&#29305;&#24449;&#29983;&#25104;&#22270;&#20998;&#31867;&#22120;&#30340;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.14849</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#35270;&#35282;&#36827;&#34892;&#22270;&#20998;&#31867;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#30340;&#20027;&#35201;&#29305;&#24449;&#29983;&#25104;&#22270;&#20998;&#31867;&#22120;&#30340;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#23376;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20135;&#29983;&#31616;&#21333;&#26131;&#25026;&#30340;&#20107;&#21518;&#35299;&#37322;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#22270;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#25913;&#21464;&#22270;&#30340;&#26368;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#21024;&#38500;&#29616;&#26377;&#36793;&#25110;&#28155;&#21152;&#19981;&#23384;&#22312;&#30340;&#36793;&#65289;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#35299;&#37322;&#26041;&#24335;&#21487;&#33021;&#22826;&#32454;&#33268;&#65292;&#36716;&#32780;&#20851;&#27880;&#30495;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#30340;&#19968;&#20123;&#20027;&#35201;&#29305;&#24449;&#65292;&#22914;&#38381;&#21512;&#19977;&#35282;&#24418;&#30340;&#36235;&#21183;&#12289;&#37325;&#22797;&#30340;&#27169;&#24335;&#20197;&#21450;&#32452;&#32455;&#25104;&#23494;&#38598;&#27169;&#22359;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22270;&#20998;&#31867;&#22120;&#29983;&#25104;&#23454;&#20363;&#32423;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#23494;&#38598;&#23376;&#32467;&#26500;&#27010;&#24565;&#23454;&#20363;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#36890;&#29992;&#26694;&#26550;&#30340;&#20004;&#20010;&#20855;&#20307;&#23454;&#20363;&#21270;&#26041;&#27861;&#65306;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#21453;&#20107;&#23454;&#22270;&#26469;&#21305;&#37197;&#23494;&#38598;&#23376;&#32467;&#26500;&#27979;&#24230;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#21453;&#20107;&#23454;&#23376;&#22270;&#26469;&#21305;&#37197;&#23494;&#38598;&#23376;&#22270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14839</link><description>&lt;p&gt;
&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#20197;&#20854;&#21487;&#36870;&#30340;&#26550;&#26500;&#32780;&#34987;&#25551;&#36848;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#36870;&#24615;&#35201;&#27714;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#26045;&#21152;&#38480;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#36798;&#21040;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26367;&#20195;&#30340;&#36716;&#25442;&#26041;&#27861;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#26377;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;&#26680;&#21270;&#27969;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#32593;&#32476;&#24615;&#33021;&#12289;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#22686;&#21152;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.14823</link><description>&lt;p&gt;
&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#20013;&#30340;&#35114;&#36864;&#35760;&#24518;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14823
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#32593;&#32476;&#24615;&#33021;&#12289;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#22686;&#21152;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#36830;&#25509;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26550;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20197;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#35757;&#32451;&#30340;&#21069;&#39304;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#29190;&#28856;&#21644;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#27531;&#24046;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#21644;&#35114;&#36864;&#35760;&#24518;&#23646;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24369;&#32806;&#21512;&#27531;&#24046;&#24490;&#29615;&#32593;&#32476;(WCRNNs)&#65292;&#20854;&#20013;&#27531;&#24046;&#36830;&#25509;&#23548;&#33268;&#20102;&#26126;&#30830;&#23450;&#20041;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#35114;&#36864;&#35760;&#24518;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;WCRNNs&#30340;&#27531;&#24046;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#32593;&#32476;&#21160;&#21147;&#23398;&#21644;&#35760;&#24518;&#23646;&#24615;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#27531;&#24046;&#36830;&#25509;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#30340;&#27531;&#24046;&#36830;&#25509;&#65306;(i) &#23548;&#33268;&#32593;&#32476;&#21160;&#24577;&#25509;&#36817;&#28151;&#27788;&#30340;&#36793;&#32536;&#65292;(ii) &#20801;&#35768;&#32593;&#32476;&#22312;&#38271;&#26102;&#38388;&#20869;&#20445;&#25345;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29305;&#24449;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25490;&#21517;&#24314;&#35758;&#65292;&#35813;&#31995;&#32479;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#34920;&#29616;&#20248;&#20110;&#26080;&#29615;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14788</link><description>&lt;p&gt;
&#21487;&#33021;&#30340;&#65292;&#36731;&#37327;&#30340;&#21644;&#20934;&#30830;&#30340;&#22522;&#20110;&#26080;&#29615;&#25991;&#27861;&#32858;&#31867;&#30340;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction. (arXiv:2307.14788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29305;&#24449;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25490;&#21517;&#24314;&#35758;&#65292;&#35813;&#31995;&#32479;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#34920;&#29616;&#20248;&#20110;&#26080;&#29615;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#38656;&#35201;&#26234;&#33021;&#26426;&#21046;&#26469;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#20197;&#39044;&#27979;&#26410;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#65306;&#36712;&#36857;&#36716;&#25442;&#25104;&#20301;&#31227;&#31354;&#38388;&#65292;&#20301;&#31227;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#65292;&#36712;&#36857;&#24314;&#35758;&#21644;&#25490;&#21517;&#24314;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29305;&#24449;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#36523;&#26465;&#20214;&#30340;GAN&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25490;&#21517;&#24314;&#35758;&#65292;&#29992;&#20110;&#20026;&#29983;&#25104;&#30340;&#36712;&#36857;&#20998;&#37197;&#27010;&#29575;&#65292;&#27604;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#25928;&#32780;&#20934;&#30830;&#12290;&#24635;&#20307;&#31995;&#32479;&#22312;&#20154;&#31867;&#21644;&#36947;&#36335;&#20195;&#29702;&#36712;&#36857;&#25968;&#25454;&#20013;&#36229;&#36807;&#20102;&#26080;&#29615;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;&#22312;&#27604;&#36739;&#26368;&#21487;&#33021;&#30340;&#36712;&#36857;&#26102;&#19982;&#28857;&#20272;&#35745;&#22120;&#30340;&#34920;&#29616;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems in the road transportation network require intelligent mechanisms that cope with uncertainty to foresee the future. In this paper, we propose a multi-stage probabilistic approach for trajectory forecasting: trajectory transformation to displacement space, clustering of displacement time series, trajectory proposals, and ranking proposals. We introduce a new deep feature clustering method, underlying self-conditioned GAN, which copes better with distribution shifts than traditional methods. Additionally, we propose novel distance-based ranking proposals to assign probabilities to the generated trajectories that are more efficient yet accurate than an auxiliary neural network. The overall system surpasses context-free deep generative models in human and road agents trajectory data while performing similarly to point estimators when comparing the most probable trajectory.
&lt;/p&gt;</description></item><item><title>Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14783</link><description>&lt;p&gt;
Emotion4MIDI&#65306;&#22522;&#20110;&#27468;&#35789;&#30340;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14783
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12k&#20010;MIDI&#27468;&#26354;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#21322;&#22823;&#23567;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#65292;&#20026;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23588;&#20854;&#26159;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#22312;&#32447;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MATNilm&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#26679;&#26412;&#22686;&#24378;&#21644;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#25552;&#39640;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14778</link><description>&lt;p&gt;
MATNilm: &#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19979;&#22810;&#35774;&#22791;&#20219;&#21153;&#30340;&#26080;&#24178;&#25200;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MATNilm&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#26679;&#26412;&#22686;&#24378;&#21644;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#25552;&#39640;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979; (NILM) &#36890;&#36807;&#23558;&#25972;&#20010;&#25151;&#23627;&#30340;&#24635;&#21151;&#32791;&#20449;&#21495;&#20998;&#35299;&#26469;&#35782;&#21035;&#21508;&#31181;&#23478;&#29992;&#30005;&#22120;&#30340;&#29366;&#24577;&#21644;&#30005;&#33021;&#28040;&#32791;&#12290;&#39640;&#25928;&#20934;&#30830;&#30340;&#36127;&#36733;&#30417;&#27979;&#26377;&#21161;&#20110;&#24314;&#31435;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#12289;&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#21644;&#23792;&#20540;&#36127;&#33655;&#36716;&#31227;&#12290;&#36825;&#23545;&#20110;&#26368;&#32456;&#29992;&#25143;&#21644;&#20844;&#29992;&#20107;&#19994;&#37117;&#26377;&#30410;&#22788;&#65292;&#21487;&#20197;&#25552;&#39640;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#27599;&#20010;&#30005;&#22120;&#24320;&#21457;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#38590;&#20197;&#25910;&#38598;&#30340;&#23478;&#24237;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35774;&#22791;&#20219;&#21153;&#26694;&#26550;&#65292;&#37197;&#21512;&#19968;&#20010;&#35757;&#32451;&#39640;&#25928;&#30340;&#26679;&#26412;&#22686;&#24378; (SA) &#26041;&#26696;&#65292;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#20998;&#35299;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#30005;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#20998;&#23618;&#25286;&#20998;&#32467;&#26500;&#65292;&#29992;&#20110;&#20854;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#32500;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mech
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20998;&#24067;&#21464;&#36801;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#39034;&#24207;&#21464;&#36801;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#37096;&#32626;&#38656;&#27714;&#65292;&#24182;&#25512;&#33616;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.14758</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#34892;&#30340;&#39034;&#24207;&#21464;&#36801;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Practicable Sequential Shift Detectors. (arXiv:2307.14758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14758
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20998;&#24067;&#21464;&#36801;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#39034;&#24207;&#21464;&#36801;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#37096;&#32626;&#38656;&#27714;&#65292;&#24182;&#25512;&#33616;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#20154;&#24847;&#35782;&#21040;&#20998;&#24067;&#21464;&#36801;&#23545;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#30456;&#20851;&#25104;&#26412;&#31215;&#32047;&#20043;&#21069;&#26816;&#27979;&#36825;&#20123;&#21464;&#36801;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#23545;&#20110;&#21487;&#34892;&#37096;&#32626;&#39034;&#24207;&#21464;&#36801;&#26816;&#27979;&#22120;&#33267;&#20851;&#37325;&#35201;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#27492;&#31867;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;&#19982;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#30456;&#20851;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing awareness of the harmful effects of distribution shift on the performance of deployed machine learning models. Consequently, there is a growing interest in detecting these shifts before associated costs have time to accumulate. However, desiderata of crucial importance to the practicable deployment of sequential shift detectors are typically overlooked by existing works, precluding their widespread adoption. We identify three such desiderata, highlight existing works relevant to their satisfaction, and recommend impactful directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14754</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#36951;&#24536;&#65306;&#22312;&#20943;&#23569;&#24046;&#24322;&#30340;&#21516;&#26102;&#21024;&#38500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20844;&#20247;&#23545;&#20225;&#19994;&#25910;&#38598;&#21644;&#20351;&#29992;&#20010;&#20154;&#20449;&#24687;&#30340;&#24847;&#35782;&#22686;&#24378;&#65292;&#28040;&#36153;&#32773;&#31215;&#26497;&#21442;&#19982;&#20225;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25968;&#25454;&#31649;&#29702;&#26694;&#26550;&#65288;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65289;&#24050;&#32463;&#25552;&#20986;&#20102;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#20801;&#35768;&#20010;&#20154;&#35831;&#27714;&#23558;&#20854;&#20010;&#20154;&#25968;&#25454;&#20174;&#32452;&#32455;&#20351;&#29992;&#30340;&#25968;&#25454;&#24211;&#21644;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36951;&#24536;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#36951;&#24536;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#36951;&#24536;&#35831;&#27714;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#22312;&#32447;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36951;&#24536;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20854;&#20182;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
&lt;/p&gt;</description></item><item><title>FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;</title><link>http://arxiv.org/abs/2307.14751</link><description>&lt;p&gt;
FLARE: &#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#23545;&#25351;&#32441;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#36827;&#34892;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14751
&lt;/p&gt;
&lt;p&gt;
FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FLARE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#65288;&#21463;&#23475;&#65289;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36890;&#36807;&#25214;&#21040;&#19981;&#21487;&#20256;&#36882;&#30340;&#12289;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25513;&#30721;&#65292;&#21363;&#25200;&#21160;&#65292;&#21487;&#20197;&#29983;&#25104;&#25104;&#21151;&#22320;&#20174;&#21463;&#23475;&#31574;&#30053;&#20256;&#36882;&#21040;&#20854;&#20462;&#25913;&#29256;&#26412;&#20294;&#19981;&#33021;&#20256;&#36882;&#21040;&#29420;&#31435;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;FLARE&#21033;&#29992;&#36825;&#20123;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#36890;&#36807;&#23545;&#36890;&#36807;&#25513;&#30721;&#25200;&#21160;&#30340;&#29366;&#24577;&#19978;&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#36827;&#34892;&#27979;&#37327;&#26469;&#39564;&#35777;&#34987;&#30423;&#30340;DRL&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLARE&#26159;&#26377;&#25928;&#30340;&#65288;&#23545;&#20110;&#34987;&#30423;&#21103;&#26412;&#20855;&#26377;100%&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#65289;&#65292;&#24182;&#19988;&#19981;&#20250;&#38169;&#35823;&#22320;&#25351;&#25511;&#29420;&#31435;&#31574;&#30053;&#65288;&#26080;&#35823;&#25253;&#65289;&#12290;FLARE&#36824;&#23545;&#27169;&#22411;&#20462;&#25913;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#34987;&#26356;&#26126;&#26234;&#30340;&#23545;&#25163;&#35268;&#36991;&#32780;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#37117;&#26159;&#36866;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#35821;&#20041;&#22270;&#20687;&#34917;&#20840;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14748</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#36827;&#34892;&#35821;&#20041;&#22270;&#20687;&#34917;&#20840;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#35821;&#20041;&#22270;&#20687;&#34917;&#20840;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20462;&#22797;&#25110;&#22270;&#20687;&#34917;&#20840;&#26159;&#25351;&#26681;&#25454;&#22270;&#20687;&#35821;&#20041;&#25512;&#26029;&#20986;&#22270;&#20687;&#20013;&#20219;&#24847;&#22823;&#23567;&#30340;&#32570;&#22833;&#21306;&#22495;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#39044;&#27979;&#22270;&#20687;&#20687;&#32032;&#38656;&#35201;&#39640;&#32423;&#19978;&#19979;&#25991;&#30340;&#25351;&#31034;&#65292;&#36825;&#20351;&#24471;&#23427;&#27604;&#22270;&#20687;&#34917;&#20840;&#26356;&#22256;&#38590;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#20851;&#27880;&#25968;&#25454;&#25439;&#22351;&#30340;&#32416;&#27491;&#21644;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#21024;&#38500;&#25972;&#20010;&#23545;&#35937;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22270;&#20687;&#22686;&#24378;&#26088;&#22312;&#28040;&#38500;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#21644;&#27169;&#31946;&#65292;&#24182;&#20445;&#30041;&#22823;&#37096;&#20998;&#22270;&#20687;&#32454;&#33410;&#12290;&#39640;&#25928;&#30340;&#22270;&#20687;&#34917;&#20840;&#21644;&#22686;&#24378;&#27169;&#22411;&#24212;&#33021;&#22815;&#24674;&#22797;&#22270;&#20687;&#20013;&#30340;&#25439;&#22351;&#21644;&#25513;&#30422;&#21306;&#22495;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#25913;&#21892;&#22270;&#20687;&#65292;&#25552;&#39640;&#36755;&#20986;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#22270;&#20687;&#34917;&#20840;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#24213;&#23618;&#30340;GAN&#26550;&#26500;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#34917;&#20840;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic inpainting or image completion alludes to the task of inferring arbitrary large missing regions in images based on image semantics. Since the prediction of image pixels requires an indication of high-level context, this makes it significantly tougher than image completion, which is often more concerned with correcting data corruption and removing entire objects from the input image. On the other hand, image enhancement attempts to eliminate unwanted noise and blur from the image, along with sustaining most of the image details. Efficient image completion and enhancement model should be able to recover the corrupted and masked regions in images and then refine the image further to increase the quality of the output image. Generative Adversarial Networks (GAN), have turned out to be helpful in picture completion tasks. In this chapter, we will discuss the underlying GAN architecture and how they can be used used for image completion tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#26469;&#20998;&#26512;&#36275;&#29699;&#20013;&#30340;1&#23545;1&#23556;&#38376;&#24773;&#20917;&#65292;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#29702;&#35770;&#27169;&#22411;&#21644;&#21338;&#24328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#25910;&#30410;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#37327;&#21270;&#20998;&#26512;&#23556;&#38376;&#20915;&#31574;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#20915;&#31574;&#25552;&#20379;&#23458;&#35266;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.14732</link><description>&lt;p&gt;
&#25112;&#30053;&#26694;&#26550;&#65306;&#22312;&#36275;&#29699;1&#23545;1&#23556;&#38376;&#22330;&#26223;&#20013;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#30340;&#32508;&#21512;&#26041;&#27861;&#8212;&#8212;&#26426;&#22120;&#23398;&#20064;&#12289;&#29702;&#35770;&#27169;&#22411;&#21644;&#21338;&#24328;&#35770;&#30340;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory. (arXiv:2307.14732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#26469;&#20998;&#26512;&#36275;&#29699;&#20013;&#30340;1&#23545;1&#23556;&#38376;&#24773;&#20917;&#65292;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#29702;&#35770;&#27169;&#22411;&#21644;&#21338;&#24328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#25910;&#30410;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#37327;&#21270;&#20998;&#26512;&#23556;&#38376;&#20915;&#31574;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#20915;&#31574;&#25552;&#20379;&#23458;&#35266;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#12289;&#21338;&#24328;&#35770;&#21644;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#20004;&#20010;&#23545;&#31435;&#20027;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#32463;&#24120;&#21457;&#29983;&#12290;&#37327;&#21270;&#20998;&#26512;&#28041;&#21450;&#30340;&#31574;&#30053;&#21487;&#20197;&#20026;&#20915;&#31574;&#25552;&#20379;&#23458;&#35266;&#20381;&#25454;&#12290;&#36275;&#29699;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#22330;&#26223;&#23601;&#26159;&#23556;&#38376;&#65292;&#20915;&#31574;&#65288;&#20363;&#22914;&#36827;&#25915;&#32773;&#26159;&#21542;&#24212;&#35813;&#23556;&#38376;&#25110;&#20256;&#29699;&#65292;&#38450;&#23432;&#32773;&#26159;&#21542;&#24212;&#35813;&#23581;&#35797;&#23553;&#22581;&#23556;&#38376;&#65289;&#22312;&#27604;&#36187;&#32467;&#26524;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#25968;&#25454;&#25110;&#22522;&#20110;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#36825;&#26679;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#25910;&#30410;&#65292;&#32780;&#22522;&#20110;&#29702;&#35770;&#30340;&#23556;&#38376;&#23553;&#22581;&#27169;&#22411;&#21017;&#25552;&#21462;&#20102;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38468;&#21152;&#29305;&#24449;&#12290;&#20256;&#32479;&#19978;&#65292;&#25104;&#21151;&#25110;&#22833;&#36133;&#65288;1&#25110;0&#65289;&#34987;&#29992;&#20316;&#25910;&#30410;&#65292;&#32780;&#22312;&#36275;&#29699;&#20013;&#65292;&#25104;&#21151;&#30340;&#23556;&#38376;&#65288;&#36827;&#29699;&#65289;&#26497;&#20026;&#32597;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex interactions between two opposing agents frequently occur in domains of machine learning, game theory, and other application domains. Quantitatively analyzing the strategies involved can provide an objective basis for decision-making. One such critical scenario is shot-taking in football, where decisions, such as whether the attacker should shoot or pass the ball and whether the defender should attempt to block the shot, play a crucial role in the outcome of the game. However, there are currently no effective data-driven and/or theory-based approaches to analyzing such situations. To address this issue, we proposed a novel framework to analyze such scenarios based on game theory, where we estimate the expected payoff with machine learning (ML) models, and additional features for ML models were extracted with a theory-based shot block model. Conventionally, successes or failures (1 or 0) are used as payoffs, while a success shot (goal) is extremely rare in football. Therefore, w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#26080;&#27861;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#24378;&#35843;&#20102;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#26681;&#26412;&#21407;&#22240;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#12290;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#30340;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.14729</link><description>&lt;p&gt;
&#29702;&#35299;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#38544;&#24615;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#26080;&#27861;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#24378;&#35843;&#20102;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#26681;&#26412;&#21407;&#22240;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#12290;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#30340;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#20998;&#31867;&#31995;&#32479;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#20351;&#29992;&#65292;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#36275;&#22815;&#31283;&#20581;&#30340;&#20998;&#31867;&#22120;&#20197;&#36991;&#20813;&#39318;&#27425;&#22833;&#36133;&#65292;&#25110;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#65288;CSFs&#65289;&#26816;&#27979;&#21097;&#20313;&#30340;&#22833;&#36133;&#26469;&#23454;&#29616;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#22833;&#36133;&#30340;&#20027;&#35201;&#28304;&#22836;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#29702;&#35299;&#21307;&#23398;&#22270;&#20687;&#20013;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#22235;&#20010;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#21644;&#21508;&#31181;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#21508;&#31181;CSFs&#12290;&#26681;&#25454;&#32467;&#26524;&#21457;&#29616;&#65292;&#27809;&#26377;&#19968;&#20010;&#34987;&#22522;&#20934;&#21270;&#30340;CSF&#33021;&#22815;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of va
&lt;/p&gt;</description></item><item><title>TimeGNN&#26159;&#19968;&#31181;&#23398;&#20064;&#21160;&#24577;&#26102;&#38388;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23427;&#22312;&#25512;&#26029;&#26102;&#38388;&#19978;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14680</link><description>&lt;p&gt;
TimeGNN: &#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14680
&lt;/p&gt;
&lt;p&gt;
TimeGNN&#26159;&#19968;&#31181;&#23398;&#20064;&#21160;&#24577;&#26102;&#38388;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23427;&#22312;&#25512;&#26029;&#26102;&#38388;&#19978;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#37325;&#35201;&#23454;&#38469;&#24212;&#29992;&#20013;&#22788;&#20110;&#26680;&#24515;&#20301;&#32622;&#12290;&#22823;&#37327;&#21253;&#21547;&#22797;&#26434;&#27169;&#24335;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#23384;&#22312;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22522;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21407;&#22987;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#22270;&#32467;&#26500;&#26469;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeGNN&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#21160;&#24577;&#30340;&#26102;&#38388;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;TimeGNN&#30340;&#25512;&#26029;&#26102;&#38388;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#26045;&#21152;&#29289;&#29702;&#32422;&#26463;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#12290;&#36825;&#23545;&#20110;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#21644;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.14675</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#35777;&#25454;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#26045;&#21152;&#29289;&#29702;&#32422;&#26463;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#12290;&#36825;&#23545;&#20110;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#21644;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#30340;&#19981;&#26029;&#22686;&#38271;&#20351;&#36890;&#36807;&#20559;&#33322;&#35282;&#25511;&#21046;&#22120;&#26469;&#20248;&#21270;&#28065;&#36718;&#26426;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#26089;&#26399;&#25925;&#38556;&#26816;&#27979;&#26469;&#36827;&#34892;&#32500;&#25252;&#21464;&#24471;&#24517;&#35201;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#34892;&#20026;&#23588;&#20854;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#29983;&#25104;&#30340;&#21151;&#29575;&#19982;&#39118;&#36895;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#32463;&#39564;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#36755;&#20837;&#21464;&#37327;&#21644;&#21151;&#29575;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#21463;&#39118;&#21464;&#24615;&#30340;&#21152;&#21095;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20026;&#36890;&#36807;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26469;&#22686;&#24378;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20197;&#27169;&#25311;&#26469;&#33258;&#19968;&#20010;&#39118;&#22330;&#30340;4&#21488;&#28065;&#36718;&#26426;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#19968;&#23450;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#24320;&#21457;&#30340;&#29992;&#20110;&#22238;&#24402;&#21151;&#29575;&#12289;&#36716;&#30697;&#21644;&#21151;&#29575;&#31995;&#25968;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#25511;&#21046;&#26041;&#31243;&#26041;&#38754;&#37117;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550; xOrder&#65292;&#29992;&#20110;&#23454;&#29616;&#20108;&#20998;&#25490;&#21517;&#30340;&#20844;&#24179;&#24615;&#19988;&#33021;&#20445;&#25345;&#31639;&#27861;&#20998;&#31867;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#30340;&#21644;&#26469;&#30830;&#23450;&#19981;&#21516;&#21463;&#20445;&#25252;&#32452;&#20043;&#38388;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#20860;&#23481;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#21644;&#25490;&#21517;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;xOrder&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#31867;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#36739;&#22909;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14668</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#25490;&#24207;&#35843;&#25972;&#23454;&#29616;&#20108;&#20998;&#25490;&#21517;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment. (arXiv:2307.14668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550; xOrder&#65292;&#29992;&#20110;&#23454;&#29616;&#20108;&#20998;&#25490;&#21517;&#30340;&#20844;&#24179;&#24615;&#19988;&#33021;&#20445;&#25345;&#31639;&#27861;&#20998;&#31867;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#30340;&#21644;&#26469;&#30830;&#23450;&#19981;&#21516;&#21463;&#20445;&#25252;&#32452;&#20043;&#38388;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#20860;&#23481;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#21644;&#25490;&#21517;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;xOrder&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#31867;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#36739;&#22909;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#27491;&#24615;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#26412;&#25991;&#38024;&#23545;&#20108;&#20998;&#25490;&#21517;&#22330;&#26223;&#65292;&#21363;&#23454;&#20363;&#26469;&#28304;&#20110;&#27491;&#31867;&#25110;&#36127;&#31867;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#25490;&#21517;&#20989;&#25968;&#65292;&#20351;&#24471;&#27491;&#23454;&#20363;&#25490;&#21517;&#39640;&#20110;&#36127;&#23454;&#20363;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26435;&#34913;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550; xOrder&#65292;&#29992;&#20110;&#23454;&#29616;&#20108;&#20998;&#25490;&#21517;&#30340;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#31639;&#27861;&#20998;&#31867;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#36807;&#31243;&#20248;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#30340;&#21644;&#65292;&#20197;&#30830;&#23450;&#19981;&#21516;&#21463;&#20445;&#25252;&#32452;&#20043;&#38388;&#26368;&#20339;&#24367;&#26354;&#36335;&#24452;&#12290;xOrder&#20860;&#23481;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#21644;&#25490;&#21517;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#38500;&#20102;&#20108;&#20803;&#32452;&#20043;&#22806;&#65292;xOrder&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#32452;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;xOrder&#22312;&#32500;&#25345;&#20998;&#31867;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this paper, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#24615;&#36136;&#21644;&#20998;&#24067;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#23478;&#26063;&#21644;&#26679;&#26412;&#25968;&#37327;&#23545;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#30456;&#20114;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2307.14657</link><description>&lt;p&gt;
&#25581;&#31034;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31192;&#23494;&#65306;&#23545;&#25968;&#25454;&#38598;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (arXiv:2307.14657v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#24615;&#36136;&#21644;&#20998;&#24067;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#23478;&#26063;&#21644;&#26679;&#26412;&#25968;&#37327;&#23545;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#24182;&#25253;&#36947;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#35013;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#20998;&#26512;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#29978;&#33267;&#22312;&#23545;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#30340;&#23450;&#20041;&#19978;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#31038;&#21306;&#20173;&#28982;&#32570;&#20047;&#23545;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#32467;&#26524;&#30340;&#29702;&#35299;&#65306;&#23427;&#20204;&#26159;&#21542;&#19982;&#25152;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#24615;&#36136;&#21644;&#20998;&#24067;&#26377;&#20851;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#23478;&#26063;&#21644;&#26679;&#26412;&#25968;&#37327;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#24615;&#33021;&#65292;&#20197;&#21450;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#34917;&#20805;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#24433;&#21709;&#22522;&#20110;ML&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#22238;&#31572;&#36825;&#20123;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24179;&#34913;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;670&#20010;&#23478;&#26063;&#30340;67,000&#20010;&#26679;&#26412;&#65288;&#27599;&#20010;&#23478;&#26063;100&#20010;&#26679;&#26412;&#65289;&#65292;&#24182;&#35757;&#32451;&#20102;&#26368;&#20808;&#36827;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#23478;&#26063;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.  This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;WRF&#27169;&#22411;&#22312;&#28595;&#22823;&#21033;&#20122;&#19996;&#21335;&#37096;&#23545;&#34920;&#38754;&#27668;&#35937;&#21464;&#37327;&#25935;&#24863;&#24615;&#30340;&#30740;&#31350;&#65292;&#20026;&#28909;&#26497;&#31471;&#20107;&#20214;&#30340;&#20934;&#30830;&#27169;&#25311;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.14654</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21306;&#22495;&#27668;&#20505;&#27169;&#22411;&#21442;&#25968;&#25935;&#24863;&#24615;&#30740;&#31350;--&#20197;WRF&#27169;&#22411;&#22312;&#28595;&#22823;&#21033;&#20122;&#19996;&#21335;&#37096;&#30340;&#28909;&#26497;&#31471;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Machine Learning based Parameter Sensitivity of Regional Climate Models -- A Case Study of the WRF Model for Heat Extremes over Southeast Australia. (arXiv:2307.14654v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;WRF&#27169;&#22411;&#22312;&#28595;&#22823;&#21033;&#20122;&#19996;&#21335;&#37096;&#23545;&#34920;&#38754;&#27668;&#35937;&#21464;&#37327;&#25935;&#24863;&#24615;&#30340;&#30740;&#31350;&#65292;&#20026;&#28909;&#26497;&#31471;&#20107;&#20214;&#30340;&#20934;&#30830;&#27169;&#25311;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#28010;&#21644;&#26862;&#26519;&#28779;&#28798;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#31038;&#20250;&#21644;&#29983;&#24577;&#31995;&#32479;&#36896;&#25104;&#37325;&#22823;&#24433;&#21709;&#12290;&#20934;&#30830;&#30340;&#28909;&#26497;&#31471;&#20449;&#24687;&#23545;&#20110;&#25903;&#25345;&#21487;&#34892;&#30340;&#20943;&#28798;&#21644;&#36866;&#24212;&#31574;&#30053;&#30340;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#22495;&#27668;&#20505;&#27169;&#22411;&#24120;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#20107;&#20214;&#30340;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#38750;&#24120;&#24222;&#22823;&#30340;&#36755;&#20837;&#21442;&#25968;&#38598;&#65292;&#20854;&#20013;&#29289;&#29702;&#26041;&#26696;&#20013;&#30340;&#21442;&#25968;&#22823;&#22823;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28909;&#26497;&#31471;&#26469;&#35828;&#65292;&#21306;&#22495;&#27169;&#22411;&#30340;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#28595;&#22823;&#21033;&#20122;&#19996;&#21335;&#37096;&#22320;&#21306;&#65292;&#36825;&#20010;&#22320;&#21306;&#26159;&#28909;&#26497;&#31471;&#30340;&#20840;&#29699;&#28909;&#28857;&#20043;&#19968;&#12290;&#22312;&#28595;&#22823;&#21033;&#20122;&#19996;&#21335;&#37096;&#65292;Weather Research and Forecasting (WRF) &#27169;&#22411;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#21306;&#22495;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#35813;&#22320;&#21306;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;WRF&#27169;&#22411;&#21442;&#25968;&#23545;&#34920;&#38754;&#27668;&#35937;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#12289;&#30456;&#23545;&#28287;&#24230;&#21644;&#39118;&#36895;&#65289;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heatwaves and bushfires cause substantial impacts on society and ecosystems across the globe. Accurate information of heat extremes is needed to support the development of actionable mitigation and adaptation strategies. Regional climate models are commonly used to better understand the dynamics of these events. These models have very large input parameter sets, and the parameters within the physics schemes substantially influence the model's performance. However, parameter sensitivity analysis (SA) of regional models for heat extremes is largely unexplored. Here, we focus on the southeast Australian region, one of the global hotspots of heat extremes. In southeast Australia Weather Research and Forecasting (WRF) model is the widely used regional model to simulate extreme weather events across the region. Hence in this study, we focus on the sensitivity of WRF model parameters to surface meteorological variables such as temperature, relative humidity, and wind speed during two extreme 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#28909;&#21147;&#23398;&#26041;&#27861;&#65292;&#26681;&#25454;&#26435;&#37325;&#20998;&#24067;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#21644;&#29109;&#20135;&#29983;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#23436;&#20840;&#35757;&#32451;&#30340;&#26368;&#22823;&#36895;&#24230;&#38480;&#21046;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#32447;&#24615;&#21644;&#21487;&#32447;&#24615;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#32553;&#25918;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.14653</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36895;&#24230;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Speed Limits for Deep Learning. (arXiv:2307.14653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14653
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#28909;&#21147;&#23398;&#26041;&#27861;&#65292;&#26681;&#25454;&#26435;&#37325;&#20998;&#24067;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#21644;&#29109;&#20135;&#29983;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#23436;&#20840;&#35757;&#32451;&#30340;&#26368;&#22823;&#36895;&#24230;&#38480;&#21046;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#32447;&#24615;&#21644;&#21487;&#32447;&#24615;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#32553;&#25918;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#38454;&#27573;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26497;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#25165;&#33021;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#24456;&#33258;&#28982;&#22320;&#24819;&#30693;&#36947;&#23427;&#20204;&#26159;&#21542;&#34987;&#26368;&#20248;&#21270;&#22320;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#26368;&#36817;&#22312;&#38543;&#26426;&#28909;&#21147;&#23398;&#20013;&#30340;&#19968;&#20010;&#36827;&#23637;&#65292;&#20801;&#35768;&#26681;&#25454;&#23427;&#20204;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#27604;&#29575;&#21644;&#36830;&#25509;&#23427;&#20204;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#29109;&#20135;&#29983;&#36895;&#29575;&#65292;&#23545;&#20174;&#21021;&#22987;&#26435;&#37325;&#20998;&#24067;&#21040;&#23436;&#20840;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#26368;&#22823;&#36895;&#24230;&#36827;&#34892;&#30028;&#23450;&#12290;&#32771;&#34385;&#20102;&#26799;&#24230;&#27969;&#21644;Langevin&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#20026;&#32447;&#24615;&#21644;&#21487;&#32447;&#24615;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65289;&#25552;&#20379;&#20102;&#36825;&#20123;&#36895;&#24230;&#38480;&#21046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22914;&#26524;&#23545;NTK&#35889;&#21644;&#26631;&#31614;&#30340;&#35889;&#20998;&#35299;&#20570;&#20986;&#19968;&#20123;&#21512;&#29702;&#30340;&#32553;&#25918;&#20551;&#35774;&#65292;&#23398;&#20064;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#26368;&#20248;&#21270;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#22312;CIFAR-10&#19978;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#21644;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;(FCNs)&#36827;&#34892;&#30340;&#23567;&#35268;&#27169;&#23454;&#39564;&#19968;&#33268;&#65292;&#26174;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#20687;&#32032;&#31354;&#38388;&#36716;&#25442;&#21040;&#23567;&#27874;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;SFUNet&#26550;&#26500;&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#31354;&#38388;&#21644;&#39057;&#29575;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2307.14648</link><description>&lt;p&gt;
&#22312;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#21033;&#29992;&#31354;&#38388;&#39057;&#29575;U-Net (SFUNet)
&lt;/p&gt;
&lt;p&gt;
Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#20687;&#32032;&#31354;&#38388;&#36716;&#25442;&#21040;&#23567;&#27874;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;SFUNet&#26550;&#26500;&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#31354;&#38388;&#21644;&#39057;&#29575;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#35270;&#21512;&#25104;&#20013;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#20174;&#20687;&#32032;&#31354;&#38388;&#36716;&#25442;&#21040;&#23567;&#27874;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#23567;&#27874;&#21464;&#25442;&#33021;&#21516;&#26102;&#34920;&#31034;&#22270;&#20687;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;SFUNet&#26550;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20004;&#20010;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#23545;&#20687;&#32032;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#30340;&#21435;&#22122;U-Net&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#31354;&#38388;&#39057;&#29575;&#24863;&#30693;&#30340;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#26469;&#34917;&#20805;2D&#21367;&#31215;&#21644;&#20165;&#31354;&#38388;&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#20849;&#21516;&#24314;&#27169;&#26469;&#33258;&#23567;&#27874;&#25968;&#25454;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#39046;&#22495;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26032;&#26550;&#26500;&#21487;&#20197;&#20316;&#20026;&#20687;&#32032;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;DDPM&#30340;&#35757;&#32451;&#36807;&#31243;&#20860;&#23481;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#23567;&#27874;&#20449;&#21495;&#24314;&#27169;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CIFAR-10&#12289;FFHQ&#12289;LSUN-Bedroom&#21644;LSUN-Church&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#29983;&#25104;&#27604;&#22522;&#20110;&#20687;&#32032;&#30340;&#23545;&#24212;&#27169;&#22411;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the denoising diffusion probabilistic model (DDPM) in wavelet space, instead of pixel space, for visual synthesis. Considering the wavelet transform represents the image in spatial and frequency domains, we carefully design a novel architecture SFUNet to effectively capture the correlation for both domains. Specifically, in the standard denoising U-Net for pixel data, we supplement the 2D convolutions and spatial-only attention layers with our spatial frequency-aware convolution and attention modules to jointly model the complementary information from spatial and frequency domains in wavelet data. Our new architecture can be used as a drop-in replacement to the pixel-based network and is compatible with the vanilla DDPM training process. By explicitly modeling the wavelet signals, we find our model is able to generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and LSUN-Church datasets, than the pixel-based counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.14643</link><description>&lt;p&gt;
MVMR-FS&#65306;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#24230;&#37327;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#26159;&#29305;&#24449;&#36873;&#25321;&#39046;&#22495;&#30340;&#19968;&#20010;&#21476;&#32769;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36807;&#28388;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24230;&#37327;&#36830;&#32493;&#25968;&#25454;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25351;&#23450;&#29305;&#24449;&#25968;&#37327;&#65292;&#36825;&#22312;&#27809;&#26377;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#31616;&#31216;&#20026;MVMR-FS&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#65292;&#20197;&#25429;&#25417;&#23427;&#20204;&#22312;&#31867;&#38388;&#21644;&#25972;&#20307;&#20998;&#24067;&#20013;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#65288;MVMR&#65289;&#30340;&#20934;&#21017;&#65292;&#20854;&#20013;&#21033;&#29992;&#31867;&#38388;&#27010;&#29575;&#20998;&#24067;&#26469;&#21453;&#26144;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#25972;&#20307;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#37327;&#21270;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.14642</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65306;&#25105;&#20204;&#24212;&#35813;&#22362;&#25345;&#21040;&#24213;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#65292;&#29305;&#21035;&#26159;&#30528;&#38470;&#31283;&#23450;&#65288;STL&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#25910;&#25947;&#20110;&#20960;&#20309;&#65288;&#20256;&#32479;&#19978;&#31216;&#20026;&#8220;&#32447;&#24615;&#8221;&#65289;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STL&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#26041;&#24046;&#30340;&#20108;&#27425;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#21253;&#25324;&#20102;&#35823;&#25351;&#23450;&#30340;&#21464;&#20998;&#26063;&#12290;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#20108;&#27425;&#26041;&#24046;&#26465;&#20214;&#30340;&#24037;&#20316;&#65292;&#36825;&#30452;&#25509;&#26263;&#31034;&#20102;&#22312;&#20351;&#29992;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;BBVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29616;&#26377;&#23545;&#20110;&#27491;&#24120;&#23553;&#38381;&#24418;&#24335;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#19982;STL&#20272;&#35745;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20026;&#20004;&#32773;&#25552;&#20379;&#26126;&#30830;&#30340;&#38750;&#28176;&#36827;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.14634</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25253;&#21578;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#65292;&#29616;&#22312;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#33258;&#21160;&#25253;&#21578;&#26469;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#36827;&#34892;&#21021;&#27493;&#38405;&#35835;&#12290;&#36825;&#21487;&#20197;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#23548;&#33268;&#29983;&#25104;&#25253;&#21578;&#20013;&#20986;&#29616;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19982;&#25551;&#36848;&#30495;&#23454;&#25110;&#28508;&#22312;&#34394;&#20551;&#21457;&#29616;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24320;&#21457;&#30340;&#26680;&#26597;&#32773;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#26680;&#26597;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20266;&#36896;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#26469;&#33258;&#36825;&#20123;&#25253;&#21578;&#30340;&#30495;&#20551;&#21477;&#23376;&#30340;&#25991;&#26412;&#32534;&#30721;&#19982;&#22270;&#20687;&#32534;&#30721;&#37197;&#23545;&#65292;&#23398;&#20064;&#26144;&#23556;&#21040;&#30495;/&#20551;&#26631;&#31614;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;AB&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#23618;&#36125;&#21494;&#26031;&#20272;&#35745;&#26469;&#20811;&#26381;&#20102;&#24120;&#35265;&#30340;AB&#27979;&#35797;&#20998;&#26512;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#22810;&#21464;&#37327;&#35774;&#35745;&#12289;&#30456;&#20851;&#24615;&#12289;&#39034;&#24207;&#27979;&#35797;&#21644;&#27719;&#24635;&#36807;&#21435;&#27979;&#35797;&#30693;&#35782;&#31561;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22686;&#21152;&#32479;&#35745;&#21151;&#25928;&#12289;&#23454;&#29616;&#39034;&#24207;&#27979;&#35797;&#21644;&#28176;&#36827;&#24335;&#26089;&#20572;&#27490;&#65292;&#21516;&#26102;&#38477;&#20302;&#38169;&#35823;&#21028;&#26029;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.14628</link><description>&lt;p&gt;
&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;AB&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Rapid and Scalable Bayesian AB Testing. (arXiv:2307.14628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;AB&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#23618;&#36125;&#21494;&#26031;&#20272;&#35745;&#26469;&#20811;&#26381;&#20102;&#24120;&#35265;&#30340;AB&#27979;&#35797;&#20998;&#26512;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#22810;&#21464;&#37327;&#35774;&#35745;&#12289;&#30456;&#20851;&#24615;&#12289;&#39034;&#24207;&#27979;&#35797;&#21644;&#27719;&#24635;&#36807;&#21435;&#27979;&#35797;&#30693;&#35782;&#31561;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22686;&#21152;&#32479;&#35745;&#21151;&#25928;&#12289;&#23454;&#29616;&#39034;&#24207;&#27979;&#35797;&#21644;&#28176;&#36827;&#24335;&#26089;&#20572;&#27490;&#65292;&#21516;&#26102;&#38477;&#20302;&#38169;&#35823;&#21028;&#26029;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AB&#27979;&#35797;&#24110;&#21161;&#19994;&#21153;&#32463;&#33829;&#32773;&#36827;&#34892;&#20915;&#31574;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25913;&#36827;&#25968;&#23383;&#29992;&#25143;&#20307;&#39564;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#32773;&#30340;&#35201;&#27714;&#36890;&#24120;&#19982;&#24120;&#29992;&#20110;AB&#27979;&#35797;&#20998;&#26512;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#23384;&#22312;&#24046;&#36317;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#22810;&#21464;&#37327;&#35774;&#35745;&#20013;&#32570;&#20047;&#32479;&#35745;&#21151;&#25928;&#12289;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#38656;&#35201;&#39034;&#24207;&#27979;&#35797;&#36827;&#34892;&#26089;&#20572;&#27490;&#65292;&#20197;&#21450;&#26080;&#27861;&#27719;&#24635;&#36807;&#21435;&#27979;&#35797;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#23618;&#36125;&#21494;&#26031;&#20272;&#35745;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#19982;&#24403;&#21069;&#30340;&#39034;&#24207;AB&#27979;&#35797;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#22686;&#21152;&#32479;&#35745;&#21151;&#25928;&#65292;&#23454;&#29616;&#20102;&#39034;&#24207;&#27979;&#35797;&#21644;&#28176;&#36827;&#24335;&#26089;&#20572;&#27490;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#38169;&#35823;&#27491;&#20363;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
AB testing aids business operators with their decision making, and is considered the gold standard method for learning from data to improve digital user experiences. However, there is usually a gap between the requirements of practitioners, and the constraints imposed by the statistical hypothesis testing methodologies commonly used for analysis of AB tests. These include the lack of statistical power in multivariate designs with many factors, correlations between these factors, the need of sequential testing for early stopping, and the inability to pool knowledge from past tests. Here, we propose a solution that applies hierarchical Bayesian estimation to address the above limitations. In comparison to current sequential AB testing methodology, we increase statistical power by exploiting correlations between factors, enabling sequential testing and progressive early stopping, without incurring excessive false positive risk. We also demonstrate how this methodology can be extended to e
&lt;/p&gt;</description></item><item><title>BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14623</link><description>&lt;p&gt;
BubbleML: &#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14623
&lt;/p&gt;
&lt;p&gt;
BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21464;&#29616;&#35937;&#39046;&#22495;&#65292;&#32570;&#20047;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#21487;&#35775;&#38382;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#36890;&#24120;&#21463;&#38480;&#65292;&#21487;&#29992;&#24615;&#26377;&#38480;&#19988;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#36825;&#31181;&#22797;&#26434;&#22810;&#29289;&#29702;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BubbleML&#25968;&#25454;&#38598;&#65288;https://github.com/HPCForge/BubbleML&#65289;&#65292;&#23427;&#21033;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;&#27169;&#25311;&#20026;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#25552;&#20379;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#21253;&#25324;&#26680;&#27873;&#27744;&#27832;&#33150;&#12289;&#27969;&#21160;&#27832;&#33150;&#21644;&#20122;&#20919;&#27832;&#33150;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#37325;&#21147;&#26465;&#20214;&#12289;&#27969;&#37327;&#12289;&#20122;&#20919;&#27700;&#24179;&#21644;&#22721;&#38754;&#36807;&#28909;&#65292;&#24635;&#20849;&#26377;51&#20010;&#27169;&#25311;&#12290;BubbleML&#24050;&#32463;&#36890;&#36807;&#23454;&#39564;&#35266;&#23519;&#21644;&#36235;&#21183;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34987;&#30830;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20419;&#36827;&#22810;&#26679;&#21270;&#38477;&#20302;&#28201;&#24230;&#27832;&#33150;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2307.14619</link><description>&lt;p&gt;
&#27169;&#20223;&#22797;&#26434;&#36712;&#36857;&#65306;&#26725;&#25509;&#20302;&#23618;&#31283;&#23450;&#24615;&#19982;&#39640;&#23618;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#22797;&#26434;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#31283;&#23450;&#27169;&#20223;&#31574;&#30053;&#24182;&#30830;&#20445;&#20934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#27169;&#20223;&#32773;&#19982;&#28436;&#31034;&#32773;&#30340;&#36712;&#36857;&#20998;&#24067;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#27169;&#20223;&#38543;&#26426;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#12289;&#28508;&#22312;&#22810;&#27169;&#24577;&#65288;&#21363;&#8220;&#22797;&#26434;&#8221;&#65289;&#19987;&#23478;&#28436;&#31034;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20302;&#23618;&#25511;&#21046;&#22120;&#65288;&#26080;&#35770;&#26159;&#23398;&#20064;&#30340;&#36824;&#26159;&#38544;&#21547;&#30340;&#65289;&#26469;&#31283;&#23450;&#22260;&#32469;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#65288;a&#65289;&#21512;&#36866;&#30340;&#20302;&#23618;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#65288;b&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#38543;&#26426;&#36830;&#32493;&#24615;&#23646;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24635;&#21464;&#24046;&#36830;&#32493;&#24615;&#8221;&#65289;&#65288;TVC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31934;&#30830;&#20272;&#35745;&#28436;&#31034;&#32773;&#29366;&#24577;&#20998;&#24067;&#19978;&#30340;&#34892;&#21160;&#30340;&#27169;&#20223;&#32773;&#20250;&#19982;&#28436;&#31034;&#32773;&#23545;&#25972;&#20010;&#36712;&#36857;&#30340;&#20998;&#24067;&#30456;&#36817;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#35268;&#21017;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#25216;&#24039;&#30456;&#32467;&#21512;&#65288;&#21363;&#22312;&#25191;&#34892;&#26102;&#28155;&#21152;&#22686;&#24378;&#22122;&#22768;&#65289;&#26469;&#30830;&#20445;TVC&#24182;&#19988;&#26368;&#23567;&#31243;&#24230;&#19978;&#38477;&#20302;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20445;&#35777;&#23454;&#20363;&#21270;&#20026;&#30001;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#23398;&#20064;&#32773;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#28436;&#31034;&#32773;&#30340;&#20998;&#24067;&#65292;&#21017;&#26368;&#32456;&#23436;&#25104;&#36825;&#31181;&#23454;&#20363;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#25193;&#25955;&#27169;&#22359;&#23454;&#29616;&#23545;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#20248;&#31168;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14613</link><description>&lt;p&gt;
&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#25193;&#25955;&#27169;&#22359;&#23454;&#29616;&#23545;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#20248;&#31168;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#22686;&#24378;&#25216;&#26415;&#21644;&#37319;&#26679;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20013;&#65292;&#22686;&#24378;&#25216;&#26415;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#65292;&#32780;&#20182;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#21482;&#33021;&#25429;&#25417;&#21040;&#19968;&#23567;&#37096;&#20998;&#20869;&#22312;&#30340;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#35774;&#35745;&#26469;&#33719;&#24471;&#25968;&#25454;&#30340;&#20004;&#20010;&#19981;&#21516;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#23545;&#27604;&#22270;&#25193;&#25955;&#32593;&#32476;&#65288;SCGDN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;AttM&#65289;&#21644;&#25193;&#25955;&#27169;&#22359;&#65288;DiFM&#65289;&#12290;AttM&#36890;&#36807;&#27719;&#38598;&#39640;&#38454;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#26469;&#33719;&#24471;&#20248;&#31168;&#30340;&#23884;&#20837;&#65292;&#32780;DiFM&#36890;&#36807;Laplacian&#25193;&#25955;&#23398;&#20064;&#24179;&#34913;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#29366;&#24577;&#65292;&#24182;&#20801;&#35768;&#22270;&#20013;&#37051;&#25509;&#21644;&#29305;&#24449;&#20449;&#24687;&#30340;&#21327;&#21516;&#28436;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;SCGDN&#26159;&#19968;&#31181;&#26080;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#37319;&#26679;&#20559;&#24046;&#8221;&#21644;&#35821;&#20041;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#20998;&#31163;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#20041;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#22810;&#26465;&#20214;&#20998;&#31163;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#26041;&#27861;&#25509;&#36817;&#20855;&#26377;&#23436;&#25972;&#35821;&#20041;&#20449;&#24687;&#30340;&#39044;&#35774;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#19982;&#26368;&#20339;&#24615;&#33021;&#30340;&#21333;&#26465;&#20214;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.14609</link><description>&lt;p&gt;
&#23436;&#25972;&#19988;&#29420;&#31435;&#65306;&#20855;&#26377;&#32570;&#22833;&#30446;&#26631;&#28304;&#23646;&#24615;&#34917;&#20840;&#30340;&#26465;&#20214;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Complete and separate: Conditional separation with missing target source attribute completion. (arXiv:2307.14609v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#20998;&#31163;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#20041;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#22810;&#26465;&#20214;&#20998;&#31163;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#26041;&#27861;&#25509;&#36817;&#20855;&#26377;&#23436;&#25972;&#35821;&#20041;&#20449;&#24687;&#30340;&#39044;&#35774;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#19982;&#26368;&#20339;&#24615;&#33021;&#30340;&#21333;&#26465;&#20214;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#21033;&#29992;&#26377;&#20851;&#36755;&#20837;&#28151;&#21512;&#29289;&#21644;&#25104;&#20998;&#28304;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#26465;&#20214;&#20998;&#31163;&#27169;&#22411;&#20013;&#20351;&#29992;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#36825;&#31867;&#26041;&#27861;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#25551;&#36848;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#28151;&#21512;&#29289;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#32473;&#23450;&#36755;&#20837;&#28151;&#21512;&#29289;&#21644;&#26377;&#20851;&#30446;&#26631;&#28304;&#30340;&#37096;&#20998;&#35821;&#20041;&#20449;&#24687;&#65292;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#25552;&#21462;&#20854;&#20182;&#30340;&#35821;&#20041;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25913;&#36827;&#26410;&#32806;&#21512;&#30340;&#22810;&#26465;&#20214;&#20998;&#31163;&#32593;&#32476;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#22810;&#26465;&#20214;&#27169;&#22411;&#30340;&#20998;&#31163;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#25509;&#36817;&#20855;&#26377;&#23436;&#25972;&#35821;&#20041;&#20449;&#24687;&#30340;&#39044;&#35774;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#26368;&#20339;&#24615;&#33021;&#30340;&#19987;&#38376;&#21333;&#26465;&#20214;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches in source separation leverage semantic information about their input mixtures and constituent sources that when used in conditional separation models can achieve impressive performance. Most approaches along these lines have focused on simple descriptions, which are not always useful for varying types of input mixtures. In this work, we present an approach in which a model, given an input mixture and partial semantic information about a target source, is trained to extract additional semantic data. We then leverage this pre-trained model to improve the separation performance of an uncoupled multi-conditional separation network. Our experiments demonstrate that the separation performance of this multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Furthermore, our approach achieves performance levels that are comparable to those of the best performing specialized single conditional models,
&lt;/p&gt;</description></item><item><title>HUTFormer&#26159;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#30340;&#20998;&#23618;U-Net Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23610;&#24230;&#34920;&#31034;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14596</link><description>&lt;p&gt;
HUTFormer&#65306;&#29992;&#20110;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#30340;&#20998;&#23618;U-Net Transformer
&lt;/p&gt;
&lt;p&gt;
HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting. (arXiv:2307.14596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14596
&lt;/p&gt;
&lt;p&gt;
HUTFormer&#26159;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#30340;&#20998;&#23618;U-Net Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23610;&#24230;&#34920;&#31034;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#30340;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#36890;&#36807;&#23558;&#39034;&#24207;&#27169;&#22411;&#19982;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;STGNNs&#20165;&#32858;&#28966;&#20110;&#30701;&#26399;&#20132;&#36890;&#39044;&#27979;&#65292;&#22914;1&#23567;&#26102;&#39044;&#27979;&#65292;&#32780;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#38271;&#26399;&#39044;&#27979;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#65292;&#20363;&#22914;1&#22825;&#30340;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22312;&#21033;&#29992;&#22810;&#23610;&#24230;&#34920;&#31034;&#26041;&#38754;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;U-Net TransFormer&#65288;HUTFormer&#65289;&#26469;&#35299;&#20915;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;HUTFormer&#30001;&#20998;&#23618;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#20849;&#21516;&#29983;&#25104;&#21644;&#21033;&#29992;&#20132;&#36890;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traff
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCPA&#30340;&#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#20108;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27169;&#22359;&#25429;&#25417;&#23616;&#37096;&#30456;&#20851;&#24615;&#65292;&#24182;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#29305;&#24449;&#65292;&#20197;&#20811;&#26381;UNet&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14588</link><description>&lt;p&gt;
MCPA: &#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20108;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation. (arXiv:2307.14588v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCPA&#30340;&#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#20108;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27169;&#22359;&#25429;&#25417;&#23616;&#37096;&#30456;&#20851;&#24615;&#65292;&#24182;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#29305;&#24449;&#65292;&#20197;&#20811;&#26381;UNet&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;UNet&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24863;&#21463;&#37326;&#21644;&#21367;&#31215;&#25805;&#20316;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#22312;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#25216;&#26415;&#24050;&#32463;&#34987;&#25972;&#21512;&#21040;UNet&#26550;&#26500;&#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#20840;&#23616;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22359;&#30340;&#25972;&#21512;&#21487;&#33021;&#23548;&#33268;&#22312;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#36807;&#31243;&#20013;&#20002;&#22833;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Multi-scale Cross Perceptron Attention Network&#65288;MCPA&#65289;&#30340;&#20108;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;MCPA&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#20132;&#21449;&#24863;&#30693;&#22120;&#12290;&#20132;&#21449;&#24863;&#30693;&#22120;&#39318;&#20808;&#20351;&#29992;&#22810;&#20010;&#22810;&#23610;&#24230;&#20132;&#21449;&#24863;&#30693;&#22120;&#27169;&#22359;&#25429;&#25417;&#23616;&#37096;&#30456;&#20851;&#24615;&#65292;&#20419;&#36827;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UNet architecture, based on Convolutional Neural Networks (CNN), has demonstrated its remarkable performance in medical image analysis. However, it faces challenges in capturing long-range dependencies due to the limited receptive fields and inherent bias of convolutional operations. Recently, numerous transformer-based techniques have been incorporated into the UNet architecture to overcome this limitation by effectively capturing global feature correlations. However, the integration of the Transformer modules may result in the loss of local contextual information during the global feature fusion process. To overcome these challenges, we propose a 2D medical image segmentation model called Multi-scale Cross Perceptron Attention Network (MCPA). The MCPA consists of three main components: an encoder, a decoder, and a Cross Perceptron. The Cross Perceptron first captures the local correlations using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of features ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#23433;&#20840;&#32422;&#26463;&#30340;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#27604;&#36739;&#20102;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#20004;&#31181;&#23398;&#20064;&#31574;&#30053;&#21518;&#21457;&#29616;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#36712;&#36857;&#65292;&#36991;&#20813;&#30896;&#25758;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14568</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#23548;&#33322;&#20013;&#23545;&#23433;&#20840;&#32422;&#26463;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#23433;&#20840;&#32422;&#26463;&#30340;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#27604;&#36739;&#20102;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#20004;&#31181;&#23398;&#20064;&#31574;&#30053;&#21518;&#21457;&#29616;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#36712;&#36857;&#65292;&#36991;&#20813;&#30896;&#25758;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#32771;&#34385;&#21040;&#23433;&#20840;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#36825;&#20123;&#38480;&#21046;&#23545;&#20110;&#36991;&#20813;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#36947;&#36335;&#19978;&#20986;&#29616;&#19981;&#23433;&#20840;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20984;&#26174;&#36825;&#20123;&#38480;&#21046;&#30340;&#37325;&#35201;&#24615;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#21487;&#23398;&#20064;&#30340;&#23548;&#33322;&#31574;&#30053;&#65306;&#23433;&#20840;&#31574;&#30053;&#21644;&#19981;&#23433;&#20840;&#31574;&#30053;&#12290;&#23433;&#20840;&#31574;&#30053;&#32771;&#34385;&#21040;&#20102;&#32422;&#26463;&#26465;&#20214;&#65292;&#32780;&#21478;&#19968;&#31181;&#31574;&#30053;&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23433;&#20840;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26356;&#22823;&#23433;&#20840;&#38388;&#38553;&#65288;&#19982;&#38556;&#30861;&#29289;&#30340;&#36317;&#31163;&#65289;&#24182;&#19988;&#21457;&#29983;&#26356;&#23569;&#30896;&#25758;&#30340;&#36712;&#36857;&#65292;&#32780;&#19981;&#25439;&#23475;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
&lt;/p&gt;</description></item><item><title>Auto-Tables&#31995;&#32479;&#33021;&#33258;&#21160;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#24335;&#34920;&#26684;&#36716;&#25442;&#20026;&#20851;&#31995;&#24335;&#34920;&#26684;&#65292;&#35299;&#20915;&#20102;&#38750;&#25216;&#26415;&#29992;&#25143;&#20351;&#29992;SQL&#20998;&#26512;&#24037;&#20855;&#30340;&#30171;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.14565</link><description>&lt;p&gt;
Auto-Tables: &#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#20197;&#20351;&#34920;&#26684;&#20851;&#31995;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14565
&lt;/p&gt;
&lt;p&gt;
Auto-Tables&#31995;&#32479;&#33021;&#33258;&#21160;&#21512;&#25104;&#22810;&#27493;&#36716;&#25442;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#24335;&#34920;&#26684;&#36716;&#25442;&#20026;&#20851;&#31995;&#24335;&#34920;&#26684;&#65292;&#35299;&#20915;&#20102;&#38750;&#25216;&#26415;&#29992;&#25143;&#20351;&#29992;SQL&#20998;&#26512;&#24037;&#20855;&#30340;&#30171;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#34920;&#26684;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#26159;&#26631;&#20934;&#30340;&#65292;&#27599;&#19968;&#34892;&#23545;&#24212;&#19968;&#20010;&#23454;&#20307;&#65292;&#27599;&#19968;&#21015;&#23545;&#24212;&#19968;&#20010;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;"&#37326;&#29983;"&#34920;&#26684;&#26102;&#65292;&#36825;&#26679;&#30340;&#26631;&#20934;&#26080;&#27861;&#20445;&#35777;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#30495;&#23454;&#30340;&#30005;&#23376;&#34920;&#26684;&#21644;&#32593;&#39029;&#34920;&#26684;&#65292;&#21457;&#29616;&#36229;&#36807;30%&#30340;&#34920;&#26684;&#19981;&#31526;&#21512;&#20851;&#31995;&#26631;&#20934;&#65292;&#36825;&#23601;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#34920;&#26684;&#37325;&#26500;&#36716;&#25442;&#65292;&#25165;&#33021;&#36731;&#26494;&#22320;&#20351;&#29992;&#22522;&#20110;SQL&#30340;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#30340;&#36716;&#25442;&#32534;&#31243;&#24182;&#19981;&#31616;&#21333;&#65292;&#36825;&#24050;&#32463;&#25104;&#20026;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#29992;&#25143;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20174;StackOverflow&#21644;Excel/Tableau&#35770;&#22363;&#30340;&#22823;&#37327;&#38382;&#39064;&#21487;&#20197;&#35777;&#26126;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Auto-Tables&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21512;&#25104;&#20855;&#26377;&#22810;&#27493;&#36716;&#25442;&#65288;&#20351;&#29992;Python&#25110;&#20854;&#20182;&#35821;&#35328;&#65289;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#38750;&#20851;&#31995;&#22411;&#34920;&#36716;&#25442;&#20026;&#26631;&#20934;&#30340;&#20851;&#31995;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#29702;&#35770;&#24615;&#33021;&#65292;&#24182;&#19988;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#12290;</title><link>http://arxiv.org/abs/2307.14549</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#23545;&#25239;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#65306;&#31639;&#27861;&#21644;&#25490;&#21517;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#29702;&#35770;&#24615;&#33021;&#65292;&#24182;&#19988;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#28216;&#25103;&#30340;&#30561;&#30496;&#36172;&#21338;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#26377;&#30028;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#26410;&#30693;&#30340;i.i.d.&#20998;&#24067;&#30340;&#27494;&#22120;&#21487;&#29992;&#24615;&#12290;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;&#21333;&#27494;&#22120;&#36873;&#25321;&#30340;&#30561;&#30496;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#20445;&#35777;&#36798;&#21040;&#29702;&#35770;&#24615;&#33021;&#65292;&#21518;&#24724;&#19978;&#30028;&#20026;$\bigO(kN^2\sqrt{T\log T})$&#65292;&#20854;&#20013;$k$&#26159;&#27599;&#20010;&#26102;&#38388;&#27493;&#36873;&#25321;&#30340;&#27494;&#22120;&#25968;&#37327;&#65292;$N$&#26159;&#24635;&#27494;&#22120;&#25968;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Modified Spectrum Kernels&#65288;MSKs&#65289;&#26500;&#36896;&#26680;&#26063;&#65292;&#36890;&#36807;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#23485;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#20559;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14531</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#25913;&#26680;&#35889;&#26469;&#25511;&#21046;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum. (arXiv:2307.14531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Modified Spectrum Kernels&#65288;MSKs&#65289;&#26500;&#36896;&#26680;&#26063;&#65292;&#36890;&#36807;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#23485;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#20559;&#24046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#29305;&#23450;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#20559;&#24046;&#65292;&#24433;&#21709;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26377;&#38480;&#35757;&#32451;&#26102;&#38388;&#20869;&#21487;&#36798;&#21040;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#20462;&#25913;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Modified Spectrum Kernels(MSKs)&#36825;&#19968;&#26032;&#39062;&#30340;&#26500;&#36896;&#26680;&#26063;&#65292;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#27809;&#26377;&#24050;&#30693;&#38381;&#21512;&#24418;&#24335;&#30340;&#26399;&#26395;&#29305;&#24449;&#20540;&#30340;&#26680;&#12290;&#25105;&#20204;&#21033;&#29992;&#23485;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#25913;&#21464;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#36712;&#36857;&#12290;&#32467;&#26524;&#26159;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#36895;&#24230;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21576;&#22810;&#39033;&#24335;&#29978;&#33267;&#25351;&#25968;&#32423;&#21152;&#36895;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#26368;&#32456;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#35745;&#31639;&#39640;&#25928;&#21448;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22312;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.14530</link><description>&lt;p&gt;
&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22312;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#29616;&#20195;&#32593;&#32476;&#31185;&#23398;&#20013;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20854;&#24212;&#29992;&#21487;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#25214;&#21040;&#65292;&#20174;&#34507;&#30333;&#36136;&#24314;&#27169;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#35770;&#25991;&#30740;&#31350;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#21363;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#23646;&#20110;&#22810;&#20010;&#31038;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30001;Airoldi&#31561;&#20154;&#65288;2008&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MMSB&#65289;&#12290;MMSB&#22312;&#22270;&#20013;&#23545;&#37325;&#21472;&#31038;&#21306;&#32467;&#26500;&#25552;&#20379;&#20102;&#30456;&#24403;&#19968;&#33324;&#30340;&#35774;&#32622;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#32593;&#32476;&#20013;&#37325;&#24314;&#31038;&#21306;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#36825;&#20010;&#19979;&#30028;&#21305;&#37197;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#32467;&#26524;&#22312;&#23545;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#30340;&#30456;&#24403;&#26222;&#36941;&#26465;&#20214;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35828;&#26126;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#25439;&#22833;&#20540;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#20307;&#65292;&#36890;&#36807;&#35299;&#20915;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#36739;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36880;&#28176;&#23398;&#20064;&#25439;&#22833;&#20540;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14528</link><description>&lt;p&gt;
&#20989;&#25968;&#20540;&#23398;&#20064;&#65306;&#22522;&#20110;Polyak&#27493;&#38271;&#21644;&#20989;&#25968;&#20998;&#21106;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM. (arXiv:2307.14528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#25439;&#22833;&#20540;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#20307;&#65292;&#36890;&#36807;&#35299;&#20915;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#36739;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36880;&#28176;&#23398;&#20064;&#25439;&#22833;&#20540;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#65288;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65289;&#21464;&#20307;&#65292;&#21033;&#29992;&#20102;&#37319;&#26679;&#25439;&#22833;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#24565;&#19978;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;$\texttt{SPS}_+$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#37319;&#26679;&#25439;&#22833;&#20540;&#65292;&#24182;&#20551;&#35774;&#23545;&#37319;&#26679;&#25439;&#22833;&#20540;&#22312;&#26368;&#20248;&#24773;&#20917;&#19979;&#26377;&#20102;&#35299;&#12290;&#36825;&#31181;$\texttt{SPS}_+$&#26159;SPS&#65288;&#38543;&#26426;Polyak&#27493;&#38271;&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#23567;&#20462;&#25913;&#65292;&#20854;&#20013;&#27493;&#38271;&#34987;&#24378;&#21046;&#20026;&#27491;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\texttt{SPS}_+$&#22312;Lipschitz&#38750;&#20809;&#28369;&#20013;&#23454;&#29616;&#20102;SGD&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;$\texttt{FUVAL}$&#65292;&#36825;&#26159;$\texttt{SPS}_+$&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#20854;&#20013;&#25439;&#22833;&#20540;&#22312;&#26368;&#20248;&#24773;&#20917;&#19979;&#36880;&#28176;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#32473;&#23450;&#30340;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;$\texttt{FUVAL}$&#30340;&#19977;&#20010;&#35270;&#35282;&#65292;&#20316;&#20026;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#36817;&#20284;&#32447;&#24615;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#20197;&#21450;&#20316;&#20026;&#29305;&#23450;&#30340;&#22312;&#32447;SGD&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;$\texttt{FUVAL}$&#12290;
&lt;/p&gt;
&lt;p&gt;
Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14527</link><description>&lt;p&gt;
&#29992;&#20110;&#33618;&#37326;SAR&#21644;&#23547;&#25214;Patricia Wu-Murad&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#23558;&#20004;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;EfficientDET&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#26469;&#33258;&#26085;&#26412;Wu-Murad&#37326;&#22806;&#25628;&#25937;&#65288;WSAR&#65289;&#21162;&#21147;&#30340;98.9 GB&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;3&#20010;&#26041;&#21521;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#33267;&#23569;19&#31181;&#26041;&#27861;&#21644;3&#20010;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#23450;&#20301;&#22833;&#36394;&#20154;&#21592;&#65292;&#20294;&#21482;&#26377;3&#31181;&#26041;&#27861;&#65288;2&#31181;&#26080;&#30417;&#30563;&#21644;1&#31181;&#26410;&#30693;&#32467;&#26500;&#65289;&#22312;&#25991;&#29486;&#20013;&#34987;&#24341;&#29992;&#20026;&#23454;&#38469;WSAR&#25805;&#20316;&#20013;&#20351;&#29992;&#36807;&#12290;&#22312;&#36825;&#20123;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;EfficientDET&#26550;&#26500;&#21644;&#26080;&#30417;&#30563;&#30340;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#34987;&#36873;&#25321;&#20026;&#26368;&#36866;&#21512;&#27492;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;EfficientDET&#27169;&#22411;&#24212;&#29992;&#20110;HERIDAL&#25968;&#25454;&#38598;&#65292;&#23613;&#31649;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#25216;&#26415;&#30456;&#24403;&#30340;&#27700;&#24179;&#65292;&#20294;&#27169;&#22411;&#22312;&#20551;&#38451;&#24615;&#26041;&#38754;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#25928;&#35782;&#21035;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#26641;&#26525;&#21644;&#23721;&#30707;&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#38169;&#35823;&#30340;&#29305;&#24449;&#21644;&#32500;&#25252;&#25361;&#25112;&#65292;&#24182;&#27604;&#36739;&#20102;ML&#21644;&#38750;ML&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#30340;GitHub&#20179;&#24211;&#36827;&#34892;&#35843;&#26597;&#65292;&#20316;&#32773;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#20851;&#38169;&#35823;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.14512</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#38169;&#35823;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bug Characterization in Machine Learning-based Systems. (arXiv:2307.14512v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#38169;&#35823;&#30340;&#29305;&#24449;&#21644;&#32500;&#25252;&#25361;&#25112;&#65292;&#24182;&#27604;&#36739;&#20102;ML&#21644;&#38750;ML&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#30340;GitHub&#20179;&#24211;&#36827;&#34892;&#35843;&#26597;&#65292;&#20316;&#32773;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#20851;&#38169;&#35823;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#36805;&#36895;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#22686;&#21152;&#20102;&#23545;&#21487;&#38752;ML&#32452;&#20214;&#30340;&#38656;&#27714;&#65292;&#21363;&#22522;&#20110;ML&#25805;&#20316;&#30340;&#36719;&#20214;&#32452;&#20214;&#12290;&#20102;&#35299;ML&#31995;&#32479;&#20013;&#30340;&#38169;&#35823;&#29305;&#24449;&#21644;&#32500;&#25252;&#25361;&#25112;&#21487;&#20197;&#24110;&#21161;&#36825;&#20123;&#31995;&#32479;&#30340;&#24320;&#21457;&#20154;&#21592;&#30830;&#23450;&#32500;&#25252;&#21644;&#27979;&#35797;&#24037;&#20316;&#30340;&#37325;&#28857;&#65292;&#36890;&#36807;&#25552;&#20379;&#23545;&#26368;&#23481;&#26131;&#20986;&#38169;&#30340;&#32452;&#20214;&#12289;&#26368;&#24120;&#35265;&#30340;&#38169;&#35823;&#31561;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#36719;&#20214;&#31995;&#32479;&#20013;&#38169;&#35823;&#30340;&#29305;&#24449;&#20197;&#21450;&#20174;&#32500;&#25252;&#35282;&#24230;&#30475;ML&#21644;&#38750;ML&#38169;&#35823;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;447,948&#20010;GitHub&#20179;&#24211;&#65292;&#36825;&#20123;&#20179;&#24211;&#20351;&#29992;&#20102;&#19977;&#20010;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#65292;&#21363;TensorFlow&#12289;Keras&#21644;PyTorch&#12290;&#32463;&#36807;&#22810;&#27425;&#36807;&#28388;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20851;&#38381;&#38382;&#39064;&#25968;&#37327;&#26368;&#22810;&#30340;&#21069;300&#20010;&#20179;&#24211;&#12290;&#25105;&#20204;&#25163;&#21160;&#35843;&#26597;&#25552;&#21462;&#30340;&#20179;&#24211;&#65292;&#25490;&#38500;&#38750;ML&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28041;&#21450;&#25163;&#21160;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#20351;&#29992;&#19982;&#22024;&#26434;&#25968;&#25454;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#19982;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14502</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#30740;&#31350;&#21475;&#35821;&#23545;&#35821;&#38899;&#22686;&#24378;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#20351;&#29992;&#19982;&#22024;&#26434;&#25968;&#25454;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#19982;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65288;SSSRs&#65289;&#20316;&#20026;&#29305;&#24449;&#36716;&#25442;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#24456;&#23569;&#27880;&#24847;&#21040;&#29992;&#20110;&#35757;&#32451;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#38899;&#39057;&#35821;&#35328;&#19982;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20351;&#29992;&#23558;&#33258;&#30417;&#30563;&#34920;&#31034;&#19982;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#22024;&#26434;&#25968;&#25454;&#30340;&#35821;&#35328;&#23436;&#20840;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22686;&#24378;&#27169;&#22411;&#34920;&#29616;&#27604;&#19981;&#23436;&#20840;&#21305;&#37197;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22686;&#24378;&#31995;&#32479;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#22909;&#65292;&#32780;&#20351;&#29992;&#20256;&#32479;&#30340;&#39057;&#35889;&#22270;&#25110;&#26102;&#38388;&#22495;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#19981;&#20250;&#20986;&#29616;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#22312;&#22810;&#31181;&#19981;&#21516;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#32463;&#36807;&#19981;&#21516;&#35821;&#35328;&#32452;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#26032;&#27169;&#22411;READ&#65292;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2307.14500</link><description>&lt;p&gt;
&#19968;&#20010;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#39044;&#27979;&#27169;&#22411;&#65306;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#29992;&#25143;&#23545;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#26032;&#27169;&#22411;READ&#65292;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#32463;&#36807;&#23454;&#35777;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#65288;IE&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;READ&#27169;&#22411;&#65292;&#20854;&#20195;&#34920;&#20102;&#21442;&#19982;&#24230;&#39640;&#30340;&#20449;&#24687;&#30340;&#22235;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#20195;&#34920;&#24615;&#12289;&#26131;&#29992;&#24615;&#12289;&#24773;&#24863;&#21644;&#20998;&#24067;&#12290;&#22312;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30340;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#35813;&#27169;&#22411;&#23558;&#20851;&#38190;&#30340;&#35748;&#30693;&#20559;&#24046;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#23637;&#20986;&#23545;&#20449;&#24687;&#21442;&#19982;&#30340;&#22810;&#32500;&#24230;&#35270;&#35282;&#12290;&#30740;&#31350;&#37319;&#29992;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#21327;&#35758;&#65292;&#36873;&#21462;&#20102;WordNet&#25968;&#25454;&#24211;&#20013;&#30340;50&#23545;&#38543;&#26426;&#21516;&#20041;&#35789;&#65288;&#20849;100&#20010;&#35789;&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#35843;&#26597;&#65288;n = 80,500&#65289;&#35780;&#20272;&#20102;&#36825;&#20123;&#35789;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24471;&#20986;&#20102;&#23454;&#35777;IE&#25351;&#26631;&#12290;&#28982;&#21518;&#23545;&#27599;&#20010;&#35789;&#30340;READ&#23646;&#24615;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#26816;&#26597;&#20854;&#39044;&#27979;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;READ&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#35789;&#30340;IE&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engagi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;TPU&#36827;&#34892;&#39640;&#24615;&#33021;&#22270;&#23884;&#20837;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#25968;&#21313;&#20159;&#33410;&#28857;&#21644;&#25968;&#19975;&#20159;&#36793;&#30340;&#22270;&#12290;&#39564;&#35777;&#20102;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.14490</link><description>&lt;p&gt;
HUGE: &#20351;&#29992;TPU&#36827;&#34892;&#24040;&#22823;&#26080;&#30417;&#30563;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
HUGE: Huge Unsupervised Graph Embeddings with TPUs. (arXiv:2307.14490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;TPU&#36827;&#34892;&#39640;&#24615;&#33021;&#22270;&#23884;&#20837;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#25968;&#21313;&#20159;&#33410;&#28857;&#21644;&#25968;&#19975;&#20159;&#36793;&#30340;&#22270;&#12290;&#39564;&#35777;&#20102;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#19968;&#31181;&#25429;&#25417;&#23545;&#35937;&#38598;&#21512;&#20043;&#38388;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#34920;&#31034;&#24418;&#24335;&#12290;&#38543;&#30528;&#21487;&#29992;&#32593;&#32476;&#25968;&#25454;&#30340;&#26222;&#21450;&#24615;&#65292;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#23545;&#20110;&#24555;&#36895;&#20998;&#26512;&#20855;&#26377;&#25968;&#21313;&#20159;&#33410;&#28857;&#21644;&#25968;&#19975;&#20159;&#36793;&#30340;&#22270;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#32593;&#32476;&#29702;&#35299;&#30340;&#24120;&#35265;&#39318;&#35201;&#27493;&#39588;&#26159;&#22270;&#23884;&#20837;&#65292;&#21363;&#21019;&#24314;&#22270;&#20013;&#33410;&#28857;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#36807;&#31243;&#12290;&#36830;&#32493;&#34920;&#31034;&#36890;&#24120;&#26356;&#26131;&#20110;&#22788;&#29702;&#65292;&#23588;&#20854;&#22312;&#35268;&#27169;&#19978;&#65292;&#29992;&#20110;&#35299;&#20915;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#37197;&#32622;&#39640;&#24102;&#23485;&#20869;&#23384;&#30340;&#24352;&#37327;&#22788;&#29702;&#21333;&#20803;&#65288;TPU&#65289;&#30340;&#39640;&#24615;&#33021;&#22270;&#23884;&#20837;&#26550;&#26500;&#65292;&#31616;&#21270;&#20102;&#22270;&#23884;&#20837;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#21313;&#20159;&#33410;&#28857;&#21644;&#25968;&#19975;&#20159;&#36793;&#30340;&#22270;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#22312;&#36866;&#24212;&#26032;&#30340;&#22270;&#20687;&#37319;&#38598;&#21644;&#30142;&#30149;&#31867;&#22411;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#22810;&#26679;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22495;&#20869;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.14482</link><description>&lt;p&gt;
&#22270;&#20687;&#37319;&#38598;&#21644;&#24739;&#32773;&#34920;&#22411;&#21464;&#24322;&#22312;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization. (arXiv:2307.14482v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#22312;&#36866;&#24212;&#26032;&#30340;&#22270;&#20687;&#37319;&#38598;&#21644;&#30142;&#30149;&#31867;&#22411;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#22810;&#26679;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22495;&#20869;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#22312;&#22495;&#22806;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23545;&#26032;&#30340;&#22270;&#20687;&#37319;&#38598;&#21644;&#30142;&#30149;&#31867;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#26448;&#26009;&#65306;&#20351;&#29992;&#20581;&#24247;&#24739;&#32773;&#21644;&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#24739;&#32773;&#30340;&#38750;&#23545;&#27604;&#22686;&#24378;&#33145;&#37096;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#12290;&#24635;&#20849;&#20351;&#29992;&#20102;400&#24352;&#22270;&#20687;&#65288;100&#24352;&#38750;&#23545;&#27604;&#23545;&#29031;&#32452;&#65292;100&#24352;&#23545;&#27604;&#23545;&#29031;&#32452;&#65292;100&#24352;&#38750;&#23545;&#27604;PKD&#32452;&#65292;100&#24352;&#23545;&#27604;PKD&#32452;&#65289;&#36827;&#34892;&#35757;&#32451;/&#39564;&#35777;&#65292;&#20197;&#20998;&#21106;&#32958;&#33039;&#12289;&#32925;&#33039;&#21644;&#33086;&#33039;&#65292;&#24182;&#23558;&#26368;&#32456;&#27169;&#22411;&#27979;&#35797;&#22312;&#21463;PKD&#24433;&#21709;&#30340;100&#24352;&#38750;&#23545;&#27604;CT&#22270;&#20687;&#19978;&#12290;&#20351;&#29992;Dice&#31995;&#25968;&#12289;Jaccard&#31995;&#25968;&#12289;TPR&#21644;Precision&#35780;&#20272;&#24615;&#33021;&#12290;&#32467;&#26524;&#65306;&#22312;&#22495;&#20869;&#27979;&#35797;&#25968;&#25454;&#19978;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#19981;&#27604;&#20165;&#20351;&#29992;&#22495;&#20869;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24046;&#12290;&#20363;&#22914;&#65292;&#23545;&#27604;25%&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;Dice&#30456;&#20284;&#24230;&#24471;&#20998;&#20302;&#20110;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: This study evaluated the out-of-domain performance and generalization capabilities of automated medical image segmentation models, with a particular focus on adaptation to new image acquisitions and disease type.  Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT scans of healthy patients and those with polycystic kidney disease (PKD) were used. A total of 400 images (100 non-contrast controls, 100 contrast controls, 100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation of models to segment kidneys, livers, and spleens, and the final models were then tested on 100 non-contrast CT images of patients affected by PKD. Performance was evaluated using Dice, Jaccard, TPR, and Precision.  Results: Models trained on a diverse range of data showed no worse performance than models trained exclusively on in-domain data when tested on in-domain data. For instance, the Dice similarity of the model trained on 25% from each dataset was foun
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;API&#30340;&#21512;&#21516;&#31867;&#22411;&#65292;&#20197;&#24110;&#21161;API&#29992;&#25143;&#22312;&#26089;&#26399;&#38454;&#27573;&#25429;&#25417;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#23545;&#22235;&#20010;ML&#24211;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25552;&#21462;&#20102;413&#20010;API&#35268;&#33539;&#65292;&#20197;&#20102;&#35299;ML&#21512;&#21516;&#36829;&#35268;&#30340;&#26681;&#26412;&#21407;&#22240;&#12289;&#24120;&#35265;&#27169;&#24335;&#21644;&#26159;&#21542;&#38656;&#35201;&#20808;&#36827;&#30340;ML&#36719;&#20214;&#19987;&#19994;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.14465</link><description>&lt;p&gt;
ML API&#38656;&#35201;&#21738;&#31181;&#31867;&#22411;&#30340;&#21512;&#21516;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Kinds of Contracts Do ML APIs Need?. (arXiv:2307.14465v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;API&#30340;&#21512;&#21516;&#31867;&#22411;&#65292;&#20197;&#24110;&#21161;API&#29992;&#25143;&#22312;&#26089;&#26399;&#38454;&#27573;&#25429;&#25417;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#23545;&#22235;&#20010;ML&#24211;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#25552;&#21462;&#20102;413&#20010;API&#35268;&#33539;&#65292;&#20197;&#20102;&#35299;ML&#21512;&#21516;&#36829;&#35268;&#30340;&#26681;&#26412;&#21407;&#22240;&#12289;&#24120;&#35265;&#27169;&#24335;&#21644;&#26159;&#21542;&#38656;&#35201;&#20808;&#36827;&#30340;ML&#36719;&#20214;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31243;&#24207;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21628;&#21505;&#20026;ML&#20195;&#30721;&#24314;&#31435;&#21512;&#21516;&#12290;&#21512;&#21516;&#65292;&#22914;&#35774;&#35745;&#21512;&#21516;&#26041;&#27861;&#35770;&#20013;&#25152;&#36848;&#65292;&#26377;&#21161;&#20110;&#35760;&#24405;API&#24182;&#24110;&#21161;API&#29992;&#25143;&#32534;&#20889;&#27491;&#30830;&#30340;&#20195;&#30721;&#12290;&#38382;&#39064;&#26159;&#65306;&#21738;&#31181;&#31867;&#22411;&#30340;&#21512;&#21516;&#33021;&#26368;&#22823;&#31243;&#24230;&#22320;&#24110;&#21161;API&#29992;&#25143;&#65311;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21738;&#31181;&#31867;&#22411;&#30340;&#21512;&#21516;&#26377;&#21161;&#20110;API&#29992;&#25143;&#22312;ML&#27969;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#25429;&#25417;&#38169;&#35823;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#39033;&#38024;&#23545;Stack Overflow&#19978;&#26368;&#32463;&#24120;&#35752;&#35770;&#30340;&#22235;&#20010;ML&#24211;&#65288;TensorFlow&#65292;Scikit-learn&#65292;Keras&#21644;PyTorch&#65289;&#30340;&#24086;&#23376;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#23545;&#20110;&#36825;&#20123;&#24211;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#21462;&#20102;413&#20010;&#38750;&#27491;&#24335;&#65288;&#33521;&#25991;&#65289;API&#35268;&#33539;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35268;&#33539;&#26469;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#12290;ML&#21512;&#21516;&#36829;&#35268;&#30340;&#26681;&#26412;&#21407;&#22240;&#21644;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#26159;&#21542;&#23384;&#22312;&#24120;&#35265;&#30340;ML&#21512;&#21516;&#36829;&#35268;&#27169;&#24335;&#65311;&#29702;&#35299;ML&#21512;&#21516;&#20309;&#26102;&#38656;&#35201;&#20808;&#36827;&#30340;ML&#36719;&#20214;&#19987;&#19994;&#30693;&#35782;&#65311;&#26816;&#26597;API&#32423;&#21035;&#30340;&#21512;&#21516;&#33021;&#21542;&#26377;&#21161;&#20110;&#26816;&#27979;&#21512;&#21516;&#36829;&#35268;&#65311;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that Machine Learning (ML) programs are error-prone and called for contracts for ML code. Contracts, as in the design by contract methodology, help document APIs and aid API users in writing correct code. The question is: what kinds of contracts would provide the most help to API users? We are especially interested in what kinds of contracts help API users catch errors at earlier stages in the ML pipeline. We describe an empirical study of posts on Stack Overflow of the four most often-discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study extracted 413 informal (English) API specifications. We used these specifications to understand the following questions. What are the root causes and effects behind ML contract violations? Are there common patterns of ML contract violations? When does understanding ML contracts require an advanced level of ML software expertise? Could checking contracts at the API level help detect t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#24212;&#29992;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#31639;&#27861;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#21033;&#29992;&#26680;&#24515;&#38598;&#21512;&#25216;&#26415;&#20195;&#26367;&#23436;&#25972;&#25968;&#25454;&#38598;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#24182;&#21152;&#36895;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.14459</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#24515;&#38598;&#21512;&#35757;&#32451;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;
&lt;/p&gt;
&lt;p&gt;
Training Quantum Boltzmann Machines with Coresets. (arXiv:2307.14459v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14459
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#24212;&#29992;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#31639;&#27861;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#21033;&#29992;&#26680;&#24515;&#38598;&#21512;&#25216;&#26415;&#20195;&#26367;&#23436;&#25972;&#25968;&#25454;&#38598;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#24182;&#21152;&#36895;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#20351;&#29992;&#26680;&#24515;&#38598;&#21512;&#25216;&#26415;&#26469;&#21152;&#36895;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#29992;&#20110;&#22788;&#29702;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24819;&#27861;&#24212;&#29992;&#20110;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;QBM&#65289;&#65292;&#20854;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#27493;&#39588;&#38656;&#35201;&#23545;Gibbs&#29366;&#24577;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#26159;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#29942;&#39048;&#12290;&#36890;&#36807;&#20351;&#29992;&#26680;&#24515;&#38598;&#21512;&#20195;&#26367;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35797;&#22270;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#24182;&#21152;&#36895;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#37327;&#23376;&#35745;&#31639;&#26102;&#38388;&#38750;&#24120;&#23453;&#36149;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#23454;&#38469;&#33410;&#30465;&#12290;&#25105;&#20204;&#20351;&#29992;QBM&#30340;36&#20010;&#21487;&#35265;&#21333;&#20803;&#21644;8&#20010;&#38544;&#34255;&#21333;&#20803;&#22312;&#19968;&#20010;&#22686;&#24378;&#30340;&#26465;&#32441;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;6x6&#20108;&#36827;&#21046;&#22270;&#20687;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21463;&#21551;&#21457;&#20110;Inception&#20998;&#25968;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#26680;&#24515;&#38598;&#21512;&#30340;QBM&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#26512;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#35013;&#30002;&#36710;&#36742;&#30340;&#32500;&#25252;&#38656;&#27714;&#65292;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#20943;&#23569;&#36710;&#36742;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14453</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35013;&#30002;&#36710;&#36742;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches. (arXiv:2307.14453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#26512;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#35013;&#30002;&#36710;&#36742;&#30340;&#32500;&#25252;&#38656;&#27714;&#65292;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#20943;&#23569;&#36710;&#36742;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#30002;&#36710;&#36742;&#26159;&#19987;&#19994;&#21644;&#22797;&#26434;&#30340;&#26426;&#26800;&#35774;&#22791;&#65292;&#35774;&#35745;&#29992;&#20110;&#22312;&#39640;&#24212;&#21147;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#24120;&#24120;&#29992;&#20110;&#25112;&#26007;&#25110;&#25112;&#26415;&#29615;&#22659;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#36825;&#20123;&#36710;&#36742;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#39044;&#27979;&#28508;&#22312;&#30340;&#32500;&#25252;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#30340;&#26550;&#26500;&#28041;&#21450;&#21508;&#31181;&#27169;&#22411;&#65292;&#22914;&#36731;&#26799;&#24230;&#25552;&#21319;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#12289;&#26497;&#31471;&#26641;&#20998;&#31867;&#22120;&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#30340;&#32500;&#25252;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;TOPSIS&#20998;&#26512;&#26469;&#35780;&#20272;&#25552;&#20986;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#20026;98.93%&#65292;&#31934;&#30830;&#24230;&#20026;99.80%&#65292;&#21484;&#22238;&#29575;&#20026;99.03%&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#32500;&#25252;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#23569;&#36710;&#36742;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#27604;&#21508;&#31181;&#31639;&#27861;&#21644;&#24314;&#35758;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the sugges
&lt;/p&gt;</description></item><item><title>VISPUR&#26159;&#19968;&#20010;&#25552;&#20379;&#35270;&#35273;&#20998;&#26512;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#30340;&#34394;&#20551;&#20851;&#32852;&#12290;&#23427;&#21253;&#25324;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#21644;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2307.14448</link><description>&lt;p&gt;
VISPUR: &#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#34394;&#20551;&#20851;&#32852;&#30340;&#35270;&#35273;&#36741;&#21161;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14448
&lt;/p&gt;
&lt;p&gt;
VISPUR&#26159;&#19968;&#20010;&#25552;&#20379;&#35270;&#35273;&#20998;&#26512;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20013;&#30340;&#34394;&#20551;&#20851;&#32852;&#12290;&#23427;&#21253;&#25324;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#21644;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20849;&#21516;&#36171;&#20104;&#20154;&#31867;&#22312;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#25429;&#25417;&#21040;&#30340;&#32463;&#39564;&#20851;&#32852;&#21487;&#33021;&#30001;&#20110;&#28151;&#28102;&#22240;&#32032;&#21644;&#23376;&#32676;&#24322;&#36136;&#24615;&#32780;&#26159;&#34394;&#20551;&#30340;&#12290;&#33879;&#21517;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#23601;&#26159;&#36825;&#26679;&#19968;&#20010;&#29616;&#35937;&#65292;&#32858;&#21512;&#21644;&#23376;&#32676;&#32423;&#21035;&#30340;&#20851;&#32852;&#30456;&#20114;&#30683;&#30462;&#65292;&#32473;&#20154;&#31867;&#24102;&#26469;&#35748;&#30693;&#22256;&#24785;&#21644;&#20915;&#31574;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23545;&#20110;&#20154;&#31867;&#22312;&#23454;&#36341;&#20013;&#23450;&#20301;&#12289;&#25512;&#29702;&#21644;&#39044;&#38450;&#34394;&#20551;&#20851;&#32852;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;VISPUR&#65292;&#19968;&#20010;&#25552;&#20379;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#21644;&#20154;&#26412;&#24037;&#20316;&#27969;&#31243;&#20197;&#24212;&#23545;&#34394;&#20551;&#20851;&#32852;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#12290;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28151;&#28102;&#22240;&#32032;&#20202;&#34920;&#30424;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#21487;&#33021;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#20197;&#21450;&#19968;&#20010;&#23376;&#32676;&#27983;&#35272;&#22120;&#65292;&#20801;&#35768;&#23545;&#21487;&#33021;&#23548;&#33268;&#23545;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#38169;&#35823;&#30340;&#22810;&#26679;&#23376;&#32676;&#27169;&#24335;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14439</link><description>&lt;p&gt;
&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23398;&#20064;&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31215;&#20998;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#31215;&#20998;&#36890;&#24120;&#26159;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#30340;&#65292;&#22240;&#20026;&#35299;&#26512;&#35745;&#31639;&#31215;&#20998;&#36807;&#31243;&#22797;&#26434;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#23398;&#20064;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968; $f$ &#35299;&#26512;&#31215;&#20998;&#30340;&#26041;&#27861;&#12290;&#36825;&#20801;&#35768;&#31934;&#30830;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#26469;&#23545;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558; $f$ &#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35768;&#22810;&#24212;&#29992;&#65288;&#20363;&#22914;&#27010;&#29575;&#20998;&#24067;&#12289;&#36317;&#31163;&#24230;&#37327;&#31561;&#65289;&#25152;&#24517;&#38656;&#30340;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20010;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#30340;&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;&#65288;FINN&#65289;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14403</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#22686;&#24378;&#20809;&#35889;&#21644;&#31354;&#38388;&#20445;&#30495;&#24230;&#30340;Pansharpening&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14403
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;Pansharpening&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#32570;&#20047;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#20998;&#36776;&#29575;&#39046;&#22495;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#65292;&#35757;&#32451;&#22312;&#38477;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#19978;&#30340;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#29616;&#22312;&#36716;&#21521;&#22312;&#20840;&#20998;&#36776;&#29575;&#39046;&#22495;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#36890;&#36807;&#23450;&#20041;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#33539;&#20363;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#26550;&#26500;&#30340;&#20840;&#20998;&#36776;&#29575;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Pansharpening&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#24182;&#25552;&#20379;&#20102;&#21069;&#27839;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#30340;&#26550;&#26500;&#25913;&#36827;&#65292;&#22914;&#20351;&#29992;&#27531;&#20313;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#20855;&#26377;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#20419;&#36827;&#20809;&#35889;&#21644;&#31354;&#38388;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.  Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#21644;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.14397</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#24314;&#27169;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#21644;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29983;&#25104;&#24314;&#27169;&#26088;&#22312;&#23398;&#20064;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#32479;&#35745;&#30456;&#20284;&#30340;&#26032;&#25968;&#25454;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#31216;&#20026;&#25968;&#25454;&#32422;&#26463;&#19979;&#30340;&#29983;&#25104;&#24314;&#27169;&#65288;GM-DC&#65289;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#24403;&#25968;&#25454;&#33719;&#21462;&#20855;&#26377;&#25361;&#25112;&#24615;&#26102;&#65292;&#20363;&#22914;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32972;&#26223;&#12289;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20998;&#31867;&#20307;&#31995;&#65306;&#19968;&#20010;&#26159;GM-DC&#20219;&#21153;&#20998;&#31867;&#65292;&#21478;&#19968;&#20010;&#26159;GM-DC&#26041;&#27861;&#20998;&#31867;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;GM-DC&#20219;&#21153;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#30740;&#31350;&#31354;&#30333;&#12289;&#30740;&#31350;&#36235;&#21183;&#21644;&#26410;&#26469;&#25506;&#32034;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://gmdc-survey.github.io&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;PDE-Net++&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#26126;&#30830;&#23884;&#20837;&#20102;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14395</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#27169;&#25311;&#37096;&#20998;&#24050;&#30693;&#30340;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;PDE-Net++&#65292;&#36890;&#36807;&#32467;&#21512;&#21487;&#35757;&#32451;&#24046;&#20998;&#31639;&#23376;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#26126;&#30830;&#23884;&#20837;&#20102;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#26102;&#31354;&#21160;&#24577;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#12290;&#36890;&#36807;&#23558;&#21487;&#35757;&#32451;&#30340;&#24046;&#20998;&#31639;&#23376;&#19982;&#40657;&#30418;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;&#24213;&#23618;PDE&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#26126;&#30830;&#23884;&#20837;&#21040;PDE-Net++&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#36873;&#25321;&#65292;&#31216;&#20026;&#21487;&#35757;&#32451;&#32763;&#36716;&#24046;&#20998;&#23618;&#65288;TFDL&#65289;&#21644;&#21487;&#35757;&#32451;&#21160;&#24577;&#24046;&#20998;&#23618;&#65288;TDDL&#65289;&#65292;&#29992;&#20110;&#24046;&#20998;&#31639;&#23376;&#12290;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;PDE-Net++&#20855;&#26377;&#27604;&#40657;&#30418;&#27169;&#22411;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#35745;&#31639;&#30340;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#30340;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14394</link><description>&lt;p&gt;
Hypergraph&#21516;&#26500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#35745;&#31639;&#30340;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#30340;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26500;&#38382;&#39064;&#26159;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#25429;&#25417;&#20302;&#38454;&#21644;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#12290;&#20174;&#25552;&#21462;&#20302;&#38454;&#32467;&#26500;&#20449;&#24687;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22270;&#21516;&#26500;&#31639;&#27861;&#20998;&#26512;&#32467;&#26500;&#31561;&#20215;&#24615;&#20197;&#38477;&#20302;&#27714;&#35299;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#12289;&#21270;&#23398;&#36884;&#24452;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#29616;&#23454;&#22330;&#26223;&#20013;&#26356;&#24120;&#35265;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20351;&#29992;&#22270;&#21516;&#26500;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#36229;&#22270;&#21516;&#26500;&#38382;&#39064;&#65292;&#21518;&#32773;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#39640;&#38454;&#32467;&#26500;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#36229;&#22270;&#20869;&#26680;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20869;&#23384;&#28040;&#32791;&#39640;&#25110;&#23376;&#32467;&#26500;&#35782;&#21035;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#65292;&#22240;&#27492;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#36229;&#22270;Weisfiler-Lehman&#27979;&#35797;&#31639;&#27861;&#29992;&#20110;&#36229;&#22270;&#21516;&#26500;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph iso
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#21457;&#23637;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.14389</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#30721;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#21457;&#23637;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#21644;&#20302;&#20449;&#22122;&#27604;&#12290;&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#20855;&#26377;&#21069;&#26223;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;DDPMs&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;Diff-E&#23545;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#36827;&#34892;&#35299;&#30721;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;Diff-E&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#24819;&#35937;&#35328;&#35821;&#33041;&#30005;&#22270;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;DDPMs&#21487;&#20197;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#24037;&#20855;&#65292;&#23545;&#20110;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HyperFed&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#36896;&#25104;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#12289;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#21644;&#19968;&#33268;&#32858;&#21512;&#31561;&#27169;&#22359;&#30340;&#32467;&#21512;&#65292;&#26469;&#35299;&#20915;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#12289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14384</link><description>&lt;p&gt;
HyperFed: &#38750;IID&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#19982;&#19968;&#33268;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HyperFed&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#36896;&#25104;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36229;&#29699;&#38754;&#21407;&#22411;&#25506;&#32034;&#12289;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#21644;&#19968;&#33268;&#32858;&#21512;&#31561;&#27169;&#22359;&#30340;&#32467;&#21512;&#65292;&#26469;&#35299;&#20915;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#12289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20197;&#20998;&#25955;&#21270;&#30340;&#26041;&#24335;&#21327;&#21516;&#23545;&#29992;&#25143;&#25968;&#25454;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#30456;&#21516;&#21644;&#29420;&#31435;&#25968;&#25454;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#38459;&#30861;&#20102;FL&#30340;&#24615;&#33021;&#65292;&#21407;&#22240;&#26377;&#19977;&#28857;&#65292;&#21363;&#65288;1&#65289;&#31867;&#21035;&#32479;&#35745;&#30340;&#21464;&#21270;&#65292;&#65288;2&#65289;&#23618;&#32423;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#65288;3&#65289;&#23458;&#25143;&#31471;&#32858;&#21512;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperFed&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65292;&#21363;&#36229;&#29699;&#38754;&#21407;&#22411;Tammes&#21021;&#22987;&#21270;&#65288;HPTI&#65289;&#65292;&#36229;&#29699;&#38754;&#21407;&#22411;&#23398;&#20064;&#65288;HPL&#65289;&#21644;&#19968;&#33268;&#32858;&#21512;&#65288;CA&#65289;&#12290;&#39318;&#20808;&#65292;&#26381;&#21153;&#22120;&#20013;&#30340;HPTI&#26500;&#36896;&#22343;&#21248;&#20998;&#24067;&#19988;&#22266;&#23450;&#30340;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#19982;&#23458;&#25143;&#31471;&#20998;&#20139;&#20197;&#21305;&#37197;&#31867;&#21035;&#32479;&#35745;&#65292;&#36827;&#19968;&#27493;&#25351;&#23548;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#19968;&#33268;&#29305;&#24449;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;HPL&#22312;&#36229;&#29699;&#38754;&#27169;&#22411;&#31354;&#38388;&#20013;&#20197;&#20849;&#20139;&#30340;&#31867;&#21035;&#21407;&#22411;&#20026;&#30417;&#30563;&#65292;&#25429;&#33719;&#26412;&#22320;&#25968;&#25454;&#20013;&#30340;&#23618;&#32423;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#26381;&#21153;&#22120;&#20013;&#30340;CA&#36827;&#34892;&#19968;&#33268;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14382</link><description>&lt;p&gt;
&#24403;&#22810;&#20219;&#21153;&#23398;&#20064;&#36935;&#21040;&#37096;&#20998;&#30417;&#30563;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#36164;&#28304;&#21516;&#26102;&#35745;&#31639;&#22810;&#20010;&#36755;&#20986;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#26356;&#20302;&#12290;&#20197;&#24448;&#30340;MTL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19978;&#65292;&#22240;&#20026;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#20197;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#26631;&#31614;&#38656;&#27714;&#12290;&#26412;&#32508;&#36848;&#30528;&#37325;&#20110;MTL&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;MTL&#20256;&#32479;&#19978;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20854;&#27425;&#65292;&#23427;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising fro
&lt;/p&gt;</description></item><item><title>EdgeConvEns&#26159;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#32593;&#32476;&#19978;&#35757;&#32451;&#21644;&#38598;&#25104;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.14381</link><description>&lt;p&gt;
EdgeConvEns: &#22522;&#20110;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14381
&lt;/p&gt;
&lt;p&gt;
EdgeConvEns&#26159;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#32593;&#32476;&#19978;&#35757;&#32451;&#21644;&#38598;&#25104;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#36793;&#32536;&#26234;&#33021;&#26088;&#22312;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#36793;&#32536;&#32593;&#32476;&#19978;&#37096;&#32626;&#38656;&#35201;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#28145;&#24230;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26080;&#27861;&#23558;&#20854;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20132;&#25442;&#23398;&#21040;&#30340;&#26435;&#37325;&#26469;&#20197;&#38598;&#20307;&#26041;&#24335;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#36718;&#32593;&#32476;&#36890;&#20449;&#25165;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;EdgeConvEns&#65292;&#23427;&#21487;&#20197;&#22312;&#36793;&#32536;&#19978;&#35757;&#32451;&#24322;&#26500;&#30340;&#24369;&#27169;&#22411;&#65292;&#24182;&#23398;&#20250;&#23558;&#23427;&#20204;&#38598;&#25104;&#36215;&#26469;&#65292;&#22312;&#36793;&#32536;&#19978;&#25968;&#25454;&#20998;&#24067;&#21576;&#24322;&#36136;&#24615;&#12290;&#36793;&#32536;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#35745;&#31639;&#33021;&#21147;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#35774;&#22791;&#36827;&#34892;&#29420;&#31435;&#23454;&#29616;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#20013;&#38169;&#35823;&#25968;&#25454;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#31354;&#38388;&#30340;&#26032;&#22411;&#26631;&#27880;&#32479;&#19968;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14380</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#21644;&#22024;&#26434;&#30340;&#26631;&#27880;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#30340;&#26631;&#31614;&#40065;&#26834;&#20998;&#27966;
&lt;/p&gt;
&lt;p&gt;
Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations. (arXiv:2307.14380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#20013;&#38169;&#35823;&#25968;&#25454;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#31354;&#38388;&#30340;&#26032;&#22411;&#26631;&#27880;&#32479;&#19968;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20840;&#29699;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#29983;&#27963;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26631;&#31614;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26159;&#19981;&#21487;&#34892;&#30340;&#25110;&#32773;&#22826;&#26114;&#36149;&#20197;&#33267;&#20110;&#26080;&#27861;&#23454;&#38469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36890;&#24120;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#20165;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#22312;&#20174;&#19987;&#23478;&#22788;&#33719;&#24471;&#30340;&#26631;&#31614;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#36275;&#22815;&#26102;&#25165;&#21487;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#22312;&#20026;&#22686;&#21152;&#26631;&#31614;&#36136;&#37327;&#32780;&#22810;&#20010;&#27880;&#37322;&#20154;&#21592;&#27880;&#37322;&#20010;&#21035;&#26679;&#26412;&#19982;&#20026;&#22686;&#21152;&#26631;&#35760;&#23454;&#20363;&#30340;&#24635;&#25968;&#32780;&#27880;&#37322;&#26032;&#26679;&#26412;&#20043;&#38388;&#20316;&#20986;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#38169;&#35823;&#25968;&#25454;&#27880;&#37322;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#31354;&#38388;&#30340;&#26032;&#22411;&#26631;&#27880;&#32479;&#19968;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised classification algorithms are used to solve a growing number of real-life problems around the globe. Their performance is strictly connected with the quality of labels used in training. Unfortunately, acquiring good-quality annotations for many tasks is infeasible or too expensive to be done in practice. To tackle this challenge, active learning algorithms are commonly employed to select only the most relevant data for labeling. However, this is possible only when the quality and quantity of labels acquired from experts are sufficient. Unfortunately, in many applications, a trade-off between annotating individual samples by multiple annotators to increase label quality vs. annotating new samples to increase the total number of labeled instances is necessary. In this paper, we address the issue of faulty data annotations in the context of active learning. In particular, we propose two novel annotation unification algorithms that utilize unlabeled parts of the sample space. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;Bregman&#32858;&#31867;&#31639;&#27861;&#65288;DBGSA&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26222;&#36866;&#24341;&#21147;&#31639;&#27861;&#12289;&#24341;&#20837;Bregman&#20998;&#35010;&#24191;&#20041;&#24130;&#20449;&#24687;&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#21644;&#26500;&#24314;&#36229;&#21442;&#25968;&#35782;&#21035;&#20248;&#21270;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#22312;&#21021;&#22987;&#36136;&#24515;&#36873;&#25321;&#25935;&#24863;&#21644;&#22788;&#29702;&#38750;&#20984;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14375</link><description>&lt;p&gt;
DBGSA:&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;Bregman&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm. (arXiv:2307.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;Bregman&#32858;&#31867;&#31639;&#27861;&#65288;DBGSA&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26222;&#36866;&#24341;&#21147;&#31639;&#27861;&#12289;&#24341;&#20837;Bregman&#20998;&#35010;&#24191;&#20041;&#24130;&#20449;&#24687;&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#21644;&#26500;&#24314;&#36229;&#21442;&#25968;&#35782;&#21035;&#20248;&#21270;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#22312;&#21021;&#22987;&#36136;&#24515;&#36873;&#25321;&#25935;&#24863;&#21644;&#22788;&#29702;&#38750;&#20984;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20998;&#26512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;&#22914;K-means&#65289;&#23545;&#21021;&#22987;&#36136;&#24515;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#22312;&#38750;&#20984;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;Bregman&#20998;&#35010;&#21442;&#25968;&#20248;&#21270;&#32858;&#31867;&#31639;&#27861;&#65288;DBGSA&#65289;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26222;&#36866;&#24341;&#21147;&#31639;&#27861;&#65292;&#23558;&#30456;&#20284;&#28857;&#22312;&#25968;&#25454;&#38598;&#20013;&#38752;&#36817;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#29305;&#27530;&#23646;&#24615;&#30340;&#24341;&#21147;&#31995;&#25968;&#26041;&#31243;&#65292;&#38543;&#30528;&#36845;&#20195;&#30340;&#36827;&#34892;&#36880;&#28176;&#38477;&#20302;&#24433;&#21709;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bregman&#20998;&#35010;&#24191;&#20041;&#24130;&#20449;&#24687;&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#26469;&#35782;&#21035;&#32858;&#31867;&#20013;&#24515;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#21442;&#25968;&#35782;&#21035;&#20248;&#21270;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20013;&#30340;&#25163;&#21160;&#35843;&#25972;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#23545;&#22235;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Big data technology, data analysis has become increasingly important. Traditional clustering algorithms such as K-means are highly sensitive to the initial centroid selection and perform poorly on non-convex datasets. In this paper, we address these problems by proposing a data-driven Bregman divergence parameter optimization clustering algorithm (DBGSA), which combines the Universal Gravitational Algorithm to bring similar points closer in the dataset. We construct a gravitational coefficient equation with a special property that gradually reduces the influence factor as the iteration progresses. Furthermore, we introduce the Bregman divergence generalized power mean information loss minimization to identify cluster centers and build a hyperparameter identification optimization model, which effectively solves the problems of manual adjustment and uncertainty in the improved dataset. Extensive experiments are conducted on four simulated datasets and six real dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#24212;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26448;&#26009;&#35774;&#35745;&#65292;&#23545;&#27431;&#27954;&#22269;&#23478;&#21644;&#21360;&#24230;&#30340;&#19981;&#21516;&#34892;&#19994;CO2&#25490;&#25918;&#36827;&#34892;&#20102;&#20840;&#38754;&#39044;&#27979;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#30005;&#21147;&#12289;&#24037;&#19994;&#21644;&#38470;&#22320;&#20132;&#36890;&#37096;&#38376;&#26159;&#25968;&#25454;&#38598;&#20013;&#21464;&#24322;&#26368;&#22823;&#30340;&#36129;&#29486;&#32773;&#12290;</title><link>http://arxiv.org/abs/2307.14374</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25429;&#33719;&#21644;&#27963;&#21270;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#65306;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26448;&#26009;&#35774;&#35745;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design. (arXiv:2307.14374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#24212;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26448;&#26009;&#35774;&#35745;&#65292;&#23545;&#27431;&#27954;&#22269;&#23478;&#21644;&#21360;&#24230;&#30340;&#19981;&#21516;&#34892;&#19994;CO2&#25490;&#25918;&#36827;&#34892;&#20102;&#20840;&#38754;&#39044;&#27979;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#30005;&#21147;&#12289;&#24037;&#19994;&#21644;&#38470;&#22320;&#20132;&#36890;&#37096;&#38376;&#26159;&#25968;&#25454;&#38598;&#20013;&#21464;&#24322;&#26368;&#22823;&#30340;&#36129;&#29486;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20174;2019&#24180;1&#26376;&#33267;2023&#24180;2&#26376;&#30340;&#34892;&#19994;&#29305;&#23450;&#30340;&#27599;&#26085;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#25490;&#25918;&#30340;&#20840;&#38754;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#27431;&#27954;&#22269;&#23478;&#65288;EU27&#21644;&#33521;&#22269;&#12289;&#24847;&#22823;&#21033;&#12289;&#24503;&#22269;&#12289;&#35199;&#29677;&#29273;&#65289;&#21644;&#21360;&#24230;&#30340;&#30005;&#21147;&#12289;&#24037;&#19994;&#12289;&#38470;&#22320;&#20132;&#36890;&#12289;&#22269;&#20869;&#33322;&#31354;&#21644;&#22269;&#38469;&#33322;&#31354;&#37096;&#38376;&#65292;&#21033;&#29992;&#30899;&#30417;&#27979;&#30740;&#31350;&#35745;&#21010;&#30340;&#36817;&#23454;&#26102;&#27963;&#21160;&#25968;&#25454;&#12290;&#20026;&#20102;&#35782;&#21035;&#24120;&#35268;&#25490;&#25918;&#27169;&#24335;&#65292;&#30001;&#20110;COVID-19&#22823;&#27969;&#34892;&#30149;&#36896;&#25104;&#30340;&#30772;&#22351;&#24615;&#24433;&#21709;&#65292;&#35813;&#30740;&#31350;&#25490;&#38500;&#20102;2020&#24180;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#30740;&#31350;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30830;&#23450;&#20102;CO2&#25490;&#25918;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#30005;&#21147;&#12289;&#24037;&#19994;&#21644;&#38470;&#22320;&#20132;&#36890;&#37096;&#38376;&#22312;&#25968;&#25454;&#38598;&#20013;&#21344;&#25454;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#20351;&#29992;&#20102;7&#22825;&#31227;&#21160;&#24179;&#22343;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#31283;&#20581;&#30340;&#39044;&#27979;&#12290;&#35813;&#25968;&#25454;&#38598;&#25429;&#25417;&#21040;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#36235;&#21183;&#65292;&#24182;&#22686;&#24378;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO$_2$ emissions from January 2019 to February 2023. The research focuses on the Power, Industry, Ground Transport, Domestic Aviation, and International Aviation sectors in European countries (EU27 &amp; UK, Italy, Germany, Spain) and India, utilizing near-real-time activity data from the Carbon Monitor research initiative. To identify regular emission patterns, the data from the year 2020 is excluded due to the disruptive effects caused by the COVID-19 pandemic. The study then performs a principal component analysis (PCA) to determine the key contributors to CO$_2$ emissions. The analysis reveals that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset. A 7-day moving averaged dataset is employed for further analysis to facilitate robust predictions. This dataset captures both short-term and long-term trends and enhances the quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#38480;&#23485;&#24230;&#12289;&#26377;&#38480;&#25104;&#26412;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#26469;&#34920;&#31034;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#24230;&#37327;&#20174;&#21442;&#25968;&#31354;&#38388;&#26144;&#23556;&#21040;&#20989;&#25968;&#23450;&#20041;&#22495;&#20013;&#30340;&#36229;&#24179;&#38754;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.14373</link><description>&lt;p&gt;
&#21487;&#30001;&#23485;&#24230;&#26080;&#38480;&#12289;&#25104;&#26412;&#26377;&#38480;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks. (arXiv:2307.14373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#38480;&#23485;&#24230;&#12289;&#26377;&#38480;&#25104;&#26412;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#26469;&#34920;&#31034;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#24230;&#37327;&#20174;&#21442;&#25968;&#31354;&#38388;&#26144;&#23556;&#21040;&#20989;&#25968;&#23450;&#20041;&#22495;&#20013;&#30340;&#36229;&#24179;&#38754;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#20219;&#24847;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#24230;&#26080;&#38480;&#12289;&#25104;&#26412;&#26377;&#38480;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23545;&#36830;&#32493;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#20854;&#31215;&#20998;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#26377;&#38480;&#31526;&#21495;&#24230;&#37327;&#22312;&#36866;&#24403;&#21442;&#25968;&#31354;&#38388;&#19978;&#35782;&#21035;&#20986;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24230;&#37327;&#26144;&#23556;&#21040;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#24230;&#37327;&#65292;&#24182;&#23558;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#28857;&#21452;&#23556;&#21040;&#20989;&#25968;&#23450;&#20041;&#22495;&#20013;&#30340;&#36229;&#24179;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ongie&#31561;&#20154;&#30340;&#29468;&#24819;&#65292;&#21363;&#27599;&#20010;&#21487;&#20197;&#30001;&#36825;&#31181;&#23485;&#24230;&#26080;&#38480;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#20063;&#21487;&#20197;&#30001;&#26377;&#38480;&#23485;&#24230;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#25233;&#37057;&#29366;&#24577;&#65292;&#35813;&#27169;&#22411;&#22312;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#25233;&#37057;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#23398;&#29983;&#32676;&#20307;&#30340;&#24515;&#29702;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2307.14371</link><description>&lt;p&gt;
&#36890;&#36807;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#25233;&#37057;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Prediction of depression status in college students using a Naive Bayes classifier based machine learning model. (arXiv:2307.14371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14371
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#25233;&#37057;&#29366;&#24577;&#65292;&#35813;&#27169;&#22411;&#22312;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#25233;&#37057;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#23398;&#29983;&#32676;&#20307;&#30340;&#24515;&#29702;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#25233;&#37057;&#27700;&#24179;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;70%&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;30%&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;519&#21517;&#22823;&#23398;&#29983;&#30340;&#19982;&#25233;&#37057;&#26377;&#20851;&#30340;&#22240;&#32032;&#65292;&#32467;&#26524;&#26174;&#31034;&#20934;&#30830;&#29575;&#20026;78.03%&#65292;&#22312;&#26816;&#27979;&#25233;&#37057;&#30340;&#38451;&#24615;&#30149;&#20363;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20013;&#24230;&#21644;&#37325;&#24230;&#27700;&#24179;&#19978;&#65292;&#20197;&#21450;&#22312;&#27491;&#30830;&#20998;&#31867;&#38452;&#24615;&#30149;&#20363;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#29305;&#24322;&#24615;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35813;&#27169;&#22411;&#22312;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#25233;&#37057;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#30410;&#20110;&#24369;&#21183;&#32676;&#20307;&#65292;&#24182;&#26377;&#21161;&#20110;&#25913;&#21892;&#23398;&#29983;&#32676;&#20307;&#30340;&#24515;&#29702;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a machine learning model based on the Naive Bayes classifier for predicting the level of depression in university students, the objective was to improve prediction accuracy using a machine learning model involving 70% training data and 30% validation data based on the Naive Bayes classifier, the collected data includes factors associated with depression from 519 university students, the results showed an accuracy of 78.03%, high sensitivity in detecting positive cases of depression, especially at moderate and severe levels, and significant specificity in correctly classifying negative cases, these findings highlight the effectiveness of the model in early detection and treatment of depression, benefiting vulnerable sectors and contributing to the improvement of mental health in the student population.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#65292;&#36890;&#36807;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14366</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24046;&#24322;&#34917;&#20607;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20844;&#24179;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#65292;&#36890;&#36807;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#25490;&#21517;&#20989;&#25968;&#24448;&#24448;&#23545;&#19981;&#21516;&#20154;&#32676;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#22522;&#30784;&#25968;&#25454;&#23384;&#22312;&#20559;&#35265;&#12290;&#35299;&#20915;&#21644;&#34917;&#20607;&#36825;&#20123;&#19981;&#21516;&#30340;&#32467;&#26524;&#23545;&#20110;&#20844;&#24179;&#20915;&#31574;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#34917;&#20607;&#25514;&#26045;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#25490;&#21517;&#20989;&#25968;&#36827;&#34892;&#19981;&#36879;&#26126;&#30340;&#36716;&#25442;&#20197;&#28385;&#36275;&#20844;&#24179;&#20445;&#35777;&#65292;&#25110;&#32773;&#20351;&#29992;&#37197;&#39069;&#25110;&#30041;&#20301;&#26469;&#20445;&#35777;&#21521;&#20195;&#34920;&#20302;&#35843;&#32676;&#20307;&#30340;&#25104;&#21592;&#25552;&#20379;&#26368;&#23569;&#25968;&#37327;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26131;&#35299;&#37322;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#25514;&#26045;&#20381;&#36182;&#20110;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#30340;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#25490;&#21517;&#20989;&#25968;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#12290;&#22870;&#21169;&#31215;&#20998;&#21487;&#20197;&#20107;&#20808;&#35774;&#32622;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#32452;&#21512;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#20195;&#34920;&#20132;&#21449;&#65292;&#24182;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#25277;&#26679;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14364</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#65292;&#20197;&#22312;&#27010;&#29575;&#20998;&#24067;&#30340;&#27169;&#31946;&#38598;&#21512;&#20013;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#25104;&#26412;&#65292;&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#32593;&#32476;&#34892;&#20026;&#20998;&#26512;&#12289;&#39118;&#38505;&#31649;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DRO&#25216;&#26415;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#22788;&#29702;&#20998;&#24067;&#29615;&#22659;&#20013;&#30340;&#24322;&#27493;&#26356;&#26032;&#65307;2&#65289;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#65307;3&#65289;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#22330;&#26223;&#36866;&#24403;&#35843;&#25972;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE)&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20197;&#22788;&#29702;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (FDRO) &#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#21363;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#24182;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;NF-cMRI&#65292;&#21033;&#29992;&#31070;&#32463;&#22330;&#34920;&#31034;&#37325;&#24314;&#21152;&#36895;&#24515;&#33039;&#21160;&#24577;MRI&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#22312;&#36739;&#22823;&#30340;&#20302;&#37319;&#26679;&#29575;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#24182;&#25913;&#36827;&#26102;&#38388;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.14363</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#37325;&#24314;&#21152;&#36895;&#24515;&#33039;&#21160;&#24577;MRI&#30340;&#31070;&#32463;&#22330;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields. (arXiv:2307.14363v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;NF-cMRI&#65292;&#21033;&#29992;&#31070;&#32463;&#22330;&#34920;&#31034;&#37325;&#24314;&#21152;&#36895;&#24515;&#33039;&#21160;&#24577;MRI&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#22312;&#36739;&#22823;&#30340;&#20302;&#37319;&#26679;&#29575;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#24182;&#25913;&#36827;&#26102;&#38388;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#21160;&#24577;MRI&#26159;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#20854;&#26412;&#36523;&#36739;&#24930;&#30340;&#37319;&#38598;&#36807;&#31243;&#20351;&#24471;&#23545;&#21152;&#36895;&#37319;&#26679;&#30340;&#37325;&#24314;&#26041;&#27861;&#25104;&#20026;&#24517;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#26102;&#31354;&#20887;&#20313;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#37325;&#24314;&#21152;&#36895;&#37319;&#26679;&#30340;&#24515;&#33039;&#21160;&#24577;MRI&#12290;&#26368;&#36817;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#37319;&#38598;&#21644;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#24515;&#33039;&#21160;&#24577;MRI&#65288;&#31216;&#20026;NF-cMRI&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20307;&#20869;&#20302;&#37319;&#26679;&#35282;&#24230;&#37329;&#23646;&#32447;&#22280;&#22810;&#27425;&#33719;&#24471;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#25551;&#36848;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#25216;&#26415;&#30456;&#24403;&#25110;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction techn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23431;&#23449;&#23398;&#25512;&#26029;&#20013;&#24212;&#29992;&#21487;&#23398;&#20064;&#30340;&#25955;&#23556;&#21464;&#25442;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20004;&#28857;&#32479;&#35745;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#26356;&#22810;&#20449;&#24687;&#24182;&#24456;&#22909;&#22320;&#28040;&#38500;&#22825;&#20307;&#29289;&#29702;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#26679;&#26412;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#25955;&#23556;&#32593;&#32476;&#30340;&#24615;&#33021;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.14362</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#23567;&#27874;&#31070;&#32463;&#32593;&#32476;&#22312;&#23431;&#23449;&#23398;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learnable wavelet neural networks for cosmological inference. (arXiv:2307.14362v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23431;&#23449;&#23398;&#25512;&#26029;&#20013;&#24212;&#29992;&#21487;&#23398;&#20064;&#30340;&#25955;&#23556;&#21464;&#25442;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20004;&#28857;&#32479;&#35745;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#26356;&#22810;&#20449;&#24687;&#24182;&#24456;&#22909;&#22320;&#28040;&#38500;&#22825;&#20307;&#29289;&#29702;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#26679;&#26412;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#25955;&#23556;&#32593;&#32476;&#30340;&#24615;&#33021;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20174;&#23431;&#23449;&#23398;&#39046;&#22495;&#20013;&#25552;&#21462;&#27604;&#20256;&#32479;&#30340;&#20004;&#28857;&#32479;&#35745;&#26356;&#22810;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#38750;&#24120;&#22909;&#22320;&#28040;&#38500;&#22825;&#20307;&#29289;&#29702;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;CNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#26114;&#36149;&#30340;&#23431;&#23449;&#23398;&#27169;&#25311;&#39046;&#22495;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#24456;&#38590;&#35299;&#37322;&#32593;&#32476;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21487;&#23398;&#20064;&#30340;&#25955;&#23556;&#21464;&#25442;&#24212;&#29992;&#20110;&#23431;&#23449;&#23398;&#25512;&#26029;&#21644;&#22825;&#20307;&#29289;&#29702;&#25928;&#24212;&#36793;&#32536;&#21270;&#30340;&#38382;&#39064;&#20013;&#65292;&#25955;&#23556;&#21464;&#25442;&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#35757;&#32451;&#23567;&#27874;&#20316;&#20026;&#28388;&#27874;&#22120;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#25955;&#23556;&#21464;&#25442;&#30340;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#20026;&#20102;&#24615;&#33021;&#32780;&#26500;&#24314;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#20026;&#20102;&#35299;&#37322;&#24615;&#32780;&#26500;&#24314;&#30340;&#65292;&#24182;&#19982;CNN&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#25955;&#23556;&#32593;&#32476;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;CNN&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#23427;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#39640;&#26031;&#21387;&#32553;&#25628;&#32034;&#65288;GCS&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#39640;&#26031;&#20998;&#24067;&#20013;&#31890;&#23376;&#30340;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#24182;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#21644;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;GCS&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14359</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#65306;&#39640;&#26031;&#21387;&#32553;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
A new derivative-free optimization method: Gaussian Crunching Search. (arXiv:2307.14359v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#39640;&#26031;&#21387;&#32553;&#25628;&#32034;&#65288;GCS&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#39640;&#26031;&#20998;&#24067;&#20013;&#31890;&#23376;&#30340;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#24182;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#21644;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;GCS&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26041;&#27861;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#39640;&#26031;&#21387;&#32553;&#25628;&#32034;&#65288;GCS&#65289;&#12290;&#21463;&#21040;&#39640;&#26031;&#20998;&#24067;&#20013;&#31890;&#23376;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;GCS&#26088;&#22312;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#65292;&#24182;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#23545;GCS&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#24037;&#20316;&#26426;&#21046;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#20248;&#21270;&#26041;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;GCS&#30340;&#20248;&#21183;&#21644;&#24378;&#39033;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20026;&#23545;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#12289;&#23454;&#36341;&#32773;&#21644;&#23398;&#29983;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39640;&#26031;&#21387;&#32553;&#25628;&#32034;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#28508;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#32593;&#26684;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32039;&#20945;&#12289;&#35268;&#21017;&#30340;&#32593;&#26684;&#65292;&#22312;&#28857;&#20113;&#22788;&#29702;&#20013;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#36755;&#20837;&#21644;&#22823;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.14354</link><description>&lt;p&gt;
&#23398;&#20064;&#32593;&#26684;&#21270;&#20197;&#25552;&#39640;&#28857;&#20113;&#22788;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#32593;&#26684;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32039;&#20945;&#12289;&#35268;&#21017;&#30340;&#32593;&#26684;&#65292;&#22312;&#28857;&#20113;&#22788;&#29702;&#20013;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#36755;&#20837;&#21644;&#22823;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#32593;&#26684;&#25968;&#25454;&#30456;&#27604;&#65292;&#28857;&#20113;&#19978;&#20381;&#36182;&#20110;&#37051;&#22495;&#20449;&#24687;&#30340;&#31070;&#32463;&#25805;&#20316;&#20250;&#26356;&#21152;&#26114;&#36149;&#65292;&#22240;&#20026;&#28857;&#20113;&#20013;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32593;&#26684;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#20165;&#35745;&#31639;&#19968;&#27425;&#21367;&#31215;&#26680;&#65292;&#28982;&#21518;&#37325;&#29992;&#23427;&#26469;&#22788;&#29702;&#25152;&#26377;&#26597;&#35810;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#19982;&#32593;&#26684;&#25968;&#25454;&#30456;&#27604;&#65292;&#20381;&#36182;&#20110;&#37051;&#22495;&#20449;&#24687;&#30340;&#25805;&#20316;&#22312;&#28857;&#20113;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#26356;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#36755;&#20837;&#21644;&#22823;&#37051;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#26469;&#35299;&#20915;&#28857;&#20113;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#23398;&#20064;&#30340;&#32593;&#26684;&#21270;&#20316;&#20026;&#28857;&#20113;&#22788;&#29702;&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#65292;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32039;&#20945;&#12289;&#35268;&#21017;&#30340;&#32593;&#26684;&#12290;&#36890;&#36807;&#32593;&#26684;&#21270;&#65292;&#21518;&#32493;&#30340;&#23618;&#21487;&#20197;&#20351;&#29992;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#23450;&#20041;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;Conv3D&#65292;&#36825;&#27604;&#21407;&#29983;&#30340;&#28857;&#20113;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32593;&#26684;&#21270;&#25193;&#23637;&#21040;&#28857;&#20113;&#21040;&#28857;&#20113;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#21106;&#65292;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.  In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#31163;&#32447;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#26410;&#30693;&#20559;&#22909;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#36164;&#28304;&#35843;&#24230;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#24449;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;MEC&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#12290;</title><link>http://arxiv.org/abs/2307.14346</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#31163;&#32447;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#26410;&#30693;&#20559;&#22909;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#36164;&#28304;&#35843;&#24230;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#24449;&#26500;&#24314;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;MEC&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#23545;&#20110;&#19979;&#19968;&#20195;&#31227;&#21160;&#32593;&#32476;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#24212;&#29992;&#20248;&#20808;&#32771;&#34385;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#24310;&#36831;&#21644;&#33021;&#32791;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#35843;&#24230;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#65292;&#22240;&#20026;&#36825;&#20123;&#24212;&#29992;&#30340;&#20559;&#22909;&#65288;&#21363;&#19981;&#21516;&#30446;&#26631;&#30340;&#26435;&#37325;&#65289;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#38590;&#20197;&#20107;&#20808;&#25351;&#23450;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#22810;&#30446;&#26631;&#31163;&#32447;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38024;&#23545;MEC&#20013;&#30340;&#22810;&#20010;&#36793;&#32536;&#26469;&#26368;&#23567;&#21270;&#39044;&#26399;&#30340;&#38271;&#26399;&#33021;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#32771;&#34385;&#26410;&#30693;&#30340;&#20559;&#22909;&#20316;&#20026;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#20559;&#22909;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#22810;&#30446;&#26631;&#36164;&#28304;&#35843;&#24230;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;MEC&#31995;&#32479;&#20013;&#22810;&#20010;&#36793;&#32536;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile edge computing (MEC) is essential for next-generation mobile network applications that prioritize various performance metrics, including delays and energy consumption. However, conventional single-objective scheduling solutions cannot be directly applied to practical systems in which the preferences of these applications (i.e., the weights of different objectives) are often unknown or challenging to specify in advance. In this study, we address this issue by formulating a multi-objective offloading problem for MEC with multiple edges to minimize expected long-term energy consumption and transmission delay while considering unknown preferences as parameters. To address the challenge of unknown preferences, we design a multi-objective (deep) reinforcement learning (MORL)-based resource scheduling scheme with proximal policy optimization (PPO). In addition, we introduce a well-designed state encoding method for constructing features for multiple edges in MEC systems, a sophisticate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21076;&#38500;&#25197;&#26354;&#21644;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14343</link><description>&lt;p&gt;
MNIST&#25163;&#20889;&#25968;&#23383;&#20013;&#30340;&#25197;&#26354;&#22270;&#20687;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21076;&#38500;&#25197;&#26354;&#21644;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20070;&#20889;&#39118;&#26684;&#30340;&#22810;&#26679;&#24615;&#21644;&#22122;&#22768;&#22270;&#20687;&#30340;&#23384;&#22312;&#65292;&#35782;&#21035;&#25163;&#20889;&#25968;&#23383;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;MNIST&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#36825;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20855;&#26377;&#19981;&#35268;&#21017;&#24418;&#29366;&#12289;&#19981;&#23436;&#25972;&#31508;&#30011;&#21644;&#21464;&#24322;&#20542;&#26012;&#24230;&#30340;&#25197;&#26354;&#25968;&#23383;&#65292;&#21516;&#26102;&#23384;&#22312;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#23548;&#33268;&#20102;&#25968;&#23383;&#35782;&#21035;&#20934;&#30830;&#24230;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#30340;&#25197;&#26354;&#25968;&#23383;&#12290;&#36825;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#21644;&#36807;&#28388;&#20986;&#36825;&#20123;&#25197;&#26354;&#21644;&#27169;&#31946;&#30340;&#22270;&#20687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#34987;&#35782;&#21035;&#20986;&#30340;&#22270;&#20687;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25490;&#38500;&#65292;&#24182;&#20351;&#29992;&#36807;&#28388;&#21518;&#30340;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#26088;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#20943;&#36731;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing handwritten digits is a challenging task primarily due to the diversity of writing styles and the presence of noisy images. The widely used MNIST dataset, which is commonly employed as a benchmark for this task, includes distorted digits with irregular shapes, incomplete strokes, and varying skew in both the training and testing datasets. Consequently, these factors contribute to reduced accuracy in digit recognition. To overcome this challenge, we propose a two-stage deep learning approach. In the first stage, we create a simple neural network to identify distorted digits within the training set. This model serves to detect and filter out such distorted and ambiguous images. In the second stage, we exclude these identified images from the training dataset and proceed to retrain the model using the filtered dataset. This process aims to improve the classification accuracy and confidence levels while mitigating issues of underfitting and overfitting. Our experimental results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#65292;&#31454;&#20105;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.14066</link><description>&lt;p&gt;
&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#30340;&#25193;&#25955;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#65292;&#31454;&#20105;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#23588;&#20854;&#26159;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#65292;&#21463;&#21040;&#26631;&#35760;&#25104;&#26412;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#38656;&#35201;&#20855;&#26377;&#29305;&#23450;&#19987;&#19994;&#30693;&#35782;&#21644;&#32321;&#37325;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#26631;&#31614;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#21644;&#21518;&#32493;&#20219;&#21153;&#20043;&#38388;&#19981;&#38656;&#35201;&#26550;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#25552;&#35758;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;DDPM&#35757;&#32451;&#30446;&#26631;&#23545;Unet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20998;&#21106;&#20219;&#21153;&#19978;&#23545;&#24471;&#21040;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#29273;&#31185;&#25918;&#23556;&#29255;&#30340;&#20998;&#21106;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.13423</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#23545;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#28165;&#26224;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034; (SSSRs) &#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20316;&#20026;&#35821;&#38899;&#36136;&#37327; (SQ) &#39044;&#27979;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#27491;&#24120;&#25110;&#26377;&#21548;&#21147;&#38556;&#30861;&#30340;&#29992;&#25143;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#23558;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#23884;&#20837;&#21040;&#36825;&#26679;&#30340;&#34920;&#31034;&#20013;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38750;&#20405;&#20837;&#24335; SQ &#35780;&#32423;&#39044;&#27979;&#25216;&#26415;&#34987;&#25193;&#23637;&#21040;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#29992;&#25143;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#21457;&#29616;&#33258;&#23398;&#20064;&#34920;&#31034;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#24615;&#33021;&#31454;&#20105;&#21147;&#24378;&#20110;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#38024;&#23545; Clarity Prediction Challenge 1 &#21463;&#35797;&#32773;&#21644;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#65288;&#21548;&#21147;&#21463;&#25439;&#30340;&#65289;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2307.10705</link><description>&lt;p&gt;
TwinLiteNet&#65306;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#23545;&#20110;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#26469;&#35828;&#65292;&#21487;&#39537;&#21160;&#21306;&#22495;&#20998;&#21106;&#21644;&#36710;&#36947;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#38656;&#35201;&#39640;&#31471;&#30828;&#20214;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#32447;&#20998;&#21106;&#27169;&#22411;&#12290;TwinLiteNet&#35774;&#35745;&#25104;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;BDD100K&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TwinLiteNet&#65292;&#24182;&#19982;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TwinLiteNet&#19982;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26174;&#33879;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TwinLiteNet&#22312;&#21487;&#39537;&#21160;&#21306;&#22495;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;91.3%&#30340;mIoU&#35780;&#20998;&#65292;&#22312;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;31.08%&#30340;IoU&#35780;&#20998;&#65292;&#20165;&#20351;&#29992;&#20102;40&#19975;&#20010;&#21442;&#25968;&#65292;&#22312;GPU RTX&#19978;&#23454;&#29616;&#20102;415 FPS&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
&lt;/p&gt;</description></item><item><title>TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09916</link><description>&lt;p&gt;
TimeTuner: &#35786;&#26029;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#34920;&#31034;&#30340;&#23545;&#29031;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09916
&lt;/p&gt;
&lt;p&gt;
TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24448;&#24448;&#24402;&#22240;&#20110;&#26377;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20419;&#36827;&#20102;&#29305;&#24449;&#24037;&#31243;&#21644;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12289;&#35782;&#21035;&#21464;&#37327;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#20197;&#30830;&#20445;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#21363;TimeTuner&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#31890;&#24230;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#21253;&#25324;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#25216;&#26415;&#65306;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23545;&#29031;&#35299;&#37322;&#26469;&#24314;&#31435;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05946</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22810;&#23618;&#26550;&#26500;&#23545;&#22797;&#26434;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#24314;&#27169;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#19981;&#25552;&#20379;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#36816;&#33829;&#21644;&#25511;&#21046;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#24402;&#19968;&#21270;&#21040;&#20854;&#38544;&#34255;&#23618;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#36890;&#36807;&#25511;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#24182;&#20943;&#23569;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03811</link><description>&lt;p&gt;
&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#26144;&#23556;&#21040;&#22120;&#20214;&#24615;&#33021;&#30340;&#37197;&#26041;&#22270;
&lt;/p&gt;
&lt;p&gt;
Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#30005;&#27744;&#30005;&#35299;&#36136;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#25972;&#20010;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#21152;&#24555;&#26032;&#21270;&#21512;&#29289;&#30340;&#21457;&#29616;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22312;&#31215;&#26497;&#23547;&#27714;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#30340;&#32452;&#21512;&#26448;&#26009;&#65288;&#22914;&#37197;&#26041;&#65289;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#39046;&#22495;&#24863;&#30693;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#32452;&#21512;&#25104;&#37197;&#26041;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#23547;&#25214;&#30446;&#26631;&#24212;&#29992;&#30340;&#26032;&#21270;&#21512;&#29289;&#65292;&#20294;&#26159;&#22312;&#20174;&#31934;&#36873;&#21270;&#23398;&#31354;&#38388;&#20013;&#35782;&#21035;&#20986;&#21512;&#36866;&#30340;&#8220;&#37197;&#26041;&#8221;&#26041;&#38754;&#20173;&#28982;&#20027;&#35201;&#26159;&#23454;&#39564;&#39537;&#21160;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;Formulation Graph Convolution Network&#65288;F-GCN&#65289;&#65292;&#21487;&#20197;&#23558;&#21508;&#20010;&#32452;&#20998;&#30340;&#32467;&#26500;&#32452;&#25104;&#20851;&#31995;&#26144;&#23556;&#21040;&#28082;&#20307;&#37197;&#26041;&#30340;&#24615;&#36136;&#12290;&#22810;&#20010;GCN&#24182;&#34892;&#32452;&#35013;&#65292;&#24182;&#26681;&#25454;&#37197;&#26041;&#20013;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#30452;&#35266;&#22320;&#23545;&#37197;&#26041;&#25104;&#20998;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#28982;&#21518;&#26681;&#25454;&#30456;&#24212;&#32452;&#25104;&#37096;&#20998;&#30340;&#25705;&#23572;&#30334;&#20998;&#27604;&#23545;&#25152;&#24471;&#30340;&#20998;&#23376;&#25551;&#36848;&#31526;&#36827;&#34892;&#32553;&#25918;&#65292;&#25509;&#19979;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#34987;&#21024;&#38500;&#21644;&#25554;&#20837;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#35823;&#24046;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#20056;&#27861;&#35823;&#24046;&#20960;&#20046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.03430</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#34987;&#21024;&#38500;&#21644;&#25554;&#20837;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#35823;&#24046;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#20056;&#27861;&#35823;&#24046;&#20960;&#20046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;$\mathbb{R}^d$&#20013;&#36827;&#34892;&#38544;&#31169;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#21644;&#21024;&#38500;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;$\varepsilon$-&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110; $k$-means &#30446;&#26631;&#12290;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#22686;&#21152;&#30340;&#35823;&#24046;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968; $T$ &#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;&#20056;&#27861;&#35823;&#24046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#20960;&#20046;&#30456;&#21516;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#25345;&#32493;&#35266;&#23519;&#20013;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#24182;&#23558;&#20854;&#19982;&#29992;&#20110; $k$-means &#30340;&#24046;&#20998;&#38544;&#31169;&#36138;&#24515;&#36924;&#36817;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#37096;&#20998;&#22320;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040; $k$-median &#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of clustering privately a dataset in $\mathbb{R}^d$ that undergoes both insertion and deletion of points. Specifically, we give an $\varepsilon$-differentially private clustering mechanism for the $k$-means objective under continual observation. This is the first approximation algorithm for that problem with an additive error that depends only logarithmically in the number $T$ of updates. The multiplicative error is almost the same as non privately. To do so we show how to perform dimension reduction under continual observation and combine it with a differentially private greedy approximation algorithm for $k$-means. We also partially extend our results to the $k$-median problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#25919;&#27835;&#36777;&#35770;&#20013;&#30340;&#25991;&#26412;&#29255;&#27573;&#26159;&#21542;&#20540;&#24471;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#35780;&#20272;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.02377</link><description>&lt;p&gt;
Fraunhofer SIT&#22312;CheckThat! 2023&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#65306;&#20197;&#21487;&#26816;&#26597;&#24615;&#20998;&#31867;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#25919;&#27835;&#36777;&#35770;&#20013;&#30340;&#25991;&#26412;&#29255;&#27573;&#26159;&#21542;&#20540;&#24471;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#35780;&#20272;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#27573;&#25919;&#27835;&#36777;&#35770;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#23545;&#20854;&#36827;&#34892;&#26816;&#26597;&#20215;&#20540;&#35780;&#20272;&#12290;&#26816;&#27979;&#21487;&#26816;&#26597;&#22768;&#26126;&#26088;&#22312;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20107;&#23454;&#26816;&#26597;&#20154;&#21592;&#24212;&#39318;&#20808;&#32771;&#34385;&#30340;&#22768;&#26126;&#26469;&#31616;&#21270;&#25163;&#21160;&#20107;&#23454;&#26816;&#26597;&#24037;&#20316;&#12290;&#23427;&#36824;&#21487;&#20197;&#34987;&#35270;&#20026;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#30340;&#20027;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#21033;&#29992;&#20102;&#20197;&#27169;&#22411;&#28151;&#21512;&#20026;&#20013;&#24515;&#30340;&#38598;&#25104;&#20998;&#31867;&#26041;&#26696;&#12290;&#22312;&#24212;&#29992;&#20110;&#33521;&#35821;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20132;&#30340;&#27169;&#22411;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#20102;0.878&#30340;&#25972;&#20307;F1&#24471;&#20998;&#65292;&#24182;&#34987;&#35780;&#20026;&#31532;&#20108;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35299;&#21387;&#32553;&#25805;&#20316;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01524</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;&#30340;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35299;&#21387;&#32553;&#25805;&#20316;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20986;&#34892;&#30340;&#26041;&#24335;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#36710;&#36742;&#30446;&#21069;&#20381;&#36182;&#20110;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#26469;&#26816;&#27979;&#21644;&#36319;&#36394;&#21608;&#22260;&#30340;&#29289;&#20307;&#12290;&#20174;&#36710;&#36742;&#25910;&#38598;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#21457;&#36865;&#21040;&#20113;&#26381;&#21153;&#22120;&#20197;&#20415;&#20110;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#25345;&#32493;/&#32456;&#36523;&#23398;&#20064;&#12290;&#32771;&#34385;&#21040;&#24102;&#23485;&#38480;&#21046;&#65292;&#25968;&#25454;&#22312;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#34987;&#21387;&#32553;&#65292;&#36890;&#24120;&#20250;&#22312;&#35757;&#32451;&#21644;&#20998;&#26512;&#26102;&#36827;&#34892;&#35299;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#26469;&#20943;&#23569;&#26631;&#20934;&#27969;&#27700;&#32447;&#20013;&#35299;&#21387;&#32553;&#25805;&#20316;&#25152;&#20135;&#29983;&#30340;&#26102;&#24310;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;&#21387;&#32553;&#34920;&#31034;&#36824;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#65292;&#20197;&#33719;&#21462;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#65292;&#22312;&#20854;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#21387;&#32553;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression facto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#65292;&#25104;&#21151;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00610</link><description>&lt;p&gt;
Fraunhofer SIT&#22312;CheckThat! 2023&#20013;&#30340;&#36129;&#29486;&#65306;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#20197;&#20272;&#35745;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#65292;&#25104;&#21151;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#25991;&#20214;&#30340;&#36873;&#39033;&#20026;&#21306;&#20998;&#32593;&#32476;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#27599;&#31186;&#20998;&#20139;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26426;&#25110;&#20154;&#31867;&#19987;&#23478;&#23545;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#21487;&#36890;&#36807;&#21487;&#38752;&#24615;&#20998;&#26512;&#20316;&#20026;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#65292;&#20197;&#21450;&#20316;&#20026;&#25552;&#39640;&#25928;&#29575;&#30340;&#36807;&#28388;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#20010;&#22312;&#21333;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;OCR&#20998;&#26512;&#25552;&#21462;&#23884;&#20837;&#30340;&#25991;&#26412;&#34920;&#29616;&#26368;&#20339;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20004;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#36798;&#21040;&#20102;0.7297&#30340;F1&#20998;&#25968;&#65292;&#22312;&#31169;&#20154;&#27979;&#35797;&#38598;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;2D-Shapley&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#20540;&#30862;&#29255;&#21270;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35745;&#31639;&#20174;&#32858;&#21512;&#25968;&#25454;&#30697;&#38453;&#20013;&#31227;&#38500;&#19968;&#20010;&#30862;&#29255;&#30340;&#23545;&#31435;&#29616;&#23454;&#65292;&#23454;&#29616;&#20102;&#23545;&#30862;&#29255;&#21270;&#25968;&#25454;&#28304;&#30340;&#20272;&#20540;&#12290;&#32780;&#19988;&#65292;2D-Shapley&#28385;&#36275;&#30862;&#29255;&#21270;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;&#19968;&#20123;&#37325;&#35201;&#20844;&#29702;&#65292;&#20026;&#36873;&#25321;&#26377;&#29992;&#30340;&#25968;&#25454;&#29255;&#27573;&#12289;&#35299;&#37322;&#26679;&#26412;&#25968;&#25454;&#20540;&#21644;&#36827;&#34892;&#31934;&#32454;&#21270;&#30340;&#25968;&#25454;&#38382;&#39064;&#35786;&#26029;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.10473</link><description>&lt;p&gt;
2D-Shapley: &#19968;&#20010;&#38024;&#23545;&#30862;&#29255;&#21270;&#25968;&#25454;&#20272;&#20540;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
2D-Shapley: A Framework for Fragmented Data Valuation. (arXiv:2306.10473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;2D-Shapley&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#20540;&#30862;&#29255;&#21270;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35745;&#31639;&#20174;&#32858;&#21512;&#25968;&#25454;&#30697;&#38453;&#20013;&#31227;&#38500;&#19968;&#20010;&#30862;&#29255;&#30340;&#23545;&#31435;&#29616;&#23454;&#65292;&#23454;&#29616;&#20102;&#23545;&#30862;&#29255;&#21270;&#25968;&#25454;&#28304;&#30340;&#20272;&#20540;&#12290;&#32780;&#19988;&#65292;2D-Shapley&#28385;&#36275;&#30862;&#29255;&#21270;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;&#19968;&#20123;&#37325;&#35201;&#20844;&#29702;&#65292;&#20026;&#36873;&#25321;&#26377;&#29992;&#30340;&#25968;&#25454;&#29255;&#27573;&#12289;&#35299;&#37322;&#26679;&#26412;&#25968;&#25454;&#20540;&#21644;&#36827;&#34892;&#31934;&#32454;&#21270;&#30340;&#25968;&#25454;&#38382;&#39064;&#35786;&#26029;&#31561;&#25552;&#20379;&#20102;&#26032;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#8212;&#8212;&#37327;&#21270;&#21333;&#20010;&#25968;&#25454;&#28304;&#23545;&#27169;&#22411;&#26576;&#20123;&#39044;&#27979;&#34892;&#20026;&#30340;&#36129;&#29486;&#8212;&#8212;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#36879;&#26126;&#24230;&#21644;&#35774;&#35745;&#25968;&#25454;&#20849;&#20139;&#30340;&#28608;&#21169;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35780;&#20272;&#20855;&#26377;&#20849;&#20139;&#29305;&#24449;&#25110;&#26679;&#26412;&#31354;&#38388;&#30340;&#25968;&#25454;&#28304;&#12290;&#22914;&#20309;&#20272;&#20540;&#30862;&#29255;&#21270;&#25968;&#25454;&#28304;&#65292;&#20854;&#20013;&#27599;&#20010;&#21482;&#21253;&#21547;&#37096;&#20998;&#29305;&#24449;&#21644;&#26679;&#26412;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20174;&#32858;&#21512;&#25968;&#25454;&#30697;&#38453;&#20013;&#31227;&#38500;&#19968;&#20010;&#30862;&#29255;&#30340;&#23545;&#31435;&#29616;&#23454;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#23545;&#31435;&#29616;&#23454;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;2D-Shapley&#65292;&#19968;&#20010;&#22312;&#30862;&#29255;&#21270;&#25968;&#25454;&#20272;&#20540;&#29615;&#22659;&#20013;&#28385;&#36275;&#19968;&#20123;&#21560;&#24341;&#20154;&#30340;&#20844;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290; 2D-Shapley&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#26032;&#30340;&#29992;&#20363;&#65292;&#27604;&#22914;&#36873;&#25321;&#26377;&#29992;&#30340;&#25968;&#25454;&#29255;&#27573;&#65292;&#20026;&#26679;&#26412;&#25968;&#25454;&#20540;&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#31934;&#32454;&#21270;&#30340;&#25968;&#25454;&#38382;&#39064;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation -- quantifying the contribution of individual data sources to certain predictive behaviors of a model -- is of great importance to enhancing the transparency of machine learning and designing incentive systems for data sharing. Existing work has focused on evaluating data sources with the shared feature or sample space. How to valuate fragmented data sources of which each only contains partial features and samples remains an open question. We start by presenting a method to calculate the counterfactual of removing a fragment from the aggregated data matrix. Based on the counterfactual calculation, we further propose 2D-Shapley, a theoretical framework for fragmented data valuation that uniquely satisfies some appealing axioms in the fragmented data context. 2D-Shapley empowers a range of new use cases, such as selecting useful data fragments, providing interpretation for sample-wise data values, and fine-grained data issue diagnosis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#32676;&#21367;&#31215;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#30340;G-FNO&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05697</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#31561;&#21464;&#24615;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Group Equivariant Fourier Neural Operators for Partial Differential Equations. (arXiv:2306.05697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#32676;&#21367;&#31215;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#30340;G-FNO&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22312;&#39057;&#29575;&#22495;&#19979;&#25805;&#20316;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#30001;&#20110;&#29289;&#29702;&#23450;&#24459;&#19981;&#20381;&#36182;&#20110;&#29992;&#20110;&#25551;&#36848;&#23427;&#20204;&#30340;&#22352;&#26631;&#31995;, &#22240;&#27492;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#36827;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26356;&#23481;&#26131;&#30340;&#23398;&#20064;&#26159;&#26377;&#30410;&#30340;&#12290;&#23613;&#31649;&#20351;&#29992;&#32676;&#35770;&#32534;&#30721;&#29289;&#29702;&#22495;&#20869;&#30340;&#23545;&#31216;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22914;&#20309;&#22312;&#39057;&#29575;&#22495;&#20869;&#25429;&#25417;&#23545;&#31216;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#23558;&#32676;&#21367;&#31215;&#25193;&#23637;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#35774;&#35745;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#38236;&#20687;&#31561;&#21464;&#24615;&#36136;&#30340;&#20613;&#37324;&#21494;&#23618;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#31561;&#21464;&#24615;&#36136;&#12290;&#20135;&#29983;&#30340;G-FNO&#26550;&#26500;&#36328;&#36755;&#20837;&#20998;&#36776;&#29575;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#23545;&#31216;&#24615;&#27700;&#24179;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20316;&#20026;AIRS&#24211;&#30340;&#19968;&#37096;&#20998;&#20844;&#24320;&#22312;Github&#65288;https://github.com/divelab/AIRS&#65289;&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider solving partial differential equations (PDEs) with Fourier neural operators (FNOs), which operate in the frequency domain. Since the laws of physics do not depend on the coordinate system used to describe them, it is desirable to encode such symmetries in the neural operator architecture for better performance and easier learning. While encoding symmetries in the physical domain using group theory has been studied extensively, how to capture symmetries in the frequency domain is under-explored. In this work, we extend group convolutions to the frequency domain and design Fourier layers that are equivariant to rotations, translations, and reflections by leveraging the equivariance property of the Fourier transform. The resulting $G$-FNO architecture generalizes well across input resolutions and performs well in settings with varying levels of symmetry. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04169</link><description>&lt;p&gt;
&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#21450;&#20854;&#22312;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#26159;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#23558;&#36816;&#34892;&#26102;&#38388;&#20174; n^2k^2 &#20248;&#21270;&#21040;&#20102; n^2k&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two matrices $U, V \in \mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized, where $\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16573</link><description>&lt;p&gt;
&#25506;&#32034;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26435;&#37325;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#26032;&#25552;&#20986;&#30340;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#32531;&#35299;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#20998;&#24067;&#36890;&#24120;&#26159;&#25351;&#25968;&#20998;&#24067;&#65292;&#38500;&#38750;&#26377;&#24847;&#22320;&#35843;&#25972;&#26679;&#26412;&#25968;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33879;&#21517;&#30340;&#32463;&#20856;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#20294;&#24050;&#30693;&#20854;&#23545;&#29616;&#26377;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#23545;&#38271;&#23614;&#25968;&#25454;&#26377;&#25928;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#22278;&#38181;&#25928;&#24212;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#30001;&#26435;&#20540;&#34928;&#20943;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#24341;&#36215;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;Fisher&#21028;&#21035;&#27604;&#30340;&#22686;&#21152;&#20197;&#21450;&#30001;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#24179;&#34913;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#38544;&#24335;&#36923;&#36753;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26435;&#37325;&#24179;&#34913;&#26041;&#27861;&#25104;&#21151;&#32531;&#35299;&#20102;&#31070;&#32463;&#23849;&#28291;&#21644;&#22278;&#38181;&#25928;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13849</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#20272;&#35745;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#21644;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#26679;&#26412;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33391;&#22909;&#27491;&#21017;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#20102;&#37325;&#22823;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;&#12289;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#26550;&#26500;&#30340;&#25913;&#21160;&#35201;&#27714;&#26368;&#23567;&#12290;&#20026;&#20102;&#24471;&#21040;&#36866;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#26031;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31867;&#20869;&#34920;&#31034;&#20998;&#20026;&#22810;&#20010;&#39640;&#26031;&#12290;&#20855;&#26377;&#38750;&#39640;&#26031;&#34920;&#31034;&#30340;&#31867;&#21035;&#34987;&#33258;&#21160;&#35782;&#21035;&#24182;&#21160;&#24577;&#32858;&#31867;&#20026;&#22810;&#20010;&#22823;&#27010;&#29575;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#31867;&#21035;&#12290;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#39550;&#39542;&#34892;&#20026;&#20998;&#24067;&#23614;&#37096;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#21644;&#27969;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13106</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#21644;&#27969;&#37327;&#23398;&#20064;&#39550;&#39542;&#34892;&#20026;&#20998;&#24067;&#30340;&#23614;&#37096;&#20998;&#20301;&#25968;
&lt;/p&gt;
&lt;p&gt;
On Learning the Tail Quantiles of Driving Behavior Distributions via Quantile Regression and Flows. (arXiv:2305.13106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#39550;&#39542;&#34892;&#20026;&#20998;&#24067;&#23614;&#37096;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#21644;&#27969;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#65292;&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#27169;&#22411;&#20197;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#21644;&#23614;&#37096;&#20998;&#20301;&#25968;&#65292;&#19982;AD&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20174;&#29366;&#24577;&#39044;&#27979;&#39550;&#39542;&#21592;&#30340;&#36830;&#32493;&#21160;&#20316;&#65292;&#23545;&#20110;&#32553;&#23567;AD&#20195;&#29702;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#29305;&#21035;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36866;&#24212;&#20102;&#20004;&#20010;&#28789;&#27963;&#30340;&#20998;&#20301;&#25968;&#23398;&#20064;&#26694;&#26550;&#65306;&#65288;1&#65289;&#22522;&#20110;&#20542;&#26012;&#32477;&#23545;&#25439;&#22833;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#21644;&#65288;2&#65289;&#33258;&#22238;&#24402;&#20998;&#20301;&#25968;&#27969;&#65288;&#19968;&#31181;&#24402;&#19968;&#21270;&#27969;&#30340;&#29256;&#26412;&#65289;&#12290;&#35757;&#32451;&#37319;&#29992;&#34892;&#20026;&#20811;&#38534;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#22810;&#26465;&#39640;&#36895;&#20844;&#36335;&#19978;&#39550;&#39542;&#21592;&#36712;&#36857;&#30340;&#39640;D&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#19968;&#27493;&#21152;&#36895;&#24230;&#39044;&#27979;&#20219;&#21153;&#21644;&#22810;&#27493;&#39550;&#39542;&#21592;&#27169;&#25311;&#25512;&#29702;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;&#20542;&#26012;&#32477;&#23545;&#25439;&#22833;&#20316;&#20026;&#25351;&#26631;&#30340;&#23450;&#37327;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#20102;&#23450;&#24615;&#31034;&#20363;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards safe autonomous driving (AD), we consider the problem of learning models that accurately capture the diversity and tail quantiles of human driver behavior probability distributions, in interaction with an AD vehicle. Such models, which predict drivers' continuous actions from their states, are particularly relevant for closing the gap between AD agent simulations and reality. To this end, we adapt two flexible quantile learning frameworks for this setting that avoid strong distributional assumptions: (1) quantile regression (based on the titled absolute loss), and (2) autoregressive quantile flows (a version of normalizing flows). Training happens in a behavior cloning-fashion. We use the highD dataset consisting of driver trajectories on several highways. We evaluate our approach in a one-step acceleration prediction task, and in multi-step driver simulation rollouts. We report quantitative results using the tilted absolute loss as metric, give qualitative examples showing tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.08890</link><description>&lt;p&gt;
&#24046;&#20998;&#21367;&#31215;&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;FTSF&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#20856;&#22411;&#39044;&#27979;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;FTSF&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#19987;&#23478;&#31995;&#32479;&#65292;&#23548;&#33268;&#22833;&#21435;&#20102;&#35782;&#21035;&#26410;&#23450;&#20041;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;FTSF&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#27169;&#31946;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DFCNN&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#12290;DFCNN&#33021;&#22815;&#35782;&#21035;&#28508;&#22312;&#20449;&#24687;&#24182;&#25913;&#21892;&#39044;&#27979;&#31934;&#24230;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#23398;&#20064;&#33021;&#21147;&#65292;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#35268;&#21017;&#30340;&#38271;&#24230;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#36825;&#26159;&#19987;&#23478;&#31995;&#32479;&#25152;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#65292;FTSF&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#20250;&#23548;&#33268;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#38598;&#22833;&#25928;&#65292;&#24182;&#23548;&#33268;&#39044;&#27979;&#22833;&#36133;&#12290;DFCNN&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.13037</link><description>&lt;p&gt;
VeML&#65306;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13037
&lt;/p&gt;
&lt;p&gt;
VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21253;&#21547;&#35768;&#22810;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20934;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21040;&#27169;&#22411;&#35757;&#32451;&#65292;&#20877;&#21040;&#37096;&#32626;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#12290;&#24403;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#29983;&#21629;&#21608;&#26399;&#26102;&#65292;&#24517;&#39035;&#35774;&#35745;&#21644;&#25191;&#34892;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#36825;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#29983;&#21629;&#21608;&#26399;&#29256;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;VeML&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#27809;&#26377;&#35299;&#20915;&#30340;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#39640;&#25104;&#26412;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#35758;&#23558;&#22312;&#25105;&#20204;&#31995;&#32479;&#20013;&#31649;&#29702;&#30340;&#31867;&#20284;&#25968;&#25454;&#38598;&#30340;&#29983;&#21629;&#21608;&#26399;&#36716;&#31227;&#21040;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24046;&#24322;&#32780;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#26041;&#27861;&#26469;&#26816;&#27979;&#20808;&#21069;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#25143;&#21487;&#20197;&#33258;&#23450;&#20041;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#24037;&#20316;&#27969;&#65292;&#24182;&#23558;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#19982;&#20854;API&#36830;&#25509;&#36215;&#26469;&#65292;&#20316;&#20026;&#29992;&#25143;&#36816;&#34892;&#33258;&#23450;&#20041;&#20195;&#30721;&#30340;&#26725;&#26753;&#12290; VeML&#24050;&#24212;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#26469;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00570</link><description>&lt;p&gt;
FedFTN: &#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#26469;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#35745;&#25968;PET&#26159;&#38477;&#20302;&#36752;&#23556;&#26292;&#38706;&#21644;&#37319;&#38598;&#26102;&#38388;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#37325;&#24314;&#22270;&#20687;&#24448;&#24448;&#21463;&#21040;&#20449;&#22122;&#27604;&#20302;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#35786;&#26029;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20174;&#22810;&#20010;&#26426;&#26500;&#33719;&#21462;&#22823;&#22411;&#12289;&#38598;&#20013;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#26426;&#26500;&#30340;&#20302;&#35745;&#25968;PET&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32858;&#21512;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22810;&#26426;&#26500;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#20294;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#24212;&#29992;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#24182;&#19988;&#36824;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-count PET is an efficient way to reduce radiation exposure and acquisition time, but the reconstructed images often suffer from low signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream tasks. Recent advances in deep learning have shown great potential in improving low-count PET image quality, but acquiring a large, centralized, and diverse dataset from multiple institutions for training a robust model is difficult due to privacy and security concerns of patient data. Moreover, low-count PET data at different institutions may have different data distribution, thus requiring personalized models. While previous federated learning (FL) algorithms enable multi-institution collaborative training without the need of aggregating local data, addressing the large domain shift in the application of multi-institutional low-count PET denoising remains a challenge and is still highly under-explored. In this work, we propose FedFTN, a personalized federated learning strategy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2303.10135</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#25928;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#65288;RASP&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#20195;&#21046;&#36896;&#19994;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#24212;&#21147;&#65292;&#38543;&#30528;&#23545;&#26356;&#22823;&#37327;&#21270;&#29983;&#20135;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#31181;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20174;&#19981;&#26029;&#22686;&#21152;&#30340;&#28508;&#22312;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#36827;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#35013;&#37197;&#36824;&#38656;&#35201;&#25104;&#26412;&#26114;&#36149;&#30340;&#21487;&#34892;&#24615;&#26816;&#26597;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#20135;&#21697;&#35013;&#37197;&#22270;&#30340;&#22270;&#24418;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;GRACE&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;GRACE&#20174;&#22270;&#24418;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#36880;&#27493;&#39044;&#27979;&#35013;&#37197;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#39044;&#27979;&#38109;&#22411;&#26448;&#20135;&#21697;&#21464;&#20307;&#30340;&#21487;&#34892;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#12289;&#39640;&#25928;&#25311;&#21512;&#27010;&#29575;&#27874;-&#20271;&#21162;&#21033;&#28508;&#22312;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#35889;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36716;&#25442;&#26679;&#26412;&#30697;&#30340;&#26041;&#24335;&#23558;&#20256;&#32479;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20271;&#21162;&#21033;&#35774;&#32622;&#20013;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#22266;&#23450;&#25104;&#26412;&#20272;&#35745;&#22120;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35889;&#20272;&#35745;&#21487;&#20197;&#20026;Laplace-EM&#25311;&#21512;&#25552;&#20379;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.02060</link><description>&lt;p&gt;
Bernoulli&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#35889;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral learning of Bernoulli linear dynamical systems models. (arXiv:2303.02060v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#12289;&#39640;&#25928;&#25311;&#21512;&#27010;&#29575;&#27874;-&#20271;&#21162;&#21033;&#28508;&#22312;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#35889;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36716;&#25442;&#26679;&#26412;&#30697;&#30340;&#26041;&#24335;&#23558;&#20256;&#32479;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20271;&#21162;&#21033;&#35774;&#32622;&#20013;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#22266;&#23450;&#25104;&#26412;&#20272;&#35745;&#22120;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35889;&#20272;&#35745;&#21487;&#20197;&#20026;Laplace-EM&#25311;&#21512;&#25552;&#20379;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;Bernoulli&#35266;&#27979;&#30340;&#28508;&#22312;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20026;&#35782;&#21035;&#20108;&#36827;&#21046;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#21160;&#24577;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#20108;&#36827;&#21046;&#20915;&#31574;&#21644;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#65288;&#20363;&#22914;&#31163;&#25955;&#31070;&#32463;&#23574;&#23792;&#35757;&#32451;&#65289;&#31561;&#21508;&#31181;&#24773;&#20917;&#19979;&#20135;&#29983;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#27010;&#29575;&#27874;/Bernoulli&#28508;&#22312;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#27169;&#22411;&#30340;&#35889;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#31532;&#19968;&#21644;&#31532;&#20108;&#20010;&#26679;&#26412;&#30697;&#30340;&#36716;&#25442;&#23558;&#20256;&#32479;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;&#26041;&#27861;&#25193;&#23637;&#21040;Bernoulli&#35774;&#32622;&#20013;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#22266;&#23450;&#25104;&#26412;&#20272;&#35745;&#22120;&#65292;&#36991;&#20813;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#21361;&#38505;&#20197;&#21450;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#31561;&#36845;&#20195;&#25311;&#21512;&#36807;&#31243;&#30340;&#38271;&#26102;&#38388;&#35745;&#31639;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#25110;&#25968;&#25454;&#30340;&#32479;&#35745;&#32467;&#26500;&#19981;&#28385;&#36275;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35889;&#20272;&#35745;&#20026;Laplace-EM&#25311;&#21512;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent linear dynamical systems with Bernoulli observations provide a powerful modeling framework for identifying the temporal dynamics underlying binary time series data, which arise in a variety of contexts such as binary decision-making and discrete stochastic processes (e.g., binned neural spike trains). Here we develop a spectral learning method for fast, efficient fitting of probit-Bernoulli latent linear dynamical system (LDS) models. Our approach extends traditional subspace identification methods to the Bernoulli setting via a transformation of the first and second sample moments. This results in a robust, fixed-cost estimator that avoids the hazards of local optima and the long computation time of iterative fitting procedures like the expectation-maximization (EM) algorithm. In regimes where data is limited or assumptions about the statistical structure of the data are not met, we demonstrate that the spectral estimate provides a good initialization for Laplace-EM fitting. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24120;&#35265;&#29702;&#30001;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;&#36890;&#36807;&#35782;&#21035;&#24120;&#35265;&#30340;&#21306;&#20998;&#24615;&#32447;&#32034;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#20811;&#26381;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01669</link><description>&lt;p&gt;
&#23398;&#20064;&#24120;&#35265;&#29702;&#30001;&#20197;&#25913;&#36827;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems. (arXiv:2303.01669v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24120;&#35265;&#29702;&#30001;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;&#36890;&#36807;&#35782;&#21035;&#24120;&#35265;&#30340;&#21306;&#20998;&#24615;&#32447;&#32034;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#20811;&#26381;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31574;&#30053;&#22312;&#21508;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21644;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#23398;&#20064;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#65288;FGVR&#65289;&#30340;&#34920;&#31034;&#26102;&#21487;&#33021;&#19981;&#22826;&#26377;&#25928;&#65292;&#22240;&#20026;&#35768;&#22810;&#26377;&#21161;&#20110;&#20248;&#21270;SSL&#30446;&#26631;&#30340;&#29305;&#24449;&#19981;&#36866;&#21512;&#25551;&#36848;&#32454;&#31890;&#24230;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#19968;&#20010;&#39069;&#22806;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;&#20197;&#35782;&#21035;&#36328;&#23454;&#20363;&#21644;&#31867;&#21035;&#20013;&#24120;&#35265;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#32447;&#32034;&#65292;&#26412;&#25991;&#31216;&#20043;&#20026;&#24120;&#35265;&#29702;&#30001;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#24120;&#35265;&#29702;&#30001;&#24448;&#24448;&#23545;&#24212;&#20110;&#21069;&#26223;&#29289;&#20307;&#30340;&#20851;&#38190;&#37096;&#20301;&#30340;&#21306;&#20998;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24120;&#35265;&#29702;&#30001;&#26816;&#27979;&#22120;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#21033;&#29992;&#20174;SSL&#30446;&#26631;&#23548;&#20986;&#30340;GradCAM&#26469;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#29289;&#20307;&#37096;&#20301;&#25110;&#26174;&#33879;&#24615;&#26816;&#27979;&#22120;&#65292;&#20174;&#32780;&#20351;&#20854;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SSL&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) strategies have demonstrated remarkable performance in various recognition tasks. However, both our preliminary investigation and recent studies suggest that they may be less effective in learning representations for fine-grained visual recognition (FGVR) since many features helpful for optimizing SSL objectives are not suitable for characterizing the subtle differences in FGVR. To overcome this issue, we propose learning an additional screening mechanism to identify discriminative clues commonly seen across instances and classes, dubbed as common rationales in this paper. Intuitively, common rationales tend to correspond to the discriminative patterns from the key parts of foreground objects. We show that a common rationale detector can be learned by simply exploiting the GradCAM induced from the SSL objective without using any pre-trained object parts or saliency detectors, making it seamlessly to be integrated with the existing SSL process. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08720</link><description>&lt;p&gt;
&#36817;&#22320;&#34920;&#39118;&#30340;&#31639;&#27861;&#24187;&#35273;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#25105;&#20204;&#30340;GANs&#26159;&#36890;&#36807;&#23545;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#36755;&#20837;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#26469;&#29983;&#25104;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306; Weather Research and Forecasting&#65288;WRF&#65289;&#27169;&#22411;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22320;&#34920;&#39118;&#12290;&#19982;&#20256;&#32479;&#30340;SR&#27169;&#22411;&#19981;&#21516;&#65292;LR&#36755;&#20837;&#22312;WRF&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#38750;&#29702;&#24819;&#21270;&#30340;LR&#21644;HR&#37197;&#23545;&#65292;&#23548;&#33268;&#30001;&#20110;&#20869;&#37096;&#21464;&#24322;&#24341;&#36215;&#30340;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#24403;&#21069;&#22522;&#20110;SR&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#24182;&#23581;&#35797;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26032;&#39062;&#39057;&#29575;&#20998;&#31163;&#65288;FS&#65289;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;SR&#27169;&#22411;&#30340;&#25216;&#33021;&#65292;&#25105;&#20204;&#31934;&#36873;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20851;&#27880;&#22522;&#20110;&#31354;&#38388;&#21151;&#29575;&#35889;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;GAN&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
&lt;/p&gt;</description></item><item><title>&#22240;&#23376;&#22330;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoBaFa&#22312;&#36924;&#36817;&#36136;&#37327;&#12289;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01226</link><description>&lt;p&gt;
&#22240;&#23376;&#22330;&#65306;&#31070;&#32463;&#22330;&#21644;&#26356;&#22810;&#39046;&#22495;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01226
&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#22330;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoBaFa&#22312;&#36924;&#36817;&#36136;&#37327;&#12289;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#23376;&#22330;&#65292;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#21644;&#34920;&#31034;&#20449;&#21495;&#30340;&#26032;&#26694;&#26550;&#12290;&#22240;&#23376;&#22330;&#23558;&#20449;&#21495;&#20998;&#35299;&#20026;&#22240;&#23376;&#30340;&#20056;&#31215;&#65292;&#27599;&#20010;&#22240;&#23376;&#30001;&#31070;&#32463;&#22330;&#25110;&#24120;&#35268;&#22330;&#34920;&#31034;&#65292;&#24182;&#23545;&#36755;&#20837;&#20449;&#21495;&#36827;&#34892;&#22352;&#26631;&#21464;&#25442;&#25805;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#35299;&#20135;&#29983;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#65292;&#21253;&#25324;NeRF&#65292;PlenOxels&#65292;EG3D&#65292;Instant-NGP&#21644;TensoRF&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26694;&#26550;&#36824;&#21487;&#20197;&#21019;&#24314;&#24378;&#22823;&#30340;&#26032;&#20449;&#21495;&#34920;&#31034;&#65292;&#20363;&#22914;&#26412;&#25991;&#25552;&#20986;&#30340;&#31995;&#25968;&#22522;&#20989;&#25968;&#20998;&#35299;&#65288;CoBaFa&#65289;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;CoBaFa&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24555;&#36895;&#37325;&#24314;&#26041;&#27861;&#22312;&#31070;&#32463;&#20449;&#21495;&#34920;&#31034;&#30340;&#19977;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#36924;&#36817;&#36136;&#37327;&#65292;&#32039;&#20945;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#22312;&#20108;&#32500;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#22270;&#20687;&#36924;&#36817;&#36136;&#37327;&#65292;&#21516;&#26102;&#22312;&#20960;&#20309;&#36136;&#37327;&#26041;&#38754;&#20063;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2302.01198</link><description>&lt;p&gt;
&#22240;&#26524;&#25552;&#21319;&#19982;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#22240;&#26524;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#19968;&#32452;&#22266;&#26377;&#30340;&#33410;&#28857;&#22240;&#23376;&#65292;&#21363;&#22312;&#33410;&#28857;&#20986;&#29983;&#26102;&#23601;&#23450;&#20041;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#23427;&#20204;&#25511;&#21046;&#30528;&#22270;&#20013;&#38142;&#36335;&#30340;&#22240;&#26524;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#22240;&#26524;&#20219;&#21153;&#20013;&#65292;&#38142;&#36335;&#24418;&#25104;&#26159;&#36335;&#24452;&#20381;&#36182;&#30340;&#65306;&#38142;&#36335;&#24178;&#39044;&#30340;&#32467;&#26524;&#21462;&#20915;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#22240;&#26524;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36335;&#24452;&#20381;&#36182;&#30340;&#38142;&#36335;&#24418;&#25104;&#65292;&#22240;&#20026;&#38142;&#36335;&#20043;&#38388;&#30340;&#32423;&#32852;&#21151;&#33021;&#20381;&#36182;&#65288;&#30001;&#36335;&#24452;&#20381;&#36182;&#24615;&#20135;&#29983;&#65289;&#35201;&#20040;&#26080;&#27861;&#35782;&#21035;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#19981;&#20999;&#23454;&#38469;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#20013;&#36335;&#24452;&#20381;&#36182;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#25552;&#21319;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#22270;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#26377;&#38480;&#30340;&#24178;&#39044;&#25968;&#25454;&#26469;&#35782;&#21035;&#22240;&#26524;&#38142;&#36335;&#39044;&#27979;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26500;&#23545;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#23884;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing causal models for link prediction assume an underlying set of inherent node factors -- an innate characteristic defined at the node's birth -- that governs the causal evolution of links in the graph. In some causal tasks, however, link formation is path-dependent: The outcome of link interventions depends on existing links. Unfortunately, these existing causal methods are not designed for path-dependent link formation, as the cascading functional dependencies between links (arising from path dependence) are either unidentifiable or require an impractical number of control variables. To overcome this, we develop the first causal model capable of dealing with path dependencies in link prediction. In this work we introduce the concept of causal lifting, an invariance in causal models of independent interest that, on graphs, allows the identification of causal link prediction queries using limited interventional data. Further, we show how structural pairwise embeddings exhibit low
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01913</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#23398;&#20064;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290;&#27714;&#35299;&#22120;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#26159;&#20998;&#25903;&#21551;&#21457;&#24335;&#65292;&#23427;&#20204;&#26088;&#22312;&#22312;&#26368;&#30701;&#30340;&#26102;&#38388;&#20869;&#23547;&#25214;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#20123;&#21551;&#21457;&#24335;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#24182;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#35768;&#22810;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#39640;&#25928;&#21551;&#21457;&#24335;&#30340;&#21162;&#21147;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#36890;&#29992;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#21364;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#24471;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#24471;&#30410;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13381</link><description>&lt;p&gt;
MixupE&#65306;&#20174;&#26041;&#21521;&#23548;&#25968;&#35282;&#24230;&#29702;&#35299;&#21644;&#25913;&#36827;Mixup&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#36755;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#29983;&#25104;&#39069;&#22806;&#30340;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#24050;&#34987;&#35777;&#23454;&#22312;&#35768;&#22810;&#23398;&#20064;&#33539;&#24335;&#21644;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;Mixup&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;&#29256;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#21644;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;Mixup&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#22312;&#20351;&#29992;&#21508;&#31181;&#26550;&#26500;&#26102;&#37117;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;ImageNet&#30340;top-1&#31934;&#24230;&#19978;&#27604;Mixup&#25552;&#39640;&#20102;0.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#21183;&#30340;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;MCMC&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;UQ&#65292;&#21487;&#20934;&#30830;&#20272;&#35745;MD&#21487;&#35266;&#27979;&#37327;&#65292;&#35201;&#27714;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22810;&#20010;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;</title><link>http://arxiv.org/abs/2212.07959</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21183;&#30340;&#30740;&#31350;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian Uncertainty Quantification for Neural Network Potentials: Promise and Pitfalls. (arXiv:2212.07959v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07959
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21183;&#30340;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;MCMC&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;UQ&#65292;&#21487;&#20934;&#30830;&#20272;&#35745;MD&#21487;&#35266;&#27979;&#37327;&#65292;&#35201;&#27714;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22810;&#20010;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21183;&#26041;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21487;&#36798;&#21040;&#32463;&#20856;&#20998;&#23376;&#21160;&#21147;&#23398;&#21147;&#22330;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#26102;&#65292;NN&#21183;&#30340;&#39044;&#27979;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#30340;&#38656;&#27714;&#12290;&#36125;&#21494;&#26031;&#24314;&#27169;&#25552;&#20379;&#20102;UQ&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20294;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#30340;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;NN&#21183;&#20013;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#31895;&#31890;&#21270;&#30340;&#28082;&#24577;&#27700;&#21644;&#19993;&#27688;&#37240;&#20108;&#32957;&#31995;&#32479;&#35757;&#32451;&#22270;NN&#21183;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;MCMC&#65288;SG-MCMC&#65289;&#23454;&#29616;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;UQ&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;MD&#21487;&#35266;&#27979;&#37327;&#12290;&#25105;&#20204;&#34920;&#26126;&#20919;&#21518;&#39564;&#20998;&#24067;&#21487;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#32780;&#20026;&#20102;&#21487;&#38752;&#30340;UQ&#65292;&#38656;&#35201;&#22810;&#20010;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SG-MCMC&#21644;&#28145;&#24230;&#38598;&#21512;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#23613;&#31649;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#19988;&#27809;&#26377;&#36807;&#24230;&#23459;&#20256;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) potentials promise highly accurate molecular dynamics (MD) simulations within the computational complexity of classical MD force fields. However, when applied outside their training domain, NN potential predictions can be inaccurate, increasing the need for Uncertainty Quantification (UQ). Bayesian modeling provides the mathematical framework for UQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are computationally intractable for NN potentials. By training graph NN potentials for coarse-grained systems of liquid water and alanine dipeptide, we demonstrate here that scalable Bayesian UQ via stochastic gradient MCMC (SG-MCMC) yields reliable uncertainty estimates for MD observables. We show that cold posteriors can reduce the required training data size and that for reliable UQ, multiple Markov chains are needed. Additionally, we find that SG-MCMC and the Deep Ensemble method achieve comparable results, despite shorter training and less hype
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28151;&#21512;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.01555</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#28151;&#21512;&#30340;&#26102;&#24207;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contrastive Domain Adaptation for Time-Series via Temporal Mixup. (arXiv:2212.01555v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28151;&#21512;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22495;&#20559;&#31227;&#38382;&#39064;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26377;&#26631;&#31614;&#30340;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#26377;&#36716;&#31227;&#30340;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23613;&#31649;UDA&#22312;&#35270;&#35273;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#22312;&#26102;&#24207;&#24212;&#29992;&#20013;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;CoTMix&#65292;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20165;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#20943;&#36731;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#28151;&#21512;&#31574;&#30053;&#65292;&#20026;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20004;&#20010;&#20013;&#38388;&#22686;&#24378;&#35270;&#22270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#27599;&#20010;&#39046;&#22495;&#19982;&#20854;&#30456;&#24212;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#29983;&#25104;&#30340;&#35270;&#22270;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#26102;&#24207;&#25968;&#25454;&#30340;&#26102;&#38388;&#21160;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation (UDA) has emerged as a powerful solution for the domain shift problem via transferring the knowledge from a labeled source domain to a shifted unlabeled target domain. Despite the prevalence of UDA for visual applications, it remains relatively less explored for time-series applications. In this work, we propose a novel lightweight contrastive domain adaptation framework called CoTMix for time-series data. Unlike existing approaches that either use statistical distances or adversarial techniques, we leverage contrastive learning solely to mitigate the distribution shift across the different domains. Specifically, we propose a novel temporal mixup strategy to generate two intermediate augmented views for the source and target domains. Subsequently, we leverage contrastive learning to maximize the similarity between each domain and its corresponding augmented view. The generated views consider the temporal dynamics of time-series data during the adaptation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#24674;&#22797;&#31639;&#27861;&#30456;&#23545;&#20110;&#30830;&#23450;&#24615;&#24674;&#22797;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#21487;&#20197;&#36798;&#21040;&#23436;&#32654;&#24863;&#30693;&#36136;&#37327;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#25552;&#39640;&#36755;&#20986;&#21487;&#21464;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#23558;&#36827;&#19968;&#27493;&#25903;&#25345;&#38543;&#26426;&#20272;&#35745;&#22120;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.08944</link><description>&lt;p&gt;
&#38543;&#26426;&#20272;&#35745;&#22120;&#20248;&#20110;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#30340;&#21407;&#22240;&#65306;&#40065;&#26834;&#24615;&#65292;&#19968;&#33268;&#24615;&#21644;&#24863;&#30693;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality. (arXiv:2211.08944v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#24674;&#22797;&#31639;&#27861;&#30456;&#23545;&#20110;&#30830;&#23450;&#24615;&#24674;&#22797;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#21487;&#20197;&#36798;&#21040;&#23436;&#32654;&#24863;&#30693;&#36136;&#37327;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#25552;&#39640;&#36755;&#20986;&#21487;&#21464;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#23558;&#36827;&#19968;&#27493;&#25903;&#25345;&#38543;&#26426;&#20272;&#35745;&#22120;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24674;&#22797;&#31639;&#27861;&#20801;&#35768;&#25506;&#32034;&#19982;&#36864;&#21270;&#36755;&#20837;&#23545;&#24212;&#30340;&#35299;&#31354;&#38388;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26041;&#27861;&#30456;&#23545;&#20110;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#39069;&#22806;&#22522;&#26412;&#20248;&#21183;&#65292;&#24182;&#36827;&#19968;&#27493;&#28608;&#21457;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#21160;&#26426;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#33021;&#36798;&#21040;&#23436;&#32654;&#24863;&#30693;&#36136;&#37327;&#19988;&#20854;&#36755;&#20986;&#19982;&#36755;&#20837;&#19968;&#33268;&#30340;&#24674;&#22797;&#31639;&#27861;&#24517;&#39035;&#26159;&#19968;&#20010;&#21518;&#39564;&#37319;&#26679;&#22120;&#65292;&#22240;&#27492;&#24517;&#39035;&#26159;&#38543;&#26426;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#30830;&#23450;&#24615;&#24674;&#22797;&#31639;&#27861;&#21487;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20294;&#21482;&#33021;&#36890;&#36807;&#20351;&#29992;&#26497;&#20854;&#25935;&#24863;&#30340;&#26144;&#23556;&#22635;&#20805;&#25152;&#26377;&#21487;&#33021;&#30340;&#28304;&#22270;&#20687;&#31354;&#38388;&#26469;&#23454;&#29616;&#65292;&#36825;&#20351;&#23427;&#20204;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#35201;&#27714;&#30830;&#23450;&#24615;&#27169;&#22411;&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#20250;&#20005;&#37325;&#38459;&#30861;&#20854;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#20351;&#38543;&#26426;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;&#20854;&#24863;&#30693;&#36136;&#37327;&#65292;&#24182;&#25913;&#21892;&#20854;&#36755;&#20986;&#30340;&#21487;&#21464;&#24615;&#12290;&#36825;&#20010;&#21457;&#29616;&#23558;&#36827;&#19968;&#27493;&#25903;&#25345;&#38543;&#26426;&#20272;&#35745;&#22120;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic restoration algorithms allow to explore the space of solutions that correspond to the degraded input. In this paper we reveal additional fundamental advantages of stochastic methods over deterministic ones, which further motivate their use. First, we prove that any restoration algorithm that attains perfect perceptual quality and whose outputs are consistent with the input must be a posterior sampler, and is thus required to be stochastic. Second, we illustrate that while deterministic restoration algorithms may attain high perceptual quality, this can be achieved only by filling up the space of all possible source images using an extremely sensitive mapping, which makes them highly vulnerable to adversarial attacks. Indeed, we show that enforcing deterministic models to be robust to such attacks profoundly hinders their perceptual quality, while robustifying stochastic models hardly influences their perceptual quality, and improves their output variability. These findings p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20854;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2211.08411</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20854;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20013;&#21253;&#21547;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20174;&#21382;&#21490;&#20154;&#29289;&#30340;&#29983;&#26085;&#21040;&#32534;&#31243;&#25945;&#31243;&#31561;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#21487;&#20197;&#30001;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26576;&#20123;&#20449;&#24687;&#22312;&#32593;&#19978;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#20182;&#20449;&#24687;&#30340;&#20986;&#29616;&#38750;&#24120;&#32597;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#20174;&#32593;&#32476;&#25235;&#21462;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#35745;&#31639;&#21253;&#21547;&#19982;&#32473;&#23450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30456;&#21516;&#23454;&#20307;&#30340;&#25991;&#26723;&#25968;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#30456;&#20851;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20247;&#22810;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914; TriviaQA&#65289;&#12289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914; ROOTS&#65289;&#21644;&#27169;&#22411;&#19978;&#65292;&#20934;&#30830;&#24615;&#19982;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model si
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#24179;&#20215;&#28216;&#25103;&#30340;&#33719;&#32988;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;900&#20010;&#38543;&#26426;&#29983;&#25104;&#30340;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.09924</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24179;&#20215;&#28216;&#25103;&#20013;&#30340;&#33719;&#32988;&#21306;&#22495; (&#25193;&#23637;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Predicting Winning Regions in Parity Games via Graph Neural Networks (Extended Abstract). (arXiv:2210.09924v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#24179;&#20215;&#28216;&#25103;&#30340;&#33719;&#32988;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;900&#20010;&#38543;&#26426;&#29983;&#25104;&#30340;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#24179;&#20215;&#28216;&#25103;&#26159;&#21453;&#24212;&#24335;&#31243;&#24207;&#39564;&#35777;&#19982;&#21512;&#25104;&#20013;&#20247;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#34429;&#28982;&#23427;&#20204;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#39640;&#25928;&#22320;&#35299;&#20915;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#26041;&#27861;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#26368;&#22351;&#24773;&#20917;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30830;&#23450;&#24179;&#20215;&#28216;&#25103;&#30340;&#33719;&#32988;&#21306;&#22495;&#30340;&#19981;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;900&#20010;&#38543;&#26426;&#29983;&#25104;&#30340;&#24179;&#20215;&#28216;&#25103;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26159;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#12290;&#23427;&#27491;&#30830;&#30830;&#23450;&#20102;&#25968;&#25454;&#38598;&#20013;&#32422;60%&#30340;&#28216;&#25103;&#30340;&#33719;&#32988;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#20854;&#20313;&#28216;&#25103;&#20013;&#21482;&#20135;&#29983;&#20102;&#36731;&#24494;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#24179;&#20215;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving parity games is a major building block for numerous applications in reactive program verification and synthesis. While they can be solved efficiently in practice, no known approach has a polynomial worst-case runtime complexity. We present a incomplete polynomial-time approach to determining the winning regions of parity games via graph neural networks.  Our evaluation on 900 randomly generated parity games shows that this approach is effective and efficient in practice. It correctly determines the winning regions of $\sim$60\% of the games in our data set and only incurs minor errors in the remaining ones. We believe that this approach can be extended to efficiently solve parity games as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#37325;&#26032;&#35299;&#37322;&#20102;&#20256;&#36882;&#31639;&#23376;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#20363;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;Frobenius-Perron&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.03124</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#23398;&#20064;&#20256;&#36882;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Transfer Operators by Kernel Density Estimation. (arXiv:2210.03124v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26680;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#37325;&#26032;&#35299;&#37322;&#20102;&#20256;&#36882;&#31639;&#23376;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#20363;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;Frobenius-Perron&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#20256;&#36882;&#31639;&#23376;&#32463;&#24120;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#22522;&#20110;&#20044;&#25289;&#22982;&#26041;&#27861;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#20256;&#32479;&#25551;&#36848;&#20013;&#30340;&#20044;&#25289;&#22982;-&#21152;&#21202;&#37329;&#26041;&#27861;&#28041;&#21450;&#21040;&#23558;&#22522;&#20989;&#25968;&#25237;&#24433;&#21040;&#20197;&#32454;&#32593;&#26684;&#30340;&#30697;&#24418;&#25903;&#25345;&#30340;&#29305;&#24449;&#20989;&#25968;&#19978;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#20044;&#25289;&#22982;-&#21152;&#21202;&#37329;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#20351;&#29992;&#30452;&#26041;&#22270;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25226;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#25918;&#21040;&#20102;&#32479;&#35745;&#23494;&#24230;&#20272;&#35745;&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#31181;&#26367;&#20195;&#30340;&#35266;&#28857;&#20801;&#35768;&#23545;&#20559;&#24046;&#21644;&#26041;&#24046;&#36827;&#34892;&#26126;&#30830;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20174;&#32780;&#20419;&#36827;&#23545;&#22343;&#26041;&#35823;&#24046;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#26144;&#23556;&#21644;&#39532;&#23572;&#21487;&#22827;&#26144;&#23556;&#30340;&#32508;&#21512;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;Frobenius-Perron&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#30452;&#26041;&#22270;&#23494;&#24230;&#20272;&#35745;&#65288;HDE&#65289;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;KDE&#26041;&#27861;&#22312;&#35813;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference of transfer operators from data is often formulated as a classical problem that hinges on the Ulam method. The conventional description, known as the Ulam-Galerkin method, involves projecting onto basis functions represented as characteristic functions supported over a fine grid of rectangles. From this perspective, the Ulam-Galerkin approach can be interpreted as density estimation using the histogram method. In this study, we recast the problem within the framework of statistical density estimation. This alternative perspective allows for an explicit and rigorous analysis of bias and variance, thereby facilitating a discussion on the mean square error. Through comprehensive examples utilizing the logistic map and a Markov map, we demonstrate the validity and effectiveness of this approach in estimating the eigenvectors of the Frobenius-Perron operator. We compare the performance of Histogram Density Estimation(HDE) and Kernel Density Estimation(KDE) methods and find that KD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;</title><link>http://arxiv.org/abs/2209.10825</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization. (arXiv:2209.10825v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#65288;GDA&#65289;&#31639;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#24179;&#28369;&#30340;&#38750;&#20984;&#20985;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21517;&#20026;&#24179;&#28369;&#30340;&#36817;&#31471;&#32447;&#24615;&#19979;&#38477;&#19978;&#21319;&#65288;smoothed PLDA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#21407;&#22987;&#20989;&#25968;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#32467;&#26500;&#65292;&#23545;&#20598;&#20989;&#25968;&#20855;&#26377;Kurdyka-L{o}jasiewicz&#65288;K\L{}&#65289;&#24615;&#36136;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#26469;&#20998;&#26512;smoothed PLDA&#31639;&#27861;&#65292;&#20854;&#20013;&#20851;&#38190;&#32452;&#20214;&#26159;&#25105;&#20204;&#26368;&#26032;&#24320;&#21457;&#30340;&#38750;&#20809;&#28369;&#21407;&#22987;&#35823;&#24046;&#30028;&#21644;&#23545;&#20598;&#35823;&#24046;&#30028;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;smoothed PLDA&#21487;&#20197;&#22312;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#21407;&#22987;&#20989;&#25968;&#21644;KL&#23545;&#20598;&#20989;&#25968;&#30340;&#24191;&#27867;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#25214;&#21040;$\varepsilon$-game-stationary&#28857;&#21644;$\varepsilon$-&#26368;&#20248;&#21270;&#31283;&#23450;&#28857;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(\varepsilon^{-2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing work focuses on variants of gradient descent-ascent (GDA) algorithms, which are only applicable in smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent $\theta \in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound properties. Using this framework, we show that smoothed PLDA can find both $\epsilon$-game-stationary points and $\epsilon$-optimization-st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#34920;&#31034;&#30340;&#30417;&#25511;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#27969;&#24320;&#22987;&#21464;&#24471;&#38750;&#24179;&#31283;&#30340;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#25968;&#25454;&#28145;&#24230;&#35745;&#31639;&#21644;&#24402;&#19968;&#21270;&#25490;&#21517;&#30340;&#22810;&#20803;&#25511;&#21046;&#22270;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#19982;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2209.07436</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#36807;&#31243;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Statistical process monitoring of artificial neural networks. (arXiv:2209.07436v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07436
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#34920;&#31034;&#30340;&#30417;&#25511;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#27969;&#24320;&#22987;&#21464;&#24471;&#38750;&#24179;&#31283;&#30340;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#25968;&#25454;&#28145;&#24230;&#35745;&#31639;&#21644;&#24402;&#19968;&#21270;&#25490;&#21517;&#30340;&#22810;&#20803;&#25511;&#21046;&#22270;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#19982;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#21019;&#26032;&#30340;&#30417;&#25511;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#26102;&#25805;&#20316;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#27169;&#22411;&#36890;&#24120;&#26159;&#20197;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#22411;&#37096;&#32626;&#26399;&#38388;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#24517;&#39035;&#20445;&#25345;&#26377;&#25928;&#12290;&#22914;&#26524;&#36825;&#20010;&#24179;&#31283;&#24615;&#20551;&#35774;&#25104;&#31435;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#32467;&#35770;&#65292;ANN&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#21542;&#21017;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#37325;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#32771;&#34385;&#30001;ANN&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28508;&#22312;&#29305;&#24449;&#34920;&#31034;&#65288;&#31216;&#20026;&#8220;&#23884;&#20837;&#8221;&#65289;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#27969;&#24320;&#22987;&#21464;&#24471;&#38750;&#24179;&#31283;&#30340;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#25968;&#25454;&#28145;&#24230;&#35745;&#31639;&#21644;&#24402;&#19968;&#21270;&#25490;&#21517;&#30340;&#22810;&#20803;&#25511;&#21046;&#22270;&#26469;&#30417;&#27979;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;&#24341;&#20837;&#30340;&#26041;&#27861;&#19982;&#21508;&#31181;ANN&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of models based on artificial intelligence demands innovative monitoring techniques which can operate in real time with low computational costs. In machine learning, especially if we consider artificial neural networks (ANNs), the models are often trained in a supervised manner. Consequently, the learned relationship between the input and the output must remain valid during the model's deployment. If this stationarity assumption holds, we can conclude that the ANN provides accurate predictions. Otherwise, the retraining or rebuilding of the model is required. We propose considering the latent feature representation of the data (called "embedding") generated by the ANN to determine the time when the data stream starts being nonstationary. In particular, we monitor embeddings by applying multivariate control charts based on the data depth calculation and normalized ranks. The performance of the introduced method is compared with benchmark approaches for various ANN 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.11838</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#26377;&#38480;&#29366;&#24577;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#12290;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#26032;&#30340;&#25552;&#28860;&#26041;&#27861;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#20855;&#26377;&#31232;&#30095;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#37327;&#22870;&#21169;&#20449;&#21495;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#25163;&#24037;&#21019;&#24314;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#24448;&#24448;&#23481;&#26131;&#20986;&#38169;&#65292;&#29305;&#21035;&#26159;&#24403;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#21482;&#26377;&#37096;&#20998;&#24050;&#30693;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#20195;&#29702;&#32463;&#39564;&#30340; episodes &#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20064;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#30340;&#31616;&#27905;&#26377;&#38480;&#29366;&#24577;&#8220;&#20219;&#21153;&#33258;&#21160;&#26426;&#8221;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#31639;&#27861;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20135;&#21697; MDP &#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979; MDP &#24182;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340; Baum-Welch &#31639;&#27861;&#26469;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#20102;&#35268;&#33539;&#33258;&#21160;&#26426;&#21644;&#29615;&#22659;&#30340; MDP&#65288;&#21021;&#22987;&#37117;&#26410;&#30693;&#65289;&#25152;&#32452;&#25104;&#30340;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#23398;&#21040;&#30340;&#20135;&#21697; MDP &#20013;&#25552;&#28860;&#20219;&#21153;&#33258;&#21160;&#26426;&#65288;&#20551;&#35774;&#20026;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65289;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#20219;&#21153;&#33258;&#21160;&#26426;&#20351;&#24471;&#20195;&#29702;&#21487;&#20197;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;PAC&#27169;&#22411;&#20013;&#30340;&#38750;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#21464;&#31181;&#30340;&#36830;&#35789;&#26597;&#35810;&#25552;&#20986;&#20102;&#36127;&#38754;&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#21487;&#20197;&#39640;&#25928;&#23398;&#20064;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#12290;</title><link>http://arxiv.org/abs/2208.10255</link><description>&lt;p&gt;
&#35770;&#36830;&#35789;&#26597;&#35810;&#30340;&#38750;&#39640;&#25928;PAC&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;PAC&#27169;&#22411;&#20013;&#30340;&#38750;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#21464;&#31181;&#30340;&#36830;&#35789;&#26597;&#35810;&#25552;&#20986;&#20102;&#36127;&#38754;&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#21487;&#20197;&#39640;&#25928;&#23398;&#20064;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26377;&#19977;&#20010;&#30446;&#30340;&#65306;(i)&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#38416;&#36848;&#65292;&#35777;&#26126;&#20102;&#36830;&#35789;&#26597;&#35810;&#22312;&#21487;&#33021;-&#36817;&#20284;&#27491;&#30830;(PAC)&#27169;&#22411;&#20013;&#27809;&#26377;&#39640;&#25928;&#21487;&#23398;&#20064;&#24615;&#65292;&#28165;&#26970;&#22320;&#27880;&#24847;&#21040;&#36825;&#20010;&#27010;&#24565;&#31867;&#22312;&#24456;&#22810;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#25991;&#29486;&#20013;&#26263;&#21547;&#30340;&#22810;&#39033;&#24335;&#35268;&#27169;&#36866;&#24212;&#24615;&#32570;&#22833;&#65307;(ii)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36127;&#38754;PAC&#21487;&#23398;&#20064;&#24615;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#38480;&#21046;&#31867;&#21035;&#30340;&#36830;&#35789;&#26597;&#35810;(CQs)&#65292;&#21253;&#25324;&#24191;&#20041;&#30340;&#8220;&#26080;&#29615;&#24615;&#8221;&#65307;(iii)&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#35789;&#26597;&#35810;&#21644;UCQs&#36890;&#36807;&#25104;&#21592;&#26597;&#35810;&#26159;&#21487;&#20197;&#39640;&#25928;PAC&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note serves three purposes: (i) we provide a self-contained exposition of the fact that conjunctive queries are not efficiently learnable in the Probably-Approximately-Correct (PAC) model, paying clear attention to the complicating fact that this concept class lacks the polynomial-size fitting property, a property that is tacitly assumed in much of the computational learning theory literature; (ii) we establish a strong negative PAC learnability result that applies to many restricted classes of conjunctive queries (CQs), including acyclic CQs for a wide range of notions of "acyclicity"; (iii) we show that CQs (and UCQs) are efficiently PAC learnable with membership queries.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#20043;&#38388;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20989;&#25968;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21487;&#20197;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.05776</link><description>&lt;p&gt;
&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#37327;&#36755;&#20837;&#21644;&#20989;&#25968;&#36755;&#20986;&#20043;&#38388;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20989;&#25968;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#21487;&#20197;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#32452;&#26631;&#37327;&#39044;&#27979;&#21464;&#37327;&#19978;&#22238;&#24402;&#20989;&#25968;&#21709;&#24212;&#21487;&#20197;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#26377;&#22823;&#37327;&#39044;&#27979;&#21464;&#37327;&#25110;&#32773;&#39044;&#27979;&#21464;&#37327;&#19982;&#21709;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#20505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#39044;&#27979;&#26631;&#37327;&#36755;&#20837;&#19979;&#30340;&#20989;&#25968;&#21709;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20989;&#25968;&#21709;&#24212;&#36716;&#21270;&#20026;&#26377;&#38480;&#32500;&#24230;&#34920;&#31034;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#36755;&#20986;&#35813;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#22343;&#21248;&#21644;&#19981;&#22343;&#21248;&#38388;&#38548;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#24179;&#28369;&#24809;&#32602;&#39033;&#26469;&#25511;&#21046;&#39044;&#27979;&#26354;&#32447;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#23454;&#29616;&#36825;&#20123;&#29305;&#24615;&#30340;&#22256;&#38590;&#22312;&#20110;&#23450;&#20041;&#21487;&#20197;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The regression of a functional response on a set of scalar predictors can be a challenging task, especially if there is a large number of predictors, or the relationship between those predictors and the response is nonlinear. In this work, we propose a solution to this problem: a feed-forward neural network (NN) designed to predict a functional response using scalar inputs. First, we transform the functional response to a finite-dimensional representation and construct an NN that outputs this representation. Then, we propose to modify the output of an NN via the objective function and introduce different objective functions for network training. The proposed models are suited for both regularly and irregularly spaced data, and a roughness penalty can be further applied to control the smoothness of the predicted curve. The difficulty in implementing both those features lies in the definition of objective functions that can be back-propagated. In our experiments, we demonstrate that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#23548;&#33322;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#23398;&#20064;&#26377;&#25928;&#22320;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#23545;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2207.00052</link><description>&lt;p&gt;
&#35270;&#35273;&#39044;&#35757;&#32451;&#29992;&#20110;&#23548;&#33322;&#65306;&#20174;&#22122;&#22768;&#20013;&#25105;&#20204;&#33021;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#23548;&#33322;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#23398;&#20064;&#26377;&#25928;&#22320;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#23545;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#23548;&#33322;&#20013;&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#26159;&#20174;&#35266;&#23519;&#20013;&#30452;&#25509;&#39044;&#27979;&#34892;&#20026;&#12290;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20135;&#29983;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#20351;&#24471;&#35813;&#31995;&#32479;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#39044;&#27979;&#19982;&#30446;&#26631;&#23545;&#24212;&#30340;&#24403;&#21069;&#35270;&#22270;&#35009;&#21098;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#23548;&#33322;&#31574;&#30053;&#25152;&#38656;&#30340;&#24403;&#21069;&#35270;&#22270;&#21644;&#30446;&#26631;&#35270;&#22270;&#30340;&#20805;&#20998;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#22122;&#22768;&#22270;&#20687;&#36827;&#34892;&#38543;&#26426;&#35009;&#21098;&#39044;&#27979;&#30340;&#35757;&#32451;&#21487;&#20197;&#24456;&#22909;&#22320;&#36801;&#31227;&#21040;&#33258;&#28982;&#23478;&#24237;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#26377;&#25928;&#22320;&#33258;&#20030;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#65292;&#20943;&#23569;&#20132;&#20114;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.12672</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#29983;&#25104;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#35774;&#32622;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#19982;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#20043;&#38388;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#22312;&#36825;&#20010;&#38543;&#26426;&#36712;&#36857;&#20013;&#24674;&#22797;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#35770;&#25991;&#23545;&#19981;&#21516;&#25104;&#26412;&#27169;&#22411;&#23545;&#36712;&#36857;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#20135;&#21697;&#22810;&#22270;&#26469;&#27604;&#36739;&#26367;&#20195;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#27599;&#20010;&#19981;&#30830;&#23450;&#27963;&#21160;&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#27491;&#30830;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#26159;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#30340;&#26377;&#21147;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#20998;&#26512;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#24182;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.12481</link><description>&lt;p&gt;
&#20998;&#26512;&#36890;&#36807;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#35780;&#20272;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#20998;&#26512;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#24182;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#22120;&#24635;&#26159;&#34987;&#20381;&#36182;&#20110;&#20026;&#36825;&#20123;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#20851;&#38190;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#35299;&#37322;&#22120;&#26412;&#36523;&#26159;&#31283;&#20581;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#31283;&#20581;&#24615;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65292;&#21363;&#35299;&#37322;&#22120;&#22312;&#30456;&#20284;&#30340;&#25968;&#25454;&#36755;&#20837;&#19978;&#24212;&#35813;&#32473;&#20986;&#31867;&#20284;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#23450;&#20041;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#35266;&#24565;&#65292;&#31867;&#20284;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#25935;&#38160;&#24615;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#35813;&#29575;&#25429;&#25417;&#20102;&#20989;&#25968;&#23616;&#37096;&#24179;&#28369;&#24615;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#26681;&#25454;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#25552;&#20379;&#23545;&#21508;&#31181;&#35299;&#37322;&#22120;&#65288;&#22914;SHAP&#65292;RISE&#65292;CXPlain&#65289;&#30340;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#26263;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#20809;&#28369;&#24615;&#39044;&#27979;&#20989;&#25968;&#30340;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26464;&#26438;&#20998;&#25968;&#31232;&#30095;&#65288;LESS&#65289;&#23884;&#20837;&#30340;&#33609;&#22270;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#26031;&#21270;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#22320;&#26500;&#24314;&#19982;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#30340;&#25968;&#25454;&#33609;&#22270;&#12290;</title><link>http://arxiv.org/abs/2206.10291</link><description>&lt;p&gt;
&#36890;&#36807;&#33609;&#22270;&#25216;&#26415;&#23454;&#29616;&#31639;&#27861;&#39640;&#26031;&#21270;&#65306;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs. (arXiv:2206.10291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26464;&#26438;&#20998;&#25968;&#31232;&#30095;&#65288;LESS&#65289;&#23884;&#20837;&#30340;&#33609;&#22270;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#26031;&#21270;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#22320;&#26500;&#24314;&#19982;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#30340;&#25968;&#25454;&#33609;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#39640;&#26031;&#21270;&#26159;&#20351;&#29992;&#38543;&#26426;&#33609;&#22270;&#26041;&#27861;&#25110;&#37319;&#26679;&#26041;&#27861;&#29983;&#25104;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36739;&#23567;&#34920;&#31034;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#29616;&#35937;&#65306;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#65292;&#35266;&#23519;&#21040;&#30340;&#36825;&#20123;&#33609;&#22270;&#34920;&#31034;&#20855;&#26377;&#35768;&#22810;&#31283;&#20581;&#24615;&#33021;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#25968;&#25454;&#26679;&#26412;&#26469;&#33258;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;&#26102;&#24050;&#34987;&#30830;&#35748;&#23384;&#22312;&#65292;&#32780;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#20010;&#24378;&#22823;&#32479;&#35745;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29616;&#35937;&#20165;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#24230;&#37327;&#26631;&#20934;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#36890;&#36807;&#24179;&#22343;&#26469;&#39640;&#26031;&#21270;&#25968;&#25454;&#20998;&#24067;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#21487;&#20197;&#39640;&#25928;&#22320;&#26500;&#24314;&#19982;&#27425;&#39640;&#26031;&#38543;&#26426;&#35774;&#35745;&#22312;&#24635;&#21464;&#24322;&#36317;&#31163;&#19978;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#30340;&#25968;&#25454;&#33609;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#20381;&#36182;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#31216;&#20026;&#26464;&#26438;&#20998;&#25968;&#31232;&#30095;&#65288;LESS&#65289;&#23884;&#20837;&#30340;&#33609;&#22270;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;n&#36924;&#30495;&#30340;&#39640;&#26031;&#21270;&#25968;&#25454;&#33609;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Gaussianization is a phenomenon that can arise when using randomized sketching or sampling methods to produce smaller representations of large datasets: For certain tasks, these sketched representations have been observed to exhibit many robust performance characteristics that are known to occur when a data sample comes from a sub-gaussian random design, which is a powerful statistical model of data distributions. However, this phenomenon has only been studied for specific tasks and metrics, or by relying on computationally expensive methods. We address this by providing an algorithmic framework for gaussianizing data distributions via averaging, proving that it is possible to efficiently construct data sketches that are nearly indistinguishable (in terms of total variation distance) from sub-gaussian random designs. In particular, relying on a recently introduced sketching technique called Leverage Score Sparsified (LESS) embeddings, we show that one can construct an $n\ti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#20102;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#26080;&#38656;&#23436;&#20840;&#25551;&#36848;&#37327;&#23376;&#24577;&#65292;&#20165;&#20351;&#29992;&#19981;&#23436;&#20840;&#30340;&#23616;&#37096;&#27979;&#37327;&#38598;&#21363;&#21487;&#23454;&#29616;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20302;&#19968;&#20010;&#37327;&#32423;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#19988;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.01462</link><description>&lt;p&gt;
&#37327;&#23376;&#32416;&#32544;&#30340;&#19981;&#23436;&#20840;&#27979;&#37327;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep learning of quantum entanglement from incomplete measurements. (arXiv:2205.01462v6 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#20102;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#26080;&#38656;&#23436;&#20840;&#25551;&#36848;&#37327;&#23376;&#24577;&#65292;&#20165;&#20351;&#29992;&#19981;&#23436;&#20840;&#30340;&#23616;&#37096;&#27979;&#37327;&#38598;&#21363;&#21487;&#23454;&#29616;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20302;&#19968;&#20010;&#37327;&#32423;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#19988;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#30784;&#30740;&#31350;&#21644;&#35768;&#22810;&#21069;&#27839;&#24212;&#29992;&#26469;&#35828;&#65292;&#37327;&#23376;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#32416;&#32544;&#31243;&#24230;&#30340;&#37327;&#21270;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#35201;&#20040;&#38656;&#35201;&#23545;&#31995;&#32479;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#35201;&#20040;&#38656;&#35201;&#38750;&#24120;&#33499;&#21051;&#30340;&#23454;&#39564;&#36807;&#31243;&#65292;&#20363;&#22914;&#20840;&#24577;&#27979;&#37327;&#25110;&#38598;&#20307;&#27979;&#37327;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#37327;&#23376;&#24577;&#30340;&#23436;&#20840;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#32416;&#32544;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#19981;&#23436;&#20840;&#30340;&#23616;&#37096;&#27979;&#37327;&#38598;&#23545;&#37327;&#23376;&#30456;&#20851;&#24615;&#36827;&#34892;&#30452;&#25509;&#37327;&#21270;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#27424;&#37319;&#26679;&#27979;&#37327;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#27604;&#26368;&#20808;&#36827;&#30340;&#37327;&#23376;&#24577;&#27979;&#37327;&#20302;&#19968;&#20010;&#37327;&#32423;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#36825;&#20010;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25509;&#21463;&#26469;&#33258;&#21508;&#31181;&#27979;&#37327;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantification of the entanglement present in a physical system is of para\-mount importance for fundamental research and many cutting-edge applications. Currently, achieving this goal requires either a priori knowledge on the system or very demanding experimental procedures such as full state tomography or collective measurements. Here, we demonstrate that by employing neural networks we can quantify the degree of entanglement without needing to know the full description of the quantum state. Our method allows for direct quantification of the quantum correlations using an incomplete set of local measurements. Despite using undersampled measurements, we achieve a quantification error of up to an order of magnitude lower than the state-of-the-art quantum tomography. Furthermore, we achieve this result employing networks trained using exclusively simulated data. Finally, we derive a method based on a convolutional network input that can accept data from various measurement scenarios 
&lt;/p&gt;</description></item><item><title>RELDEC&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#39034;&#24207;&#35843;&#24230;CN&#38598;&#32676;&#65292;&#20197;&#20248;&#21270;&#35299;&#30721;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#26356;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;AM-RELDEC&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.13934</link><description>&lt;p&gt;
RELDEC: &#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13934
&lt;/p&gt;
&lt;p&gt;
RELDEC&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#31561;&#38271;&#24230;LDPC&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#39034;&#24207;&#35843;&#24230;CN&#38598;&#32676;&#65292;&#20197;&#20248;&#21270;&#35299;&#30721;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#26356;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;AM-RELDEC&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELDEC&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20013;&#31561;&#38271;&#24230;&#20302;&#23494;&#24230;&#22855;&#20598;&#26657;&#39564;&#65288;LDPC&#65289;&#30721;&#30340;&#39034;&#24207;&#35299;&#30721;&#12290; RELDEC&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#20248;&#21270;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#19982;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#26234;&#33021;&#20307;&#20165;&#23398;&#20064;&#19968;&#27425;&#36845;&#20195;&#20013;&#38598;&#32676;&#20869;&#30340;&#21333;&#20010;&#26816;&#26597;&#33410;&#28857;&#65288;CN&#65289;&#30340;&#35843;&#24230;&#65292;&#32780;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#26234;&#33021;&#20307;&#35843;&#24230;&#27599;&#20010;&#38598;&#32676;&#20013;&#30340;&#25152;&#26377;CN&#20197;&#21450;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25152;&#26377;&#38598;&#32676;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;RELDEC&#30340;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#20013;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#19982;&#35843;&#24230;&#29305;&#23450;&#38598;&#32676;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26469;&#39034;&#24207;&#23398;&#20064;&#35843;&#24230;CN&#38598;&#32676;&#12290;&#25105;&#20204;&#36824;&#20462;&#25913;&#20102;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20351;RELDEC&#36866;&#29992;&#20110;&#27604;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#30340;&#36739;&#22823;&#22359;&#38271;&#30340;LDPC&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#30340;&#35299;&#30721;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28789;&#27963;&#30340;&#20803;-RELDEC&#65288;AM-RELDEC&#65289;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#30340;&#26159;&#25935;&#25463;&#30340;&#20803;-RELDEC&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose RELDEC, a novel approach for sequential decoding of moderate length low-density parity-check (LDPC) codes. The main idea behind RELDEC is that an optimized decoding policy is subsequently obtained via reinforcement learning based on a Markov decision process (MDP). In contrast to our previous work, where an agent learns to schedule only a single check node (CN) within a group (cluster) of CNs per iteration, in this work we train the agent to schedule all CNs in a cluster, and all clusters in every iteration. That is, in each learning step of RELDEC an agent learns to schedule CN clusters sequentially depending on a reward associated with the outcome of scheduling a particular cluster. We also modify the state space representation of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes than those studied in our previous work. Furthermore, to address decoding under varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that empl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#21040;&#24403;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#36825;&#26159;&#23545;&#35813;&#38382;&#39064;&#30340;&#39318;&#27425;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#38382;&#39064;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#21457;&#23637;&#12289;&#35780;&#20272;&#31243;&#24207;&#20197;&#21450;&#39046;&#22495;&#30340;&#24847;&#20041;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2108.13624</link><description>&lt;p&gt;
&#36208;&#21521;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Out-Of-Distribution Generalization: A Survey. (arXiv:2108.13624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#21040;&#24403;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#36825;&#26159;&#23545;&#35813;&#38382;&#39064;&#30340;&#39318;&#27425;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#38382;&#39064;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#21457;&#23637;&#12289;&#35780;&#20272;&#31243;&#24207;&#20197;&#21450;&#39046;&#22495;&#30340;&#24847;&#20041;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#30340;&#32479;&#35745;&#27169;&#24335;&#30340;&#20551;&#35774;&#65292;&#25968;&#23398;&#19978;&#31216;&#20026;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;$i.i.d.$&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#26410;&#39044;&#26009;&#21040;&#30340;&#20998;&#24067;&#36716;&#21464;&#65292;&#36825;&#31181;$i.i.d.$&#20551;&#35774;&#32463;&#24120;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#24615;&#33021;&#22823;&#22823;&#38477;&#20302;&#12290;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#24046;&#24322;&#34920;&#26126;&#20102;&#30740;&#31350;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;OOD&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#20852;&#20027;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#26412;&#25991;&#26159;&#23545;OOD&#27867;&#21270;&#30340;&#39318;&#27425;&#20840;&#38754;&#12289;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#20174;&#38382;&#39064;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#21457;&#23637;&#21644;&#35780;&#20272;&#31243;&#24207;&#21040;&#39046;&#22495;&#30340;&#24847;&#20041;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#19968;&#31995;&#21015;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2107.11277</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#21363;&#20351;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#35768;&#22810;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#65292;&#24212;&#36991;&#20813;&#36825;&#31181;&#34892;&#20026;&#65292;&#22240;&#20026;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#12290;&#23613;&#31649;&#22312;1970&#24180;&#24050;&#32463;&#30740;&#31350;&#36807;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#36991;&#20813;&#20570;&#20986;&#39044;&#27979;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#20013;&#25298;&#32477;&#36873;&#39033;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23548;&#33268;&#20004;&#31181;&#25298;&#32477;&#24773;&#20917;&#65288;&#27169;&#31946;&#21644;&#26032;&#22855;&#25298;&#32477;&#65289;&#30340;&#26465;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#21644;&#25298;&#32477;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#29616;&#26377;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#25551;&#36848;&#20102;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ComFedL&#30340;&#32452;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21644;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#65292;&#23558;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#24050;&#35777;&#26126;ComFedL&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#20026;O(1/&#8730;T)&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#32452;&#21512;&#38543;&#26426;&#20248;&#21270;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2106.11264</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#65306;&#22312;&#20998;&#24067;&#40065;&#26834;&#24179;&#22343;&#21644;&#20803;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ComFedL&#30340;&#32452;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21644;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#65292;&#23558;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#24050;&#35777;&#26126;ComFedL&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#20026;O(1/&#8730;T)&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#32452;&#21512;&#38543;&#26426;&#20248;&#21270;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#32452;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;ComFedL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#35768;&#22810;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22914;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21644;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;ComFedL&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#23427;&#36798;&#21040;&#20102;$O(\frac{1}{\sqrt{T}})$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$T$&#34920;&#31034;&#36845;&#20195;&#27425;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26032;&#30340;&#32452;&#21512;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#26159;&#39318;&#20010;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#32452;&#21512;&#38543;&#26426;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#24037;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#23558;&#20998;&#24067;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65289;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#23558;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;MAML&#38382;&#39064;&#65288;&#21363;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
In the paper, we propose an effective and efficient Compositional Federated Learning (ComFedL) algorithm for solving a new compositional Federated Learning (FL) framework, which frequently appears in many data mining and machine learning problems with a hierarchical structure such as distributionally robust FL and model-agnostic meta learning (MAML). Moreover, we study the convergence analysis of our ComFedL algorithm under some mild conditions, and prove that it achieves a convergence rate of $O(\frac{1}{\sqrt{T}})$, where $T$ denotes the number of iteration. To the best of our knowledge, our new Compositional FL framework is the first work to bridge federated learning with composition stochastic optimization. In particular, we first transform the distributionally robust FL (i.e., a minimax optimization problem) into a simple composition optimization problem by using KL divergence regularization. At the same time, we also first transform the distribution-agnostic MAML problem (i.e., a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Multi-RoundSecAgg&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#36718;&#38544;&#31169;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20171;&#32461;&#26032;&#30340;&#38544;&#31169;&#25351;&#26631;&#21644;&#24320;&#21457;&#32467;&#26500;&#21270;&#30340;&#29992;&#25143;&#36873;&#25321;&#31574;&#30053;&#23454;&#29616;&#20102;&#22810;&#36718;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.03328</link><description>&lt;p&gt;
&#20445;&#25252;&#23433;&#20840;&#32858;&#21512;&#65306;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#36718;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Multi-RoundSecAgg&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#22810;&#36718;&#38544;&#31169;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20171;&#32461;&#26032;&#30340;&#38544;&#31169;&#25351;&#26631;&#21644;&#24320;&#21457;&#32467;&#26500;&#21270;&#30340;&#29992;&#25143;&#36873;&#25321;&#31574;&#30053;&#23454;&#29616;&#20102;&#22810;&#36718;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20351;&#24471;&#26381;&#21153;&#22120;&#33021;&#22815;&#22312;&#19981;&#35266;&#23519;&#29992;&#25143;&#26412;&#22320;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;&#29992;&#25143;&#30340;&#32858;&#21512;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;&#32858;&#21512;&#31639;&#27861;&#20165;&#20851;&#27880;&#21333;&#36718;&#35757;&#32451;&#20013;&#20010;&#20307;&#29992;&#25143;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#22240;&#20026;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#37117;&#20250;&#36873;&#25321;/&#21442;&#19982;&#37096;&#20998;&#29992;&#25143;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#38543;&#26426;&#29992;&#25143;&#36873;&#25321;&#31574;&#30053;&#23548;&#33268;&#20102;&#29992;&#25143;&#20010;&#20307;&#27169;&#22411;&#30340;&#27844;&#28431;&#65292;&#20854;&#27844;&#28431;&#30340;&#36718;&#25968;&#19982;&#29992;&#25143;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;Multi-RoundSecAgg&#65292;&#20855;&#26377;&#22810;&#36718;&#38544;&#31169;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#38544;&#31169;&#20445;&#35777;&#30340;&#26032;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#29992;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#29992;&#25143;&#30340;&#38271;&#26399;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#20102;Acrobot&#30340;&#35282;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#26410;&#39537;&#21160;&#25670;&#26438;&#30340;&#25671;&#25670;&#25110;&#23436;&#20840;&#26059;&#36716;&#65292;&#24182;&#28145;&#20837;&#25506;&#31350;&#20102;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2011.09246</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Acrobot&#25511;&#21046;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Experimental Study on Reinforcement Learning-based Control of an Acrobot. (arXiv:2011.09246v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#20102;Acrobot&#30340;&#35282;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#23454;&#29616;&#20102;&#26410;&#39537;&#21160;&#25670;&#26438;&#30340;&#25671;&#25670;&#25110;&#23436;&#20840;&#26059;&#36716;&#65292;&#24182;&#28145;&#20837;&#25506;&#31350;&#20102;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22914;&#20309;&#23398;&#20064;&#25511;&#21046;Acrobot&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#31995;&#32479;&#20316;&#20026;&#23454;&#39564;&#35013;&#32622;&#65292;&#35813;&#31995;&#32479;&#23545;&#26426;&#22120;&#20154;&#21644;&#33021;&#28304;&#25910;&#38598;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25511;&#21046;Acrobot&#35282;&#36895;&#24230;&#20197;&#21450;&#20854;&#24635;&#33021;&#37327;&#65288;&#21363;&#21160;&#33021;&#21644;&#21183;&#33021;&#20043;&#21644;&#65289;&#12290;&#36890;&#36807;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;Acrobot&#30340;&#31532;&#19968;&#20010;&#25670;&#26438;&#30340;&#35282;&#36895;&#24230;&#25110;&#33021;&#37327;&#39537;&#21160;&#21040;&#26399;&#26395;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;Acrobot&#26410;&#39537;&#21160;&#25670;&#26438;&#30340;&#25671;&#25670;&#25110;&#23436;&#20840;&#26059;&#36716;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;Acrobot&#25511;&#21046;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36825;&#20123;&#30740;&#31350;&#25105;&#20204;&#23545;&#29366;&#24577;&#31354;&#38388;&#31163;&#25955;&#21270;&#12289;&#22238;&#21512;&#38271;&#24230;&#12289;&#21160;&#20316;&#31354;&#38388;&#20197;&#21450;&#39537;&#21160;&#25670;&#26438;&#36136;&#37327;&#23545;RL&#25511;&#21046;&#30340;&#24433;&#21709;&#26377;&#20102;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#12290;&#36890;&#36807;&#26356;&#22810;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21442;&#25968;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present computational and experimental results on how artificial intelligence (AI) learns to control an Acrobot using reinforcement learning (RL). Thereby the experimental setup is designed as an embedded system, which is of interest for robotics and energy harvesting applications. Specifically, we study the control of angular velocity of the Acrobot, as well as control of its total energy, which is the sum of the kinetic and the potential energy. By this means the RL algorithm is designed to drive the angular velocity or the energy of the first pendulum of the Acrobot towards a desired value. With this, libration or full rotation of the unactuated pendulum of the Acrobot is achieved. Moreover, investigations of the Acrobot control are carried out, which lead to insights about the influence of the state space discretization, the episode length, the action space or the mass of the driven pendulum on the RL control. By further numerous simulations and experiments the effects of parame
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#21487;&#20197;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#65307;&#32780;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#21017;&#36890;&#36807;&#36215;&#21040;&#27491;&#21017;&#21270;&#20316;&#29992;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2005.00695</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#20013;&#32447;&#24615;&#36716;&#25442;&#30340;&#27867;&#21270;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#21487;&#20197;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#65307;&#32780;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#21017;&#36890;&#36807;&#36215;&#21040;&#27491;&#21017;&#21270;&#20316;&#29992;&#26469;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#31561;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21508;&#31181;&#22686;&#24378;&#26041;&#27861;&#20026;&#20309;&#26377;&#25928;&#20197;&#21450;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#20005;&#26684;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#23545;&#23725;&#20272;&#35745;&#37327;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#24352;&#37327;&#26469;&#23637;&#31034;&#20102;&#33021;&#22815;&#20445;&#25345;&#25968;&#25454;&#26631;&#31614;&#30340;&#36716;&#25442;&#20250;&#25913;&#21892;&#20272;&#35745;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#30340;&#36716;&#25442;&#23637;&#31034;&#20102;&#23545;&#20272;&#35745;&#37327;&#36215;&#21040;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;MNIST&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27169;&#22411;&#23545;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25628;&#32034;&#36716;&#25442;&#31354;&#38388;&#30340;&#22686;&#24378;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;Wide-ResNet&#23545;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;1.24%&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>