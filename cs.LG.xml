<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#24179;&#21488;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01683</link><description>&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32959;&#30244;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25104;&#20687;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data. (arXiv:2311.01683v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#24179;&#21488;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#37327;&#21270;&#21270;&#23398;&#20132;&#25442;&#39281;&#21644;&#36716;&#31227;&#65288;CEST&#65289;&#25928;&#24212;&#12290;ML&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#23454;&#27979;&#25968;&#25454;&#25110;&#23436;&#20840;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23454;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20351;&#29992;&#23436;&#20840;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#30001;&#20110;&#26377;&#38480;&#30340;&#27169;&#25311;&#36164;&#28304;&#32780;&#24341;&#20837;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#24179;&#21488;&#65292;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;CEST&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#29992;&#20110;&#35757;&#32451;ML&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#30340;&#21487;&#34892;&#24615;&#12290;&#37096;&#20998;&#21512;&#25104;&#30340;CEST&#20449;&#21495;&#20351;&#29992;&#20174;&#27169;&#25311;&#20013;&#30340;APT&#25928;&#24212;&#30340;&#36870;&#27714;&#21644;&#21644;&#26469;&#33258;&#23454;&#27979;&#25968;&#25454;&#30340;&#20854;&#20182;&#25104;&#20998;&#21019;&#24314;&#12290;&#36890;&#36807;&#25913;&#21464;APT&#27169;&#25311;&#21442;&#25968;&#24182;&#24212;&#29992;&#32553;&#25918;&#22240;&#23376;&#26469;&#35843;&#25972;&#23454;&#27979;&#25968;&#25454;&#30340;&#25104;&#20998;&#65292;&#29983;&#25104;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has been increasingly used to quantify chemical exchange saturation transfer (CEST) effect. ML models are typically trained using either measured data or fully simulated data. However, training with measured data often lacks sufficient training data, while training with fully simulated data may introduce bias due to limited simulations pools. This study introduces a new platform that combines simulated and measured components to generate partially synthetic CEST data, and to evaluate its feasibility for training ML models to predict amide proton transfer (APT) effect. Partially synthetic CEST signals were created using an inverse summation of APT effects from simulations and the other components from measurements. Training data were generated by varying APT simulation parameters and applying scaling factors to adjust the measured components, achieving a balance between simulation flexibility and fidelity. First, tissue-mimicking CEST signals along with ground trut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20092</link><description>&lt;p&gt;
&#36229;&#36234;U&#65306;&#20351;&#25193;&#25955;&#27169;&#22411;&#26356;&#24555;&#26356;&#36731;
&lt;/p&gt;
&lt;p&gt;
Beyond U: Making Diffusion Models Faster &amp; Lighter. (arXiv:2310.20092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;U-Net&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#26174;&#33879;&#20943;&#23569;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#36136;&#37327;&#35299;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21019;&#32426;&#24405;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#22791;&#36825;&#20123;&#33021;&#21147;&#65292;&#20294;&#20854;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#36870;&#21521;&#21435;&#22122;&#36807;&#31243;&#20013;&#65292;&#20173;&#28982;&#38754;&#20020;&#30528;&#24930;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#26469;&#35774;&#35745;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#22122;&#22768;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23545;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#21442;&#25968;&#32422;&#20026;&#26631;&#20934;&#21435;&#22122;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;DDPM&#65289;&#20013;&#26631;&#20934;U-Net&#30340;&#22235;&#20998;&#20043;&#19968;&#65292;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#32422;&#20026;&#26631;&#20934;U-Net&#30340;30%&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#27979;&#37327;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#27604;&#22522;&#20934;&#27169;&#22411;&#24555;70&#65285;&#65292;&#21516;&#26102;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#36136;&#37327;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16121</link><description>&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#65306;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics. (arXiv:2310.16121v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16121
&lt;/p&gt;
&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31890;&#23376;&#21152;&#36895;&#22120;&#22686;&#21152;&#30896;&#25758;&#36895;&#29575;&#21644;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#24471;&#21040;&#35777;&#23454;&#65292;&#23545;&#20110;&#20302;&#24310;&#36831;&#20219;&#21153;&#65288;&#22914;&#35302;&#21457;&#22120;&#65289;&#65292;&#38656;&#35201;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#27931;&#20262;&#20857;&#21644;&#32622;&#25442;&#23545;&#31216;&#26550;&#26500;PELICAN&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#30340;&#20960;&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#20165;&#20026;19&#20010;&#65292;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24615;&#33021;&#36229;&#36807;&#20102;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#21464;&#21270;&#24182;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#65292;&#35299;&#20915;&#21518;&#38376;&#25915;&#20987;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.06372</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21464;&#21270;&#20026;&#40065;&#26834;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#21464;&#21270;&#24182;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#65292;&#35299;&#20915;&#21518;&#38376;&#25915;&#20987;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#65292;&#23427;&#20204;&#22312;&#27169;&#22411;&#20013;&#31192;&#23494;&#24341;&#20837;&#38544;&#34255;&#21151;&#33021;&#12290;&#36825;&#20123;&#21518;&#38376;&#22312;&#23545;&#24178;&#20928;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#26102;&#20445;&#25345;&#27785;&#40664;&#65292;&#30001;&#20110;&#38544;&#34109;&#30340;&#34892;&#20026;&#32780;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#36755;&#20837;&#25968;&#25454;&#20013;&#20986;&#29616;&#29305;&#23450;&#30340;&#35302;&#21457;&#27169;&#24335;&#65292;&#21518;&#38376;&#23601;&#20250;&#28608;&#27963;&#65292;&#23548;&#33268;&#27169;&#22411;&#25191;&#34892;&#20854;&#38544;&#34255;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25163;&#21160;&#26816;&#26597;&#65292;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21040;&#36825;&#31181;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#23454;&#29616;&#23545;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#19978;&#21019;&#24314;&#21512;&#25104;&#21464;&#21270;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#35302;&#21457;&#27169;&#24335;&#20855;&#26377;&#22266;&#26377;&#30340;&#24377;&#24615;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#29983;&#25104;&#26041;&#27861;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#20445;&#25345;&#20219;&#21153;&#19978;&#30340;&#24635;&#20307;&#24615;&#33021;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhib
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#65292;&#19981;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#30340;&#20551;&#35774;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.04723</link><description>&lt;p&gt;
&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Subspace Identification for Multi-Source Domain Adaptation. (arXiv:2310.04723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#65292;&#19981;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#30340;&#20551;&#35774;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#26041;&#27861;&#26088;&#22312;&#23558;&#22810;&#20010;&#26377;&#26631;&#31614;&#30340;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#22495;&#20013;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22495;&#20043;&#38388;&#26045;&#21152;&#26368;&#23567;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#30446;&#26631;&#32852;&#21512;&#20998;&#24067;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#20005;&#26684;&#30340;&#26465;&#20214;&#65292;&#22914;&#36275;&#22815;&#25968;&#37327;&#30340;&#22495;&#12289;&#28508;&#22312;&#21464;&#37327;&#30340;&#21333;&#35843;&#21464;&#25442;&#21644;&#19981;&#21464;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;&#36825;&#20123;&#35201;&#27714;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36825;&#20123;&#20005;&#26684;&#20551;&#35774;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#65292;&#23427;&#22312;&#20851;&#20110;&#22495;&#25968;&#37327;&#21644;&#21464;&#25442;&#29305;&#24615;&#26041;&#38754;&#20855;&#26377;&#36739;&#23485;&#26494;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#26469;&#20419;&#36827;&#22495;&#33258;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;&#20445;&#35777;&#65288;SIG&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Although current methods achieve target joint distribution identifiability by enforcing minimal changes across domains, they often necessitate stringent conditions, such as an adequate number of domains, monotonic transformation of latent variables, and invariant label distributions. These requirements are challenging to satisfy in real-world applications. To mitigate the need for these strict assumptions, we propose a subspace identification theory that guarantees the disentanglement of domain-invariant and domain-specific variables under less restrictive constraints regarding domain numbers and transformation properties, thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that leverages variational inference. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01794</link><description>&lt;p&gt;
GNNX-BENCH: &#36890;&#36807;&#28145;&#24230;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#22120;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#35780;&#20272;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#25581;&#31034;GNN&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#37117;&#21253;&#21547;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#30340;&#35810;&#38382;&#26041;&#38754;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;GNN&#35299;&#37322;&#24615;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#23545;&#20107;&#23454;&#27714;&#35777;&#25512;&#29702;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;&#12289;&#23427;&#20204;&#23545;&#19981;&#21516;GNN&#26550;&#26500;&#12289;&#22122;&#22768;&#12289;&#38750;&#20984;&#25439;&#22833;&#34920;&#38754;&#20013;&#30340;&#38543;&#26426;&#24615;&#12289;&#22312;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#21487;&#34892;&#24615;&#31561;&#31561;&#65292;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#30340;&#30740;&#31350;&#12290;&#21463;&#27492;&#38656;&#27714;&#30340;&#28608;&#21457;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#25200;&#21160;&#30340;GNN&#35299;&#37322;&#24615;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#21508;&#31181;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20851;&#38190;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#25928;&#21147;&#21644;&#31283;&#23450;&#24615;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25152;&#26377;&#31639;&#27861;&#37117;&#21463;&#21040;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stabi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08628</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#25513;&#30721;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#23545;&#20110;&#22788;&#29702;&#20195;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24212;&#65292;&#29992;&#25143;&#30340;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#25110;&#26412;&#22320;&#35774;&#22791;&#19978;&#65292;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#25968;&#25454;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#21521;&#23545;&#25163;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#30340;&#39069;&#22806;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#36890;&#29992;&#26631;&#35760;&#26367;&#25442;&#25991;&#26412;&#20013;&#30340;&#26631;&#35782;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#35758;&#26367;&#25442;&#25513;&#30721;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28151;&#28102;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07579</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#24207;&#21015;&#30340;SPD&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25968;&#25454;&#31867;&#22411;&#30340;&#20998;&#26512;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#31561;&#65292;&#21253;&#25324;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#22312;&#25972;&#20010;&#20998;&#26512;&#36807;&#31243;&#20013;&#20445;&#25345;&#23427;&#20204;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#33041;&#30005;&#22270;&#21327;&#26041;&#24046;&#30697;&#38453;&#24207;&#21015;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.06869</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#25511;&#21046;&#25311;&#26230;&#32467;&#26500;&#30340;&#33258;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning. (arXiv:2309.06869v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#65288;DDQC&#65289;&#12290;&#36825;&#20123;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#19982;&#20854;&#20182;&#39063;&#31890;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#24418;&#25104;DDQC&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#31283;&#24577;&#19979;&#30340;&#32467;&#26500;&#21463;&#20854;&#32467;&#26500;&#24418;&#25104;&#30340;&#21160;&#21147;&#23398;&#36335;&#24452;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;Q&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#20102;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20272;&#35745;&#30340;&#31574;&#30053;&#29983;&#25104;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;DDQC&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#65288;&#22914;&#36864;&#28779;&#65289;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#38416;&#26126;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#20010;&#25551;&#36848;&#32467;&#26500;&#21464;&#21270;&#21160;&#21147;&#23398;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#36816;&#21160;&#26159;&#22312;&#19977;&#20117;&#21183;&#33021;&#20013;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#33258;&#20027;&#22320;&#21457;&#29616;&#22686;&#24378;&#32467;&#26500;&#27874;&#21160;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06157</link><description>&lt;p&gt;
Robust-MBDL:&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;(RUL)&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;(CO)&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#20027;&#35201;&#32452;&#20214;&#65306;(1)&#37319;&#29992;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65307;(2)&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#20174;&#21435;&#22122;&#25968;&#25454;&#20013;&#29983;&#25104;&#26102;&#22495;&#12289;&#39057;&#22495;&#21644;&#26102;&#39057;&#22495;&#29305;&#24449;&#65307;(3)&#37319;&#29992;&#26032;&#39062;&#32780;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#26469;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;XJTU-SY&#21644;PRONOSTIA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#20855;&#26377;&#22312;&#36724;&#25215;&#26426;&#22120;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05395</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Practical Homomorphic Aggregation for Byzantine ML. (arXiv:2309.05395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25308;&#21344;&#24237;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29992;&#21516;&#24577;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25308;&#21344;&#24237;&#33410;&#28857;&#21644;&#26381;&#21153;&#22120;&#38544;&#31169;&#20405;&#29359;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21487;&#29992;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27491;&#22312;&#20998;&#24067;&#24335;&#25299;&#25169;&#20013;&#37096;&#32626;&#65292;&#19981;&#21516;&#30340;&#33410;&#28857;&#36890;&#36807;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20132;&#25442;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#26799;&#24230;&#65289;&#26469;&#20849;&#21516;&#35757;&#32451;&#20854;&#20010;&#20307;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#20004;&#31181;&#23041;&#32961;&#12290;&#39318;&#20808;&#65292;&#25308;&#21344;&#24237;&#24335;&#33410;&#28857;&#21487;&#20197;&#36890;&#36807;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#38169;&#35823;&#30340;&#26799;&#24230;&#65289;&#21333;&#29420;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#32531;&#35299;&#27492;&#31867;&#34892;&#20026;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#38750;&#32447;&#24615;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#20405;&#29359;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#26368;&#36817;&#30340;&#25915;&#20987;&#24050;&#32463;&#34920;&#26126;&#65292;&#20132;&#25442;&#65288;&#26410;&#21152;&#23494;&#30340;&#65289;&#26799;&#24230;&#20351;&#24471;&#19968;&#20010;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#33021;&#22815;&#24674;&#22797;&#20986;&#25152;&#26377;&#33410;&#28857;&#30340;&#25968;&#25454;&#12290;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#19968;&#31181;&#37329;&#26631;&#20934;&#23433;&#20840;&#21407;&#35821;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#38750;&#25308;&#21344;&#24237;&#22330;&#26223;&#20013;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. Howev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06221</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#38454;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#21644;&#35757;&#32451;&#39640;&#25928;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22238;&#24402;&#25214;&#21040;&#19968;&#20010;&#21021;&#22987;&#30340;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#36890;&#36807;&#20462;&#21098;&#19981;&#24517;&#35201;&#30340;&#36755;&#20837;&#26469;&#26368;&#23567;&#21270;&#39564;&#35777;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;Ho-Kashyap&#35268;&#21017;&#30340;&#26041;&#27861;&#25913;&#21892;&#26399;&#26395;&#36755;&#20986;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#36755;&#20986;&#21028;&#21035;&#24335;&#32553;&#25918;&#20026;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#25209;&#37327;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#20462;&#21098;&#19982;&#22686;&#38271;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#23558;&#36755;&#20837;&#21333;&#20803;&#32553;&#25918;&#20026;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#39304;&#36865;&#21040;MLP&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20013;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#26550;&#26500;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;d&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#21017;&#21644;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d
&lt;/p&gt;</description></item><item><title>ReCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#31354;&#38388;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03793</link><description>&lt;p&gt;
ReCLIP: &#20351;&#29992;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20248;&#21270;&#23545;&#27604;&#24615;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03793
&lt;/p&gt;
&lt;p&gt;
ReCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#31354;&#38388;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#30475;&#21040;&#20219;&#20309;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;76.3&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#36825;&#20026;&#35768;&#22810;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#23558;CLIP&#24212;&#29992;&#20110;&#19979;&#28216;&#30446;&#26631;&#39046;&#22495;&#26102;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#24046;&#24322;&#20197;&#21450;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReCLIP&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#28304;&#25968;&#25454;&#25110;&#30446;&#26631;&#26631;&#27880;&#25968;&#25454;&#12290;ReCLIP&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#25237;&#24433;&#31354;&#38388;&#26469;&#20943;&#36731;&#19981;&#23545;&#40784;&#30340;&#35270;&#35273;-&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#23398;&#20064;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#20266;&#26631;&#31614;&#37096;&#32626;&#20132;&#21449;&#27169;&#24577;&#33258;&#35757;&#32451;&#65292;&#20197;&#36845;&#20195;&#22320;&#26356;&#26032;&#35270;&#35273;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20248;&#21270;&#26631;&#31614;&#65292;&#24182;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ReCLIP&#33021;&#22815;&#20943;&#23569;&#24179;&#22343;...
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the avera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03443</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#32972;&#26223;&#19979;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25240;&#34935;&#38382;&#39064;&#12290;&#21442;&#25968;&#21270;&#26041;&#27861;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#27169;&#22411;&#32780;&#23548;&#33268;&#20559;&#24046;&#65292;&#32780;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#30001;&#20110;&#26041;&#24046;&#32780;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21028;&#21035;&#24335;&#30340;&#19981;&#33391;&#34892;&#20026;&#25233;&#21046;&#22120;&#65288;MIPS&#65289;&#26469;&#36890;&#36807;&#23545;&#21160;&#20316;&#30340;&#23884;&#20837;&#26469;&#20943;&#23567;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20351;&#20272;&#35745;&#22120;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIPS&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#8212;&#8212;&#36793;&#38469;&#21270;&#21452;&#37325;&#31283;&#20581;&#65288;MDR&#65289;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27604;MIPS&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#26159;&#26080;&#20559;&#30340;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;IPS&#30340;&#26041;&#24046;&#20943;&#23567;&#65292;&#36825;&#26159;MIPS&#30340;&#20027;&#35201;&#20248;&#21183;&#12290;&#32463;&#39564;&#23454;&#39564;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02668</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20027;&#23548;&#33539;&#24335;&#26159;&#20381;&#36182;&#20110;&#23436;&#20840;&#24102;&#27880;&#37322;&#30340;&#35757;&#32451;&#22270;&#20687;&#65292;&#36825;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20381;&#36182;&#24182;&#25552;&#39640;&#32467;&#26524;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20197;&#38480;&#21046;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#39062;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#26174;&#33879;&#25913;&#36827;&#24072;&#29983;&#33976;&#39311;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;(i)&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#25913;&#36827;&#20102;&#33976;&#39311;&#26041;&#27861;&#65292;(ii)&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23454;&#20363;&#20998;&#21106;&#26550;&#26500;&#12289;&#20027;&#24178;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#20043;&#21069;&#21482;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#26469;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#39044;&#28903;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#23548;&#24072;&#27169;&#22411;&#30340;&#25351;&#23548;&#22312;&#39044;&#28903;&#38454;&#27573;&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#30340;&#33976;&#39311;&#26041;&#27861;&#22312;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on 
&lt;/p&gt;</description></item><item><title>DualCoOp++&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#37319;&#29992;Evidence-guided Dual Context Optimization&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01890</link><description>&lt;p&gt;
DualCoOp++: &#38024;&#23545;&#26377;&#38480;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#36827;&#34892;&#24555;&#36895;&#26377;&#25928;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01890
&lt;/p&gt;
&lt;p&gt;
DualCoOp++&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#37319;&#29992;Evidence-guided Dual Context Optimization&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#26631;&#31614;&#22270;&#20687;&#35782;&#21035;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#21644;&#23454;&#38469;&#24847;&#20041;&#30340;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#35273;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20197;&#24357;&#34917;&#22270;&#20687;&#26631;&#31614;&#26377;&#38480;&#25152;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#22810;&#26631;&#31614;&#27880;&#37322;&#31232;&#32570;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29992;&#25968;&#30334;&#19975;&#20010;&#36741;&#21161;&#22270;&#20687;-&#25991;&#26412;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#20043;&#38388;&#30340;&#24378;&#22823;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Evidence-guided Dual Context Optimization (DualCoOp++)&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#26631;&#31614;&#21644;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#22312;DualCoOp++&#20013;&#65292;&#25105;&#20204;&#21333;&#29420;&#23545;&#30446;&#26631;&#31867;&#21035;&#30340;&#35777;&#25454;&#12289;&#27491;&#38754;&#21644;&#36127;&#38754;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#65292;&#20316;&#20026;&#35821;&#35328;&#36755;&#20837;&#65288;&#21363;&#25552;&#31034;&#65289;&#30340;&#21442;&#25968;&#32452;&#20214;&#12290;&#35777;&#25454;&#19978;&#19979;&#25991;&#26088;&#22312;&#21457;&#29616;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#25152;&#26377;&#35270;&#35273;&#20869;&#23481;&#65292;&#24182;&#20316;&#20026;&#32858;&#21512;&#27491;&#38754;&#21644;&#36127;&#38754;&#19978;&#19979;&#25991;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive 
&lt;/p&gt;</description></item><item><title>PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.16506</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;PELICAN
&lt;/p&gt;
&lt;p&gt;
Explainable Equivariant Neural Networks for Particle Physics: PELICAN. (arXiv:2307.16506v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16506
&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#25442;&#31561;&#21464;&#19988;&#27931;&#20262;&#20857;&#19981;&#21464;&#25110;&#21327;&#21464;&#30340;&#32858;&#21512;&#32593;&#32476;&#65292;&#26088;&#22312;&#20811;&#26381;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#38480;&#21046;&#12290;&#19982;&#35768;&#22810;&#20351;&#29992;&#38750;&#19987;&#29992;&#26550;&#26500;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PELICAN&#37319;&#29992;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20307;&#29616;&#20102;&#22797;&#26434;&#24230;&#38477;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#21644;&#24615;&#33021;&#25552;&#21319;&#31561;&#20248;&#21183;&#65292;&#32780;&#38750;&#20197;&#24222;&#22823;&#30340;&#21442;&#25968;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#22312;&#26631;&#35760;&#65288;&#20998;&#31867;&#65289;&#21644;&#37325;&#26500;&#65288;&#22238;&#24402;&#65289;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#30340;&#32972;&#26223;&#19979;&#23545;PELICAN&#31639;&#27861;&#26550;&#26500;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#22312;&#27931;&#20262;&#20857;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#24378;&#23376;&#26411;&#24577;&#30340;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#23558;PELICAN&#24212;&#29992;&#20110;&#35782;&#21035;&#22840;&#20811;-&#24341;&#21457;&#19982;&#33014;&#23376;-&#24341;&#21457;&#21943;&#27880;&#20197;&#21450;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. We present a comprehensive study of the PELICAN algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the Lorentz-boosted top-quark hadronic final state. We also extend the application of PELICAN to the tasks of identifying quark-initiated vs.~gluon-initiated jets, and a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.12971</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#30340;&#39044;&#27979;&#65306;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#27604;&#36739;&#20998;&#26512;&#26368;&#20808;&#36827;&#30340;&#20379;&#24212;&#38142;&#39044;&#27979;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#65292;&#21253;&#25324;&#38382;&#39064;&#35782;&#21035;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#20197;&#21450;&#39044;&#27979;&#23545;&#20154;&#21147;&#12289;&#24211;&#23384;&#21644;&#25972;&#20010;&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#26681;&#25454;&#20379;&#24212;&#38142;&#31574;&#30053;&#25910;&#38598;&#25968;&#25454;&#30340;&#38656;&#27714;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#26681;&#25454;&#21608;&#26399;&#25110;&#20379;&#24212;&#38142;&#30446;&#26631;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#27979;&#12290;&#25512;&#33616;&#20351;&#29992;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#21644;&#35823;&#24046;&#27979;&#37327;&#31995;&#32479;&#26469;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#36824;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#31649;&#29702;&#20915;&#31574;&#20381;&#36182;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#21442;&#25968;&#21644;&#25913;&#36827;&#36816;&#33829;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65292;&#21253;&#25324;&#38750;&#39640;&#26031;&#24615;&#12289;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#36827;&#34892;&#24494;&#23567;&#30772;&#22351;&#65292;&#21487;&#20197;&#24471;&#21040;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#65292;&#36825;&#31181;&#23637;&#24320;&#26041;&#27861;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;$1/N$&#23637;&#24320;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#20851;&#32852;&#20989;&#25968;&#21487;&#20197;&#31995;&#32479;&#22320;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.03223</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65306;&#38750;&#39640;&#26031;&#24615;&#65292;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;
&lt;/p&gt;
&lt;p&gt;
Neural Network Field Theories: Non-Gaussianity, Actions, and Locality. (arXiv:2307.03223v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65292;&#21253;&#25324;&#38750;&#39640;&#26031;&#24615;&#12289;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#36827;&#34892;&#24494;&#23567;&#30772;&#22351;&#65292;&#21487;&#20197;&#24471;&#21040;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#65292;&#36825;&#31181;&#23637;&#24320;&#26041;&#27861;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;$1/N$&#23637;&#24320;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#20851;&#32852;&#20989;&#25968;&#21487;&#20197;&#31995;&#32479;&#22320;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#35770;&#20013;&#30340;&#36335;&#24452;&#31215;&#20998;&#27979;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#25551;&#36848;&#30340;&#26159;&#20989;&#25968;&#20998;&#24067;&#12290;&#24403;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#36866;&#29992;&#20110;&#26080;&#38480;&#23485;&#24230;&#65288;&#26080;&#38480;$N$&#65289;&#26497;&#38480;&#26102;&#65292;&#32593;&#32476;&#30340;&#38598;&#21512;&#23545;&#24212;&#20110;&#33258;&#30001;&#22330;&#29702;&#35770;&#12290;&#34429;&#28982;&#22312;$1/N$&#30340;&#23637;&#24320;&#20013;&#23545;&#24212;&#20110;&#22330;&#35770;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#20854;&#20182;&#23637;&#24320;&#65292;&#22914;&#32593;&#32476;&#21442;&#25968;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24494;&#23567;&#30772;&#22351;&#65292;&#20063;&#21487;&#20197;&#23548;&#33268;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#12290;&#36825;&#20123;&#20854;&#20182;&#30340;&#23637;&#24320;&#21487;&#20197;&#27604;$1/N$&#23637;&#24320;&#26356;&#20855;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#32473;&#23450;&#22330;&#35770;&#30340;&#20851;&#32852;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#36153;&#26364;&#22270;&#35268;&#21017;&#65292;&#39030;&#28857;&#20026;&#20851;&#32852;&#20989;&#25968;&#65292;&#31995;&#32479;&#22320;&#25353;&#29031;&#23637;&#24320;&#21442;&#25968;&#36880;&#38454;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;Edgeworth&#23637;&#24320;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#23548;&#20986;&#20316;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the co
&lt;/p&gt;</description></item><item><title>AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01146</link><description>&lt;p&gt;
AVSegFormer: &#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01146
&lt;/p&gt;
&lt;p&gt;
AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#32467;&#21512;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#23450;&#20301;&#21644;&#20998;&#21106;&#32473;&#23450;&#35270;&#39057;&#20013;&#30340;&#26377;&#22768;&#23545;&#35937;&#12290;&#36825;&#20010;&#20219;&#21153;&#39318;&#27425;&#35201;&#27714;&#22312;&#20687;&#32032;&#32423;&#21035;&#23545;&#38899;&#39057;&#39537;&#21160;&#30340;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AVSegFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;AVS&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#24863;&#20852;&#36259;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30456;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#21644;&#25233;&#21046;&#26080;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#26469;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#65292;&#40723;&#21169;&#32593;&#32476;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha
&lt;/p&gt;</description></item><item><title>AutoST&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.00293</link><description>&lt;p&gt;
AutoST&#65306;&#26080;&#38656;&#35757;&#32451;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00293
&lt;/p&gt;
&lt;p&gt;
AutoST&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#22240;&#21516;&#26102;&#20855;&#22791;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#33021;&#25928;&#21644;&#21464;&#21387;&#22120;&#30340;&#39640;&#23481;&#37327;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#25512;&#23548;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#26550;&#26500;&#24046;&#36317;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#21450;&#20854;ANN&#23545;&#24212;&#29289;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#25163;&#21160;&#36807;&#31243;&#25110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;&#26550;&#26500;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#35201;&#20040;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoST&#65292;&#19968;&#31181;&#29992;&#20110;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;NAS&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#30340;&#33033;&#20914;&#21464;&#21387;&#22120;&#26550;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;NAS&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#20316;&#20026;&#19968;&#31181;&#25351;&#23548;&#21487;&#34892;&#24615;&#30340;&#24230;&#37327;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from ANNs, exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Traditional approaches to discovering optimal architectures primarily rely on either manual procedures, which are time-consuming, or Neural Architecture Search (NAS) methods, which are usually expensive in terms of memory footprints and computation time. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance and energy-efficient Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992; Transformer &#27169;&#22411;&#22312;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#36890;&#36807; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640; 2 &#20493;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.10007</link><description>&lt;p&gt;
&#20855;&#26377;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Learning with Sensorimotor Pre-training. (arXiv:2306.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992; Transformer &#27169;&#22411;&#22312;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#36890;&#36807; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640; 2 &#20493;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026; RPT&#65292;&#26159;&#19968;&#31181; Transformer&#65292;&#23427;&#23545;&#24863;&#35273;&#36816;&#21160;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#25805;&#20316;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#30456;&#26426;&#22270;&#20687;&#12289;&#26412;&#20307;&#24863;&#35273;&#26426;&#22120;&#20154;&#29366;&#24577;&#21644;&#36807;&#21435;&#30340;&#21160;&#20316;&#65292;&#25105;&#20204;&#23558;&#20132;&#38169;&#30340;&#24207;&#21015;&#32534;&#30721;&#20026;&#20196;&#29260;&#65292;&#25513;&#27169;&#20986;&#38543;&#26426;&#23376;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#25513;&#27169;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#20869;&#23481;&#65292;&#23427;&#24050;&#32463;&#33719;&#24471;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#20854;&#34892;&#21160;&#30340;&#29289;&#29702;&#19990;&#30028;&#30340;&#33391;&#22909;&#27169;&#22411;&#12290;RPT &#30340;&#35774;&#35745;&#26159;&#22312;&#28508;&#22312;&#30340;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#39044;&#27979;&#21464;&#24471;&#21487;&#34892;&#65292;&#33021;&#22815;&#23454;&#29616; 10 &#20493;&#30340;&#27169;&#22411;&#25193;&#23637;&#65292;&#24182;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#27599;&#31186; 10 &#27425;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#36816;&#21160;&#35268;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25235;&#21462;&#31639;&#27861;&#65292;&#25910;&#38598;&#20102; 9 &#20010;&#26376;&#20869;&#30340; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#22987;&#32456;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#23548;&#33268; 2 &#20493;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and past actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content. We hypothesize that if the robot can predict the missing content it has acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to 10x larger models, and 10 Hz inference on a real robot. To evaluate our approach, we collect a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and model-based grasping algorithms. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task,
&lt;/p&gt;</description></item><item><title>GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03480</link><description>&lt;p&gt;
GSHOT: &#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03480
&lt;/p&gt;
&lt;p&gt;
GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#29983;&#25104;&#24314;&#27169;&#22240;&#20854;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#38544;&#34255;&#22270;&#20998;&#24067;&#30340;&#24778;&#20154;&#33021;&#21147;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26368;&#21021;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#19968;&#26679;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#21457;&#29616;&#31561;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#19981;&#24635;&#26159;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23569;&#26679;&#26412;&#22270;&#29983;&#25104;&#24314;&#27169;&#36825;&#19968;&#36804;&#20170;&#26410;&#26366;&#25506;&#32034;&#30340;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GSHOT&#65292;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#12290;GSHOT&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#20123;&#20808;&#21069;&#30340;&#32463;&#39564;&#65292;GSHOT&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#25105;&#35843;&#25972;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LFlows&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#21644;&#21487;&#36870;&#30340;&#21464;&#25442;&#65292;&#22312;&#26102;&#38388;&#19978;&#35268;&#23450;&#21442;&#25968;&#21270;&#30340;&#24494;&#20998;&#21516;&#32986;&#21464;&#25442;&#26469;&#23545;&#22522;&#30784;&#23494;&#24230;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#36830;&#32493;&#22320;&#24314;&#27169;&#27969;&#20307;&#23494;&#24230;&#21644;&#36895;&#24230;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#20248;&#21183;&#22312;&#20110;&#36895;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#24635;&#26159;&#19982;&#23494;&#24230;&#20445;&#25345;&#19968;&#33268;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20063;&#26080;&#38656;&#20351;&#29992;&#24809;&#32602;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16846</link><description>&lt;p&gt;
&#25289;&#26684;&#26391;&#26085;&#27969;&#32593;&#32476;&#29992;&#20110;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Lagrangian Flow Networks for Conservation Laws. (arXiv:2305.16846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LFlows&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#21644;&#21487;&#36870;&#30340;&#21464;&#25442;&#65292;&#22312;&#26102;&#38388;&#19978;&#35268;&#23450;&#21442;&#25968;&#21270;&#30340;&#24494;&#20998;&#21516;&#32986;&#21464;&#25442;&#26469;&#23545;&#22522;&#30784;&#23494;&#24230;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#36830;&#32493;&#22320;&#24314;&#27169;&#27969;&#20307;&#23494;&#24230;&#21644;&#36895;&#24230;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#20248;&#21183;&#22312;&#20110;&#36895;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#24635;&#26159;&#19982;&#23494;&#24230;&#20445;&#25345;&#19968;&#33268;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20063;&#26080;&#38656;&#20351;&#29992;&#24809;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25289;&#26684;&#26391;&#26085;&#27969;&#32593;&#32476;&#65288;LFlows&#65289;&#65292;&#29992;&#20110;&#36830;&#32493;&#22320;&#24314;&#27169;&#27969;&#20307;&#23494;&#24230;&#21644;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;LFlows&#22522;&#20110;&#36830;&#32493;&#26041;&#31243;&#30340;&#35299;&#65292;&#20854;&#20013;&#36830;&#32493;&#26041;&#31243;&#26159;&#25551;&#36848;&#19981;&#21516;&#24418;&#24335;&#30340;&#36136;&#37327;&#23432;&#24658;&#24615;&#36136;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#24605;&#36335;&#65306;&#36830;&#32493;&#26041;&#31243;&#30340;&#35299;&#21487;&#20197;&#36890;&#36807;&#21487;&#24494;&#21644;&#21487;&#36870;&#30340;&#21464;&#25442;&#34920;&#31034;&#20026;&#26102;&#38388;&#20381;&#36182;&#30340;&#23494;&#24230;&#21464;&#25442;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#38388;&#19978;&#35268;&#23450;&#21442;&#25968;&#21270;&#30340;&#24494;&#20998;&#21516;&#32986;&#21464;&#25442;&#26469;&#23545;&#22522;&#30784;&#23494;&#24230;&#36827;&#34892;&#36716;&#25442;&#20197;&#24314;&#27169;&#27969;&#20307;&#23494;&#24230;&#12290;&#19982;&#20381;&#36182;&#20110;Neural-ODE&#25110;PINNs&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20851;&#38190;&#30340;&#20248;&#21183;&#22312;&#20110;&#36895;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#22987;&#32456;&#19982;&#23494;&#24230;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20063;&#26080;&#38656;&#20351;&#29992;&#24809;&#32602;&#26041;&#27861;&#26469;&#23454;&#26045;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25289;&#26684;&#26391;&#26085;&#27969;&#32593;&#32476;&#22312;&#21512;&#25104;&#23494;&#24230;&#25968;&#25454;&#19978;&#26174;&#31034;&#20986;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lagrangian Flow Networks (LFlows) for modeling fluid densities and velocities continuously in space and time. The proposed LFlows satisfy by construction the continuity equation, a PDE describing mass conservation in its differentiable form. Our model is based on the insight that solutions to the continuity equation can be expressed as time-dependent density transformations via differentiable and invertible maps. This follows from classical theory of existence and uniqueness of Lagrangian flows for smooth vector fields. Hence, we model fluid densities by transforming a base density with parameterized diffeomorphisms conditioned on time. The key benefit compared to methods relying on Neural-ODE or PINNs is that the analytic expression of the velocity is always consistent with the density. Furthermore, there is no need for expensive numerical solvers, nor for enforcing the PDE with penalty methods. Lagrangian Flow Networks show improved predictive accuracy on synthetic densi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02506</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#35299;&#23494;&#24230;&#30340;&#23383;&#31526;&#20018;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#25193;&#23637;&#23450;&#21521;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#37117;&#23450;&#20041;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#26377;&#20851;&#27010;&#29575;&#26144;&#23556;&#30340;&#39532;&#23572;&#21487;&#22827;&#33539;&#30068;&#30340;&#24037;&#20316;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33539;&#30068;&#65292;&#20854;&#24577;&#23556;&#23558;&#20998;&#21035;&#30001;&#27599;&#20010;&#26679;&#26412;&#31354;&#38388;&#20998;&#35299;&#30340;&#32852;&#21512;&#23494;&#24230;&#19982;&#20174;&#26679;&#26412;&#21040;&#36820;&#22238;&#20540;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#32452;&#21512;&#12290;&#36825;&#26159;&#36808;&#21521;&#26368;&#36817;&#30340;&#33539;&#30068;&#35770;&#27010;&#29575;&#27979;&#24230;&#25551;&#36848;&#21644;&#36890;&#24120;&#22312;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#20998;&#35299;&#23494;&#24230;&#30340;&#25805;&#20316;&#23450;&#20041;&#20043;&#38388;&#30340;&#32553;&#23567;&#24046;&#36317;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21453;&#21521;&#39044;&#27979;&#23431;&#23449;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;</title><link>http://arxiv.org/abs/2303.13056</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#23431;&#23449;&#30340;&#21021;&#22987;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Predicting the Initial Conditions of the Universe using Deep Learning. (arXiv:2303.13056v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21453;&#21521;&#39044;&#27979;&#23431;&#23449;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#23548;&#33268;&#24403;&#21069;&#23431;&#23449;&#29366;&#24577;&#30340;&#21021;&#22987;&#26465;&#20214;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25628;&#32034;&#19968;&#20010;&#24040;&#22823;&#30340;&#21021;&#22987;&#26465;&#20214;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#35832;&#22914;N-&#20307;&#27169;&#25311;&#31561;&#24037;&#20855;&#24314;&#27169;&#23427;&#20204;&#30340;&#28436;&#21270;&#65292;&#36825;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26367;&#20195;&#24314;&#27169;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;N-&#20307;&#27169;&#25311;&#30340;&#32447;&#24615;&#36755;&#20837;&#19982;&#32418;&#31227;&#20026;&#38646;&#26102;&#30340;&#26368;&#32456;&#38750;&#32447;&#24615;&#20301;&#31227;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#21521;&#21069;&#30340;&#27169;&#25311;&#12290;&#20294;&#26159;&#65292;&#36825;&#24182;&#19981;&#33021;&#20943;&#23569;&#21021;&#22987;&#26465;&#20214;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#34987;&#29992;&#20110;&#21453;&#21521;&#26144;&#23556;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;V-Net&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#22312;&#32473;&#23450;&#31995;&#32479;&#24403;&#21069;&#26102;&#38388;&#30340;&#38750;&#32447;&#24615;&#20301;&#31227;&#21644;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36755;&#20986;N-&#20307;&#31995;&#32479;&#30340;&#32447;&#24615;&#20301;&#31227;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#21021;&#22987;&#32447;&#24615;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the initial conditions that led to the current state of the universe is challenging because it involves searching over a vast input space of initial conditions, along with modeling their evolution via tools such as N-body simulations which are computationally expensive. Deep learning has emerged as an alternate modeling tool that can learn the mapping between the linear input of an N-body simulation and the final nonlinear displacements at redshift zero, which can significantly accelerate the forward modeling. However, this does not help reduce the search space for initial conditions. In this paper, we demonstrate for the first time that a deep learning model can be trained for the reverse mapping. We train a V-Net based convolutional neural network, which outputs the linear displacement of an N-body system, given the current time nonlinear displacement and the cosmological parameters of the system. We demonstrate that this neural network accurately recovers the initial linear 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;softmax&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#24182;&#23545;&#20854;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.04416</link><description>&lt;p&gt;
&#36890;&#36807;Softmax&#36924;&#36817;&#23454;&#29616;&#21160;&#24577;&#26368;&#20248;&#31574;&#30053;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference on Optimal Dynamic Policies via Softmax Approximation. (arXiv:2303.04416v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;softmax&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#24182;&#23545;&#20854;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#31574;&#30053;&#26159;&#21160;&#24577;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#22240;&#26524;&#25512;&#26029;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#38382;&#39064;&#34987;&#31216;&#20026;&#20272;&#35745;&#26368;&#20248;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#12290;&#21363;&#20351;&#23384;&#22312;&#22823;&#37327;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#20272;&#35745;&#26368;&#20248;&#31574;&#30053;&#30340;&#20215;&#20540;&#21450;&#20854;&#30456;&#20851;&#30340;&#32467;&#26500;&#21442;&#25968;&#26412;&#36136;&#19978;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#26410;&#30693;&#37327;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#21487;&#24494;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#20122;&#26679;&#26412;&#26041;&#27861;&#65292;&#20294;&#21487;&#33021;&#20250;&#38477;&#20302;&#20272;&#35745;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#20010;&#36866;&#24403;&#22686;&#38271;&#30340;&#28201;&#24230;&#21442;&#25968;&#65292;&#26368;&#20248;&#27835;&#30103;&#26041;&#26696;&#30340;&#31616;&#21333;softmax&#36924;&#36817;&#21487;&#20197;&#23454;&#29616;&#23545;&#30495;&#27491;&#26368;&#20248;&#26041;&#26696;&#30340;&#26377;&#25928;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#29992;&#20110;&#20004;&#26399;&#30340;&#26368;&#20248;&#21160;&#24577;&#26041;&#26696;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#30452;&#25509;&#25512;&#24191;&#21040;&#26377;&#38480;&#30340;&#26102;&#38388;&#27573;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#20102;&#21322;&#21442;&#25968;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating optimal dynamic policies from offline data is a fundamental problem in dynamic decision making. In the context of causal inference, the problem is known as estimating the optimal dynamic treatment regime. Even though there exists a plethora of methods for estimation, constructing confidence intervals for the value of the optimal regime and structural parameters associated with it is inherently harder, as it involves non-linear and non-differentiable functionals of un-known quantities that need to be estimated. Prior work resorted to sub-sample approaches that can deteriorate the quality of the estimate. We show that a simple soft-max approximation to the optimal treatment regime, for an appropriately fast growing temperature parameter, can achieve valid inference on the truly optimal regime. We illustrate our result for a two-period optimal dynamic regime, though our approach should directly extend to the finite horizon case. Our work combines techniques from semi-parametric
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01903</link><description>&lt;p&gt;
&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#36229;&#20986;&#22270;&#20687;&#33539;&#22260;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#26816;&#32034;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#36825;&#32463;&#24120;&#20250;&#24341;&#20837;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-3&#65289;&#20316;&#20026;&#38544;&#21547;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#22238;&#31572;&#25152;&#38656;&#30340;&#24517;&#35201;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36824;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;GPT-3&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#36755;&#20837;&#20449;&#24687;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prophet&#8212;&#8212;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#21551;&#21457;&#24335;&#26041;&#24335;&#65292;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29305;&#23450;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#31572;&#26696;&#21551;&#21457;&#24335;&#65306;&#31572;&#26696;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.01141</link><description>&lt;p&gt;
DeepSaDe: &#23398;&#20064;&#30830;&#20445;&#28385;&#36275;&#39046;&#22495;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#34892;&#20026;&#24517;&#39035;&#26159;&#23433;&#20840;&#30340;&#12290;&#24403;&#21069;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32422;&#26463;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#65288;&#21363;&#20351;&#22312;&#26410;&#30475;&#36807;&#30340;&#25968;&#25454;&#19978;&#65289;&#65292;&#25110;&#32773;&#23427;&#20204;&#23545;&#21487;&#24378;&#21046;&#25191;&#34892;&#30340;&#32422;&#26463;&#31867;&#22411;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#32422;&#26463;&#24182;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#24448;&#23558;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#35270;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#22686;&#21152;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#26032;&#20803;&#32032;&#65306;&#32593;&#32476;&#23618;&#19978;&#30340;&#32422;&#26463;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00919</link><description>&lt;p&gt;
QCM-SGM+: &#22522;&#20110;&#24471;&#20998;&#29983;&#25104;&#27169;&#22411;&#30340;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models. (arXiv:2302.00919v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#21387;&#32553;&#24863;&#30693;&#36807;&#31243;&#20013;&#65292;&#33719;&#24471;&#30340;&#27979;&#37327;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22312;&#20256;&#36755;&#25110;&#23384;&#20648;&#21069;&#38480;&#21046;&#20026;&#26377;&#38480;&#27604;&#29305;&#30340;&#37327;&#21270;&#12290;&#36825;&#20010;&#38750;&#32447;&#24615;&#37327;&#21270;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#24674;&#22797;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#24230;&#31895;&#31961;&#30340;&#37327;&#21270;&#22914;1&#27604;&#29305;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;QCS-SGM&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#12290;&#30001;&#20110;SGM&#22312;&#25429;&#25417;&#33258;&#28982;&#20449;&#21495;&#30340;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;QCS-SGM&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;QCS&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;QCS-SGM&#23616;&#38480;&#20110;(&#36817;&#20284;)&#34892;&#27491;&#20132;&#20256;&#24863;&#30697;&#38453;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20998;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QCS-SGM+&#30340;&#39640;&#32423;&#21464;&#20307;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20284;&#28982;&#20998;&#25968;&#35745;&#31639;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35266;&#28857;&#65292;&#20854;&#20013;&#35745;&#31639;&#26399;&#26395;&#24471;&#20998;&#20197;&#35299;&#20915;&#27599;&#20010;&#27979;&#37327;&#30340;&#32467;&#26500;&#38750;&#27491;&#20132;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein an expectatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#23545;&#20998;&#24067;&#24335;&#38750;&#20984;&#30446;&#26631;&#36827;&#34892;&#20102;&#38543;&#26426;&#20248;&#21270;&#65292;&#24314;&#31435;&#20102;&#20165;&#28385;&#36275;&#38543;&#26426;&#26799;&#24230;&#28201;&#21644;&#26465;&#20214;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.12677</link><description>&lt;p&gt;
&#36890;&#29992;&#26041;&#24046;&#26465;&#20214;&#19979;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributed Stochastic Optimization under a General Variance Condition. (arXiv:2301.12677v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12677
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#23545;&#20998;&#24067;&#24335;&#38750;&#20984;&#30446;&#26631;&#36827;&#34892;&#20102;&#38543;&#26426;&#20248;&#21270;&#65292;&#24314;&#31435;&#20102;&#20165;&#28385;&#36275;&#38543;&#26426;&#26799;&#24230;&#28201;&#21644;&#26465;&#20214;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#19968;&#33324;&#23454;&#38469;&#38382;&#39064;&#30340;&#31639;&#27861;&#24456;&#22810;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#26576;&#20123;&#26377;&#30028;&#26465;&#20214;&#65292;&#20174;&#22343;&#21248;&#26377;&#30028;&#24615;&#21040;&#25918;&#26494;&#22686;&#38271;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#20195;&#29702;&#20043;&#38388;&#34920;&#24449;&#25968;&#25454;&#24322;&#36136;&#24615;&#21450;&#20854;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20986;&#20110;&#36825;&#26679;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#24179;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#38543;&#26426;&#26799;&#24230;&#20165;&#28385;&#36275;&#28201;&#21644;&#26041;&#24046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;&#22312;&#27492;&#26465;&#20214;&#19979;&#65292;&#36824;&#24314;&#31435;&#20102;&#25509;&#36817;&#30830;&#23450;&#30340;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#24577;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic optimization has drawn great attention recently due to its effectiveness in solving large-scale machine learning problems. Though numerous algorithms have been proposed and successfully applied to general practical problems, their theoretical guarantees mainly rely on certain boundedness conditions on the stochastic gradients, varying from uniform boundedness to the relaxed growth condition. In addition, how to characterize the data heterogeneity among the agents and its impacts on the algorithmic performance remains challenging. In light of such motivations, we revisit the classical Federated Averaging (FedAvg) algorithm for solving the distributed stochastic optimization problem and establish the convergence results under only a mild variance condition on the stochastic gradients for smooth nonconvex objective functions. Almost sure convergence to a stationary point is also established under the condition. Moreover, we discuss a more informative measurement for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12562</link><description>&lt;p&gt;
&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31616;&#21270;&#20197;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Simplifying Subgraph Representation Learning for Scalable Link Prediction. (arXiv:2301.12562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23558;&#38142;&#25509;&#39044;&#27979;&#36716;&#21270;&#20026;&#22312;&#38142;&#25509;&#21608;&#22260;&#23376;&#22270;&#19978;&#30340;&#22270;&#20998;&#31867;&#26469;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#24182;&#19988;&#30001;&#20110;&#23376;&#22270;&#27700;&#24179;&#25805;&#20316;&#30340;&#20195;&#20215;&#32780;&#19981;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31867;&#65292;&#31216;&#20026;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;S3GRL&#31616;&#21270;&#20102;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#20316;&#20026;&#21487;&#25193;&#23637;&#24615;&#26694;&#26550;&#65292;S3GRL&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#36816;&#31639;&#31526;&#26469;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;S3GRL&#23454;&#20363;&#65292;&#24182;&#22312;&#23567;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call Scalable Simplified SGRL (S3GRL). Aimed at faster training and inference, S3GRL simplifies the message passing and aggregation operations in each link's subgraph. S3GRL, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of S3GRL and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed S3GRL models scale up SGRLs without significant performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#34394;&#25311;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#20010;VN&#20849;&#23384;&#21487;&#33021;&#23548;&#33268;&#38548;&#31163;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.14158</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Isolation-Aware Online Virtual Network Embedding via Deep Reinforcement Learning. (arXiv:2211.14158v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#34394;&#25311;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#20010;VN&#20849;&#23384;&#21487;&#33021;&#23548;&#33268;&#38548;&#31163;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21270;&#25216;&#26415;&#26159;&#29616;&#20195;ICT&#22522;&#30784;&#35774;&#26045;&#30340;&#22522;&#30784;&#65292;&#23427;&#20351;&#26381;&#21153;&#25552;&#20379;&#21830;&#33021;&#22815;&#21019;&#24314;&#25903;&#25345;&#21508;&#31181;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#19987;&#29992;&#34394;&#25311;&#32593;&#32476;&#65288;VN&#65289;&#12290;&#36825;&#20123;VN&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#20005;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#34394;&#25311;&#21270;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;VN&#21487;&#33021;&#20849;&#23384;&#20110;&#21516;&#19968;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#19978;&#65292;&#22914;&#26524;&#38548;&#31163;&#19981;&#24403;&#65292;&#21017;&#21487;&#33021;&#30456;&#20114;&#24178;&#25200;&#25110;&#25552;&#20379;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12290;&#21069;&#32773;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#21518;&#32773;&#20250;&#30772;&#22351;VN&#30340;&#23433;&#20840;&#24615;&#12290;&#24403;&#29305;&#23450;VN&#36829;&#21453;&#38548;&#31163;&#35201;&#27714;&#26102;&#65292;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21830;&#30340;&#26381;&#21153;&#20445;&#38556;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#35299;&#20915;&#38548;&#31163;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#22312;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#65288;VNE&#65289;&#26399;&#38388;&#36827;&#34892;&#38548;&#31163;&#65292;&#21363;&#23558;VN&#20998;&#37197;&#21040;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtualization technologies are the foundation of modern ICT infrastructure, enabling service providers to create dedicated virtual networks (VNs) that can support a wide range of smart city applications. These VNs continuously generate massive amounts of data, necessitating stringent reliability and security requirements. In virtualized network environments, however, multiple VNs may coexist on the same physical infrastructure and, if not properly isolated, may interfere with or provide unauthorized access to one another. The former causes performance degradation, while the latter compromises the security of VNs. Service assurance for infrastructure providers becomes significantly more complicated when a specific VN violates the isolation requirement.  In an effort to address the isolation issue, this paper proposes isolation during virtual network embedding (VNE), the procedure of allocating VNs onto physical infrastructure. We define a simple abstracted concept of isolation levels t
&lt;/p&gt;</description></item></channel></rss>