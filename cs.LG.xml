<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01727</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36882;&#24402;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24191;&#25773;
&lt;/p&gt;
&lt;p&gt;
Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#36890;&#36807;&#20174;&#29616;&#26377;&#33410;&#28857;&#20013;&#22343;&#21248;&#38543;&#26426;&#36873;&#25321;$k$&#20010;&#29238;&#33410;&#28857;&#26469;&#25512;&#24191;&#22343;&#21248;&#30340;&#38543;&#26426;&#36882;&#24402;&#26641;&#12290;&#23427;&#20197;$k$&#20010;&#8220;&#26681;&#8221;&#24320;&#22987;&#12290;&#27599;&#20010;$k$&#20010;&#26681;&#33410;&#28857;&#37117;&#34987;&#20998;&#37197;&#19968;&#20010;&#20301;&#12290;&#36825;&#20123;&#20301;&#36890;&#36807;&#19968;&#20010;&#22024;&#26434;&#30340;&#20449;&#36947;&#20256;&#25773;&#12290;&#27599;&#20010;&#29238;&#33410;&#28857;&#30340;&#20301;&#37117;&#20197;&#27010;&#29575;$p$&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#36827;&#34892;&#22823;&#22810;&#25968;&#34920;&#20915;&#12290;&#24403;&#25152;&#26377;&#33410;&#28857;&#37117;&#25509;&#25910;&#21040;&#23427;&#20204;&#30340;&#20301;&#21518;&#65292;$k$-dag&#34987;&#26174;&#31034;&#65292;&#19981;&#35782;&#21035;&#26681;&#33410;&#28857;&#12290;&#30446;&#26631;&#26159;&#20272;&#35745;&#25152;&#26377;&#26681;&#33410;&#28857;&#20013;&#30340;&#22823;&#22810;&#25968;&#20301;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;$p$&#30340;&#38408;&#20540;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#20110;$k$&#30340;&#20989;&#25968;&#65292;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#30340;&#22823;&#22810;&#25968;&#35268;&#21017;&#20135;&#29983;&#38169;&#35823;$c+o(1)$&#30340;&#27010;&#29575;&#23567;&#20110;$1/2$&#12290;&#22312;&#38408;&#20540;&#20197;&#19978;&#65292;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#38169;&#35823;&#27010;&#29575;&#20026;$1/2+o(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c&lt;1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2306.01726</link><description>&lt;p&gt;
&#35780;&#20272;&#26377;&#22122;&#22768;&#21028;&#21035;&#22120;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27969;&#24335;&#31639;&#27861; -- &#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#35780;&#20272;&#20316;&#20026;&#27969;&#24335;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;: &#32473;&#23450;&#19968;&#20010;&#20998;&#31867;&#22120;&#20915;&#31574;&#30340;&#25968;&#25454;&#33609;&#22270;&#65292;&#20272;&#35745;&#26631;&#31614;&#30340;&#30495;&#23454;&#27969;&#34892;&#24230;&#20197;&#21450;&#27599;&#20010;&#20998;&#31867;&#22120;&#23545;&#23427;&#20204;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#31181;&#23436;&#20840;&#20195;&#25968;&#21270;&#30340;&#35780;&#20272;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20004;&#31181;&#35780;&#20272;&#22120;&#37117;&#22522;&#20110;&#20998;&#31867;&#22120;&#20135;&#29983;&#29420;&#31435;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#22810;&#25968;&#25237;&#31080;&#30340;&#12290;&#32780;&#31532;&#20108;&#31181;&#21017;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#24182;&#34987;&#20445;&#35777;&#26159;&#27491;&#30830;&#30340;&#12290;&#20294;&#26159;&#22914;&#20309;&#30830;&#20445;&#20998;&#31867;&#22120;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#27979;&#35797;&#20013;&#26159;&#29420;&#31435;&#30340;&#21602;&#65311;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#26469;&#32531;&#35299;&#36825;&#20010;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#25968;&#25925;&#38556;&#27169;&#24335;&#26469;&#25298;&#32477;&#22826;&#30456;&#20851;&#30340;&#35780;&#20272;&#38598;&#21512;&#65292;&#20351;&#29992; \texttt{adult}&#65292;\texttt{mushroom} &#21644; \texttt{two-norm} &#25968;&#25454;&#38598;&#23545;&#19968;&#32452;&#20960;&#20046;&#26080;&#35823;&#24046;&#19977;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#35777;&#25628;&#32034;&#12290;&#36825;&#20123;&#25628;&#32034;&#36890;&#36807;&#26500;&#24314;&#35780;&#20272;&#31354;&#38388;&#20013;&#30340;&#34920;&#38754;&#26469;&#36827;&#34892;&#31934;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fiedler&#25968;&#30340;&#22270;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#23436;&#25972;&#30340;&#22270;&#26680;&#20013;&#21024;&#38500;&#36793;&#32536;&#65292;&#20197;&#38477;&#20302;GCN&#35757;&#32451;&#21644;&#25191;&#34892;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20892;&#19994;&#20135;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01725</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31232;&#30095;&#21270;&#30340;GCN&#31639;&#27861;&#29992;&#20110;&#26368;&#20248;&#20892;&#20316;&#29289;&#25910;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Sparsification for GCN Towards Optimal Crop Yield Predictions. (arXiv:2306.01725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fiedler&#25968;&#30340;&#22270;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#23436;&#25972;&#30340;&#22270;&#26680;&#20013;&#21024;&#38500;&#36793;&#32536;&#65292;&#20197;&#38477;&#20302;GCN&#35757;&#32451;&#21644;&#25191;&#34892;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20892;&#19994;&#20135;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#23398;&#39046;&#22495;&#65292;&#39044;&#27979;&#27599;&#20010;&#30000;&#22320;/&#21439;&#30340;&#20316;&#29289;&#20135;&#37327;&#23545;&#20110;&#20892;&#27665;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#20026;&#19979;&#19968;&#36718;&#20316;&#29289;&#31181;&#26893;&#35745;&#21010;&#20316;&#20986;&#23433;&#25490;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fiedler&#25968;&#30340;&#22270;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#23436;&#25972;&#30340;&#22270;&#26680;&#20013;&#21024;&#38500;&#36793;&#32536;&#65292;&#20197;&#38477;&#20302;GCN&#35757;&#32451;&#21644;&#25191;&#34892;&#30340;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;Fiedler&#30340;&#26041;&#27861;&#33021;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;GCN&#24615;&#33021;&#30340;&#31232;&#30095;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In agronomics, predicting crop yield at a per field/county granularity is important for farmers to minimize uncertainty and plan seeding for the next crop cycle. While state-of-the-art prediction techniques employ graph convolutional nets (GCN) to predict future crop yields given relevant features and crop yields of previous years, a dense underlying graph kernel requires long training and execution time. In this paper, we propose a graph sparsification method based on the Fiedler number to remove edges from a complete graph kernel, in order to lower the complexity of GCN training/execution. Specifically, we first show that greedily removing an edge at a time that induces the minimal change in the second eigenvalue leads to a sparse graph with good GCN performance. We then propose a fast method to choose an edge for removal per iteration based on an eigenvalue perturbation theorem. Experiments show that our Fiedler-based method produces a sparse graph with good GCN performance compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#35813;&#24230;&#37327;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#35782;&#21035;&#20986;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01710</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#27979;&#24230;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Measure of Relative Uncertainty for Misclassification Detection. (arXiv:2306.01710v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#35813;&#24230;&#37327;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#35782;&#21035;&#20986;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20998;&#31867;&#26816;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#24230;&#22914;&#39321;&#20892;&#29109;&#24182;&#19981;&#33021;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25512;&#26029;&#27169;&#22411;&#39044;&#27979;&#30340;&#23454;&#38469;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#30456;&#23545;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#35823;&#20998;&#31867;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#36719;&#39044;&#27979;&#30340;&#20998;&#24067;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21487;&#20197;&#22522;&#20110;&#39044;&#27979;&#30340;&#31867;&#27010;&#29575;&#26631;&#35782;&#34987;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#65292;&#19982;&#35823;&#20998;&#31867;&#23454;&#20363;&#23545;&#24212;&#30340;&#36719;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#24456;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#39321;&#20892;&#29109;&#21487;&#33021;&#24456;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23454;&#35777;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35823;&#20998;&#31867;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of relative uncertainty to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions that correspond to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65292;&#21457;&#29616;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#27604;&#65292;&#20854;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.01706</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?. (arXiv:2306.01706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65292;&#21457;&#29616;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#27604;&#65292;&#20854;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#22312;&#27809;&#26377;&#30446;&#26631;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#65306;&#36755;&#20837;&#32423;&#21035;&#23545;&#40784;&#65288;&#22914;&#29983;&#25104;&#24314;&#27169;&#21644;&#39118;&#26684;&#21270;&#65289;&#21644;&#29305;&#24449;&#32423;&#21035;&#23545;&#40784;&#65288;&#21305;&#37197;&#29305;&#24449;&#26144;&#23556;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#26799;&#24230;&#21453;&#36716;&#23618;&#65289;&#26469;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36755;&#20837;&#32423;&#21035;&#23545;&#40784;&#23545;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) aims to bridge the gap between source and target domains in the absence of target domain labels using two main techniques: input-level alignment (such as generative modeling and stylization) and feature-level alignment (which matches the distribution of the feature maps, e.g. gradient reversal layers). Motivated from the success of generative modeling for image classification, stylization-based methods were recently proposed for regression tasks, such as pose estimation. However, use of input-level alignment via generative modeling and stylization incur additional overhead and computational complexity which limit their use in real-world DA tasks. To investigate the role of input-level alignment for DA, we ask the following question: Is generative modeling-based stylization necessary for visual domain adaptation in regression? Surprisingly, we find that input-alignment has little effect on regression tasks as compared to classification. Based on thes
&lt;/p&gt;</description></item><item><title>Transformer&#20351;&#29992;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#24471;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65292;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#23558;&#36830;&#25509;&#27169;&#24335;&#20998;&#35299;&#20026;&#30456;&#20114;&#29420;&#31435;&#30340;&#20449;&#24687;&#36890;&#36335;&#65292;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01705</link><description>&lt;p&gt;
&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65306;Transformer&#26159;&#21160;&#24577;&#33258;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01705
&lt;/p&gt;
&lt;p&gt;
Transformer&#20351;&#29992;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#24471;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65292;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#23558;&#36830;&#25509;&#27169;&#24335;&#20998;&#35299;&#20026;&#30456;&#20114;&#29420;&#31435;&#30340;&#20449;&#24687;&#36890;&#36335;&#65292;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36171;&#20104;&#20102;&#23427;&#22312;&#36828;&#36317;&#31163;&#36830;&#25509;&#26041;&#38754;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#28145;&#24230;Transformer&#30340;&#22810;&#20010;&#23618;&#20013;&#65292;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#24456;&#23569;&#26377;&#36830;&#25509;&#27169;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#65292;&#20854;&#20013;&#26377;&#29992;&#30340;&#26356;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20551;&#35774;&#65292;&#22312;&#19968;&#20010;Transformer&#20013;&#23384;&#22312;&#30528;&#21517;&#20026;&#20449;&#24687;&#36890;&#36335;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36890;&#36335;&#30340;&#21160;&#24577;&#65288;&#21363;&#20381;&#36182;&#20110;&#36755;&#20837;&#65289;&#29305;&#24615;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#21098;&#26525;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#36890;&#36335;&#30340;&#25972;&#20307;&#20998;&#24067;&#24448;&#24448;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#25552;&#20986;&#20102;&#38543;&#26426;&#23376;&#37319;&#26679;&#33258;&#27880;&#24847;&#21147;&#65288;SSA&#65289;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#33258;&#27880;&#24847;&#21147;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;4&#21040;8&#20493;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20146;&#21644;&#24615;&#32858;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#36890;&#36807;&#21516;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#20146;&#21644;&#32858;&#31867;&#21644;&#20445;&#25252;&#25968;&#25454;&#30340;&#20849;&#20139;&#36798;&#21040;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01699</link><description>&lt;p&gt;
&#22522;&#20110;&#25104;&#23545;&#20998;&#24067;&#24046;&#24322;&#30340;&#25968;&#25454;&#21435;&#20559;&#35265;&#20146;&#21644;&#24615;&#32858;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy. (arXiv:2306.01699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20146;&#21644;&#24615;&#32858;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#36890;&#36807;&#21516;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#20146;&#21644;&#32858;&#31867;&#21644;&#20445;&#25252;&#25968;&#25454;&#30340;&#20849;&#20139;&#36798;&#21040;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#19981;&#36275;&#25110;&#19981;&#20855;&#20195;&#34920;&#24615;&#24120;&#23548;&#33268;&#36523;&#20221;&#32452;&#19981;&#24179;&#34913;&#65292;&#24418;&#25104;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#20110;&#19968;&#20010;&#25110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#20043;&#38388;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#32467;&#26524;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#21033;&#29992;&#20146;&#21644;&#24615;&#32858;&#31867;&#24179;&#34913;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#38750;&#20445;&#25252;&#32452;&#21644;&#20445;&#25252;&#32452;&#34920;&#24449;&#12290;&#36890;&#36807;&#23558;&#21516;&#19968;&#20445;&#25252;&#23646;&#24615;&#30340;&#23454;&#20363;&#20174;&#30456;&#20284;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#20849;&#20139;&#26469;&#33258;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#23454;&#20363;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#37327;&#21270;&#25968;&#25454;&#38598;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#26500;&#24314;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23545;&#31216;&#25104;&#23545;&#30456;&#20284;&#24615;&#30697;&#38453;&#12290;&#20351;&#29992;&#38750;&#21442;&#25968;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group imbalance, resulting from inadequate or unrepresentative data collection methods, is a primary cause of representation bias in datasets. Representation bias can exist with respect to different groups of one or more protected attributes and might lead to prejudicial and discriminatory outcomes toward certain groups of individuals; in cases where a learning model is trained on such biased data. This paper presents MASC, a data augmentation approach that leverages affinity clustering to balance the representation of non-protected and protected groups of a target dataset by utilizing instances of the same protected attributes from similar datasets that are categorized in the same cluster as the target dataset by sharing instances of the protected attribute. The proposed method involves constructing an affinity matrix by quantifying distribution discrepancies between dataset pairs and transforming them into a symmetric pairwise similarity matrix. A non-parametric spectral clustering i
&lt;/p&gt;</description></item><item><title>MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.01697</link><description>&lt;p&gt;
MutateNN&#65306;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#31361;&#21464;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01697
&lt;/p&gt;
&lt;p&gt;
MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24182;&#25512;&#21160;&#25216;&#26415;&#21457;&#23637;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#29305;&#21035;&#26159;&#34987;&#20998;&#37197;&#20102;&#24863;&#30693;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24182;&#23548;&#33268;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#38656;&#27714;&#20063;&#26377;&#25152;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20248;&#21270;&#21644;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#27010;&#24565;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35753;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#22312;&#19981;&#21516;&#30828;&#20214;&#21152;&#36895;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MutateNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27492;&#30446;&#30340;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#21151;&#33021;&#65292;&#25105;&#20204;&#23545;7&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;21&#31181;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;4&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#21464;&#24322;&#20307;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;&#20102;MutateNN&#22312;&#26816;&#27979;&#20986;&#19981;&#27491;&#30830;&#25110;&#19981;&#31934;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35780;&#20272;LLMs&#30340;&#21407;&#22411;&#24179;&#21488;CheckMate&#65292;&#24182;&#21033;&#29992;&#35813;&#24179;&#21488;&#35780;&#20272;&#20102;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#24067;&#20102;&#32467;&#26524;&#25968;&#25454;&#38598;MathConverse&#65292;&#24182;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#21644;LMM&#29983;&#25104;&#27491;&#30830;&#24615;&#19982;&#24863;&#30693;&#24110;&#21161;&#24615;&#20998;&#27495;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01694</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#35780;&#20272;&#25968;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35780;&#20272;LLMs&#30340;&#21407;&#22411;&#24179;&#21488;CheckMate&#65292;&#24182;&#21033;&#29992;&#35813;&#24179;&#21488;&#35780;&#20272;&#20102;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#24067;&#20102;&#32467;&#26524;&#25968;&#25454;&#38598;MathConverse&#65292;&#24182;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#21644;LMM&#29983;&#25104;&#27491;&#30830;&#24615;&#19982;&#24863;&#30693;&#24110;&#21161;&#24615;&#20998;&#27495;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#38745;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#24320;&#21457;&#21161;&#25163;&#65306;&#36825;&#31181;&#35780;&#20272;&#26410;&#33021;&#32771;&#34385;&#37096;&#32626;&#20013;&#30340;&#22522;&#26412;&#20132;&#20114;&#24615;&#36136;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CheckMate&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#24212;&#24615;&#30340;&#20154;&#26426;&#20132;&#20114;&#21407;&#22411;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#21033;&#29992;CheckMate&#23545;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#12289;ChatGPT&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#23427;&#20204;&#20316;&#20026;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#30340;&#33021;&#21147;&#65292;&#21442;&#19982;&#32773;&#28151;&#21512;&#20102;&#20174;&#26412;&#31185;&#29983;&#21040;&#25968;&#23398;&#25945;&#25480;&#30340;&#21508;&#23618;&#27425;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#32467;&#26524;&#20132;&#20114;&#21644;&#35780;&#20998;&#25968;&#25454;&#38598;MathConverse&#12290;&#36890;&#36807;&#23545;MathConverse&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#26222;&#36941;&#23384;&#22312;&#31215;&#26497;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;LLM&#29983;&#25104;&#30340;&#27491;&#30830;&#24615;&#21644;&#24863;&#30693;&#24110;&#21161;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;Lipschitz&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#21644;&#21487;&#21464;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#20805;&#20998;&#26465;&#20214;&#20197;&#21450;Lipschitz&#24120;&#25968;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01692</link><description>&lt;p&gt;
&#20855;&#26377;Lipschitz&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#21644;&#21487;&#21464;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Uniform Convergence of Deep Neural Networks with Lipschitz Continuous Activation Functions and Variable Widths. (arXiv:2306.01692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;Lipschitz&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#21644;&#21487;&#21464;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#20805;&#20998;&#26465;&#20214;&#20197;&#21450;Lipschitz&#24120;&#25968;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;Lipschitz&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#21644;&#21487;&#21464;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#65292;&#20854;&#20013;&#25552;&#20379;&#20102;&#20851;&#20110;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#32622;&#21521;&#37327;&#30340;&#20805;&#20998;&#26465;&#20214;&#20197;&#21450;Lipschitz&#24120;&#25968;&#65292;&#20197;&#30830;&#20445;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23618;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#23545;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20989;&#25968;&#30340;&#32479;&#19968;&#25910;&#25947;&#12290;&#22312;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;&#22266;&#23450;&#23485;&#24230;&#12289;&#26377;&#30028;&#23485;&#24230;&#21644;&#26080;&#30028;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#30340;&#26435;&#37325;&#30697;&#38453;&#30340;&#29305;&#27530;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25513;&#30721;&#24207;&#21015;&#30340;&#26465;&#20214;&#65292;&#20174;&#32780;&#23548;&#33268;&#25152;&#24471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25910;&#25947;&#12290;&#28608;&#27963;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#20801;&#35768;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#20013;&#21253;&#25324;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider deep neural networks with a Lipschitz continuous activation function and with weight matrices of variable widths. We establish a uniform convergence analysis framework in which sufficient conditions on weight matrices and bias vectors together with the Lipschitz constant are provided to ensure uniform convergence of the deep neural networks to a meaningful function as the number of their layers tends to infinity. In the framework, special results on uniform convergence of deep neural networks with a fixed width, bounded widths and unbounded widths are presented. In particular, as convolutional neural networks are special deep neural networks with weight matrices of increasing widths, we put forward conditions on the mask sequence which lead to uniform convergence of resulting convolutional neural networks. The Lipschitz continuity assumption on the activation functions allows us to include in our theory most of commonly used activation functions in applications.
&lt;/p&gt;</description></item><item><title>GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01690</link><description>&lt;p&gt;
GateON: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01690
&lt;/p&gt;
&lt;p&gt;
GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#23545;&#26089;&#26399;&#20219;&#21153;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25353;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32463;&#36807;CL&#35757;&#32451;&#21518;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;'Gate and Obstruct Network'&#65288;GateON&#65289;&#12290;GateON&#23558;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#19982;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#20043;&#38388;&#29983;&#25104;&#37096;&#20998;&#37325;&#21472;&#30340;&#36335;&#24452;&#65292;&#20801;&#35768;&#22312;&#39034;&#24207;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#31227;&#12290;GateON&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#26469;&#35299;&#20915;&#21442;&#25968;&#22266;&#23450;&#21518;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#12290;GateON&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;CNN&#12289;Transformers&#65289;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#39640;&#36798;100&#20010;MNIST&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;BERT&#20013;&#21462;&#24471;&#20102;&#39030;&#23574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#31169;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#30340;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#21487;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.01684</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31169;&#26377;&#30340;&#21512;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#31169;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#30340;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#21487;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#26041;&#27861;&#65292;&#22914;DP-SGD&#65292;&#21487;&#20197;&#36890;&#36807;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#20250;&#36879;&#38706;&#31169;&#26377;&#20449;&#24687;&#26469;&#20445;&#25252;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#20445;&#30456;&#23545;&#20110;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#12290;&#36825;&#26679;&#20570;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65288;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#65289;&#65292;&#21487;&#20197;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;&#20294;&#26159;&#65292;&#33719;&#21462;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#27604;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#20854;&#22312;&#25991;&#26412;&#20013;&#21487;&#34892;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#24320;&#22987;&#65292;&#24182;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#31169;&#20154;&#35843;&#25972;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#25277;&#26679;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#36825;&#20010;&#31574;&#30053;&#20284;&#20046;&#24456;&#31616;&#21333;&#65292;&#20294;&#25191;&#34892;&#23427;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#65292;&#35201;&#20040;...
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;ODE&#30340;RNN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#26102;&#38388;&#27493;&#38271;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#23574;&#23792;&#22411;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;Hawkes&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24378;&#24230;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#35745;&#31639;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.01674</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#31070;&#32463;&#24494;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural Differential Recurrent Neural Network with Adaptive Time Steps. (arXiv:2306.01674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;ODE&#30340;RNN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#26102;&#38388;&#27493;&#38271;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#23574;&#23792;&#22411;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;Hawkes&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24378;&#24230;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#35745;&#31639;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#27169;&#22411;&#24050;&#32463;&#22312;&#20174;&#31163;&#25955;&#26102;&#38388;&#25139;&#30340;&#35266;&#27979;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#36830;&#32493;&#26102;&#38388;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#32771;&#34385;&#24314;&#27169;&#21644;&#39044;&#27979;&#19968;&#20123;&#21487;&#33021;&#20855;&#26377;&#23574;&#23792;&#31561;&#38160;&#21464;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;RNN-ODE-Adap&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;ODE&#26469;&#34920;&#31034;&#38544;&#34255;&#29366;&#24577;&#30340;&#26102;&#38388;&#21457;&#23637;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#38497;&#23789;&#31243;&#24230;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;"&#23574;&#23792;"&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;RNN-ODE-Adap&#21487;&#26377;&#25928;&#22320;&#20272;&#35745;Hawkes&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24378;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;RNN-ODE&#27169;&#22411;&#30340;&#36817;&#20284;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#20248;&#21183;&#12290;&#22312;&#27169;&#25311;&#21160;&#24577;&#31995;&#32479;&#25968;&#25454;&#21644;&#28857;&#36807;&#31243;&#25968;&#25454;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#35777;&#26126;&#20102;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural Ordinary Differential Equation (ODE) model has shown success in learning complex continuous-time processes from observations on discrete time stamps. In this work, we consider the modeling and forecasting of time series data that are non-stationary and may have sharp changes like spikes. We propose an RNN-based model, called RNN-ODE-Adap, that uses a neural ODE to represent the time development of the hidden states, and we adaptively select time steps based on the steepness of changes of the data over time so as to train the model more efficiently for the "spike-like" time series. Theoretically, RNN-ODE-Adap yields provably a consistent estimation of the intensity function for the Hawkes-type time series data. We also provide an approximation analysis of the RNN-ODE model showing the benefit of adaptive steps. The proposed model is demonstrated to achieve higher prediction accuracy with reduced computational cost on simulated dynamic system data and point process data and on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#26469;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#20449;&#20219;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#25216;&#26415;&#28508;&#21147;&#24040;&#22823;&#65292;&#26377;&#26395;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#30340;&#26684;&#23616;&#65292;&#20174;&#32780;&#25552;&#39640;&#24739;&#32773;&#30340;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2306.01668</link><description>&lt;p&gt;
XAI&#25991;&#33402;&#22797;&#20852;&#65306;&#37325;&#26032;&#23450;&#20041;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models. (arXiv:2306.01668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#26469;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#20449;&#20219;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#25216;&#26415;&#28508;&#21147;&#24040;&#22823;&#65292;&#26377;&#26395;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#30340;&#26684;&#23616;&#65292;&#20174;&#32780;&#25552;&#39640;&#24739;&#32773;&#30340;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#38656;&#27714;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;XAI&#25991;&#33402;&#22797;&#20852;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#21457;&#29983;&#20102;&#37325;&#22823;&#36716;&#21464;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20869;&#27491;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#12290;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;XAI&#25216;&#26415;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#12289;&#20449;&#20219;&#24182;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#21487;&#38752;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;XAI&#22312;&#21307;&#23398;&#35786;&#26029;&#26041;&#38754;&#30340;&#20851;&#38190;&#36827;&#23637;&#21450;&#20854;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#26684;&#23616;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models become increasingly prevalent in medical diagnostics, the need for interpretability and transparency becomes paramount. The XAI Renaissance signifies a significant shift in the field, aiming to redefine the interpretability of medical diagnostic models. This paper explores the innovative approaches and methodologies within the realm of Explainable AI (XAI) that are revolutionizing the interpretability of medical diagnostic models. By shedding light on the underlying decision-making process, XAI techniques empower healthcare professionals to understand, trust, and effectively utilize these models for accurate and reliable medical diagnoses. This review highlights the key advancements in XAI for medical diagnostics and their potential to transform the healthcare landscape, ultimately improving patient outcomes and fostering trust in AI-driven diagnostic systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#30693;&#26631;&#31614;&#65292;&#24182;&#36866;&#24212;&#26102;&#38388;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#28418;&#31227;&#24133;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01658</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28418;&#31227;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Method for Weak Supervision with Drifting Data. (arXiv:2306.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#30693;&#26631;&#31614;&#65292;&#24182;&#36866;&#24212;&#26102;&#38388;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#28418;&#31227;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#20855;&#26377;&#27491;&#24335;&#36136;&#37327;&#20445;&#35777;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25968;&#25454;&#24207;&#21015;&#36827;&#34892;&#24369;&#30417;&#30563;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#27599;&#20010;&#25968;&#25454;&#28857;&#27491;&#30830;&#20998;&#31867;&#30340;&#29420;&#31435;&#22024;&#26434;&#20449;&#21495;&#30340;&#24369;&#30417;&#30563;&#28304;&#26469;&#25512;&#26029;&#26410;&#30693;&#26631;&#31614;&#12290;&#36825;&#31181;&#24773;&#20917;&#21253;&#25324;&#20247;&#21253;&#21644;&#32534;&#31243;&#24335;&#24369;&#30417;&#30563;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#38750;&#31283;&#24577;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24369;&#30417;&#30563;&#28304;&#30340;&#31934;&#24230;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#28418;&#31227;&#65292;&#20363;&#22914;&#30001;&#20110;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#30001;&#20110;&#28418;&#31227;&#65292;&#26087;&#25968;&#25454;&#21487;&#33021;&#20250;&#25552;&#20379;&#35823;&#23548;&#24615;&#20449;&#24687;&#26469;&#25512;&#26029;&#24403;&#21069;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#20808;&#39564;&#23545;&#28418;&#31227;&#24133;&#24230;&#30340;&#20551;&#35774;&#65292;&#20197;&#20915;&#23450;&#20351;&#29992;&#22810;&#23569;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#28418;&#31227;&#20551;&#35774;&#65292;&#32780;&#26159;&#26681;&#25454;&#36755;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#24369;&#30417;&#30563;&#28304;&#24403;&#21069;&#20934;&#30830;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. Comparatively, our algorithm does not require any assumptions on the drift, and it adapts based on the input. In particular, at each step, our algorithm guarantees an estimation of the current accuracies of the weak supervisio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#32593;&#32476;&#27969;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#29983;&#25104;&#26377;&#25928;&#35302;&#21457;&#22120;&#27169;&#24335;&#21644;&#29983;&#25104;&#38544;&#31192;&#35302;&#21457;&#22120;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#23545;&#25163;&#24178;&#25200;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01655</link><description>&lt;p&gt;
&#32593;&#32476;&#27969;&#20998;&#31867;&#22120;&#30340;&#21361;&#23475;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Poisoning Network Flow Classifiers. (arXiv:2306.01655v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01655
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#32593;&#32476;&#27969;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#29983;&#25104;&#26377;&#25928;&#35302;&#21457;&#22120;&#27169;&#24335;&#21644;&#29983;&#25104;&#38544;&#31192;&#35302;&#21457;&#22120;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#23545;&#25163;&#24178;&#25200;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#30417;&#25511;&#32593;&#32476;&#27969;&#37327;&#65292;&#30740;&#31350;&#23427;&#20204;&#23545;&#25239;&#25915;&#20987;&#30340;&#38887;&#24615;&#23601;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#21361;&#23475;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#20928;&#26631;&#31614;&#27745;&#26579;&#22330;&#26223;&#65292;&#20854;&#20013;&#23545;&#25163;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20165;&#24178;&#25200;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#27861;&#20219;&#24847;&#20462;&#25913;&#35757;&#32451;&#26631;&#31614;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#20219;&#20309;&#20854;&#20182;&#32452;&#20214;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35302;&#21457;&#22120;&#21046;&#20316;&#31574;&#30053;&#65292;&#21033;&#29992;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#29983;&#25104;&#21363;&#20351;&#22312;&#26497;&#20302;&#30340;&#27745;&#26579;&#29575;&#19979;&#20063;&#26377;&#25928;&#30340;&#35302;&#21457;&#22120;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29983;&#25104;&#38544;&#31192;&#35302;&#21457;&#22120;&#30340;&#26032;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#29983;&#25104;&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35302;&#21457;&#22120;&#30340;&#26174;&#33879;&#24615;&#65292;&#20174;&#32780;&#20351;&#27491;&#22312;&#36827;&#34892;&#30340;&#21361;&#23475;&#25915;&#20987;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) classifiers increasingly oversee the automated monitoring of network traffic, studying their resilience against adversarial attacks becomes critical. This paper focuses on poisoning attacks, specifically backdoor attacks, against network traffic flow classifiers. We investigate the challenging scenario of clean-label poisoning where the adversary's capabilities are constrained to tampering only with the training data - without the ability to arbitrarily modify the training labels or any other component of the training process. We describe a trigger crafting strategy that leverages model interpretability techniques to generate trigger patterns that are effective even at very low poisoning rates. Finally, we design novel strategies to generate stealthy triggers, including an approach based on generative Bayesian network models, with the goal of minimizing the conspicuousness of the trigger, and thus making detection of an ongoing poisoning campaign more challengi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.01654</link><description>&lt;p&gt;
GANs&#35299;&#20915;&#20998;&#25968;&#20105;&#35758;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30001;&#19968;&#20010;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#21028;&#21035;&#22120;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#26399;&#26395;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#32780;&#21028;&#21035;&#22120;&#21017;&#34987;&#35757;&#32451;&#20197;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#29983;&#25104;&#22120;&#36755;&#20986;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#20998;&#26512;&#29983;&#25104;&#22120;&#20248;&#21270;&#12290;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270; GAN &#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#24471;&#20998;&#36827;&#34892;&#21305;&#37197;&#24471;&#21040;&#30340;&#12290;&#22312;IPM GAN&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#26368;&#20248;&#29983;&#25104;&#22120;&#21305;&#37197;&#24471;&#20998;&#22411;&#20989;&#25968;&#65292;&#21253;&#25324;&#19982;&#25152;&#36873;IPM&#32422;&#26463;&#31354;&#38388;&#30456;&#20851;&#30340;&#26680;&#27969;&#22330;&#12290;&#27492;&#22806;&#65292;IPM-GAN&#20248;&#21270;&#21487;&#20197;&#30475;&#20316;&#26159;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#20013;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#29983;&#25104;&#22120;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#22312;&#26680;&#20989;&#25968;&#19978;&#36827;&#34892;&#21367;&#31215;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) comprise a generator, trained to learn the underlying distribution of the desired data, and a discriminator, trained to distinguish real samples from those output by the generator. A majority of GAN literature focuses on understanding the optimality of the discriminator through integral probability metric (IPM) or divergence based analysis. In this paper, we propose a unified approach to analyzing the generator optimization through variational approach. In $f$-divergence-minimizing GANs, we show that the optimal generator is the one that matches the score of its output distribution with that of the data distribution, while in IPM GANs, we show that this optimal generator matches score-like functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, the IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#35821;&#35328;&#32500;&#22522;&#30334;&#31185;&#30772;&#22351;&#26816;&#27979;&#30340;&#26032;&#22411;&#31995;&#32479;&#35774;&#35745;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#20854;&#35206;&#30422;&#20102;&#26356;&#22810;&#30340;&#35821;&#35328;&#65292;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#27604;&#32500;&#22522;&#30334;&#31185;&#20013;&#29983;&#20135;&#20351;&#29992;&#30340;ORES&#31995;&#32479;&#26356;&#21152;&#20934;&#30830;&#21644;&#20844;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.01650</link><description>&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#20844;&#27491;&#30340;&#22810;&#35821;&#35328;&#30772;&#22351;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Fair multilingual vandalism detection system for Wikipedia. (arXiv:2306.01650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#35821;&#35328;&#32500;&#22522;&#30334;&#31185;&#30772;&#22351;&#26816;&#27979;&#30340;&#26032;&#22411;&#31995;&#32479;&#35774;&#35745;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#20854;&#35206;&#30422;&#20102;&#26356;&#22810;&#30340;&#35821;&#35328;&#65292;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#27604;&#32500;&#22522;&#30334;&#31185;&#20013;&#29983;&#20135;&#20351;&#29992;&#30340;ORES&#31995;&#32479;&#26356;&#21152;&#20934;&#30830;&#21644;&#20844;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#26088;&#22312;&#25903;&#25345;&#32500;&#22522;&#30334;&#31185;&#31038;&#21306;&#35299;&#20915;&#24179;&#21488;&#19978;&#30340;&#30772;&#22351;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;47&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#20102;&#20808;&#36827;&#30340;&#36807;&#28388;&#21644;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20174;&#20154;&#29983;&#25104;&#30340;&#25968;&#25454;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19982;&#32500;&#22522;&#30334;&#31185;&#29983;&#20135;&#20013;&#20351;&#29992;&#30340;&#31216;&#20026;ORES&#30340;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#33879;&#22686;&#21152;&#20102;&#35206;&#30422;&#35821;&#35328;&#30340;&#25968;&#37327;&#65292;&#20351;&#32500;&#22522;&#30334;&#31185;&#30340;&#24033;&#36923;&#26356;&#26377;&#25928;&#22320;&#26381;&#21153;&#20110;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;ORES&#65292;&#30830;&#20445;&#25552;&#20379;&#30340;&#32467;&#26524;&#19981;&#20165;&#26356;&#20934;&#30830;&#65292;&#32780;&#19988;&#20063;&#19981;&#20250;&#23545;&#26576;&#20123;&#36129;&#29486;&#32773;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel design of the system aimed at supporting the Wikipedia community in addressing vandalism on the platform. To achieve this, we collected a massive dataset of 47 languages, and applied advanced filtering and feature engineering techniques, including multilingual masked language modeling to build the training dataset from human-generated data. The performance of the system was evaluated through comparison with the one used in production in Wikipedia, known as ORES. Our research results in a significant increase in the number of languages covered, making Wikipedia patrolling more efficient to a wider range of communities. Furthermore, our model outperforms ORES, ensuring that the results provided are not only more accurate but also less biased against certain groups of contributors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedMSA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#24207;&#21015;&#38543;&#26426;&#36924;&#36817;&#65288;MSA&#65289;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#26412;&#22320;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;FedMSA&#23454;&#29616;&#20102;BLO&#21644;MCO&#20013;&#36229;&#26799;&#24230;&#30340;&#21487;&#35777;&#20272;&#35745;&#12290;&#25991;&#20013;&#36824;&#32467;&#21512;&#21160;&#37327;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#26469;&#21152;&#36895;&#65292;&#23548;&#33268;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01648</link><description>&lt;p&gt;
&#29992;&#26412;&#22320;&#36229;&#26799;&#24230;&#20272;&#35745;&#30340;&#32852;&#37030;&#22810;&#24207;&#21015;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation. (arXiv:2306.01648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedMSA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#24207;&#21015;&#38543;&#26426;&#36924;&#36817;&#65288;MSA&#65289;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#26412;&#22320;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;FedMSA&#23454;&#29616;&#20102;BLO&#21644;MCO&#20013;&#36229;&#26799;&#24230;&#30340;&#21487;&#35777;&#20272;&#35745;&#12290;&#25991;&#20013;&#36824;&#32467;&#21512;&#21160;&#37327;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#26469;&#21152;&#36895;&#65292;&#23548;&#33268;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24207;&#21015;&#38543;&#26426;&#36924;&#36817;&#65288;MSA&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20102;&#35768;&#22810;&#38382;&#39064;&#30340;&#20016;&#23500;&#31867;&#21035;&#65292;&#21253;&#25324;&#21452;&#23618;&#20248;&#21270;&#12289;&#22810;&#23618;&#32452;&#21512;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21452;&#24207;&#21015;&#36924;&#36817;&#65288;DSA&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#35774;&#35745;&#20986;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#32852;&#37030;&#31639;&#27861;&#20063;&#19968;&#30452;&#26159;&#19968;&#20010;&#38590;&#20197;&#25417;&#25720;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FedMSA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;MSA&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20316;&#20026;&#26680;&#24515;&#21019;&#26032;&#65292;(i) FedMSA&#36890;&#36807;&#26412;&#22320;&#23458;&#25143;&#31471;&#26356;&#26032;&#23454;&#29616;&#20102;BLO&#21644;MCO&#20013;&#36229;&#26799;&#24230;&#30340;&#21487;&#35777;&#20272;&#35745;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#29702;&#35770;&#20013;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#29942;&#39048;&#65292; (ii) &#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#23545;&#38382;&#39064;&#30340;&#24322;&#36136;&#24615;&#27700;&#24179;&#26159;&#25935;&#24863;&#30340;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#21160;&#37327;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#26469;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#21152;&#36895;&#65292;&#23548;&#33268;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation with multiple coupled sequences (MSA) has found broad applications in machine learning as it encompasses a rich class of problems including bilevel optimization (BLO), multi-level compositional optimization (MCO), and reinforcement learning (specifically, actor-critic methods). However, designing provably-efficient federated algorithms for MSA has been an elusive question even for the special case of double sequence approximation (DSA). Towards this goal, we develop FedMSA which is the first federated algorithm for MSA, and establish its near-optimal communication complexity. As core novelties, (i) FedMSA enables the provable estimation of hypergradients in BLO and MCO via local client updates, which has been a notable bottleneck in prior theory, and (ii) our convergence guarantees are sensitive to the heterogeneity-level of the problem. We also incorporate momentum and variance reduction techniques to achieve further acceleration leading to near-optimal rates.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01646</link><description>&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#23457;&#26680;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auditing for Human Expertise. (arXiv:2306.01646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01646
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#24739;&#32773;&#35786;&#26029;&#65289;&#36890;&#24120;&#30001;&#25509;&#21463;&#22521;&#35757;&#30340;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#33258;&#21160;&#21270;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#65292;&#19987;&#23478;&#21487;&#33021;&#36816;&#29992;&#24456;&#38590;&#24314;&#27169;&#30340;&#30452;&#35273;&#65292;&#24182;&#19988;/&#25110;&#32773;&#21487;&#20197;&#33719;&#21462;&#20449;&#24687;&#65288;&#20363;&#22914;&#19982;&#24739;&#32773;&#30340;&#20132;&#35848;&#65289;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65292;&#20154;&#31867;&#19987;&#23478;&#26159;&#21542;&#22686;&#21152;&#20102;&#26080;&#27861;&#34987;&#31639;&#27861;&#39044;&#27979;&#22120;&#25429;&#25417;&#21040;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20026;&#19968;&#20010;&#33258;&#28982;&#30340;&#20551;&#35774;&#26816;&#39564;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#26694;&#26550;&#25152;&#24378;&#35843;&#30340;&#37027;&#26679;&#65292;&#26816;&#27979;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#27604;&#31616;&#21333;&#27604;&#36739;&#19987;&#23478;&#39044;&#27979;&#20934;&#30830;&#24615;&#19982;&#29305;&#23450;&#23398;&#20064;&#31639;&#27861;&#20570;&#20986;&#30340;&#20934;&#30830;&#24615;&#26356;&#21152;&#24494;&#22937;&#12290;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#65292;&#27979;&#35797;&#19987;&#23478;&#39044;&#27979;&#26159;&#21542;&#22312;&#8220;&#29305;&#24449;&#8221;&#21487;&#29992;&#32780;&#26465;&#20214;&#19979;&#26159;&#21542;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#32479;&#35745;&#19978;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#25298;&#32477;&#34920;&#26126;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30830;&#23454;&#22686;&#21152;&#20102;&#36229;&#20986;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that huma
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01639</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Reduction of finite sampling noise in quantum neural networks. (arXiv:2306.01639v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01639
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36755;&#20986;, &#36890;&#36807;&#35745;&#31639;&#26399;&#26395;&#20540;&#24102;&#26469;&#20102;&#22522;&#26412;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#21363;&#20351;&#22312;&#26080;&#35823;&#24046;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#20063;&#20250;&#20986;&#29616;&#27492;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26469;&#20943;&#23569;&#36825;&#31181;&#22122;&#22768;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20943;&#23567;&#37327;&#23376;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26399;&#26395;&#20540;&#30340;&#26041;&#24046;&#12290;&#22914;&#26524;QNN&#24050;&#32463;&#22949;&#21892;&#26500;&#36896;&#65292;&#21017;&#27492;&#25216;&#26415;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30005;&#36335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38477;&#20302;&#26041;&#24046;&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#38477;&#20302;&#36755;&#20986;&#22122;&#22768;&#65292;&#20943;&#23569;&#26799;&#24230;&#30005;&#36335;&#35780;&#20272;&#20013;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#25105;&#20204;&#23545;&#22810;&#39033;&#24335;&#20989;&#25968;&#22238;&#24402;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#31034;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35268;&#33539;&#21270;&#26041;&#27861;&#24179;&#22343;&#21487;&#20197;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;&#20102;&#22122;&#22768;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) use parameterized quantum circuits with data-dependent inputs and generate outputs through the evaluation of expectation values. Calculating these expectation values necessitates repeated circuit evaluations, thus introducing fundamental finite-sampling noise even on error-free quantum computers. We reduce this noise by introducing the variance regularization, a technique for reducing the variance of the expectation value during the quantum model training. This technique requires no additional circuit evaluations if the QNN is properly constructed. Our empirical findings demonstrate the reduced variance speeds up the training and lowers the output noise as well as decreases the number of measurements in the gradient circuit evaluation. This regularization method is benchmarked on the regression of multiple functions. We show that in our examples, it lowers the variance by an order of magnitude on average and leads to a significantly reduced noise level of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#32972;&#26223;&#30693;&#35782;&#26469;&#38480;&#21046;&#31561;&#20215;&#31867;&#65292;&#20174;&#32780;&#26377;&#25928;&#31616;&#21270;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#32972;&#26223;&#30693;&#35782;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.01638</link><description>&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#25105;&#20204;&#26159;&#21542;&#21464;&#24471;&#26356;&#21152;&#32874;&#26126;&#65311;&#20851;&#20110;&#20998;&#23618;&#32972;&#26223;&#30693;&#35782;&#30340;&#22240;&#26524;&#31561;&#20215;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Do we become wiser with time? On causal equivalence with tiered background knowledge. (arXiv:2306.01638v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#32972;&#26223;&#30693;&#35782;&#26469;&#38480;&#21046;&#31561;&#20215;&#31867;&#65292;&#20174;&#32780;&#26377;&#25928;&#31616;&#21270;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#32972;&#26223;&#30693;&#35782;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31561;&#20215;&#31867;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#36890;&#36807;CPDAGs&#34920;&#31034;&#65289;&#21487;&#33021;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#25552;&#20379;&#26377;&#29992;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#25972;&#21512;&#20998;&#23618;&#32972;&#26223;&#30693;&#35782;&#26469;&#38480;&#21046;&#31561;&#20215;&#31867;&#65292;&#20174;&#32780;&#24471;&#20986;&#30001;&#8220;&#20998;&#23618;MPDAGs&#8221;&#34920;&#31034;&#30340;&#31561;&#20215;&#31867;&#12290;&#20351;&#29992;&#20998;&#23618;&#30693;&#35782;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20449;&#24687;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#65306;&#25105;&#20204;&#34920;&#26126;&#65292;&#26500;&#24314;&#20998;&#23618;MPDAGs&#21482;&#38656;&#35201;&#24212;&#29992;Meek&#30340;&#31532;&#19968;&#27861;&#21017;&#65292;&#24182;&#19988;&#20998;&#23618;MPDAG&#65288;&#19981;&#21516;&#20110;&#19968;&#33324;&#30340;MPDAG&#65289;&#26159;&#20855;&#26377;&#24358;&#22270;&#32452;&#25104;&#37096;&#20998;&#30340;&#38142;&#22270;&#12290;&#36825;&#24102;&#26469;&#20102;&#31616;&#21270;&#65292;&#20363;&#22914;&#30830;&#23450;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#26377;&#25928;&#35843;&#25972;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#20309;&#26102;&#19968;&#31181;&#20998;&#23618;&#25490;&#24207;&#27604;&#21478;&#19968;&#31181;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#32972;&#26223;&#30693;&#35782;&#26377;&#29992;&#30340;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivalence classes of DAGs (represented by CPDAGs) may be too large to provide useful causal information. Here, we address incorporating tiered background knowledge yielding restricted equivalence classes represented by 'tiered MPDAGs'. Tiered knowledge leads to considerable gains in informativeness and computational efficiency: We show that construction of tiered MPDAGs only requires application of Meek's 1st rule, and that tiered MPDAGs (unlike general MPDAGs) are chain graphs with chordal components. This entails simplifications e.g. of determining valid adjustment sets for causal effect estimation. Further, we characterise when one tiered ordering is more informative than another, providing insights into useful aspects of background knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#26144;&#23556;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#21516;&#21464;&#24615;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01623</link><description>&lt;p&gt;
HomE: &#21516;&#21464;&#24615;&#21333;&#24212;&#24615;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#26144;&#23556;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#21516;&#21464;&#24615;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#26356;&#39640;&#25928;&#21644;&#26356;&#24378;&#20581;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#20173;&#28982;&#38598;&#20013;&#22312;&#22270;&#20687;&#19978;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#20851;&#27880;&#35270;&#39057;&#65292;&#29978;&#33267;&#26356;&#23569;&#30340;&#24037;&#20316;&#20851;&#27880;&#22810;&#35270;&#35282;&#35270;&#39057;&#65292;&#20854;&#20013;&#21487;&#20197;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#36827;&#34892;&#33258;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#26174;&#24335;&#22320;&#27169;&#25311;&#34920;&#31034;&#31354;&#38388;&#20197;&#20445;&#25345;&#21516;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#19981;&#21516;&#35270;&#35282;&#20043;&#38388;&#30340;&#38544;&#24335;&#26144;&#23556;&#65292;&#26368;&#32456;&#24418;&#25104;&#19968;&#20010;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20445;&#25345;&#30456;&#37051;&#35270;&#35282;&#20043;&#38388;&#30340;&#21333;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;HomE&#34920;&#31034;&#12290;&#22312;UCF101&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;3&#27425;&#20132;&#21449;&#39564;&#35777;&#30340;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;96.4&#65285;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#21516;&#26679;&#65292;&#22312;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;STIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#26512;&#39564;&#35777;&#25253;&#21578;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#21644;&#20998;&#31867;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01618</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#38382;&#39064;&#20998;&#26512;&#65306;&#26469;&#33258;&#39564;&#35777;&#25253;&#21578;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Analyzing Credit Risk Model Problems through NLP-Based Clustering and Machine Learning: Insights from Validation Reports. (arXiv:2306.01618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#26512;&#39564;&#35777;&#25253;&#21578;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#21644;&#20998;&#31867;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#36890;&#36807;&#39564;&#35777;&#25253;&#21578;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#35782;&#21035;&#21644;&#20998;&#31867;&#20449;&#36151;&#39118;&#38505;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#19968;&#20221;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#22823;&#22411;&#22269;&#38469;&#38134;&#34892;&#38598;&#22242;&#22312;2019&#24180;1&#26376;&#33267;2022&#24180;12&#26376;&#26399;&#38388;&#39564;&#35777;&#22242;&#38431;&#25552;&#20986;&#30340;657&#20010;&#21457;&#29616;&#12290;&#23558;&#36825;&#20123;&#21457;&#29616;&#20998;&#20026;&#20061;&#20010;&#39564;&#35777;&#32500;&#24230;&#65292;&#24182;&#30001;&#39564;&#35777;&#20154;&#21592;&#20351;&#29992;&#20854;&#19987;&#19994;&#30693;&#35782;&#20998;&#37197;&#20102;&#19968;&#20010;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#12290;&#20316;&#32773;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21457;&#29616;&#26631;&#39064;&#21644;&#35266;&#23519;&#32467;&#26524;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#21253;&#25324;TensorFlow Hub&#20013;&#30340;&#8220;module_url&#8221;&#21644;SentenceTransformer&#24211;&#20013;&#30340;&#19977;&#20010;&#27169;&#22411;&#65292;&#21363;&#8220;all-mpnet-base-v2&#8221;&#12289;&#8220;all-MiniLM-L6-v2&#8221;&#21644;&#8220;paraphrase-mpnet-base-v2&#8221;&#12290;&#26412;&#25991;&#20351;&#29992;&#21644;&#27604;&#36739;&#20102;&#21508;&#31181;&#32858;&#31867;&#26041;&#27861;&#26469;&#23545;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#21457;&#29616;&#36827;&#34892;&#20998;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27599;&#20010;&#39564;&#35777;&#32500;&#24230;&#20869;&#35782;&#21035;&#24120;&#35265;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of clustering methods and machine learning algorithms, including Natural Language Processing (NLP), to identify and classify problems identified in credit risk models through textual information contained in validation reports. Using a unique dataset of 657 findings raised by validation teams in a large international banking group between January 2019 and December 2022. The findings are classified into nine validation dimensions and assigned a severity level by validators using their expert knowledge. The authors use embedding generation for the findings' titles and observations using four different pre-trained models, including "module\_url" from TensorFlow Hub and three models from the SentenceTransformer library, namely "all-mpnet-base-v2", "all-MiniLM-L6-v2", and "paraphrase-mpnet-base-v2". The paper uses and compares various clustering methods in grouping findings with similar characteristics, enabling the identification of common problems within each v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#24433;&#21709;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#23398;&#20064;&#36229;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.01613</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#19979;&#30340;&#36229;&#21442;&#25968;&#23398;&#20064;&#65306;&#22522;&#20110;&#22810;&#30446;&#26631;&#20108;&#23618;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (arXiv:2306.01613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#24433;&#21709;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#23398;&#20064;&#36229;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23481;&#26131;&#36973;&#21463;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21363;&#36890;&#36807;&#25805;&#32437;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#26469;&#26377;&#24847;&#30772;&#22351;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#20248;&#25915;&#20987;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#26377;&#21161;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#31639;&#27861;&#30340;&#24378;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#36229;&#21442;&#25968;&#20445;&#25345;&#19981;&#21464;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#27491;&#21017;&#21270;&#24433;&#21709;&#30340;&#36807;&#20110;&#24754;&#35266;&#30340;&#35266;&#28857;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20801;&#35768;&#21046;&#23450;&#26368;&#20248;&#25915;&#20987;&#12289;&#23398;&#20064;&#36229;&#21442;&#25968;&#24182;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#25915;&#20987;&#20844;&#24335;&#24212;&#29992;&#20110;&#20351;&#29992;$L_2$&#21644;$L_1$&#27491;&#21017;&#21270;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19978;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#30830;&#35748;&#20102;&#20808;&#21069;&#31574;&#30053;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#21644;&#22312;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26102;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#36229;&#21442;&#25968;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst-case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our evaluation on multiple datasets confirms the limitations of previous strategies and evidences the ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#24515;&#21270;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#32416;&#38169;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#28040;&#38500;transformers&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#26550;&#26500;&#27604;&#35768;&#22810;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.01610</link><description>&lt;p&gt;
&#20013;&#24515;&#21270;&#33258;&#27880;&#24847;&#21147;&#23618;
&lt;/p&gt;
&lt;p&gt;
Centered Self-Attention Layers. (arXiv:2306.01610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#24515;&#21270;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#32416;&#38169;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#28040;&#38500;transformers&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#26550;&#26500;&#27604;&#35768;&#22810;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20013;&#65292;transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#34987;&#21453;&#22797;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#24212;&#29992;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#22312;transformer&#20013;&#19981;&#21516;&#20196;&#29260;&#21644;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21516;&#33410;&#28857;&#30340;&#28145;&#23618;&#34920;&#31034;&#38750;&#24120;&#30456;&#20284;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#36825;&#20123;&#26426;&#21046;&#32858;&#21512;&#36816;&#31639;&#31526;&#30340;&#32416;&#27491;&#39033;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#26415;&#35821;&#28040;&#38500;&#20102;&#22270;&#20687;transformers&#20013;&#20984;&#26174;&#30340;&#38382;&#39064;&#65292;&#22312;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#38754;&#33719;&#24471;&#20102;&#36229;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#32593;&#32476;&#21644;&#35757;&#32451;&#38454;&#27573;&#30340;&#31934;&#24515;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#32416;&#27491;&#39033;&#20351;&#24471;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#26550;&#26500;&#27604;&#35768;&#22810;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.
&lt;/p&gt;</description></item><item><title>&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#28040;&#38500;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#36890;&#20449;&#20174;&#32780;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;DFL&#30340;&#20840;&#38754;&#35843;&#26597;&#12289;&#28145;&#20837;&#23637;&#26395;&#21644;&#25193;&#23637;&#21464;&#20307;&#19982;&#20998;&#31867;&#30340;&#20171;&#32461;&#65292;&#37325;&#28857;&#22312;&#20110;DFL&#30340;&#31995;&#32479;&#19982;&#35814;&#32454;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01603</link><description>&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#20221;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: A Survey and Perspective. (arXiv:2306.01603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01603
&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#28040;&#38500;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#36890;&#20449;&#20174;&#32780;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;DFL&#30340;&#20840;&#38754;&#35843;&#26597;&#12289;&#28145;&#20837;&#23637;&#26395;&#21644;&#25193;&#23637;&#21464;&#20307;&#19982;&#20998;&#31867;&#30340;&#20171;&#32461;&#65292;&#37325;&#28857;&#22312;&#20110;DFL&#30340;&#31995;&#32479;&#19982;&#35814;&#32454;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#20849;&#20139;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#12289;&#20445;&#25252;&#38544;&#31169;&#12289;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19982;&#38598;&#20013;&#24335;FL&#65288;CFL&#65289;&#30456;&#27604;&#65292;DFL&#28040;&#38500;&#20102;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;DFL&#20351;&#23458;&#25143;&#31471;&#20043;&#38388;&#21487;&#20197;&#30452;&#25509;&#36890;&#20449;&#65292;&#20174;&#32780;&#26174;&#33879;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#28145;&#20837;&#30340;&#23637;&#26395;&#12290;&#39318;&#20808;&#65292;&#23545;CFL&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#21464;&#20307;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#22880;&#23450;&#20102;DFL&#30340;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;DFL&#30340;&#31995;&#32479;&#21644;&#35814;&#32454;&#30340;&#21069;&#26223;&#65292;&#21253;&#25324;&#36845;&#20195;&#39034;&#24207;&#12289;&#36890;&#20449;&#21327;&#35758;&#12289;&#32593;&#32476;&#25299;&#25169;&#12289;&#33539;&#20363;&#25552;&#35758;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;DFL&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25193;&#23637;&#21464;&#20307;&#21644;&#20998;&#31867;&#24182;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#38500;&#20102;&#24635;&#32467;DFL&#30340;&#20248;&#21183;&#19982;&#21155;&#21183;&#65292;&#23545;DFL&#26410;&#26469;&#30740;&#31350;&#21644;&#24212;&#29992;&#21069;&#26223;&#36827;&#34892;&#20102;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized FL (DFL) is a decentralized network architecture that eliminates the need for a central server in contrast to centralized FL (CFL). DFL enables direct communication between clients, resulting in significant savings in communication resources. In this paper, a comprehensive survey and profound perspective is provided for DFL. First, a review of the methodology, challenges, and variants of CFL is conducted, laying the background of DFL. Then, a systematic and detailed perspective on DFL is introduced, including iteration order, communication protocols, network topologies, paradigm proposals, and temporal variability. Next, based on the definition of DFL, several extended variants and categorizations are proposed with state-of-the-art technologies. Lastly, in addition to sum
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01589</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#21407;&#23376;&#27169;&#25311;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#21442;&#32771;&#35745;&#31639;&#26159;&#35745;&#31639;&#19978;&#35201;&#27714;&#24456;&#39640;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25551;&#36848;&#21270;&#23398;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#26680;&#22343;&#20540;&#23884;&#20837;&#12290;&#25105;&#20204;&#20174;&#39044;&#20808;&#22312;OC20&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#25552;&#21462;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20174;&#20652;&#21270;&#36807;&#31243;&#30340;&#31995;&#32479;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#28789;&#27963;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#65292;&#35813;&#26680;&#20989;&#25968;&#21253;&#25324;&#21270;&#23398;&#29289;&#31181;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#65292;&#25913;&#36827;&#20102;&#20381;&#36182;GNNs&#25110;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#65292;&#36890;&#36807;&#24314;&#31435;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#20013;&#27010;&#24565;&#23384;&#22312;&#27169;&#31946;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;CBM&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01574</link><description>&lt;p&gt;
&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Concept Bottleneck Models. (arXiv:2306.01574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#65292;&#36890;&#36807;&#24314;&#31435;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#20013;&#27010;&#24565;&#23384;&#22312;&#27169;&#31946;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;CBM&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26088;&#22312;&#20197;&#21487;&#35835;&#30340;&#26041;&#24335;&#20570;&#20986;&#20915;&#31574;&#12290;&#20854;&#20013;&#65292;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26681;&#25454;&#39044;&#27979;&#30340;&#27010;&#24565;&#36827;&#34892;&#27010;&#24565;&#39044;&#27979;&#21644;&#31867;&#39044;&#27979;&#20004;&#27493;&#39588;&#12290;CBM&#20351;&#29992;&#20174;&#27010;&#24565;&#39044;&#27979;&#20013;&#24471;&#20986;&#30340;&#39640;&#32423;&#27010;&#24565;&#25552;&#20379;&#35299;&#37322;&#65307;&#22240;&#27492;&#65292;&#21487;&#38752;&#30340;&#27010;&#24565;&#39044;&#27979;&#23545;&#20110;&#21487;&#20449;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21487;&#33021;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;&#34429;&#28982;&#25968;&#25454;&#20013;&#27010;&#24565;&#30340;&#23384;&#22312;&#24448;&#24448;&#26159;&#27169;&#31946;&#30340;&#65292;&#20294;CBM&#22312;&#19981;&#32771;&#34385;&#27492;&#27169;&#31946;&#24615;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#30830;&#23450;&#24615;&#26041;&#27861;&#39044;&#27979;&#27010;&#24565;&#12290;&#20026;&#20102;&#38024;&#23545;&#36825;&#31181;&#27169;&#31946;&#24615;&#25552;&#20379;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#65292;ProbCBM&#23545;&#27010;&#24565;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22522;&#20110;&#27010;&#24565;&#21450;&#20854;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#35299;&#37322;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31867;&#21035;&#39044;&#27979;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;ProbCBM&#36824;&#25552;&#20379;&#20102;&#31867;&#21035;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable models are designed to make decisions in a human-interpretable manner. Representatively, Concept Bottleneck Models (CBM) follow a two-step process of concept prediction and class prediction based on the predicted concepts. CBM provides explanations with high-level concepts derived from concept predictions; thus, reliable concept predictions are important for trustworthiness. In this study, we address the ambiguity issue that can harm reliability. While the existence of a concept can often be ambiguous in the data, CBM predicts concepts deterministically without considering this ambiguity. To provide a reliable interpretation against this ambiguity, we propose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging probabilistic concept embeddings, ProbCBM models uncertainty in concept prediction and provides explanations based on the concept and its corresponding uncertainty. This uncertainty enhances the reliability of the explanations. Furthermore, as class unc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#21463;&#38480;&#26426;&#32452;&#32452;&#21512;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#27979;&#35797;&#31995;&#32479;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01570</link><description>&lt;p&gt;
&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#23433;&#20840;&#21463;&#38480;&#26426;&#32452;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Deep Learning-Assisted Reduced Security-Constrained Unit Commitment. (arXiv:2306.01570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#21463;&#38480;&#26426;&#32452;&#32452;&#21512;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#27979;&#35797;&#31995;&#32479;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21463;&#38480;&#26426;&#32452;&#32452;&#21512;&#20248;&#21270;&#65288;SCUC&#65289;&#26159;&#30005;&#21147;&#31995;&#32479;&#26085;&#21069;&#35843;&#24230;&#21644;&#24066;&#22330;&#28165;&#31639;&#20013;&#25152;&#38656;&#30340;&#19968;&#20010;&#35745;&#31639;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#38656;&#35201;&#27599;&#26085;&#36816;&#34892;&#65292;&#24182;&#38656;&#35201;&#20808;&#36827;&#30340;&#31639;&#27861;&#26469;&#21152;&#24555;&#36825;&#20010;&#36807;&#31243;&#12290;&#19982;SCUC&#30456;&#20851;&#30340;&#32422;&#26463;&#21644;&#25968;&#25454;&#37117;&#20855;&#26377;&#22320;&#29702;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#20854;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#30740;&#31350;&#30005;&#21147;&#31995;&#32479;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#22218;&#25324;&#32422;&#26463;&#26465;&#20214;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#27169;&#24335;&#12290;&#26102;&#31354;&#30456;&#20851;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#29702;&#35299;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#27979;&#35797;&#31995;&#32479;&#65288;&#22914;IEEE 24-Bus&#31995;&#32479;&#12289;IEEE 73-Bus&#31995;&#32479;&#12289;IEEE 118-Bus&#31995;&#32479;&#21644;SC 500-Bus&#31995;&#32479;&#65289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#22343;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security-constrained unit commitment (SCUC) is a computationally complex process utilized in power system day-ahead scheduling and market clearing. SCUC is run daily and requires state-of-the-art algorithms to speed up the process. The constraints and data associated with SCUC are both geographically and temporally correlated to ensure the reliability of the solution, which further increases the complexity. In this paper, an advanced machine learning (ML) model is used to study the patterns in power system historical data, which inherently considers both spatial and temporal (ST) correlations in constraints. The ST-correlated ML model is trained to understand spatial correlation by considering graph neural networks (GNN) whereas temporal sequences are studied using long short-term memory (LSTM) networks. The proposed approach is validated on several test systems namely, IEEE 24-Bus system, IEEE-73 Bus system, IEEE 118-Bus system, and synthetic South-Carolina (SC) 500-Bus system. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;AUC-opt&#65292;&#29992;&#20110;&#22312;$\mathbb{R}^2$&#20013;&#25214;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;AUC&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;$\mathbb{R}^d$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;AUC-opt&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;AUC&#20540;&#12290;&#20248;&#21270;AUC&#30830;&#23454;&#26377;&#20215;&#20540;&#65292;&#24182;&#19988;&#20808;&#21069;&#30740;&#31350;&#30340;&#38480;&#21046;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01528</link><description>&lt;p&gt;
AUC&#20248;&#21270;&#26159;&#21542;&#20540;&#24471;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does it pay to optimize AUC?. (arXiv:2306.01528v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;AUC-opt&#65292;&#29992;&#20110;&#22312;$\mathbb{R}^2$&#20013;&#25214;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;AUC&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;$\mathbb{R}^d$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;AUC-opt&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;AUC&#20540;&#12290;&#20248;&#21270;AUC&#30830;&#23454;&#26377;&#20215;&#20540;&#65292;&#24182;&#19988;&#20808;&#21069;&#30740;&#31350;&#30340;&#38480;&#21046;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#26159;&#20108;&#20803;&#20998;&#31867;&#22120;&#35780;&#20272;&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22411;&#25351;&#26631;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#36817;&#20284;&#20248;&#21270;AUC&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20808;&#21069;&#30740;&#31350;&#35266;&#23519;&#21040;&#30340;&#26222;&#36941;&#24494;&#19981;&#36275;&#36947;&#30340;&#25910;&#30410;&#26159;&#30001;&#25351;&#26631;&#22266;&#26377;&#30340;&#38480;&#21046;&#36824;&#26159;&#30001;&#20248;&#21270;&#30340;&#19981;&#36275;&#36136;&#37327;&#24341;&#36215;&#30340;&#65311;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20248;&#21270;AUC&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;AUC-opt&#65292;&#22312;$\mathbb{R}^2$&#20013;&#25214;&#21040;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;AUC&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$\mathcal{O}(n_+ n_- \log (n_+ n_-))$&#65292;&#20854;&#20013;$n_+$&#21644;$n_-$&#20998;&#21035;&#26159;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36882;&#24402;&#35843;&#29992;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;AUC-opt&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;$\mathbb{R}^d$&#20013;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;$d$&#19981;&#22266;&#23450;&#26102;&#65292;&#35813;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#30340;&#65292;&#20174;&#8220;&#24320;&#25918;&#21322;&#29699;&#38382;&#39064;&#8221;&#20013;&#20943;&#23569;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;AUC-opt&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;AUC&#20540;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;AUC&#30830;&#23454;&#26377;&#20215;&#20540;&#65292;&#24182;&#19988;&#20808;&#21069;&#30740;&#31350;&#30340;&#38480;&#21046;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Area Under the ROC Curve (AUC) is an important model metric for evaluating binary classifiers, and many algorithms have been proposed to optimize AUC approximately. It raises the question of whether the generally insignificant gains observed by previous studies are due to inherent limitations of the metric or the inadequate quality of optimization.  To better understand the value of optimizing for AUC, we present an efficient algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier in $\mathbb{R}^2$, which runs in $\mathcal{O}(n_+ n_- \log (n_+ n_-))$ where $n_+$ and $n_-$ are the number of positive and negative samples respectively. Furthermore, it can be naturally extended to $\mathbb{R}^d$ in $\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$ by calling AUC-opt in lower-dimensional spaces recursively. We prove the problem is NP-complete when $d$ is not fixed, reducing from the \textit{open hemisphere problem}.  Experiments show that compared with other methods, AUC-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01523</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#29992;&#20110;&#22810;&#26631;&#31614;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification. (arXiv:2306.01523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLC&#65289;&#26694;&#26550;&#19979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#27493;&#31867;&#20196;&#29260;&#34701;&#21512;&#65288;SCT Fusion&#65289;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#22312;&#27599;&#20010;Transformer&#32534;&#30721;&#22120;&#22359;&#21518;&#36890;&#36807;&#21516;&#27493;&#29305;&#27530;&#31867;&#20196;&#29260;&#26469;&#36328;&#27169;&#24577;&#20132;&#25442;&#20449;&#24687;&#12290;&#21516;&#27493;&#21253;&#25324;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#21464;&#25442;&#23558;&#31867;&#20196;&#29260;&#34701;&#21512;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#30340;&#21516;&#27493;&#31867;&#20196;&#29260;&#12290;&#30001;&#20110;&#34701;&#21512;&#21464;&#25442;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20801;&#35768;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#27169;&#24577;MLC&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#27604;&#21333;&#27169;&#24577;&#26550;&#26500;&#21644;&#26089;&#26399;&#34701;&#21512;&#22810;&#27169;&#24577;&#26550;&#26500;&#26356;&#26377;&#25928;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20195;&#30721;&#24050;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel Synchronized Class Token Fusion (SCT Fusion) architecture in the framework of multi-modal multi-label classification (MLC) of remote sensing (RS) images. The proposed architecture leverages modality-specific attention-based transformer encoders to process varying input modalities, while exchanging information across modalities by synchronizing the special class tokens after each transformer encoder block. The synchronization involves fusing the class tokens with a trainable fusion transformation, resulting in a synchronized class token that contains information from all modalities. As the fusion transformation is trainable, it allows to reach an accurate representation of the shared features among different modalities. Experimental results show the effectiveness of the proposed architecture over single-modality architectures and an early fusion multi-modal architecture when evaluated on a multi-modal MLC dataset.  The code of the proposed architectur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#21155;&#21270;&#29616;&#35937;&#65292;&#36825;&#20250;&#24433;&#21709;&#32593;&#32476;&#35757;&#32451;&#24182;&#23548;&#33268;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#31639;&#27861;&#26469;&#39044;&#27979;&#32593;&#32476;&#21155;&#21270;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.01513</link><description>&lt;p&gt;
&#32593;&#32476;&#21155;&#21270;&#20316;&#20026;&#35757;&#32451;&#24615;&#33021;&#35780;&#20272;&#30340;&#25351;&#26631;&#65306;&#26377;&#38480;&#21644;&#26080;&#38480;&#23485;&#24230;&#35282;&#24230;&#39044;&#27979;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions. (arXiv:2306.01513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#21155;&#21270;&#29616;&#35937;&#65292;&#36825;&#20250;&#24433;&#21709;&#32593;&#32476;&#35757;&#32451;&#24182;&#23548;&#33268;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#31639;&#27861;&#26469;&#39044;&#27979;&#32593;&#32476;&#21155;&#21270;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#21151;&#33021;&#24378;&#22823;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#29702;&#35770;&#34892;&#20026;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#36890;&#36807;&#22534;&#21472;&#35768;&#22810;&#23618;&#65292;&#21487;&#20197;&#21019;&#24314;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#20419;&#25104;&#20102;&#26368;&#36817;&#36825;&#20123;&#26041;&#27861;&#30340;&#29190;&#28856;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#21487;&#20197;&#25351;&#25968;&#32423;&#22686;&#21152;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#36234;&#26469;&#36234;&#28145;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#23481;&#26131;&#21464;&#24471;&#21155;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#36864;&#21270;&#29616;&#35937;&#65292;&#22240;&#20026;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#36755;&#20837;&#20542;&#21521;&#20110;&#22312;&#36890;&#36807;&#32593;&#32476;&#30340;&#23618;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#30456;&#20851;&#12290;&#22914;&#26524;&#19968;&#20010;&#32593;&#32476;&#26377;&#22826;&#22810;&#23618;&#65292;&#23427;&#20542;&#21521;&#20110;&#36924;&#36817;&#19968;&#20010;&#65288;&#38543;&#26426;&#30340;&#65289;&#24120;&#25968;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#26080;&#27861;&#21306;&#20998;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20284;&#20046;&#24433;&#21709;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#24182;&#23548;&#33268;&#23427;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#32593;&#32476;&#36798;&#21040;&#30340;&#21155;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are powerful functions with widespread use, but the theoretical behaviour of these functions is not fully understood. Creating deep neural networks by stacking many layers has achieved exceptional performance in many applications and contributed to the recent explosion of these methods. Previous works have shown that depth can exponentially increase the expressibility of the network. However, as networks get deeper and deeper, they are more susceptible to becoming degenerate. We observe this degeneracy in the sense that on initialization, inputs tend to become more and more correlated as they travel through the layers of the network. If a network has too many layers, it tends to approximate a (random) constant function, making it effectively incapable of distinguishing between inputs. This seems to affect the training of the network and cause it to perform poorly, as we empirically investigate in this paper. We use a simple algorithm that can accurately predict the leve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22914;GPT-4&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#30446;&#21069;&#36824;&#26080;&#27861;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.01499</link><description>&lt;p&gt;
LLMs&#65288;&#22914;GPT-4&#65289;&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#20013;&#33021;&#21542;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#65311;&#25110;&#35768;&#26377;&#28508;&#21147;&#65292;&#20294;&#29616;&#22312;&#36824;&#19981;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. (arXiv:2306.01499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22914;GPT-4&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#30446;&#21069;&#36824;&#26080;&#27861;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36824;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GPT-4&#26159;&#21542;&#33021;&#30452;&#25509;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#26367;&#20195;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#65288;&#22914;GPT-4&#65289;&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#24037;&#20855;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#20840;&#38754;&#25506;&#31350;&#20102;GPT-4&#21644;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#20248;&#32570;&#28857;&#12290;&#20004;&#20010;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;LLMs&#22312;&#26410;&#26469;&#30340;&#30196;&#21574;&#30151;&#35786;&#26029;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#20173;&#26080;&#27861;&#36229;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35780;&#20272;&#20102;GPT-4&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#21516;&#26102;&#20063;&#25351;&#20986;&#20102;&#23558;LLMs&#38598;&#25104;&#21040;&#21307;&#30103;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24490;&#29615;&#22270;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01494</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Local Message Passing on Frustrated Systems. (arXiv:2306.01494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#22522;&#20110;&#22256;&#38590;&#31995;&#32479;&#30340;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24490;&#29615;&#22270;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#22270;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26159;&#27010;&#29575;&#25512;&#29702;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26159;&#27714;&#21644;-&#31215;&#31639;&#27861;&#65288;SPA&#65289;&#65292;&#23427;&#22312;&#26641;&#19978;&#21487;&#24471;&#21040;&#31934;&#30830;&#32467;&#26524;&#65292;&#20294;&#22312;&#20855;&#26377;&#35768;&#22810;&#23567;&#24490;&#29615;&#30340;&#22270;&#19978;&#24448;&#24448;&#22833;&#36133;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26367;&#20195;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36825;&#31181;&#24490;&#29615;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;SPA&#30340;&#22806;&#37096;&#21407;&#21017;&#65292;&#36825;&#22833;&#21435;&#20102;&#22312;&#24490;&#29615;&#22270;&#19978;&#30340;&#23458;&#35266;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23558;&#24213;&#23618;&#22270;&#30340;&#22240;&#23376;&#33410;&#28857;&#22788;&#30340;&#26412;&#22320;SPA&#28040;&#24687;&#26356;&#26032;&#35268;&#21017;&#26367;&#25442;&#20026;&#36890;&#29992;&#26144;&#23556;&#12290;&#36825;&#20123;&#20462;&#25913;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;SPA&#30340;&#31616;&#21333;&#24615;&#12290;&#25105;&#20204;&#23545;&#20004;&#31867;&#24490;&#29615;&#22270;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;2x2&#23436;&#20840;&#36830;&#25509;&#30340;Ising&#32593;&#26684;&#21644;&#32447;&#24615;&#36890;&#20449;&#36890;&#36947;&#19978;&#30340;&#31526;&#21495;&#26816;&#27979;&#22240;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing on factor graphs is a powerful framework for probabilistic inference, which finds important applications in various scientific domains. The most wide-spread message passing scheme is the sum-product algorithm (SPA) which gives exact results on trees but often fails on graphs with many small cycles. We search for an alternative message passing algorithm that works particularly well on such cyclic graphs. Therefore, we challenge the extrinsic principle of the SPA, which loses its objective on graphs with cycles. We further replace the local SPA message update rule at the factor nodes of the underlying graph with a generic mapping, which is optimized in a data-driven fashion. These modifications lead to a considerable improvement in performance while preserving the simplicity of the SPA. We evaluate our method for two classes of cyclic graphs: the 2x2 fully connected Ising grid and factor graphs for symbol detection on linear communication channels with inter-symbol interf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#27169;&#22411;&#20013;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#20998;&#26512;&#20102;&#20887;&#20313;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20943;&#23569;&#29305;&#24449;&#38598;&#30340;&#20887;&#20313;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01489</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;&#33021;&#37327;&#27169;&#22411;&#20013;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
On Feature Diversity in Energy-based Models. (arXiv:2306.01489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#27169;&#22411;&#20013;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#20998;&#26512;&#20102;&#20887;&#20313;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20943;&#23569;&#29305;&#24449;&#38598;&#30340;&#20887;&#20313;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#21253;&#25324;&#20102;&#21508;&#31181;&#21028;&#21035;&#21644;&#29983;&#25104;&#26041;&#27861;&#12290;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#36890;&#24120;&#30001;&#20869;&#37096;&#27169;&#22411;&#32452;&#25104;&#65292;&#23398;&#20064;&#32452;&#21512;&#19981;&#21516;&#30340;&#29305;&#24449;&#26469;&#20026;&#27599;&#20010;&#36755;&#20837;&#37197;&#32622;&#29983;&#25104;&#33021;&#37327;&#26144;&#23556;&#12290;&#26412;&#25991;&#20851;&#27880;&#20135;&#29983;&#29305;&#24449;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;EBMs&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#29702;&#35770;&#65292;&#20998;&#26512;&#20102;&#20943;&#23569;&#20887;&#20313;&#23545;EBMs&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#36793;&#30028;&#65292;&#21363;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#38544;&#24335;&#22238;&#24402;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#30830;&#23454;&#20943;&#23569;&#29305;&#24449;&#38598;&#30340;&#20887;&#20313;&#21487;&#20197;&#19968;&#33268;&#22320;&#20943;&#23569;&#30495;&#23454;&#26399;&#26395;&#21644;&#32463;&#39564;&#26399;&#26395;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based learning is a powerful learning paradigm that encapsulates various discriminative and generative approaches. An energy-based model (EBM) is typically formed of inner-model(s) that learn a combination of the different features to generate an energy mapping for each input configuration. In this paper, we focus on the diversity of the produced feature set. We extend the probably approximately correct (PAC) theory of EBMs and analyze the effect of redundancy reduction on the performance of EBMs. We derive generalization bounds for various learning contexts, i.e., regression, classification, and implicit regression, with different energy functions and we show that indeed reducing redundancy of the feature set can consistently decrease the gap between the true and empirical expectation of the energy and boosts the performance of the model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36861;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21069;&#25552;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#19982;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01485</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#30340;&#27491;&#20132;&#32422;&#26463;&#23454;&#29616;&#31283;&#20581;&#30340;&#20302;&#31209;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust low-rank training via approximate orthonormal constraints. (arXiv:2306.01485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01485
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36861;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21069;&#25552;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#19982;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#35774;&#35745;&#21098;&#26525;&#25216;&#26415;&#20197;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#30340;&#36164;&#28304;&#38656;&#27714;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#24050;&#25104;&#20026;&#24191;&#27867;&#21162;&#21147;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#38477;&#20302;&#25512;&#29702;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#20027;&#35201;&#30340;&#24037;&#20316;&#26041;&#21521;&#20043;&#19968;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#26469;&#34920;&#31034;&#32593;&#32476;&#26435;&#37325;&#12290;&#23613;&#31649;&#33021;&#22815;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#20302;&#31209;&#26041;&#27861;&#24448;&#24448;&#20250;&#25439;&#23475;&#27169;&#22411;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#31283;&#20581;&#24615;&#24314;&#27169;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#25968;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31283;&#20581;&#24615;&#25439;&#22833;&#26159;&#30001;&#20110;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#29190;&#28856;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#20302;&#31209;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#32593;&#32476;&#26435;&#37325;&#20301;&#20110;&#20302;&#31209;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#24378;&#21046;&#26045;&#21152;&#36817;&#20284;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#38477;&#20302;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#33391;&#22909;&#30340;&#26465;&#20214;&#24615;&#21644;&#26356;&#22909;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of model and data sizes, a broad effort has been made to design pruning techniques that reduce the resource demand of deep learning pipelines, while retaining model performance. In order to reduce both inference and training costs, a prominent line of work uses low-rank matrix factorizations to represent the network weights. Although able to retain accuracy, we observe that low-rank methods tend to compromise model robustness against adversarial perturbations. By modeling robustness in terms of the condition number of the neural network, we argue that this loss of robustness is due to the exploding singular values of the low-rank weight matrices. Thus, we introduce a robust low-rank training algorithm that maintains the network's weights on the low-rank matrix manifold while simultaneously enforcing approximate orthonormal constraints. The resulting model reduces both training and inference costs while ensuring well-conditioning and thus better adversarial robustness, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#23618;&#27425;&#26032;&#22855;&#23547;&#27714;&#24847;&#22270;&#24182;&#35843;&#25972;&#25512;&#33616;&#31574;&#30053;&#20197;&#25552;&#39640;&#25512;&#33616;&#39033;&#30446;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01476</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#26032;&#22855;&#23547;&#27714;&#24847;&#22270;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems. (arXiv:2306.01476v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#23618;&#27425;&#26032;&#22855;&#23547;&#27714;&#24847;&#22270;&#24182;&#35843;&#25972;&#25512;&#33616;&#31574;&#30053;&#20197;&#25552;&#39640;&#25512;&#33616;&#39033;&#30446;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#26032;&#39062;&#20869;&#23481;&#21487;&#20197;&#36890;&#36807;&#21521;&#29992;&#25143;&#20171;&#32461;&#26032;&#30340;&#20852;&#36259;&#28857;&#26469;&#25913;&#21892;&#29992;&#25143;&#22312;&#25512;&#33616;&#24179;&#21488;&#19978;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#24182;&#19981;&#24635;&#26159;&#24819;&#35201;&#25506;&#32034;&#26032;&#39062;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#20182;&#20204;&#30340;&#23547;&#27714;&#26032;&#22855;&#30340;&#24847;&#22270;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#25512;&#33616;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#23618;&#27425;&#26032;&#22855;&#23547;&#27714;&#24847;&#22270;&#65292;&#24182;&#26681;&#25454;&#25552;&#21462;&#30340;&#29992;&#25143;&#23547;&#27714;&#26032;&#22855;&#20542;&#21521;&#24615;&#26469;&#35843;&#25972;&#25512;&#33616;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#39033;&#30446;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommending novel content, which expands user horizons by introducing them to new interests, has been shown to improve users' long-term experience on recommendation platforms \cite{chen2021values}. Users however are not constantly looking to explore novel content. It is therefore crucial to understand their novelty-seeking intent and adjust the recommendation policy accordingly. Most existing literature models a user's propensity to choose novel content or to prefer a more diverse set of recommendations at individual interactions. Hierarchical structure, on the other hand, exists in a user's novelty-seeking intent, which is manifested as a static and intrinsic user preference for seeking novelty along with a dynamic session-based propensity. To this end, we propose a novel hierarchical reinforcement learning-based method to model the hierarchical user novelty-seeking intent, and to adapt the recommendation policy accordingly based on the extracted user novelty-seeking propensity. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#23558;&#38754;&#21521;&#26041;&#38754;&#30340;&#25552;&#21462;&#21644;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#26032;&#30340;&#27169;&#22411;&#26469;&#20026;&#26368;&#32456;&#30340;&#25512;&#33616;&#20219;&#21153;&#29983;&#25104;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.01475</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#24615;&#21270;&#26041;&#38754;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations. (arXiv:2306.01475v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#23558;&#38754;&#21521;&#26041;&#38754;&#30340;&#25552;&#21462;&#21644;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#26032;&#30340;&#27169;&#22411;&#26469;&#20026;&#26368;&#32456;&#30340;&#25512;&#33616;&#20219;&#21153;&#29983;&#25104;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26041;&#38754;&#25552;&#21462;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26174;&#24335;&#25110;&#22522;&#30784;&#20107;&#23454;&#26041;&#38754;&#20449;&#24687;&#65292;&#25110;&#32773;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#38544;&#21547;&#29992;&#25143;&#21453;&#39304;&#65288;&#20363;&#22914;&#29992;&#25143;&#35780;&#35770;&#65289;&#20013;&#25552;&#21462;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#25552;&#21462;&#20986;&#30340;&#26041;&#38754;&#29983;&#25104;&#26356;&#26377;&#24847;&#20041;&#30340;&#25512;&#33616;&#32473;&#29992;&#25143;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#29420;&#30340;&#26041;&#38754;&#25552;&#21462;&#27169;&#22411;&#25110;&#20551;&#35774;&#26041;&#38754;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#26368;&#20339;&#26041;&#38754;&#38598;&#21487;&#33021;&#21462;&#20915;&#20110;&#25163;&#22836;&#30340;&#25512;&#33616;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26041;&#38754;&#25552;&#21462;&#19982;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#32467;&#21512;&#36215;&#26469;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#22312;&#21333;&#20010;&#26694;&#26550;&#20013;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;&#23545;&#20110;&#26041;&#38754;&#25552;&#21462;&#32452;&#20214;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#26469;&#20026;&#26368;&#32456;&#30340;&#25512;&#33616;&#20219;&#21153;&#29983;&#25104;&#26041;&#38754;&#12290;&#23545;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#32452;&#20214;&#65292;&#25105;&#20204;&#24341;&#36827;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#38754;&#25552;&#21462;&#27169;&#22411;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26041;&#38754;&#25552;&#21462;&#21644;&#22522;&#20110;&#26041;&#38754;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#37117;&#20248;&#20110;&#20960;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand.  In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01474</link><description>&lt;p&gt;
&#36890;&#29992;&#31561;&#21464;Transformer&#65306;&#29992;&#20110;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#35768;&#22810;&#36807;&#31243;&#28041;&#21450;&#19981;&#21516;&#20998;&#23376;&#20043;&#38388;&#30340;&#21508;&#31181;3D&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#19982;&#34507;&#30333;&#36136;&#65292;&#34507;&#30333;&#36136;&#19982;&#23567;&#20998;&#23376;&#31561;&#12290;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26469;&#23398;&#20064;&#26222;&#36866;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20998;&#23376;&#36890;&#24120;&#20197;&#19981;&#21516;&#31890;&#24230;&#34920;&#31034;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#23558;3D&#20998;&#23376;&#36890;&#29992;&#34920;&#31034;&#20026;&#38598;&#21512;&#30340;&#20960;&#20309;&#22270;&#24418;&#22270;&#65292;&#19982;&#20256;&#32479;&#21333;&#23618;&#34920;&#31034;&#24418;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#34920;&#31034;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#31561;&#21464;Transformer&#65288;GET&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#31232;&#30095;&#22359;&#32423;&#21644;&#23494;&#38598;&#21407;&#23376;&#32423;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GET&#30001;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#32452;&#25104;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#20197;&#28385;&#36275;3D&#19990;&#30028;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;GET&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#20197;&#25552;&#39640;&#26367;&#25442;&#30340;&#21477;&#27861;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01471</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Guiding Text-to-Text Privatization by Syntax. (arXiv:2306.01471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#20197;&#25552;&#39640;&#26367;&#25442;&#30340;&#21477;&#27861;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#26631;&#24046;&#20998;&#38544;&#31169;&#26159;&#38024;&#23545;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#32780;&#35774;&#35745;&#30340;&#24046;&#20998;&#38544;&#31169;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#36890;&#36807;&#21521;&#23884;&#20837;&#30340;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21333;&#35789;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#65292;&#21333;&#35789;&#34987;&#26367;&#25442;&#20026;&#22312;&#22122;&#22768;&#34920;&#31034;&#30340;&#25509;&#36817;&#20301;&#32622;&#30340;&#21333;&#35789;&#12290;&#30001;&#20110;&#23884;&#20837;&#24335;&#26159;&#22522;&#20110;&#21333;&#35789;&#20849;&#29616;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#26426;&#21046;&#30830;&#20445;&#26367;&#25442;&#28304;&#20110;&#30456;&#21516;&#30340;&#35821;&#20041;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#21333;&#35789;&#30340;&#35821;&#27861;&#31867;&#21035;&#65292;&#36825;&#31181;&#26426;&#21046;&#23601;&#26080;&#27861;&#20445;&#35777;&#26367;&#25442;&#25198;&#28436;&#30456;&#20284;&#30340;&#21477;&#27861;&#35282;&#33394;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#22312;&#26367;&#25442;&#21518;&#20445;&#30041;&#21333;&#35789;&#35821;&#27861;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20195;&#29992;&#25991;&#26412;&#20013;&#20960;&#20046;&#23436;&#20840;&#30001;&#21517;&#35789;&#26500;&#25104;&#12290;&#30001;&#20110;&#32570;&#23569;&#20135;&#29983;&#19982;&#25935;&#24863;&#25991;&#26412;&#32467;&#26500;&#30456;&#20851;&#30340;&#20195;&#29992;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20445;&#25252;&#27493;&#39588;&#36716;&#25442;&#20026;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric Differential Privacy is a generalization of differential privacy tailored to address the unique challenges of text-to-text privatization. By adding noise to the representation of words in the geometric space of embeddings, words are replaced with words located in the proximity of the noisy representation. Since embeddings are trained based on word co-occurrences, this mechanism ensures that substitutions stem from a common semantic context. Without considering the grammatical category of words, however, this mechanism cannot guarantee that substitutions play similar syntactic roles. We analyze the capability of text-to-text privatization to preserve the grammatical category of words after substitution and find that surrogate texts consist almost exclusively of nouns. Lacking the capability to produce surrogate texts that correlate with the structure of the sensitive texts, we encompass our analysis by transforming the privatization step into a candidate selection problem in whic
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;MLP&#26377;&#28508;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;MLP-Mixer &#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.01470</link><description>&lt;p&gt;
MLP-Mixer&#20316;&#20026;&#23485;&#19988;&#31232;&#30095;&#30340;MLP
&lt;/p&gt;
&lt;p&gt;
MLP-Mixer as a Wide and Sparse MLP. (arXiv:2306.01470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01470
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;MLP&#26377;&#28508;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;MLP-Mixer &#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#38382;&#39064;&#30340;&#22522;&#30784;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;MLP&#30340;&#26550;&#26500;(&#29305;&#21035;&#26159;MLP-Mixer)&#30340;&#23454;&#35777;&#25104;&#21151;&#34920;&#26126;&#65292;&#25552;&#39640;MLP&#30340;&#24615;&#33021;&#20173;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MLP-Mixer&#26377;&#25928;&#22320;&#20316;&#20026;&#20855;&#26377;&#26576;&#20123;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#28548;&#28165;Mixer&#30340;&#28151;&#21512;&#23618;&#21487;&#20197;&#20316;&#20026;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#19988;&#30001;Kronecker&#20056;&#31215;&#34920;&#31034;&#30340;&#26356;&#23485;MLP&#30340;&#26377;&#25928;&#34920;&#36798;&#12290;&#35813;&#34920;&#36798;&#24335;&#33258;&#28982;&#22320;&#23450;&#20041;&#20102;&#19968;&#32452;&#32622;&#25442;-Kronecker(PK)&#23478;&#26063;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#28151;&#21512;&#23618;&#30340;&#19968;&#33324;&#31867;&#65292;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;Monarch&#30697;&#38453;&#30340;&#19968;&#31181;&#36817;&#20284;&#12290;&#38543;&#21518;&#65292;&#30001;&#20110;PK&#23478;&#26063;&#26377;&#25928;&#26500;&#25104;&#20855;&#26377;&#31232;&#30095;&#26435;&#37325;&#30340;&#23485;MLP&#65292;&#22240;&#27492;&#65292;&#21487;&#20197;&#24212;&#29992;Golubeva&#12289;Neyshabur&#21644;Gur-Ari(2021)&#25552;&#20986;&#30340;&#20551;&#35774;&#65292;&#21363;&#39044;&#27979;&#24615;&#33021;&#65306;
&lt;/p&gt;
&lt;p&gt;
Multi-layer perceptron (MLP) is a fundamental component of deep learning that has been extensively employed for various problems. However, recent empirical successes in MLP-based architectures, particularly the progress of the MLP-Mixer, have revealed that there is still hidden potential in improving MLPs to achieve better performance. In this study, we reveal that the MLP-Mixer works effectively as a wide MLP with certain sparse weights. Initially, we clarify that the mixing layer of the Mixer has an effective expression as a wider MLP whose weights are sparse and represented by the Kronecker product. This expression naturally defines a permuted-Kronecker (PK) family, which can be regarded as a general class of mixing layers and is also regarded as an approximation of Monarch matrices. Subsequently, because the PK family effectively constitutes a wide MLP with sparse weights, one can apply the hypothesis proposed by Golubeva, Neyshabur and Gur-Ari (2021) that the prediction performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26080;&#25439;&#27979;&#35797;&#39046;&#22495;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#22522;&#20110;&#21322;&#35299;&#26512;&#27169;&#25311;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#25968;&#25454;&#20998;&#31867;&#12290;&#20854;&#20013;&#31532;&#19968;&#31181;&#26041;&#27861;&#20462;&#25913;&#20102;CycleGAN&#65292;&#20174;&#29289;&#29702;&#27169;&#25311;&#21040;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.01469</link><description>&lt;p&gt;
GANs&#21644;&#26367;&#20195;&#26041;&#27861;&#30340;&#21512;&#25104;&#22122;&#22768;&#29983;&#25104;&#29992;&#20110;&#26080;&#25439;&#36229;&#22768;&#27979;&#35797;&#20013;&#32570;&#38519;&#20998;&#31867;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
GANs and alternative methods of synthetic noise generation for domain adaption of defect classification of Non-destructive ultrasonic testing. (arXiv:2306.01469v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#25439;&#27979;&#35797;&#39046;&#22495;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#22522;&#20110;&#21322;&#35299;&#26512;&#27169;&#25311;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#25968;&#25454;&#20998;&#31867;&#12290;&#20854;&#20013;&#31532;&#19968;&#31181;&#26041;&#27861;&#20462;&#25913;&#20102;CycleGAN&#65292;&#20174;&#29289;&#29702;&#27169;&#25311;&#21040;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22797;&#21512;&#26448;&#26009;&#32452;&#20214;&#26080;&#25439;&#36229;&#22768;&#27979;&#35797;&#20013;&#25968;&#25454;&#35757;&#32451;&#19981;&#36275;&#30340;&#25361;&#25112;&#30340;&#26041;&#26696;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30452;&#25509;&#30340;&#27169;&#25311;&#26041;&#27861;&#30001;&#20110;&#22122;&#22768;&#37325;&#26500;&#24046;&#32780;&#26080;&#27861;&#20135;&#29983;&#20195;&#34920;&#23454;&#39564;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#22522;&#20110;&#21322;&#35299;&#26512;&#27169;&#25311;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#29305;&#21035;&#20462;&#25913;&#20102;CycleGAN&#20197;&#20174;&#22522;&#20110;&#29289;&#29702;&#30340;&#32570;&#38519;&#27169;&#25311;&#21040;&#23454;&#39564;&#32570;&#38519;&#30340;&#26144;&#23556;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21017;&#22522;&#20110;&#23558;&#30495;&#23454;&#23454;&#39564;&#26080;&#32570;&#38519;&#22270;&#29255;&#19982;&#27169;&#25311;&#30340;&#32570;&#38519;&#21709;&#24212;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#20004;&#31181;&#26041;&#27861;&#26159;&#23436;&#20840;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a solution to the challenge of small amounts of training data in Non-Destructive Ultrasonic Testing for composite components. It was demonstrated that direct simulation alone is ineffective at producing training data that was representative of the experimental domain due to poor noise reconstruction. Therefore, four unique synthetic data generation methods were proposed which use semi-analytical simulated data as a foundation. Each method was evaluated on its classification performance of real experimental images when trained on a Convolutional Neural Network which underwent hyperparameter optimization using a genetic algorithm. The first method introduced task specific modifications to CycleGAN, to learn the mapping from physics-based simulations of defect indications to experimental indications in resulting ultrasound images. The second method was based on combining real experimental defect free images with simulated defect responses. The final two methods fully si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#30740;&#31350;&#34920;&#26126;&#65292;XAI &#26041;&#27861;&#22312;&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102;&#35299;&#37322;&#21487;&#33021;&#20986;&#29616;&#35823;&#23548;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#21152;&#29702;&#35770;&#21270;&#21644;&#32463;&#39564;&#21270;&#30340;&#30740;&#31350;&#65292;&#30830;&#20445;&#20854;&#24212;&#29992;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01464</link><description>&lt;p&gt;
&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102; XAI &#26041;&#27861;&#30340;&#29702;&#35770;&#34892;&#20026;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables. (arXiv:2306.01464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#30740;&#31350;&#34920;&#26126;&#65292;XAI &#26041;&#27861;&#22312;&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102;&#35299;&#37322;&#21487;&#33021;&#20986;&#29616;&#35823;&#23548;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#21152;&#29702;&#35770;&#21270;&#21644;&#32463;&#39564;&#21270;&#30340;&#30740;&#31350;&#65292;&#30830;&#20445;&#20854;&#24212;&#29992;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#8220;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#8221;&#65288;XAI&#65289;&#31038;&#21306;&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#37327;&#30340;&#26041;&#27861;&#26469;&#24357;&#21512;&#27169;&#22411;&#8220;&#22797;&#26434;&#24230;&#8221;&#21644;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;XAI &#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#30340;&#20855;&#20307;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#35828;&#26126;&#12290;&#22240;&#27492;&#65292;XAI &#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#20197;&#39564;&#35777;&#20854;&#35299;&#37322;&#30340;&#8220;&#27491;&#30830;&#24615;&#8221;&#65292;&#38480;&#21046;&#20102;&#20854;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#21644;&#36879;&#26126;&#24230;&#30446;&#30340;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;Haufe&#31561;&#20154;&#65288;2014&#65289;&#20351;&#29992;&#31616;&#21333;&#30340;&#29609;&#20855;&#20363;&#23376;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#27169;&#22411;&#30340;&#26631;&#20934;&#35299;&#37322;&#20063;&#21487;&#33021;&#26497;&#20855;&#35823;&#23548;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#33021;&#20250;&#34987;&#24402;&#22240;&#20110;&#25152;&#35859;&#30340;&#25233;&#21046;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#19982;&#39044;&#27979;&#30446;&#26631;&#32570;&#20047;&#20219;&#20309;&#32479;&#35745;&#20851;&#31995;&#12290;Wilming&#31561;&#20154;&#65288;2022&#65289;&#24050;&#32463;&#32463;&#39564;&#35777;&#20102;&#36825;&#31181;&#34892;&#20026;&#22312;&#22823;&#37327; XAI &#26041;&#27861;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#22810;&#31181;&#27969;&#34892;&#30340; XAI &#26041;&#27861;&#22312;&#31616;&#21333;&#30340; toy dataset &#19978;&#30340;&#34892;&#20026;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the community of 'explainable artificial intelligence' (XAI) has created a vast body of methods to bridge a perceived gap between model 'complexity' and 'interpretability'. However, a concrete problem to be solved by XAI methods has not yet been formally stated. As a result, XAI methods are lacking theoretical and empirical evidence for the 'correctness' of their explanations, limiting their potential use for quality-control and transparency purposes. At the same time, Haufe et al. (2014) showed, using simple toy examples, that even standard interpretations of linear models can be highly misleading. Specifically, high importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target. This behavior has been confirmed empirically for a large array of XAI methods in Wilming et al. (2022). Here, we go one step further by deriving analytical expressions for the behavior of a variety of popular XAI methods on a simple tw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01460</link><description>&lt;p&gt;
ReLU&#25327;&#25937;&#65306;&#29992;&#27491;&#25968;&#20248;&#21183;&#25913;&#36827;&#24744;&#30340;On-Policy Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26126;&#30830;&#22320;&#25972;&#21512;&#35880;&#24910;&#30340;&#29615;&#22659;&#20132;&#20114;&#26469;&#35299;&#20915;&#24403;&#21069;On-Policy&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#21644;Asynchronous Advantage Actor-Critic&#65289;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#30495;&#23454;&#20215;&#20540;&#20989;&#25968;&#21152;&#19978;&#24120;&#37327;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#20419;&#36827;&#8220;&#20445;&#23432;&#20540;&#20272;&#35745;&#8221;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Thompson&#37319;&#26679;&#26469;&#36827;&#34892;&#35880;&#24910;&#25506;&#32034;&#12290;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#23545;A3C&#31639;&#27861;&#36827;&#34892;&#19977;&#20010;&#24778;&#20154;&#31616;&#21333;&#30340;&#20462;&#25913;&#23454;&#29616;&#65306;&#36890;&#36807;ReLU&#20989;&#25968;&#22788;&#29702;&#20248;&#21183;&#20272;&#35745;&#65292;&#36827;&#34892;&#35889;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#22833;&#27963;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26368;&#22823;&#21270;&#20102;&#19979;&#30028;&#65292;&#36825;&#20063;&#26159;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;Regret Matching Policy Gradients&#65288;RMPG&#65289;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel method for enhancing the effectiveness of on-policy Deep Reinforcement Learning (DRL) algorithms. Current on-policy algorithms, such as Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), do not sufficiently account for cautious interaction with the environment. Our method addresses this gap by explicitly integrating cautious interaction in two critical ways: by maximizing a lower-bound on the true value function plus a constant, thereby promoting a \textit{conservative value estimation}, and by incorporating Thompson sampling for cautious exploration. These features are realized through three surprisingly simple modifications to the A3C algorithm: processing advantage estimates through a ReLU function, spectral normalization, and dropout. We provide theoretical proof that our algorithm maximizes the lower bound, which also grounds Regret Matching Policy Gradients (RMPG), a discrete-action on-policy method for multi-agen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#20197;&#25552;&#39640;&#27495;&#20041;&#21333;&#35789;&#26367;&#20195;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01457</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#25991;&#26412;-&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#20013;
&lt;/p&gt;
&lt;p&gt;
Driving Context into Text-to-Text Privatization. (arXiv:2306.01457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#20197;&#25552;&#39640;&#27495;&#20041;&#21333;&#35789;&#26367;&#20195;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#8221;&#36890;&#36807;&#21521;&#20174;&#23884;&#20837;&#31354;&#38388;&#20013;&#23548;&#20986;&#30340;&#21333;&#35789;&#21521;&#37327;&#28155;&#21152;&#32463;&#36807;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#23558;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#21521;&#37327;&#25237;&#24433;&#22238;&#31163;&#25955;&#35789;&#27719;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#12290;&#30001;&#20110;&#21333;&#35789;&#26159;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#26367;&#20195;&#65292;&#22240;&#27492;&#35813;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#20855;&#26377;&#27495;&#20041;&#21547;&#20041;&#30340;&#21333;&#35789;&#30340;&#26367;&#20195;&#35789;&#65292;&#20363;&#22914;&#8220;bank&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27169;&#26865;&#20004;&#21487;&#30340;&#21333;&#35789;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#35821;&#20041;&#23884;&#20837;&#24182;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#23545;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#30340;&#20462;&#25913;&#12290;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21333;&#35789;&#20041;&#28040;&#27495;&#35797;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#36798;&#21040;&#20102;$6.05&#65285;$&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Metric Differential Privacy} enables text-to-text privatization by adding calibrated noise to the vector of a word derived from an embedding space and projecting this noisy vector back to a discrete vocabulary using a nearest neighbor search. Since words are substituted without context, this mechanism is expected to fall short at finding substitutes for words with ambiguous meanings, such as \textit{'bank'}. To account for these ambiguous words, we leverage a sense embedding and incorporate a sense disambiguation step prior to noise injection. We encompass our modification to the privatization mechanism with an estimation of privacy and utility. For word sense disambiguation on the \textit{Words in Context} dataset, we demonstrate a substantial increase in classification accuracy by $6.05\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01443</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#25913;&#20889;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25913;&#20889;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35821;&#26009;&#24211;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#65292;&#22914;&#23383;&#20856;&#12290;&#25105;&#20204;&#22312;SemEval 2022&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#31995;&#32479;&#24182;&#19982;&#26377;&#30417;&#30563;&#31995;&#32479;&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TVC-GMM&#30340;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;FastSpeech 2&#21512;&#25104;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;Mel&#39057;&#35889;&#24179;&#28369;&#24230;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#30340;&#21548;&#35273;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01442</link><description>&lt;p&gt;
&#36890;&#36807;&#24314;&#27169;&#27531;&#24046;&#22810;&#27169;&#24577;&#23454;&#29616;&#40065;&#26834;&#30340;FastSpeech 2
&lt;/p&gt;
&lt;p&gt;
Towards Robust FastSpeech 2 by Modelling Residual Multimodality. (arXiv:2306.01442v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TVC-GMM&#30340;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;FastSpeech 2&#21512;&#25104;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;Mel&#39057;&#35889;&#24179;&#28369;&#24230;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#30340;&#21548;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;FastSpeech 2&#30340;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#33258;&#28982;&#24230;&#30340;&#35821;&#38899;&#65292;&#20294;&#23545;&#20110;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#29305;&#24449;&#38899;&#39057;&#22833;&#30495;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20123;&#20266;&#24433;&#26159;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;Mel&#39057;&#35889;&#39044;&#27979;&#24341;&#20837;&#30340;&#65292;&#32780;&#36825;&#26159;&#30001;&#20110;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26469;&#35757;&#32451;Mel&#39057;&#35889;&#35299;&#30721;&#22120;&#25152;&#33268;&#12290;FastSpeech 2&#20351;&#29992;MSE&#25439;&#22833;&#34987;&#38480;&#21046;&#20026;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#30340;&#26465;&#20214;&#24179;&#22343;&#20540;&#65292;&#22914;&#26524;&#25152;&#26377;&#30340;&#35843;&#21046;&#20449;&#21495;&#21518;&#20998;&#24067;&#20173;&#28982;&#21576;&#29616;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#21017;&#36825;&#20123;&#20540;&#21487;&#33021;&#19982;&#33258;&#28982;&#26679;&#26412;&#24182;&#19981;&#25509;&#36817;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TVC-GMM&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#27531;&#24046;&#22810;&#27169;&#24577;&#12290;TVC-GMM&#38477;&#20302;&#20102;&#39057;&#35889;&#30340;&#24179;&#28369;&#24615;&#65292;&#24182;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#23545;&#34920;&#29616;&#24615;&#25968;&#25454;&#38598;&#29305;&#21035;&#26377;&#25928;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21548;&#35273;&#38899;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art non-autoregressive text-to-speech (TTS) models based on FastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For expressive speech datasets however, we observe characteristic audio distortions. We demonstrate that such artefacts are introduced to the vocoder reconstruction by over-smooth mel-spectrogram predictions, which are induced by the choice of mean-squared-error (MSE) loss for training the mel-spectrogram decoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of the training distribution, which might not lie close to a natural sample if the distribution still appears multimodal after all conditioning signals. To alleviate this problem, we introduce TVC-GMM, a mixture model of Trivariate-Chain Gaussian distributions, to model the residual multimodality. TVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in particular for expressive datasets as shown by both objective and subjective evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#29256;&#22522;&#22240;&#32676;&#20307;&#35757;&#32451;&#31639;&#27861;MO-PBT&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#31181;&#20914;&#31361;&#30446;&#26631;&#19979;&#22343;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01436</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#22522;&#22240;&#32676;&#20307;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Population Based Training. (arXiv:2306.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01436
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#29256;&#22522;&#22240;&#32676;&#20307;&#35757;&#32451;&#31639;&#27861;MO-PBT&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#31181;&#20914;&#31361;&#30446;&#26631;&#19979;&#22343;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32676;&#20307;&#35757;&#32451;&#65288;PBT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#12290;PBT&#26159;&#19968;&#31181;&#21333;&#30446;&#26631;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#28041;&#21450;&#20004;&#20010;&#25110;&#26356;&#22810;&#20914;&#31361;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#29256;&#30340;PBT&#65288;MO-PBT&#65289;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65288;&#31934;&#30830;&#24230;/&#21484;&#22238;&#29575;&#12289;&#20934;&#30830;&#24230;/&#20844;&#24179;&#24615;&#12289;&#20934;&#30830;&#24230;/&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MO-PBT&#20248;&#20110;&#38543;&#26426;&#25628;&#32034;&#12289;&#21333;&#30446;&#26631;PBT&#21644;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;MO-ASHA&#12290;
&lt;/p&gt;
&lt;p&gt;
Population Based Training (PBT) is an efficient hyperparameter optimization algorithm. PBT is a single-objective algorithm, but many real-world hyperparameter optimization problems involve two or more conflicting objectives. In this work, we therefore introduce a multi-objective version of PBT, MO-PBT. Our experiments on diverse multi-objective hyperparameter optimization problems (Precision/Recall, Accuracy/Fairness, Accuracy/Adversarial Robustness) show that MO-PBT outperforms random search, single-objective PBT, and the state-of-the-art multi-objective hyperparameter optimization algorithm MO-ASHA.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#26356;&#26032;&#36755;&#20837;&#26469;&#38477;&#20302;&#39044;&#27979;&#29109;&#65292;&#20174;&#32780;&#25552;&#39640;DEQ&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01435</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#26174;&#24335;&#35268;&#23450;&#26469;&#25552;&#39640;DEQ&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (arXiv:2306.01435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#26356;&#26032;&#36755;&#20837;&#26469;&#38477;&#20302;&#39044;&#27979;&#29109;&#65292;&#20174;&#32780;&#25552;&#39640;DEQ&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#65288; DEQ &#65289;&#27169;&#22411;&#23558;&#20256;&#32479;&#28145;&#23618;&#32593;&#32476;&#30340;&#22810;&#23618;&#22534;&#21472;&#26367;&#25442;&#20026;&#21333;&#23618;&#21464;&#25442;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#12290;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013; DEQ &#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#22240;&#27492;&#19968;&#33324; DEQ &#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288; AT&#65289;&#26694;&#26550;&#26469;&#25552;&#39640;&#19968;&#33324; DEQ &#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21033;&#29992; DEQ &#27169;&#22411;&#30340;&#32467;&#26500;&#29420;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#35270;&#35282;&#35299;&#37322; DEQs&#65292;&#24182;&#21457;&#29616; AT &#23545;&#20013;&#38388;&#29366;&#24577;&#36827;&#34892;&#20102;&#19981;&#20805;&#20998;&#30340;&#35268;&#23450;&#12290;&#27492;&#22806;&#65292;&#20013;&#38388;&#29366;&#24577;&#36890;&#24120;&#25552;&#20379;&#20855;&#26377;&#39640;&#39044;&#27979;&#29109;&#30340;&#39044;&#27979;&#12290;&#21463;&#21160;&#24577;&#31995;&#32479;&#29109;&#19982;&#20854;&#31283;&#23450;&#24615;&#36136;&#20043;&#38388;&#20851;&#32852;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#27839;&#30528;&#31070;&#32463;&#21160;&#21147;&#23398;&#36880;&#27493;&#26356;&#26032;&#36755;&#20837;&#26469;&#38477;&#20302;&#39044;&#27979;&#29109;&#12290;&#22312; AT &#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#38543;&#26426;&#20013;&#38388;&#29366;&#24577;t
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general DEQs becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general DEQ models with the widely-used adversarial training (AT) framework, but they fail to exploit the structural uniquenesses of DEQ models. To this end, we interpret DEQs through the lens of neural dynamics and find that AT under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During AT, we also utilize random intermediate states t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01433</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Blind Audio Bandwidth Extension. (arXiv:2306.01433v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#28041;&#21450;&#20174;&#24102;&#23485;&#21463;&#38480;&#30340;&#35266;&#27979;&#20449;&#21495;&#20013;&#23454;&#29616;&#39640;&#39057;&#35889;&#30340;&#36924;&#30495;&#37325;&#24314;&#12290;&#22312;&#20302;&#36890;&#20449;&#21495;&#36864;&#21270;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#23545;&#21382;&#21490;&#38899;&#39057;&#35760;&#24405;&#30340;&#24674;&#22797;&#65292;&#36825;&#20415;&#25104;&#20102;&#19968;&#20010;&#30450;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65288;Blind Audio Bandwidth Extension&#65289;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#30450;&#38382;&#39064;&#65292;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;BABE&#21033;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#29256;&#26412;&#30340;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#65292;&#22312;&#20854;&#20013;&#36864;&#21270;&#31639;&#23376;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#26159;&#36890;&#36807;&#36845;&#20195;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#25512;&#26029;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#26159;&#29992;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#35780;&#20272;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;BABE&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#21512;&#25104;&#25968;&#25454;&#26102;&#19982;&#38750;&#30450;&#28388;&#27874;&#22120;&#30693;&#24773;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;BABE&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio bandwidth extension involves the realistic reconstruction of high-frequency spectra from bandlimited observations. In cases where the lowpass degradation is unknown, such as in restoring historical audio recordings, this becomes a blind problem. This paper introduces a novel method called BABE (Blind Audio Bandwidth Extension) that addresses the blind problem in a zero-shot setting, leveraging the generative priors of a pre-trained unconditional diffusion model. During the inference process, BABE utilizes a generalized version of diffusion posterior sampling, where the degradation operator is unknown but parametrized and inferred iteratively. The performance of the proposed method is evaluated using objective and subjective metrics, and the results show that BABE surpasses state-of-the-art blind bandwidth extension baselines and achieves competitive performance compared to non-blind filter-informed methods when tested with synthetic data. Moreover, BABE exhibits robust generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35270;&#35273;&#20449;&#24687;&#20026;&#26465;&#20214;&#30340;&#38899;&#35270;&#39057;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#20102;&#35821;&#38899;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2306.01432</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#38899;&#35270;&#39057;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Speech Enhancement with Score-Based Generative Models. (arXiv:2306.01432v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35270;&#35273;&#20449;&#24687;&#20026;&#26465;&#20214;&#30340;&#38899;&#35270;&#39057;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#20102;&#35821;&#38899;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#38899;&#35270;&#39057;&#35821;&#38899;&#22686;&#24378;&#30340;&#31995;&#32479;&#65292;&#35813;&#27169;&#22411;&#20197;&#35270;&#35273;&#20449;&#24687;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#20013;&#33719;&#24471;&#30340;&#38899;&#35270;&#39057;&#23884;&#20837;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#22312;&#21475;&#22411;&#35782;&#21035;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23427;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32534;&#30721;&#22120;&#30340;&#36880;&#23618;&#29305;&#24449;&#34987;&#32858;&#21512;&#12289;&#26102;&#38388;&#23545;&#40784;&#24182;&#34701;&#21512;&#21040;&#22122;&#22768;&#26465;&#20214;&#20998;&#20540;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38899;&#35270;&#39057;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#25913;&#36827;&#20316;&#29992;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#20165;&#38899;&#39057;&#31561;&#25928;&#30340;&#29983;&#25104;&#30011;&#38754;&#20013;&#30340;&#35821;&#38899;&#28151;&#28102;&#12290;&#36825;&#24471;&#21040;&#20102;&#19979;&#28216;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#30340;&#25903;&#25345;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20449;&#22122;&#27604;&#36755;&#20837;&#26102;&#65292;&#21333;&#35789;&#38169;&#35823;&#29575;&#26126;&#26174;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an audio-visual speech enhancement system that leverages score-based generative models, also known as diffusion models, conditioned on visual information. In particular, we exploit audio-visual embeddings obtained from a self-super\-vised learning model that has been fine-tuned on lipreading. The layer-wise features of its transformer-based encoder are aggregated, time-aligned, and incorporated into the noise conditional score network. Experimental evaluations show that the proposed audio-visual speech enhancement system yields improved speech quality and reduces generative artifacts such as phonetic confusions with respect to the audio-only equivalent. The latter is supported by the word error rate of a downstream automatic speech recognition model, which decreases noticeably, especially at low input signal-to-noise ratios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.01431</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#23637;&#26395;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#36879;&#35270;
&lt;/p&gt;
&lt;p&gt;
On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26102;&#65292;&#22312;&#25972;&#20010;&#26694;&#26550;&#19978;&#31616;&#21333;&#22320;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20250;&#23548;&#33268;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;FL&#30740;&#31350;&#38598;&#20013;&#20110;&#35774;&#35745;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#24182;&#22686;&#21152;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36951;&#24536;&#24182;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#36951;&#24536;&#65292;&#21253;&#25324;&#28040;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21487;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#24182;&#20026;&#33719;&#21462;&#26032;&#30693;&#35782;&#21019;&#36896;&#39069;&#22806;&#30340;&#8220;&#31354;&#38388;&#8221;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#24191;&#27867;&#35843;&#26597;&#28085;&#30422;&#26368;&#26032;&#36827;&#23637;&#24182;&#23545;&#27492;&#38382;&#39064;&#36827;&#34892;&#20840;&#38754;&#26816;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#65288;&#22686;&#24378;/&#21024;&#38500;&#65289;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#26088;&#22312;&#24635;&#32467;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20272;&#35745;&#20013;&#38388;&#26799;&#24230;&#20197;&#25913;&#36827;&#25915;&#20987;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01429</link><description>&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20877;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Adversarial Robustness of Deep Equilibrium Models. (arXiv:2306.01429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20272;&#35745;&#20013;&#38388;&#26799;&#24230;&#20197;&#25913;&#36827;&#25915;&#20987;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#25682;&#24323;&#20102;&#20256;&#32479;&#30340;&#23618;&#21472;&#26041;&#27861;&#65292;&#36716;&#32780;&#23547;&#25214;&#21333;&#19968;&#23618;&#30340;&#22266;&#23450;&#28857;&#12290;DEQ&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#29305;&#24449;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;DEQ&#30340;&#23545;&#25239;&#24615;&#28431;&#27934;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#21333;&#35843;DEQ&#36827;&#34892;&#40065;&#26834;&#24615;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#19968;&#33324;DEQ&#30340;&#32463;&#39564;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#25239;&#35757;&#32451;&#30340;DEQ&#38656;&#35201;&#26356;&#22810;&#30340;&#21069;&#21521;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#29978;&#33267;&#21487;&#33021;&#36829;&#21453;&#20854;&#22266;&#23450;&#28857;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#40657;&#30418;&#27714;&#35299;&#22120;&#23548;&#33268;DEQ&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36712;&#36857;&#19981;&#23545;&#40784;&#12290;&#36825;&#20123;&#20107;&#23454;&#22312;&#35780;&#20272;&#25110;&#23545;&#25239;&#24615;&#35757;&#32451;DEQ&#26102;&#20250;&#23548;&#33268;&#26799;&#24230;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20272;&#35745;DEQ&#30340;&#20013;&#38388;&#26799;&#24230;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#25915;&#20987;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20805;&#20998;&#35780;&#20272;DEQ&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium models (DEQs) refrain from the traditional layer-stacking paradigm and turn to find the fixed point of a single layer. DEQs have achieved promising performance on different applications with featured memory efficiency. At the same time, the adversarial vulnerability of DEQs raises concerns. Several works propose to certify robustness for monotone DEQs. However, limited efforts are devoted to studying empirical robustness for general DEQs. To this end, we observe that an adversarially trained DEQ requires more forward steps to arrive at the equilibrium state, or even violates its fixed-point structure. Besides, the forward and backward tracks of DEQs are misaligned due to the black-box solvers. These facts cause gradient obfuscation when applying the ready-made attacks to evaluate or adversarially train DEQs. Given this, we develop approaches to estimate the intermediate gradients of DEQs and integrate them into the attacking pipelines. Our approaches facilitate fully w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20351;&#29992;Whiper&#27169;&#22411;&#20316;&#20026;DF&#26816;&#27979;&#30340;&#21069;&#31471;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#27604;&#35757;&#32451;3&#20010;&#26816;&#27979;&#27169;&#22411;&#21518;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ASVspoof 2021 DF&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#65292;&#22312;DF In-The-Wild&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21518;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;Whisper&#30340;&#29305;&#24449;&#33021;&#22815;&#25552;&#39640;&#27599;&#20010;&#27169;&#22411;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#23558;&#31561;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;21&#65285;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#26368;&#36817;&#22312;In-The-Wild&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01428</link><description>&lt;p&gt;
&#20351;&#29992;Whisper&#29305;&#24449;&#26469;&#25552;&#39640;DeepFake&#26816;&#27979;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improved DeepFake Detection Using Whisper Features. (arXiv:2306.01428v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20351;&#29992;Whiper&#27169;&#22411;&#20316;&#20026;DF&#26816;&#27979;&#30340;&#21069;&#31471;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#27604;&#35757;&#32451;3&#20010;&#26816;&#27979;&#27169;&#22411;&#21518;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ASVspoof 2021 DF&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#65292;&#22312;DF In-The-Wild&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21518;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;Whisper&#30340;&#29305;&#24449;&#33021;&#22815;&#25552;&#39640;&#27599;&#20010;&#27169;&#22411;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#23558;&#31561;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;21&#65285;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#26368;&#36817;&#22312;In-The-Wild&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#22768;&#29983;&#25104;&#26041;&#27861;&#30340;&#19981;&#26029;&#28044;&#29616;&#65292;&#38899;&#39057;DeepFake&#65288;DF&#65289;&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#27491;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#37117;&#22522;&#20110;&#25152;&#35859;&#30340;&#21069;&#31471;&#25216;&#26415;&#65292;&#36890;&#36807;&#36716;&#25442;&#21407;&#22987;&#38899;&#39057;&#65292;&#24378;&#35843;&#35780;&#20272;&#38899;&#39057;&#26679;&#26412;&#30495;&#23454;&#24615;&#25152;&#24517;&#38656;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23558;&#26368;&#20808;&#36827;&#30340;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20316;&#20026;DF&#26816;&#27979;&#30340;&#21069;&#31471;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#32452;&#21512;&#30340;Whisper&#21644;&#24050;&#24314;&#31435;&#30340;&#21069;&#31471;&#65292;&#36890;&#36807;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ASVspoof 2021 DF&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;3&#20010;&#26816;&#27979;&#27169;&#22411;&#65288;LCNN&#65292;SpecRNet&#21644;MesoNet&#65289;&#65292;&#24182;&#22312;DF In-The-Wild&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21518;&#32493;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;Whisper&#30340;&#29305;&#24449;&#21487;&#20197;&#25913;&#36827;&#27599;&#20010;&#27169;&#22411;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23558;&#31561;&#35823;&#24046;&#29575;&#38477;&#20302;21&#65285;&#26469;&#36229;&#36234;&#26368;&#36817;&#22312;In-The-Wild&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a recent influx of voice generation methods, the threat introduced by audio DeepFake (DF) is ever-increasing. Several different detection methods have been presented as a countermeasure. Many methods are based on so-called front-ends, which, by transforming the raw audio, emphasize features crucial for assessing the genuineness of the audio sample. Our contribution contains investigating the influence of the state-of-the-art Whisper automatic speech recognition model as a DF detection front-end. We compare various combinations of Whisper and well-established front-ends by training 3 detection models (LCNN, SpecRNet, and MesoNet) on a widely used ASVspoof 2021 DF dataset and later evaluating them on the DF In-The-Wild dataset. We show that using Whisper-based features improves the detection for each model and outperforms recent results on the In-The-Wild dataset by reducing Equal Error Rate by 21%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.01424</link><description>&lt;p&gt;
&#24102;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#30340;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#26088;&#22312;&#22238;&#31572;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#65292;&#22240;&#27492;&#23646;&#20110;Pearl&#22240;&#26524;&#20851;&#31995;&#38454;&#26799;&#20013;&#26368;&#31934;&#32454;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#32467;&#26524;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#36827;&#34892;&#28857;&#35782;&#21035;&#65292;&#22240;&#27492;&#23545;&#22522;&#30784;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#20102;&#24378;&#26377;&#21147;&#19988;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#26088;&#22312;&#36827;&#34892;&#36830;&#32493;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#65292;&#21363;&#24403;&#21453;&#20107;&#23454;&#26597;&#35810;&#23384;&#22312;&#20855;&#26377;&#20449;&#24687;&#36793;&#30028;&#30340;&#26080;&#30693;&#21306;&#38388;&#20013;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#36830;&#32493;&#21487;&#24494;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20989;&#25968;&#30340;&#32423;&#38598;&#30340;&#26354;&#29575;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#65292;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#26080;&#30693;&#21306;&#38388;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#31216;&#20026;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#28857;&#21453;&#20107;&#23454;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#35270;&#20026;&#25105;&#20204;&#25552;&#20986;&#26694;&#26550;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01423</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20248;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#30452;&#25509;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#31181;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22810;&#31181;&#20248;&#21270;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#26799;&#24230;&#36235;&#21183;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;FAME&#65289;&#65292;&#23427;&#39318;&#27425;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#12290;&#23558;TEMA&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#19982;&#30446;&#21069;&#25152;&#26377;&#20027;&#35201;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;FAME&#20248;&#21270;&#22120;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#65292;PASCAL-VOC&#65292;MS-COCO&#21644;Cityscapes&#12290;
&lt;/p&gt;
&lt;p&gt;
Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29616;&#22312;&#23384;&#22312;&#30340;&#32570;&#38519;&#36827;&#34892;&#25506;&#35752;&#65292;&#25351;&#20986;&#20102;&#30740;&#31350;&#32773;&#26410;&#33021;&#27491;&#30830;&#29702;&#35299;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#21644;&#32452;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#35813;&#26435;&#34913;&#20851;&#31995;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#26500;&#25104;&#23454;&#36136;&#24615;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2306.01417</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#23384;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
The Flawed Foundations of Fair Machine Learning. (arXiv:2306.01417v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01417
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29616;&#22312;&#23384;&#22312;&#30340;&#32570;&#38519;&#36827;&#34892;&#25506;&#35752;&#65292;&#25351;&#20986;&#20102;&#30740;&#31350;&#32773;&#26410;&#33021;&#27491;&#30830;&#29702;&#35299;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#21644;&#32452;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#35813;&#26435;&#34913;&#20851;&#31995;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#26500;&#25104;&#23454;&#36136;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#20915;&#31574;&#20013;&#20844;&#24179;&#24615;&#30340;&#23450;&#20041;&#21644;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#30340;&#22522;&#30784;&#23384;&#22312;&#30528;&#38169;&#35823;&#30340;&#25512;&#29702;&#12289;&#35823;&#23548;&#24615;&#30340;&#26029;&#35328;&#21644;&#21487;&#30097;&#30340;&#23454;&#36341;&#12290;&#36825;&#20123;&#32570;&#38519;&#26159;&#30001;&#20110;&#27809;&#26377;&#29702;&#35299;&#22312;&#23384;&#22312;&#32479;&#35745;&#19978;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#32452;&#30456;&#20284;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#26159;&#19968;&#31181;&#29420;&#31435;&#30340;&#22806;&#37096;&#38480;&#21046;&#32780;&#38750;&#20027;&#35266;&#24432;&#26174;&#30340;&#32467;&#26524;&#25152;&#23548;&#33268;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#21482;&#23384;&#22312;&#19968;&#31181;&#20844;&#24179;&#30340;&#27010;&#24565;&#65306;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#32467;&#26524;&#32452;&#30456;&#20284;&#24615;&#65292;&#20854;&#20013;&#30456;&#20284;&#24615;&#26377;&#30410;&#20110;&#19968;&#20010;&#24369;&#21183;&#32452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#20219;&#20309;&#23384;&#22312;&#32452;&#24046;&#24322;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#32479;&#35745;&#19978;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#32452;&#30456;&#20284;&#30340;&#32467;&#26524;&#20043;&#38388;&#30830;&#23454;&#23384;&#22312;&#26435;&#34913;&#65292;&#32780;&#36825;&#31181;&#26435;&#34913;&#23545;&#20844;&#24179;&#23384;&#22312;&#30528;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
The definition and implementation of fairness in automated decisions has been extensively studied by the research community. Yet, there hides fallacious reasoning, misleading assertions, and questionable practices at the foundations of the current fair machine learning paradigm. Those flaws are the result of a failure to understand that the trade-off between statistically accurate outcomes and group similar outcomes exists as independent, external constraint rather than as a subjective manifestation as has been commonly argued. First, we explain that there is only one conception of fairness present in the fair machine learning literature: group similarity of outcomes based on a sensitive attribute where the similarity benefits an underprivileged group. Second, we show that there is, in fact, a trade-off between statistically accurate outcomes and group similar outcomes in any data setting where group disparities exist, and that the trade-off presents an existential threat to the equita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21270;&#23398;&#24615;&#36136;&#20449;&#24687;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#32593;&#32476;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#30707;&#33041;&#27833;&#25104;&#20998;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01391</link><description>&lt;p&gt;
&#21270;&#23398;&#24615;&#36136;&#23548;&#21521;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30707;&#33041;&#27833;&#25104;&#20998;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical Property-Guided Neural Networks for Naphtha Composition Prediction. (arXiv:2306.01391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21270;&#23398;&#24615;&#36136;&#20449;&#24687;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#32593;&#32476;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#30707;&#33041;&#27833;&#25104;&#20998;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#33041;&#27833;&#35010;&#35299;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#30707;&#33041;&#27833;&#30340;&#25104;&#20998;&#65292;&#32780;&#36825;&#26159;&#19968;&#31181;&#30001;&#19981;&#21516;&#28867;&#31867;&#32452;&#25104;&#30340;&#22797;&#26434;&#28151;&#21512;&#29289;&#12290;&#20934;&#30830;&#22320;&#39044;&#27979;&#30707;&#33041;&#27833;&#25104;&#20998;&#23545;&#20110;&#26377;&#25928;&#25511;&#21046;&#35010;&#35299;&#36807;&#31243;&#21644;&#23454;&#29616;&#26368;&#22823;&#21270;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#38656;&#35201;&#23567;&#22411;&#35797;&#39564;&#25110;&#25104;&#26412;&#38480;&#21046;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#27668;&#30456;&#33394;&#35889;&#21644;&#30495;&#27832;&#28857;&#26354;&#32447;&#20998;&#26512;&#65292;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#21270;&#23398;&#24615;&#36136;&#20449;&#24687;&#26469;&#25552;&#39640;&#30707;&#33041;&#27833;&#25104;&#20998;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65306;Watson K&#22240;&#23376;&#20272;&#35745;&#32593;&#32476;&#21644;&#30707;&#33041;&#27833;&#25104;&#20998;&#39044;&#27979;&#32593;&#32476;&#12290;&#36825;&#20004;&#20010;&#32593;&#32476;&#20849;&#29992;&#19968;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#30340;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#65292;&#32780;&#36755;&#20986;&#23618;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#20004;&#20010;&#19981;&#21516;&#30340;&#36755;&#20986;&#8211;Watson K&#22240;&#23376;&#21644;&#30707;&#33041;&#27833;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The naphtha cracking process heavily relies on the composition of naphtha, which is a complex blend of different hydrocarbons. Predicting the naphtha composition accurately is crucial for efficiently controlling the cracking process and achieving maximum performance. Traditional methods, such as gas chromatography and true boiling curve, are not feasible due to the need for pilot-plant-scale experiments or cost constraints. In this paper, we propose a neural network framework that utilizes chemical property information to improve the performance of naphtha composition prediction. Our proposed framework comprises two parts: a Watson K factor estimation network and a naphtha composition prediction network. Both networks share a feature extraction network based on Convolutional Neural Network (CNN) architecture, while the output layers use Multi-Layer Perceptron (MLP) based networks to generate two different outputs - Watson K factor and naphtha composition. The naphtha composition is exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01381</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28040;&#24687;&#37327;&#21270;&#21644;&#24182;&#34892;&#21270;&#22312;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#32593;&#32476;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20998;&#24067;&#24335;&#20840;&#22270;&#35757;&#32451;&#20855;&#26377;&#24102;&#23485;&#38656;&#27714;&#39640;&#21644;&#32791;&#26102;&#38271;&#30340;&#29305;&#28857;&#12290;&#36328;&#35774;&#22791;&#39057;&#32321;&#20132;&#25442;&#33410;&#28857;&#29305;&#24449;&#12289;&#23884;&#20837;&#21644;&#23884;&#20837;&#26799;&#24230;&#65288;&#22343;&#31216;&#20026;&#28040;&#24687;&#65289;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#23545;&#20110;&#22312;&#20854;&#20182;&#35774;&#22791;&#19978;&#20855;&#26377;&#36828;&#31243;&#37051;&#23621;&#30340;&#33410;&#28857;&#65288;&#36793;&#32536;&#33410;&#28857;&#65289;&#32780;&#35328;&#65292;&#32780;&#23545;&#20110;&#27809;&#26377;&#36828;&#31243;&#37051;&#23621;&#30340;&#33410;&#28857;&#65288;&#20013;&#24515;&#33410;&#28857;&#65289;&#32780;&#35328;&#21017;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#31561;&#24453;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24555;&#36895;&#35757;&#32451;&#25910;&#25947;&#65288;&#20197;O&#65288;T ^ {-1}&#65289;&#30340;&#36895;&#29575;&#65292;&#20854;&#20013;T&#20026;&#35757;&#32451;&#21608;&#26399;&#30340;&#24635;&#25968;&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37327;&#21270;&#20301;&#23485;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#32454;&#24494;&#30340;&#30315;&#30187;&#30149;&#28790;&#36827;&#34892;&#35782;&#21035;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01375</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#29992;&#20110;&#32454;&#24494;&#30315;&#30187;&#30149;&#28790;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach. (arXiv:2306.01375v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#32454;&#24494;&#30340;&#30315;&#30187;&#30149;&#28790;&#36827;&#34892;&#35782;&#21035;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#32454;&#24494;&#30340;&#23616;&#38480;&#24615;&#30382;&#36136;&#21457;&#32946;&#19981;&#33391;&#65288;FCD&#65289;&#30149;&#21464;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#28155;&#21152;&#36741;&#21161;&#25439;&#22833;&#21644;&#24369;&#30417;&#30563;&#20998;&#31867;&#25439;&#22833;&#30340;&#26041;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. "Ground truth" manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex- or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based feature
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DWT-CompCNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#23545;&#20351;&#29992;HTJ2K&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01359</link><description>&lt;p&gt;
DWT-CompCNN&#65306;&#29992;&#20110;&#39640;&#21534;&#21520;&#37327;JPEG 2000&#21387;&#32553;&#25991;&#26723;&#30340;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DWT-CompCNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#23545;&#20351;&#29992;HTJ2K&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#21253;&#21547;&#25991;&#26723;&#22270;&#20687;&#30340;&#25968;&#23383;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#26816;&#32034;&#65292;&#25991;&#26723;&#22270;&#20687;&#30340;&#20998;&#31867;&#25104;&#20026;&#24517;&#35201;&#30340;&#38454;&#27573;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25991;&#26723;&#30340;&#23436;&#25972;&#29256;&#26412;&#65292;&#21363;&#26410;&#21387;&#32553;&#30340;&#25991;&#26723;&#22270;&#20687;&#26500;&#25104;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#22240;&#25968;&#25454;&#37327;&#22823;&#32780;&#24102;&#26469;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#25991;&#26723;&#30340;&#21387;&#32553;&#34920;&#31034;&#65288;&#22312;&#37096;&#20998;&#35299;&#21387;&#32553;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#30452;&#25509;&#23436;&#25104;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20197;&#20351;&#25972;&#20010;&#36807;&#31243;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#65292;&#37027;&#23558;&#20250;&#26159;&#19968;&#39033;&#21019;&#26032;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;DWT-CompCNN&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#21534;&#21520;&#37327;JPEG 2000&#65288;HTJ2K&#65289;&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#30340;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;DWT-CompCNN&#21253;&#25324;&#20116;&#20010;&#21367;&#31215;&#23618;&#65292;&#21367;&#31215;&#26680;&#22823;&#23567;&#20998;&#21035;&#20026;16&#12289;32&#12289;64&#12289;128&#21644;256&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23567;&#27874;&#31995;&#25968;&#20013;&#25552;&#39640;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model, DWT CompCNN is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27745;&#26579;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#34109;&#36890;&#20449;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#34109;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#20004;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#23454;&#29616;100%&#30340;&#38544;&#34109;&#28040;&#24687;&#20256;&#36755;&#20934;&#30830;&#24615;&#65292;&#36825;&#22312;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#23384;&#22312;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#34109;&#36890;&#20449;&#30340;&#28508;&#22312;&#23041;&#32961;&#21644;&#23545;&#24212;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01342</link><description>&lt;p&gt;
&#22522;&#20110;&#27745;&#26579;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#34109;&#36890;&#20449;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Covert Communication Based on the Poisoning Attack in Federated Learning. (arXiv:2306.01342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27745;&#26579;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#34109;&#36890;&#20449;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#34109;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#20004;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#23454;&#29616;100%&#30340;&#38544;&#34109;&#28040;&#24687;&#20256;&#36755;&#20934;&#30830;&#24615;&#65292;&#36825;&#22312;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#23384;&#22312;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#34109;&#36890;&#20449;&#30340;&#28508;&#22312;&#23041;&#32961;&#21644;&#23545;&#24212;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#34109;&#36890;&#20449;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#28041;&#21450;&#23558;&#29305;&#23450;&#20449;&#24687;&#38544;&#34255;&#22312;&#36733;&#20307;&#20013;&#36827;&#34892;&#20256;&#36755;&#65292;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#31169;&#20154;&#25968;&#25454;&#12289;&#20891;&#20107;&#26426;&#23494;&#29978;&#33267;&#24694;&#24847;&#36719;&#20214;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;&#26041;&#27861;&#26469;&#38544;&#34255;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20197;&#23454;&#29616;&#38544;&#34109;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#22240;&#20026;&#27169;&#22411;&#32858;&#21512;&#20250;&#20351;&#23458;&#25143;&#31471;&#23884;&#20837;&#30340;&#30830;&#20999;&#20449;&#24687;&#22833;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27745;&#26579;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#34109;&#36890;&#20449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#23454;&#29616;&#20102;100&#65285;&#30340;&#38544;&#34109;&#28040;&#24687;&#20256;&#36755;&#20934;&#30830;&#24615;&#65292;&#24182;&#32463;&#36807;&#20102;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#38544;&#34109;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#23545;&#25105;&#20204;&#30340;&#25915;&#20987;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#38450;&#25252;&#26041;&#27861;&#30340;&#32039;&#36843;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#34109;&#36890;&#20449;&#30340;&#28508;&#22312;&#23041;&#32961;&#21644;&#30456;&#20851;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covert communication has become an important area of research in computer security. It involves hiding specific information on a carrier for message transmission and is often used to transmit private data, military secrets, and even malware. In deep learning, many methods have been developed for hiding information in models to achieve covert communication. However, these methods are not applicable to federated learning, where model aggregation invalidates the exact information embedded in the model by the client. To address this problem, we propose a novel method for covert communication in federated learning based on the poisoning attack. Our approach achieves 100% accuracy in covert message transmission between two clients and is shown to be both stealthy and robust through extensive experiments. However, existing defense methods are limited in their effectiveness against our attack scheme, highlighting the urgent need for new protection methods to be developed. Our study emphasizes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#21512;&#36229;&#32500;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#23567;&#30340;HDC&#23376;&#27169;&#22411;&#24182;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;dropout&#27969;&#31243;&#65292;&#23454;&#29616;&#21487;&#19982;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#21644;&#26080;&#32447;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.01339</link><description>&lt;p&gt;
&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#21512;&#36229;&#32500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Resource-Efficient Federated Hyperdimensional Computing. (arXiv:2306.01339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#21512;&#36229;&#32500;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#23567;&#30340;HDC&#23376;&#27169;&#22411;&#24182;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;dropout&#27969;&#31243;&#65292;&#23454;&#29616;&#21487;&#19982;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28040;&#32791;&#26356;&#23569;&#30340;&#35745;&#31639;&#21644;&#26080;&#32447;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#20013;&#65292;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#24102;&#26469;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#33021;&#28304;&#36164;&#28304;&#12290;&#22914;&#26524;&#31995;&#32479;&#36164;&#28304;&#26377;&#38480;&#65292;&#20154;&#20204;&#21487;&#33021;&#19981;&#24471;&#19981;&#36890;&#36807;&#20943;&#23567;HDC&#27169;&#22411;&#22823;&#23567;&#26469;&#29306;&#29298;&#39044;&#27979;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#21512;&#36229;&#32500;&#35745;&#31639;&#65288;RE-FHDC&#65289;&#26694;&#26550;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#26356;&#23567;&#30340;&#29420;&#31435;&#30340;HDC&#23376;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#31867;&#20284;&#20110;dropout&#30340;&#36807;&#31243;&#31934;&#28860;&#21512;&#24182;&#30340;HDC&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#31181;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#28040;&#32791;&#27604;&#22522;&#32447;&#32852;&#21512;HDC&#23454;&#29616;&#26356;&#23569;&#30340;&#35745;&#31639;&#21644;&#26080;&#32447;&#36164;&#28304;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conventional federated hyperdimensional computing (HDC), training larger models usually results in higher predictive performance but also requires more computational, communication, and energy resources. If the system resources are limited, one may have to sacrifice the predictive performance by reducing the size of the HDC model. The proposed resource-efficient federated hyperdimensional computing (RE-FHDC) framework alleviates such constraints by training multiple smaller independent HDC sub-models and refining the concatenated HDC model using the proposed dropout-inspired procedure. Our numerical comparison demonstrates that the proposed framework achieves a comparable or higher predictive performance while consuming less computational and wireless resources than the baseline federated HDC implementation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01334</link><description>&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20998;&#24067;&#26159;&#30456;&#21516;&#30340;&#65292;&#25968;&#25454;&#26159;&#38598;&#20013;&#23384;&#20648;&#20379;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#35774;&#22791;&#12289;&#32452;&#32455;&#25110;&#36793;&#32536;&#33410;&#28857;&#19978;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270; (FDG) &#30340;&#26497;&#22823;&#20852;&#36259;&#12290;FDG &#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064; (FL) &#21644;&#39046;&#22495;&#27867;&#21270; (DG) &#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#20351;&#22810;&#20010;&#28304;&#39046;&#22495;&#33021;&#22815;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#32780;&#21448;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;Aequitas&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#24182;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;AI&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01333</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#23548;&#33322;&#65306;&#27010;&#24565;&#12289;&#21518;&#26524;&#21644;&#20851;&#38190;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations. (arXiv:2306.01333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;Aequitas&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#24182;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;AI&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#38761;&#21629;&#24615;&#36827;&#23637;&#65292;&#25215;&#35834;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#21644;&#31616;&#21270;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#38450;&#27490;&#38544;&#34109;&#30340;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#29616;&#35937;&#23548;&#33268;&#19981;&#24179;&#31561;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;AI&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#37325;&#28857;&#35752;&#35770;&#20351;&#29992;Aequitas&#24037;&#20855;&#21253;&#36827;&#34892;&#20559;&#24046;&#23457;&#35745;&#21450;&#20854;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#30142;&#30149;&#31579;&#26597;&#22330;&#26223;&#20013;&#30340;&#24433;&#21709;&#12290;Aequitas&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#65292;&#23457;&#26597;AI&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#35782;&#21035;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#21644;&#25104;&#20687;&#35774;&#22791;&#21697;&#29260;&#20043;&#38388;&#24046;&#24322;&#30340;&#38544;&#34255;&#20559;&#35265;&#12290;&#35813;&#24037;&#20855;&#21253;&#22522;&#20110;&#32479;&#35745;&#29702;&#35770;&#65292;&#20998;&#26512;&#22823;&#22411;&#25968;&#25454;&#38598;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#22312;&#21516;&#26102;&#22788;&#29702;&#22810;&#31181;&#21464;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#25918;&#23556;&#23398;&#36825;&#26679;&#30340;&#22810;&#20803;&#21270;&#39046;&#22495;&#20013;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65306;&#24179;&#31561;&#21644;&#27604;&#20363;&#24179;&#31561;&#12289;&#20551;&#38451;&#24615;&#29575;&#24179;&#31561;&#12289;&#20551;&#38452;&#24615;&#29575;&#24179;&#31561;&#21644;&#39044;&#27979;&#24179;&#31561;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#22312;&#25918;&#23556;&#23398;AI&#20013;&#32771;&#34385;&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#39118;&#38505;&#20197;&#21450;&#22312;AI&#20915;&#31574;&#36807;&#31243;&#20013;&#38656;&#35201;&#36879;&#26126;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has significantly revolutionized radiology, promising improved patient outcomes and streamlined processes. However, it's critical to ensure the fairness of AI models to prevent stealthy bias and disparities from leading to unequal outcomes. This review discusses the concept of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and its real-world implications in radiology, particularly in disease screening scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models' decisions, identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands. This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness. It excels in its versatility to handle various variables simultaneously, especially in a field as diverse as radiology. The review explicates essential fairness metrics: Equal and Proportional Parity, False Positive Rate Parity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37319;&#29992;AutoML&#20013;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#23637;&#31034;&#20102;&#37319;&#29992;HPO&#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2306.01324</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#21450;&#20854;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Reinforcement Learning and How To Tune Them. (arXiv:2306.01324v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#37319;&#29992;AutoML&#20013;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#23637;&#31034;&#20102;&#37319;&#29992;HPO&#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#21487;&#37325;&#22797;&#24615;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#24050;&#32463;&#37319;&#29992;&#20102;&#26356;&#22909;&#30340;&#31185;&#23398;&#23454;&#36341;&#65292;&#22914;&#26631;&#20934;&#21270;&#35780;&#20272;&#25351;&#26631;&#21644;&#25253;&#21578;&#12290;&#28982;&#32780;&#65292;RL &#20013;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#20844;&#24179;&#27604;&#36739; RL &#31639;&#27861;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102; RL &#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#20195;&#29702;&#30340;&#26368;&#32456;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#19988;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#20250;&#24378;&#28872;&#20381;&#36182;&#20110;&#35843;&#25972;&#31181;&#23376;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992; AutoML &#20013;&#24050;&#26377;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20363;&#22914;&#20998;&#31163;&#35843;&#25972;&#21644;&#27979;&#35797;&#31181;&#23376;&#65292;&#20197;&#21450;&#22312;&#24191;&#27867;&#25628;&#32034;&#31354;&#38388;&#20869;&#36827;&#34892;&#21512;&#29702;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270; (HPO)&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#20010;&#26368;&#26032;&#30340; HPO &#24037;&#20855;&#22312;&#19968;&#31995;&#21015; RL &#31639;&#27861;&#21644;&#29615;&#22659;&#19978;&#19982;&#25163;&#21160;&#35843;&#25972;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102; HPO &#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38544;&#31169;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#22270;&#20687;&#65292;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#26102;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.01322</link><description>&lt;p&gt;
&#38544;&#31169;&#33976;&#39311;&#65306;&#38477;&#20302;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models. (arXiv:2306.01322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38544;&#31169;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#22270;&#20687;&#65292;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#26102;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#26159;&#25351;&#23558;&#22823;&#22411;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#20854;&#33258;&#36523;&#30340;&#36739;&#23567;&#29256;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25945;&#25480;&#21478;&#19968;&#20010;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#26292;&#38706;&#21487;&#35782;&#21035;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#36890;&#36807;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20849;&#20139;&#25968;&#25454;&#26102;&#25152;&#38754;&#20020;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#19968;&#20010;&#31435;&#21363;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#65292;&#8220;&#25968;&#25454;&#25552;&#20379;&#32773;&#22914;&#20309;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#19981;&#20250;&#27844;&#38706;&#19982;&#24739;&#32773;&#26377;&#20851;&#30340;&#21487;&#35782;&#21035;&#20449;&#24687;&#65311;&#8221;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#31532;&#19968;&#25193;&#25955;&#27169;&#22411;&#65307;&#65288;2&#65289;&#20351;&#29992;&#35813;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#25490;&#38500;&#20855;&#26377;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#22270;&#20687;&#65307;&#65288;3&#65289;&#20165;&#22312;&#36807;&#28388;&#21518;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#31532;&#20108;&#27425;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38544;&#31169;&#33976;&#39311;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#65292;&#21516;&#26102;&#20445;&#25345;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation in neural networks refers to compressing a large model or dataset into a smaller version of itself. We introduce Privacy Distillation, a framework that allows a text-to-image generative model to teach another model without exposing it to identifiable data. Here, we are interested in the privacy issue faced by a data provider who wishes to share their data via a multimodal generative model. A question that immediately arises is ``How can a data provider ensure that the generative model is not leaking identifiable information about a patient?''. Our solution consists of (1) training a first diffusion model on real data (2) generating a synthetic dataset using this model and filtering it to exclude images with a re-identifiability risk (3) training a second diffusion model on the filtered synthetic data only. We showcase that datasets sampled from models trained with privacy distillation can effectively reduce re-identification risk whilst maintaining downstream per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#39118;&#26684;&#36716;&#21270;&#27169;&#22411;&#25913;&#21892;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.01318</link><description>&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer Back-Translation. (arXiv:2306.01318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#39118;&#26684;&#36716;&#21270;&#27169;&#22411;&#25913;&#21892;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Back Translation (BT)&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65292;&#25152;&#20197;BT&#20027;&#35201;&#25913;&#21892;&#20849;&#20139;&#30456;&#20284;&#39118;&#26684;&#36755;&#20837;&#65288;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#26159;&#31867;&#20284;&#32763;&#35793;&#30340;&#36755;&#20837;&#65289;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#23545;&#20110;&#33258;&#28982;&#36755;&#20837;&#65292;BT&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#25913;&#36827;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#24102;&#26469;&#19981;&#21033;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#23427;&#20351;&#29992;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#26469;&#20462;&#25913;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#28304;&#35821;&#35328;&#37096;&#20998;&#30340;&#25991;&#26412;&#39118;&#26684;&#26356;&#33258;&#28982;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#21892;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#23545;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;TST BT&#36824;&#35777;&#26126;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#20063;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-like inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer model to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a general data augmenta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#26469;&#23481;&#32435;&#25968;&#25454;&#30340;&#20998;&#35299;&#24615;&#36136;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#20811;&#26381;&#29616;&#26377;&#27169;&#22359;&#21270;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01316</link><description>&lt;p&gt;
&#29420;&#31435;&#27169;&#22359;&#21270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Independent Modular Networks. (arXiv:2306.01316v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#26469;&#23481;&#32435;&#25968;&#25454;&#30340;&#20998;&#35299;&#24615;&#36136;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#20811;&#26381;&#29616;&#26377;&#27169;&#22359;&#21270;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21333;&#19968;&#26435;&#37325;&#38598;&#21512;&#30340;&#25972;&#20307;&#31070;&#32463;&#32593;&#32476;&#65292;&#26126;&#30830;&#22320;&#25918;&#24323;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#25104;&#24615;&#36136;&#65292;&#36825;&#22312;&#25968;&#25454;&#20013;&#23384;&#22312;&#65292;&#27599;&#20010;&#23454;&#20363;&#21487;&#20197;&#34987;&#35270;&#20026;&#36523;&#20221;&#27010;&#24565;&#65288;&#20363;&#22914;&#23545;&#35937;&#30340;&#24418;&#29366;&#65289;&#19982;&#20462;&#25913;&#27010;&#24565;&#65288;&#20363;&#22914;&#26041;&#21521;&#12289;&#39068;&#33394;&#21644;&#22823;&#23567;&#65289;&#30340;&#32467;&#21512;&#12290;&#36825;&#31181;&#20998;&#35299;&#24615;&#36136;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#29366;&#24577;&#20272;&#35745;&#20005;&#37325;&#20381;&#36182;&#20110;&#29289;&#29702;&#26426;&#21046;&#65288;&#20363;&#22914;&#26059;&#36716;&#21644;&#36716;&#21270;&#65289;&#30340;&#32452;&#21512;&#24615;&#36136;&#26469;&#24314;&#27169;&#20132;&#20114;&#12290;&#20026;&#20102;&#23481;&#32435;&#36825;&#31181;&#25968;&#25454;&#29305;&#24449;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22359;&#30340;&#35282;&#33394;&#32570;&#20047;&#32467;&#26500;&#65292;&#20197;&#21450;&#27169;&#22359;&#21270;&#32593;&#32476;&#29305;&#26377;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#27169;&#22359;&#23849;&#28291;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23481;&#32435;&#19978;&#36848;&#20998;&#35299;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Monolithic neural networks that make use of a single set of weights to learn useful representations for downstream tasks explicitly dismiss the compositional nature of data generation processes. This characteristic exists in data where every instance can be regarded as the combination of an identity concept, such as the shape of an object, combined with modifying concepts, such as orientation, color, and size. The dismissal of compositionality is especially detrimental in robotics, where state estimation relies heavily on the compositional nature of physical mechanisms (e.g., rotations and transformations) to model interactions. To accommodate this data characteristic, modular networks have been proposed. However, a lack of structure in each module's role, and modular network-specific issues such as module collapse have restricted their usability. We propose a modular network architecture that accommodates the mentioned decompositional concept by proposing a unique structure that split
&lt;/p&gt;</description></item><item><title>EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01310</link><description>&lt;p&gt;
EPIC: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#30340;&#22270;&#24418;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01310
&lt;/p&gt;
&lt;p&gt;
EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#22270;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#32463;&#24120;&#38480;&#21046;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EPIC&#65288;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#32534;&#36753;&#36317;&#31163;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#32467;&#26500;&#26377;&#25152;&#21464;&#21270;&#30340;&#26032;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24102;&#26631;&#31614;&#30340;&#22270;&#26469;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#22312;&#21407;&#22987;&#22270;&#23545;&#20043;&#38388;&#21019;&#24314;&#20102;&#22270;&#32534;&#36753;&#36335;&#24452;&#12290;&#36890;&#36807;&#20174;&#22270;&#32534;&#36753;&#36335;&#24452;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#20016;&#23500;&#20102;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#35299;&#20915;&#24322;&#26500;&#36890;&#20449;&#29615;&#22659;&#19979;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#30456;&#31227;&#37197;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22240;&#26524;&#34920;&#31034;&#26469;&#23398;&#20064;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26368;&#32456;&#39044;&#27979;RIS&#30340;&#30456;&#20301;&#37197;&#32622;&#12290;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.01306</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#34920;&#31034;&#30340;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#32852;&#37030;&#23398;&#20064;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Games for Reconfigurable Intelligent Surfaces via Causal Representations. (arXiv:2306.01306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#35299;&#20915;&#24322;&#26500;&#36890;&#20449;&#29615;&#22659;&#19979;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#30456;&#31227;&#37197;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22240;&#26524;&#34920;&#31034;&#26469;&#23398;&#20064;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26368;&#32456;&#39044;&#27979;RIS&#30340;&#30456;&#20301;&#37197;&#32622;&#12290;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#36890;&#20449;&#29615;&#22659;&#19979;&#23545;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#30456;&#31227;&#37197;&#32622;&#36827;&#34892;&#24378;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#34987;&#35268;&#23450;&#20026;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35774;&#32622;&#19979;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30456;&#24403;&#20110;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;RIS&#20316;&#20026;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#21338;&#24328;&#30340;&#28216;&#25103;&#12290;&#20351;&#29992;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#21450;&#20854;FL&#31561;&#20215;&#29289;&#65292;&#34987;&#31216;&#20026;FL Games&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#19981;&#21464;&#30340;&#22240;&#26524;&#34920;&#31034;&#65292;&#39044;&#27979;&#30456;&#31227;&#26469;&#35299;&#20915;RIS&#37197;&#32622;&#38382;&#39064;&#12290;&#35299;&#20915;&#26041;&#26696;&#23545;&#24212;&#20110;&#26681;&#25454; &#26368;&#20339;&#21709;&#24212;&#21160;&#21147;&#23398;&#65288;BRD&#65289;&#36827;&#34892;&#28216;&#25103;&#65292;&#20174;&#32780;&#20135;&#29983;FL&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#29366;&#24577;&#12290;&#34920;&#31034;&#23398;&#20064;&#22120;&#21644;&#30456;&#20301;&#39044;&#27979;&#22120;&#30001;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#65292;&#22522;&#20110;&#23545;&#20854;&#20182;&#26469;&#33258;&#25991;&#29486;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#27169;&#25311;&#39564;&#35777;&#20854;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;RIS&#30456;&#31227;&#37197;&#32622;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of robust Reconfigurable Intelligent Surface (RIS) phase-shifts configuration over heterogeneous communication environments. The problem is formulated as a distributed learning problem over different environments in a Federated Learning (FL) setting. Equivalently, this corresponds to a game played between multiple RISs, as learning agents, in heterogeneous environments. Using Invariant Risk Minimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS configuration problem by learning invariant causal representations across multiple environments and then predicting the phases. The solution corresponds to playing according to Best Response Dynamics (BRD) which yields the Nash Equilibrium of the FL game. The representation learner and the phase predictor are modeled by two neural networks, and their performance is validated via simulations against other benchmarks from the literature. Our results show that causality-based learning y
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#27169;&#22411;&#65292;&#25104;&#20026;&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.01295</link><description>&lt;p&gt;
&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Egocentric Planning for Scalable Embodied Task Achievement. (arXiv:2306.01295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#27169;&#22411;&#65292;&#25104;&#20026;&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#23545;&#20110;&#37319;&#21462;&#34892;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35937;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#20197;&#21450;&#25191;&#34892;&#36866;&#24403;&#30340;&#34892;&#21160;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#24863;&#30693;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;ALFRED&#65288;&#19968;&#20010;&#27169;&#25311;&#30340;&#23478;&#24237;&#20219;&#21153;&#29615;&#22659;&#65289;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;ALFRED&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;36.07%&#30340;&#26410;&#35265;&#36807;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;CVPR Emobodied AI workshop&#20013;&#33719;&#24471;&#20102;ALFRED&#25361;&#25112;&#36187;&#30340;&#32988;&#21033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#21487;&#38752;&#30340;&#24863;&#30693;&#65292;&#24182;&#35268;&#23450;&#25110;&#23398;&#20064;&#20851;&#20110;&#20195;&#29702;&#30340;&#34892;&#21160;&#21069;&#25552;&#21644;&#25928;&#26524;&#65292;&#20197;&#21450;&#23545;&#35937;&#31867;&#22411;&#30340;&#31526;&#21495;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24050;&#20986;&#29616;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.01282</link><description>&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#20013;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems. (arXiv:2306.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01282
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24050;&#20986;&#29616;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#37319;&#29992;&#36890;&#20449;&#25216;&#26415;&#65292;&#20449;&#24687;&#22788;&#29702;&#21644;&#25511;&#21046;&#31995;&#32479;&#26469;&#31649;&#29702;&#20132;&#36890;&#32593;&#32476;&#12290;&#36825;&#31181;&#38598;&#25104;&#21508;&#31181;&#32452;&#20214;&#65288;&#22914;&#36947;&#36335;&#65292;&#36710;&#36742;&#21644;&#36890;&#20449;&#31995;&#32479;&#65289;&#30340;&#26041;&#27861;&#65292;&#39044;&#35745;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#21644;&#20132;&#36890;&#27169;&#24335;&#30340;&#21327;&#35843;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#26412;&#31456;&#33410;&#25552;&#20379;ITS&#35774;&#35745;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20197;&#21450;&#28085;&#30422;&#20174;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#21040;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#27861;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#23457;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intelligent Transportation System (ITS) is an important part of modern transportation infrastructure, employing a combination of communication technology, information processing and control systems to manage transportation networks. This integration of various components such as roads, vehicles, and communication systems, is expected to improve efficiency and safety by providing better information, services, and coordination of transportation modes. In recent years, graph-based machine learning has become an increasingly important research focus in the field of ITS aiming at the development of complex, data-driven solutions to address various ITS-related challenges. This chapter presents background information on the key technical challenges for ITS design, along with a review of research methods ranging from classic statistical approaches to modern machine learning and deep learning-based approaches. Specifically, we provide an in-depth review of graph-based machine learning metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLARIFIER&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39564;&#35777;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20154;&#31867;&#20132;&#20114;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.01277</link><description>&lt;p&gt;
&#36229;&#36234;&#20027;&#21160;&#23398;&#20064;&#65306;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#12289;&#20154;&#24037;&#20462;&#27491;&#21644;&#20154;&#24037;&#39564;&#35777;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#20132;&#20114;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Active Learning: Leveraging the Full Potential of Human Interaction via Auto-Labeling, Human Correction, and Human Verification. (arXiv:2306.01277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLARIFIER&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39564;&#35777;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20154;&#31867;&#20132;&#20114;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#20154;&#31867;&#21442;&#19982;&#30340;&#26694;&#26550;&#65292;&#21487;&#20132;&#20114;&#22320;&#21644;&#36866;&#24212;&#24615;&#22320;&#26631;&#35760;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#24615;&#33021;&#27604;&#38543;&#26426;&#37319;&#26679;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#38590;&#20197;&#26631;&#27880;&#30340;&#23454;&#20363;&#26469;&#36215;&#20316;&#29992;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#26679;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#30340;&#36825;&#20123;&#20027;&#21160;&#23398;&#20064;&#33539;&#20363;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#33258;&#21160;&#26631;&#27880;&#24314;&#35758;&#25152;&#36171;&#20104;&#30340;&#20154;&#31867;&#20132;&#20114;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#22810;&#25968;&#20154;&#22312;&#39564;&#35777;&#33258;&#21160;&#24314;&#35758;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#26102;&#25152;&#38656;&#26102;&#38388;&#20026;&#26356;&#27491;&#19981;&#27491;&#30830;&#30340;&#24314;&#35758;&#21040;&#27491;&#30830;&#26631;&#31614;&#65288;&#25110;&#19981;&#20351;&#29992;&#20219;&#20309;&#24314;&#35758;&#36827;&#34892;&#26631;&#27880;&#65289;&#25152;&#38656;&#26102;&#38388;&#30340;3&#21040;4&#20493;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLARIFIER&#65288;&#26469;&#33258;&#20998;&#23618;&#38590;&#24230;&#30340;&#20027;&#21160;&#23398;&#20064;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39564;&#35777;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#31867;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Learning (AL) is a human-in-the-loop framework to interactively and adaptively label data instances, thereby enabling significant gains in model performance compared to random sampling. AL approaches function by selecting the hardest instances to label, often relying on notions of diversity and uncertainty. However, we believe that these current paradigms of AL do not leverage the full potential of human interaction granted by automated label suggestions. Indeed, we show that for many classification tasks and datasets, most people verifying if an automatically suggested label is correct take $3\times$ to $4\times$ less time than they do changing an incorrect suggestion to the correct label (or labeling from scratch without any suggestion). Utilizing this result, we propose CLARIFIER (aCtive LeARnIng From tIEred haRdness), an Interactive Learning framework that admits more effective use of human interaction by leveraging the reduced cost of verification. By targeting the hard (un
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01276</link><description>&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#20013;&#23545;&#31216;&#25506;&#32034;&#26159;&#20813;&#36153;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20813;&#36153;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#22686;&#24378;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20445;&#30041;&#22870;&#21169;&#30340;&#21464;&#25442;&#26469;&#22686;&#24378;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#24433;&#21709;&#21147;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#65292;&#26131;&#20110;&#19982;&#29616;&#26377;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;NP&#38590;&#30340;&#36335;&#32447;&#20248;&#21270;&#65292;&#35843;&#24230;&#20248;&#21270;&#21644;&#26032;&#22411;&#20998;&#23376;&#20248;&#21270;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36731;&#26494;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01272</link><description>&lt;p&gt;
DeepfakeArt Challenge: &#29992;&#20110;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#21644;&#21069;&#26223;&#65292;&#33539;&#22260;&#20174;&#20250;&#35805;&#20195;&#29702;&#21644;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#21040;&#35821;&#38899;&#21644;&#35270;&#35273;&#21512;&#25104;&#12290;&#22312;&#29983;&#25104;AI&#30340;&#23835;&#36215;&#21644;&#20854;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#37319;&#29992;&#20013;&#65292;&#23545;&#20110;&#29983;&#25104;AI&#30340;&#24694;&#24847;&#29992;&#36884;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#22312;&#20351;&#29992;&#29983;&#25104;AI&#36827;&#34892;&#35270;&#35273;&#20869;&#23481;&#21512;&#25104;&#30340;&#39046;&#22495;&#20013;&#65292;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#26159;&#22270;&#20687;&#20266;&#36896;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#21253;&#21547;&#25110;&#27966;&#29983;&#33258;&#29256;&#26435;&#20869;&#23481;&#30340;&#22270;&#20687;&#65289;&#21644;&#25968;&#25454;&#27745;&#26579;&#65288;&#21363;&#29983;&#25104;&#34987;&#25932;&#23545;&#27745;&#26579;&#30340;&#22270;&#20687;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#40723;&#21169;&#36127;&#36131;&#20219;&#30340;&#29983;&#25104;AI&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepfakeArt Challenge&#65292;&#19968;&#20010;&#22823;&#22411;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous recent advances in generative artificial intelligence techniques have led to significant successes and promise in a wide range of different applications ranging from conversational agents and textual content generation to voice and visual synthesis. Amid the rise in generative AI and its increasing widespread adoption, there has been significant growing concern over the use of generative AI for malicious purposes. In the realm of visual content synthesis using generative AI, key areas of significant concern has been image forgery (e.g., generation of images containing or derived from copyright content), and data poisoning (i.e., generation of adversarially contaminated images). Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. 
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.01271</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20250;&#21516;&#26102;&#20986;&#29616;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01271
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#19982;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#29615;&#22659;&#20013;&#20986;&#29616;&#24778;&#20154;&#30340;&#24178;&#20928;&#27867;&#21270;&#33021;&#21147;&#31867;&#20284;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#20928;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#24178;&#20928;&#27867;&#21270;&#19981;&#21516;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#20302;&#40065;&#26834;&#35757;&#32451;&#35823;&#24046;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#40065;&#26834;&#27867;&#21270;&#36317;&#31163;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#35757;&#32451;&#22914;&#20309;&#23548;&#33268;&#32593;&#32476;&#23398;&#20064;&#32773;&#36827;&#20837;&#21040;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#36843;&#20351;&#23398;&#20064;&#22120;&#25104;&#20026;&#24378;&#39044;&#27979;&#32593;&#32476;&#65292;&#23545;&#25239;&#35757;&#32451;&#23558;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01270</link><description>&lt;p&gt;
&#32452;&#21512;&#21551;&#21457;&#24335;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning. (arXiv:2306.01270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#22312;&#31227;&#21160;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36991;&#20813;&#19982;&#20854;&#20182;&#31227;&#21160;&#26426;&#22120;&#20154;&#21457;&#29983;&#30896;&#25758;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23427;&#20204;&#30340;&#34892;&#39542;&#36317;&#31163;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#19981;&#26029;&#37325;&#26032;&#35268;&#21010;&#36335;&#24452;&#20197;&#36991;&#20813;&#20914;&#31361;&#65292;&#35201;&#20040;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#36873;&#25321;&#36866;&#24403;&#30340;&#36991;&#30896;&#31574;&#30053;&#12290;&#21069;&#32773;&#21487;&#33021;&#30001;&#20110;&#39057;&#32321;&#30340;&#37325;&#26032;&#35268;&#21010;&#23548;&#33268;&#34892;&#39542;&#36317;&#31163;&#36739;&#38271;&#65292;&#32780;&#21518;&#32773;&#21487;&#33021;&#30001;&#20110;&#20302;&#26679;&#26412;&#25506;&#32034;&#21644;&#21033;&#29992;&#29575;&#32780;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#23618;&#27425;&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MAPPO&#30340;&#23454;&#26102;&#35268;&#21010;&#22120;&#65292;&#20854;&#23558;&#32463;&#39564;&#35268;&#21017;&#23884;&#20837;&#21040;&#21160;&#20316;&#36755;&#20986;&#23618;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#65307;&#20197;&#21450;&#19968;&#20010;&#21551;&#21457;&#24335;&#35268;&#21010;&#22120;&#65292;&#23427;&#29983;&#25104;&#21021;&#22987;&#36335;&#24452;&#24182;&#21521;MAPPO&#35268;&#21010;&#22120;&#28155;&#21152;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01266</link><description>&lt;p&gt;
&#33258;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Self Contrastive Learning for Session-based Recommendation. (arXiv:2306.01266v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#29616;&#26377;&#39033;&#30446;&#20132;&#20114;&#24207;&#21015;&#30340;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#24212;&#29992;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#25552;&#39640;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23545;&#27604;&#30446;&#26631;&#65306;&#65288;1&#65289;&#36215;&#21040;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#31867;&#20284;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#24573;&#30053;&#20102;&#39033;&#30446;&#34920;&#31034;&#31354;&#38388;&#20248;&#21270;&#65307;&#65288;2&#65289;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#24314;&#27169;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#27491;/&#36127;&#26679;&#26412;&#26500;&#24314;&#21644;&#39069;&#22806;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#31616;&#21270;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#24182;&#22686;&#24378;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#25512;&#33616;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SCL&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65292;&#30452;&#25509;&#20419;&#36827;&#39033;&#30446;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#24182;&#26377;&#25928;&#22320;&#26367;&#25442;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#23545;&#27604;&#30446;&#26631;&#32452;&#20214;&#30340;&#29366;&#24577;-&#33402;&#26415;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SCL&#28040;&#38500;&#20102;&#20219;&#20309;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#21644;SCL&#28040;&#38500;&#20102;&#20219;&#20309;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#39044;&#27979;&#32622;&#20449;&#24230;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01265</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26657;&#20934;&#12290; &#65288;arXiv&#65306;2306.01265v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Calibrating Multimodal Learning. (arXiv:2306.01265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#39044;&#27979;&#32622;&#20449;&#24230;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26041;&#27861;&#23384;&#22312;&#39044;&#27979;&#32622;&#20449;&#24230;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#24448;&#24448;&#22312;&#20272;&#35745;&#32622;&#20449;&#24230;&#26102;&#20381;&#36182;&#20110;&#37096;&#20998;&#27169;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#20272;&#35745;&#30340;&#32622;&#20449;&#24230;&#29978;&#33267;&#21487;&#20197;&#22686;&#21152;&#24403;&#26576;&#20123;&#27169;&#24577;&#21463;&#21040;&#25439;&#22351;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#30452;&#35266;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21407;&#21017;&#65292;&#21363;&#24403;&#21435;&#38500;&#19968;&#20010;&#27169;&#24577;&#26102;&#65292;&#32622;&#20449;&#24230;&#19981;&#24212;&#35813;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#26657;&#20934;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;CML&#65289;&#27491;&#21017;&#21270;&#65292;&#26469;&#26657;&#20934;&#20808;&#21069;&#26041;&#27861;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#28789;&#27963;&#37197;&#32622;&#65292;&#24182;&#25552;&#39640;&#32622;&#20449;&#24230;&#26657;&#20934;&#12289;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#22810;&#27169;&#24577;&#38598;&#25104;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine learning has achieved remarkable progress in a wide range of scenarios. However, the reliability of multimodal learning remains largely unexplored. In this paper, through extensive empirical studies, we identify current multimodal classification methods suffer from unreliable predictive confidence that tend to rely on partial modalities when estimating confidence. Specifically, we find that the confidence estimated by current models could even increase when some modalities are corrupted. To address the issue, we introduce an intuitive principle for multimodal learning, i.e., the confidence should not increase when one modality is removed. Accordingly, we propose a novel regularization technique, i.e., Calibrating Multimodal Learning (CML) regularization, to calibrate the predictive confidence of previous methods. This technique could be flexibly equipped by existing models and improve the performance in terms of confidence calibration, classification accuracy, and mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#25512;&#24191;&#20102;&#24191;&#20041;&#24179;&#28369;&#24230;&#26465;&#20214;&#65292;&#20351;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#26356;&#24378;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#26465;&#20214;&#19979;&#65292;&#33719;&#24471;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01264</link><description>&lt;p&gt;
&#24191;&#20041;&#24179;&#28369;&#24230;&#19979;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convex and Non-Convex Optimization under Generalized Smoothness. (arXiv:2306.01264v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#25512;&#24191;&#20102;&#24191;&#20041;&#24179;&#28369;&#24230;&#26465;&#20214;&#65292;&#20351;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#26356;&#24378;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#26465;&#20214;&#19979;&#65292;&#33719;&#24471;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#26799;&#24230;&#30340;Lipshitz&#24615;&#36136;&#65292;&#36825;&#38480;&#21046;&#20102;&#20998;&#26512;&#33539;&#22260;&#20165;&#38480;&#20110;&#20108;&#27425;&#20989;&#25968;&#30340;&#30028;&#38480;&#20869;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25918;&#26494;&#20102;&#36825;&#20010;&#35201;&#27714;&#65292;&#36716;&#32780;&#20351;&#29992;&#19968;&#31181;&#38750;&#22343;&#21248;&#24179;&#28369;&#26465;&#20214;&#65292;&#20854;&#20013;Hessian&#33539;&#25968;&#21463;&#26799;&#24230;&#33539;&#25968;&#30340;&#20223;&#23556;&#20989;&#25968;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#35009;&#21098;&#35777;&#26126;&#20102;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20551;&#35774;&#23384;&#22312;&#26377;&#30028;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#31181;&#38750;&#22343;&#21248;&#24179;&#28369;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#27839;&#36712;&#36857;&#26041;&#21521;&#38480;&#21046;&#26799;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#24378;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#20010;&#24191;&#20041;&#24179;&#28369;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#20984;&#21644;&#65288;&#25110;&#65289;&#38750;&#20984;&#35774;&#23450;&#12290;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26799;&#24230;&#35009;&#21098;&#65292;&#24182;&#20801;&#35768;&#26377;&#37325;&#23614;&#22122;&#22768;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#24120;&#23454;&#29992;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical analysis of convex and non-convex optimization methods often requires the Lipshitzness of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#26469;&#35299;&#20915;MPE&#20013;&#19981;&#21487;&#31616;&#32422;&#24615;&#20551;&#35774;&#19979;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01253</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21487;&#31616;&#32422;&#24615;&#30340;&#28151;&#21512;&#27604;&#20363;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mixture Proportion Estimation Beyond Irreducibility. (arXiv:2306.01253v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#26469;&#35299;&#20915;MPE&#20013;&#19981;&#21487;&#31616;&#32422;&#24615;&#20551;&#35774;&#19979;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27604;&#20363;&#20272;&#35745;&#65288;MPE&#65289;&#30340;&#20219;&#21153;&#26159;&#20272;&#35745;&#28151;&#21512;&#29289;&#20013;&#32452;&#25104;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32473;&#23450;&#26469;&#33258;&#32452;&#25104;&#20998;&#24067;&#21644;&#28151;&#21512;&#29289;&#30340;&#35266;&#27979;&#12290;&#20197;&#21069;&#30340;MPE&#24037;&#20316;&#37319;&#29992;&#20102;&#19981;&#21487;&#31616;&#32422;&#24615;&#20551;&#35774;&#65292;&#36825;&#30830;&#20445;&#20102;&#28151;&#21512;&#27604;&#20363;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#36866;&#29992;&#20110;&#20960;&#31181;&#24863;&#20852;&#36259;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19981;&#21487;&#31616;&#32422;&#24615;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37325;&#37319;&#26679;&#30340;&#20803;&#31639;&#27861;&#65292;&#23427;&#25509;&#21463;&#20219;&#20309;&#29616;&#26377;&#30340;&#22312;&#19981;&#21487;&#31616;&#32422;&#24615;&#19979;&#35774;&#35745;&#30340;MPE&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20026;&#22312;&#25105;&#20204;&#26356;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32463;&#39564;&#19978;&#23637;&#29616;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#37325;&#26032;&#20998;&#32452;&#30340;&#31639;&#27861;&#30340;&#20272;&#35745;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of mixture proportion estimation (MPE) is to estimate the weight of a component distribution in a mixture, given observations from both the component and mixture. Previous work on MPE adopts the irreducibility assumption, which ensures identifiablity of the mixture proportion. In this paper, we propose a more general sufficient condition that accommodates several settings of interest where irreducibility does not hold. We further present a resampling-based meta-algorithm that takes any existing MPE algorithm designed to work under irreducibility and adapts it to work under our more general condition. Our approach empirically exhibits improved estimation performance relative to baseline methods and to a recently proposed regrouping-based algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;ECG&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#23545;ECG&#20449;&#21495;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.01249</link><description>&lt;p&gt;
&#21464;&#38761;&#24515;&#30005;&#22270;&#35786;&#26029;: Transformer-based&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30340;&#28145;&#20837;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection. (arXiv:2306.01249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;ECG&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#23545;ECG&#20449;&#21495;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#26174;&#33879;&#22686;&#36827;&#20102;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30340;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#35780;&#20272;&#24515;&#33039;&#20581;&#24247;&#29366;&#20917;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;ECG&#30340;&#35299;&#37322;&#36739;&#20026;&#22797;&#26434;&#65292;&#20294;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;ECG&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#21644;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#35786;&#26029;&#38656;&#35201;&#25506;&#32034;&#26356;&#21152;&#24378;&#22823;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;transformers&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#24212;&#29992;&#20110;ECG&#20998;&#31867;&#30340;transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#35780;&#20272;&#12290;&#36825;&#20123;&#27169;&#22411;&#26368;&#21021;&#26159;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#65292;&#23427;&#20204;&#25429;&#25417;ECG&#20449;&#21495;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#65292;&#20854;&#20182;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#30053;&#23427;&#20204;&#12290;&#25105;&#20204;&#23545;&#26368;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25628;&#32034;&#21644;&#24635;&#32467;&#65292;&#20197;&#35752;&#35770;&#23427;&#20204;&#30340;&#24212;&#29992;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#24182;&#24314;&#35758;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#12290;&#36825;&#31687;&#32508;&#36848;&#26159;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#26088;&#22312;&#38416;&#26126;&#20351;&#29992;transformer-based&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;ECG&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of deep learning has significantly enhanced the analysis of electrocardiograms (ECGs), a non-invasive method that is essential for assessing heart health. Despite the complexity of ECG interpretation, advanced deep learning models outperform traditional methods. However, the increasing complexity of ECG data and the need for real-time and accurate diagnosis necessitate exploring more robust architectures, such as transformers. Here, we present an in-depth review of transformer architectures that are applied to ECG classification. Originally developed for natural language processing, these models capture complex temporal relationships in ECG signals that other models might overlook. We conducted an extensive search of the latest transformer-based models and summarize them to discuss the advances and challenges in their application and suggest potential future improvements. This review serves as a valuable resource for researchers and practitioners and aims to shed light on
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.01248</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;LLMs&#22312;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#20934;&#22791;&#24773;&#20917;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#19968;&#30452;&#26159;&#37319;&#29992;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#29983;&#25104;&#26356;&#33258;&#28982;&#21644;&#36830;&#36143;&#25688;&#35201;&#33021;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#24182;&#20855;&#26377;&#25991;&#26412;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#38382;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#24050;&#20934;&#22791;&#22909;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#21028;&#20915;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#25277;&#35937;&#24615;&#25688;&#35201;&#27169;&#22411;&#21644;&#36890;&#29992;&#39046;&#22495;&#30340;LLMs&#24212;&#29992;&#20110;&#21360;&#24230;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#65292;&#24182;&#26816;&#26597;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#25688;&#35201;&#36136;&#37327;&#30340;&#26631;&#20934;&#24230;&#37327;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#34394;&#26500;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
&lt;/p&gt;</description></item><item><title>CREST&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#21644;&#22810;&#20010;&#23567;&#25209;&#37327;&#26680;&#24515;&#38598;&#65292;&#20445;&#35777;&#38750;&#20984;&#27169;&#22411;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01244</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#25345;&#32493;&#23398;&#20064;&#65306;&#29992;&#20110;&#25968;&#25454;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#30340;&#26680;&#24515;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. (arXiv:2306.01244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01244
&lt;/p&gt;
&lt;p&gt;
CREST&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#21644;&#22810;&#20010;&#23567;&#25209;&#37327;&#26680;&#24515;&#38598;&#65292;&#20445;&#35777;&#38750;&#20984;&#27169;&#22411;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CREST&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20005;&#26684;&#29702;&#35770;&#20445;&#35777;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#35757;&#32451;&#38750;&#20984;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#28145;&#24230;&#32593;&#32476;&#65289;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#20445;&#35777;&#25910;&#25947;&#21040;&#38750;&#20984;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;CREST&#23558;&#38750;&#20984;&#25439;&#22833;&#27169;&#25311;&#20026;&#19968;&#31995;&#21015;&#20108;&#27425;&#20989;&#25968;&#65292;&#24182;&#20026;&#27599;&#20010;&#20108;&#27425;&#23376;&#21306;&#22495;&#25552;&#21462;&#19968;&#20010;&#26680;&#24515;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65288;&#22914;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65289;&#30340;&#26356;&#24555;&#25910;&#25947;&#65292;CREST&#20174;&#26356;&#22823;&#30340;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#20013;&#36845;&#20195;&#22320;&#25552;&#21462;&#22810;&#20010;&#23567;&#25209;&#37327;&#26680;&#24515;&#38598;&#65292;&#20197;&#30830;&#20445;&#36817;&#20284;&#26080;&#20559;&#30340;&#26799;&#24230;&#21644;&#23567;&#26041;&#24046;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#65292;CREST&#30830;&#23450;&#24182;&#25490;&#38500;&#20174;&#26680;&#24515;&#38598;&#36873;&#25321;&#27969;&#31243;&#20013;&#23398;&#20064;&#21040;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#28145;&#24230;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR-10&#12289;CIFAR-100&#12289;TinyImageNet&#21644;SNLI&#31561;&#35270;&#35273;&#21644;NLP&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the efficiency and sustainability of learning deep models, we propose CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.01243</link><description>&lt;p&gt;
&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65306;&#23398;&#20064;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29615;&#22659;&#20013;&#20570;&#20986;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#24182;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#21508;&#31181;&#24418;&#24335;&#30340;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#21487;&#33021;&#20250;&#20351;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#12290;&#24403;&#20195;&#29702;&#30001;&#20110;&#24310;&#36831;&#25110;&#20002;&#22833;&#30340;&#36890;&#36947;&#32780;&#26080;&#27861;&#35266;&#23519;&#21040;&#31995;&#32479;&#30340;&#26368;&#26032;&#29366;&#24577;&#26102;&#65292;&#36825;&#20123;&#24773;&#20917;&#20250;&#20986;&#29616;&#65292;&#20294;&#20195;&#29702;&#20173;&#24517;&#39035;&#20570;&#20986;&#23454;&#26102;&#20915;&#31574;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#36827;&#34892;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#24517;&#39035;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#29366;&#24577;&#35266;&#23519;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#65292;&#24418;&#24335;&#20026;$ \tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK}) $&#65292;&#20197;&#22312;&#24310;&#36831;&#21644;&#32570;&#22833;&#35266;&#23519;&#35774;&#32622;&#19979;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#12290;&#23613;&#31649;&#26377;&#25439;&#21487;&#35266;&#23519;&#24615;&#23545;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#31867;&#21035;&#36896;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#23398;&#20064;&#20173;&#28982;&#26159;&#39640;&#25928;&#30340;&#65292;&#36951;&#25022;&#36793;&#30028;&#26368;&#20248;&#22320;&#20381;&#36182;&#20110;&#21407;&#22987;&#31995;&#32479;&#30340;&#29366;&#24577;-&#21160;&#20316;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21463;&#24433;&#21709;&#30340;&#35266;&#23519;&#19979;&#26368;&#20339;&#31574;&#30053;&#30340;&#24615;&#33021;&#19982;&#26368;&#20248;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val
&lt;/p&gt;</description></item><item><title>&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#23454;&#29616;&#26412;&#22320;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01240</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#24615;&#22270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs. (arXiv:2306.01240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01240
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#23454;&#29616;&#26412;&#22320;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#26102;&#65292;&#22312;&#31169;&#26377;&#21644;&#20998;&#25955;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#26377;&#25928;&#30340;&#20840;&#23616;&#27169;&#22411;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#38754;&#20020;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#36890;&#36807;&#27169;&#22411;&#32858;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#23545;&#23458;&#25143;&#31471;&#26045;&#21152;&#20102;&#19968;&#31181;&#24378;&#21046;&#24615;&#30340;&#24314;&#27169;&#21516;&#36136;&#24615;&#21644;&#21516;&#27493;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning an effective global model on private and decentralized datasets has become an increasingly important challenge of machine learning when applied in practice. Existing distributed learning paradigms, such as Federated Learning, enable this via model aggregation which enforces a strong form of modeling homogeneity and synchronicity across clients. This is however not suitable to many practical scenarios. For example, in distributed sensing, heterogeneous sensors reading data from different views of the same phenomenon would need to use different models for different data modalities. Local learning therefore happens in isolation but inference requires merging the local models to achieve consensus. To enable consensus among local models, we propose a feature fusion approach that extracts local representations from local models and incorporates them into a global representation that improves the prediction performance. Achieving this requires addressing two non-trivial problems. Fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01237</link><description>&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#20013;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#31639;&#27861;&#24517;&#39035;&#20165;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#20248;&#21270;&#20915;&#31574;&#12290;&#31163;&#32447;&#36172;&#21338;&#20013;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#19988;&#36880;&#28176;&#27969;&#34892;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23454;&#29616;&#20302;&#36125;&#21494;&#26031;&#36951;&#25022;&#24182;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#21033;&#29992;&#39640;&#25928;&#30340;&#38181;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#33719;&#24471;&#20102;&#26356;&#20248;&#30340;&#31163;&#32447;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB&#65288;lower confidence bound&#65289;-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#21512;&#31163;&#32447;&#36172;&#21338;&#20013;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;UnMixMatch&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#22823;&#22810;&#25968;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#12289;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.01222</link><description>&lt;p&gt;
&#29992;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;UnMixMatch&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#22823;&#22810;&#25968;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#12289;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UnMixMatch&#65292;&#21487;&#20197;&#20174;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#24449;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#22522;&#20110;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#36890;&#36807;&#20351;&#29992;&#33258;&#30001;&#29983;&#27963;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35813;&#20551;&#35774;&#32463;&#24120;&#38480;&#21046;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#24182;&#26377;&#25928;&#21033;&#29992;&#38750;&#32422;&#26463;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;UnMixMatch&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#20855;&#26377;&#24378;&#27491;&#21017;&#21270;&#20316;&#29992;&#30340;&#30828;&#22686;&#24378;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#30784;&#34920;&#24449;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22120;&#20197;&#21450;&#36890;&#36807;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#22686;&#24378;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;UnMixMatch&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#26377;&#25152;&#19981;&#21516;&#65292;&#32467;&#26524;&#21457;&#29616;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#30830;&#35748;&#20102;&#25200;&#21160;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;LLMs&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#33021;&#21147;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01220</link><description>&lt;p&gt;
&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#19968;&#33268;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation. (arXiv:2306.01220v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#26377;&#25152;&#19981;&#21516;&#65292;&#32467;&#26524;&#21457;&#29616;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#30830;&#35748;&#20102;&#25200;&#21160;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;LLMs&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#33021;&#21147;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#38750;&#24120;&#26377;&#25928;&#12290;&#30001;&#20110;LLMs&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#20195;&#30721;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30456;&#21516;&#22320;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;HumanEval&#19978;&#30340;&#20116;&#20010;LLMs&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;LLMs&#30340;&#27880;&#24847;&#21147;&#19982;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#24615;&#19982;&#23427;&#20204;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31243;&#24230;&#20043;&#38388;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#35748;&#65292;&#22312;12&#31181;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#22987;&#32456;&#21463;&#21040;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#30340;LLMs&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been demonstrated effective for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. To deepen our understanding, we investigate whether LLMs attend to the same parts of a natural language description as human programmers during code generation. An analysis of five LLMs on a popular benchmark, HumanEval, revealed a consistent misalignment between LLMs' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of LLMs and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;ALAVI&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#32422;&#26463;VI&#27169;&#22411;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#23384;&#22312;&#26576;&#20123;&#24191;&#20041;&#21333;&#35843;&#24615;&#36136;&#30340;&#38382;&#39064;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2306.01214</link><description>&lt;p&gt;
&#24102;&#22278;&#38181;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#22686;&#24191;Lagrangian&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Augmented Lagrangian Approach to Conically Constrained Non-monotone Variational Inequality Problems. (arXiv:2306.01214v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;ALAVI&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#32422;&#26463;VI&#27169;&#22411;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#23384;&#22312;&#26576;&#20123;&#24191;&#20041;&#21333;&#35843;&#24615;&#36136;&#30340;&#38382;&#39064;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24102;&#26377;&#20984;&#22278;&#38181;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#20998;&#19981;&#31561;&#24335;&#27169;&#22411;&#12290;&#36890;&#36807;&#20026;&#25152;&#35752;&#35770;&#30340;VI&#27169;&#22411;&#24320;&#21457;&#31867;&#20284;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#38797;&#28857;&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65288;&#31216;&#20026;ALAVI&#65289;&#65292;&#29992;&#20110;&#27714;&#35299;&#19968;&#33324;&#32422;&#26463;VI&#27169;&#22411;&#12290;&#22312;&#19968;&#20010;&#34987;&#31216;&#20026;&#21407;&#22987;-&#23545;&#20598;&#21464;&#20998;&#30456;&#24178;&#24615;&#26465;&#20214;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ALAVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#24191;&#20041;&#21333;&#35843;&#24615;&#36136;&#26159;&#36275;&#22815;&#30340;&#65292;&#34429;&#28982;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#27492;&#26469;&#26263;&#31034;&#19978;&#36848;&#30456;&#24178;&#26465;&#20214;&#65292;&#24182;&#19988;&#36275;&#20197;&#30830;&#20445;ALAVI&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#27492;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;ALAVI&#23454;&#38469;&#19978;&#20855;&#26377;$O(1/\sqrt{k})$&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$k$&#26159;&#36845;&#20195;&#35745;&#25968;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#8220;&#38388;&#38553;&#20989;&#25968;&#8221;&#65292;&#22914;&#26524;&#26144;&#23556;&#26159;&#21333;&#35843;&#30340;&#65292;&#36825;&#20010;&#36895;&#24230;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#21040;$O(1/k)$&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In this paper we consider a non-monotone (mixed) variational inequality model with (nonlinear) convex conic constraints. Through developing an equivalent Lagrangian function-like primal-dual saddle-point system for the VI model in question, we introduce an augmented Lagrangian primal-dual method, to be called ALAVI in the current paper, for solving a general constrained VI model. Under an assumption, to be called the primal-dual variational coherence condition in the paper, we prove the convergence of ALAVI. Next, we show that many existing generalized monotonicity properties are sufficient -- though by no means necessary -- to imply the above mentioned coherence condition, thus are sufficient to ensure convergence of ALAVI. Under that assumption, we further show that ALAVI has in fact an $o(1/\sqrt{k})$ global rate of convergence where $k$ is the iteration count. By introducing a new gap function, this rate further improves to be $O(1/k)$ if the mapping is monotone. Finally, we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;</title><link>http://arxiv.org/abs/2306.01213</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35299;&#32544;&#32469;&#30340;&#22240;&#26524;&#34920;&#31034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22240;&#20854;&#23545;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35299;&#32544;&#32469;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#22240;&#26524;&#20851;&#31995;&#35266;&#23519;&#26631;&#31614;&#26469;&#30417;&#30563;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#27969;&#30340;&#24494;&#20998;&#21516;&#32986;&#20989;&#25968;&#23558;&#22122;&#22768;&#21464;&#37327;&#26144;&#23556;&#21040;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20013;&#26469;&#24314;&#27169;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#22240;&#26524;&#35201;&#32032;&#30340;&#35299;&#32544;&#32469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#32469;&#20808;&#39564;&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#26469;&#40723;&#21169;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#22240;&#26524;&#20998;&#35299;&#20998;&#24067;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#22240;&#26524;&#35201;&#32032;&#21644;&#26426;&#21046;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#36880;&#20803;&#37325;&#21442;&#25968;&#21270;&#30340;&#38480;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Learning disentangled causal representations is a challenging problem that has gained significant attention recently due to its implications for extracting meaningful information for downstream tasks. In this work, we define a new notion of causal disentanglement from the perspective of independent causal mechanisms. We propose ICM-VAE, a framework for learning causally disentangled representations supervised by causally related observed labels. We model causal mechanisms using learnable flow-based diffeomorphic functions to map noise variables to latent causal variables. Further, to promote the disentanglement of causal factors, we propose a causal disentanglement prior that utilizes the known causal structure to encourage learning a causally factorized distribution in the latent space. Under relatively mild conditions, we provide theoretical results showing the identifiability of causal factors and mechanisms up to permutation and elementwise reparameterization. We empirically demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#25509;&#28145;&#39640;&#26031;&#36807;&#31243;&#27169;&#25311;&#65288;LDGP&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#20223;&#30495;&#20855;&#26377;&#38750;&#31283;&#24577;&#34892;&#20026;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01212</link><description>&lt;p&gt;
&#38142;&#25509;&#28145;&#39640;&#26031;&#36807;&#31243;&#27169;&#25311;&#29992;&#20110;&#27169;&#22411;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Linked Deep Gaussian Process Emulation for Model Networks. (arXiv:2306.01212v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#25509;&#28145;&#39640;&#26031;&#36807;&#31243;&#27169;&#25311;&#65288;LDGP&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#20223;&#30495;&#20855;&#26377;&#38750;&#31283;&#24577;&#34892;&#20026;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31185;&#23398;&#38382;&#39064;&#36890;&#24120;&#26159;&#22810;&#23398;&#31185;&#30340;&#65292;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#21151;&#33021;&#22797;&#26434;&#24615;&#65292;&#32534;&#31243;&#29615;&#22659;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#38142;&#25509;&#39640;&#26031;&#36807;&#31243;&#65288;LGP&#65289;&#20223;&#30495;&#36890;&#36807;&#23558;&#21508;&#20010;&#35745;&#31639;&#26426;&#27169;&#22411;&#30340;&#39640;&#26031;&#36807;&#31243;&#20223;&#30495;&#22120;&#38598;&#25104;&#21040;&#32593;&#32476;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;LGP&#26694;&#26550;&#20013;&#65292;&#32452;&#20214;&#39640;&#26031;&#36807;&#31243;&#20223;&#30495;&#22120;&#30340;&#25152;&#38656;&#31283;&#23450;&#24615;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#27169;&#22411;&#32593;&#32476;&#27010;&#24565;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#38544;&#34255;&#23618;&#26333;&#20809;&#30340;&#28145;&#39640;&#26031;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25512;&#26029;&#36825;&#20123;&#37096;&#20998;&#26292;&#38706;&#30340;&#28145;&#24230;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;LGP&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#65292;&#21363;&#27599;&#20010;&#27169;&#22411;&#37117;&#21487;&#20197;&#20351;&#29992;DGP&#20998;&#21035;&#20223;&#30495;&#65292;&#28982;&#21518;&#38142;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38142;&#25509;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#27169;&#25311;&#65288;LDGP&#65289;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#20223;&#30495;&#20855;&#26377;&#38750;&#31283;&#24577;&#34892;&#20026;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#30456;&#27604;&#20043;&#19979;&#65292;LGP&#26694;&#26550;&#30340;&#25928;&#26524;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern scientific problems are often multi-disciplinary and require integration of computer models from different disciplines, each with distinct functional complexities, programming environments, and computation times. Linked Gaussian process (LGP) emulation tackles this challenge through a divide-and-conquer strategy that integrates Gaussian process emulators of the individual computer models in a network. However, the required stationarity of the component Gaussian process emulators within the LGP framework limits its applicability in many real-world applications. In this work, we conceptualize a network of computer models as a deep Gaussian process with partial exposure of its hidden layers. We develop a method for inference for these partially exposed deep networks that retains a key strength of the LGP framework, whereby each model can be emulated separately using a DGP and then linked together. We show in both synthetic and empirical examples that our linked deep Gaussian proces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.01206</link><description>&lt;p&gt;
&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#26679;&#26412;&#20043;&#38388;&#20272;&#35745;&#35821;&#20041;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#26469;&#33258;&#25968;&#25454;&#38598;&#25110;&#28304;&#19982;&#35757;&#32451;&#38598;&#19981;&#21516;&#20294;&#29992;&#20110;&#21516;&#19968;&#20219;&#21153;&#30340;&#22495;&#22806;&#65288;OOD&#65289;&#25110;&#22495;&#22806;&#20998;&#24067;&#65288;OODist&#65289;&#26679;&#26412;&#25551;&#36848;&#20026;&#22495;&#22806;&#65292;&#19982;&#22495;&#20869;&#65288;ID&#65289;&#26679;&#26412;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;OOD&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#65292;&#23613;&#31649;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#20851;&#27880;&#20110;OOD&#26816;&#27979;&#65292;&#20294;&#22823;&#22810;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#24182;&#21576;&#29616;&#20102;&#22810;&#20010;&#20851;&#20110;OOD&#21644;OODist&#30340;&#22810;&#37325;&#23450;&#20041;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;ID&#21644;OOD / OODist&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#35782;&#21035;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;12&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#24230;&#37327;&#22312;&#35813;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; UNet &#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#33021;&#22815;&#36890;&#36807;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#21644;&#24212;&#21464;&#22270;&#20687;&#30340;&#20998;&#26512;&#65292;&#31934;&#30830;&#22320;&#25512;&#26029;&#29983;&#29289;&#36719;&#32452;&#32455;&#20013;&#26448;&#26009;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#20248;&#21155;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01204</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340; UNet &#29992;&#20110;&#21457;&#29616;&#24322;&#36136;&#26448;&#26009;&#20013;&#38544;&#34255;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials. (arXiv:2306.01204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; UNet &#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#33021;&#22815;&#36890;&#36807;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#21644;&#24212;&#21464;&#22270;&#20687;&#30340;&#20998;&#26512;&#65292;&#31934;&#30830;&#22320;&#25512;&#26029;&#29983;&#29289;&#36719;&#32452;&#32455;&#20013;&#26448;&#26009;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#20248;&#21155;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36719;&#32452;&#32455;&#24120;&#24120;&#30001;&#20110;&#32467;&#26500;&#25104;&#20998;&#30340;&#21464;&#21270;&#32780;&#20855;&#26377;&#22797;&#26434;&#30340;&#26426;&#26800;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; UNet &#30340;&#21453;&#28436;&#24377;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#23558;&#24212;&#21464;&#22270;&#20316;&#20026;&#36755;&#20837;&#22270;&#20687;&#12289;&#27491;&#24120;&#24212;&#21147;&#36793;&#30028;&#26465;&#20214;&#21644;&#21306;&#22495;&#29289;&#29702;&#20449;&#24687;&#65292;&#20197;&#25512;&#26029;&#21147;&#23398;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; El-UNet &#22312;&#20272;&#35745;&#31561;&#21521;&#24615;&#32447;&#24615;&#24377;&#24615;&#30340;&#26410;&#30693;&#21442;&#25968;&#21644;&#24212;&#21147;&#20998;&#24067;&#26041;&#38754;&#65292;&#19982;&#20840;&#36830;&#25509;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#31934;&#24230;&#36824;&#26159;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545; El-UNet &#30340;&#19981;&#21516;&#21464;&#20307;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31354;&#38388;&#25439;&#22833;&#21152;&#26435;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#21453;&#28436;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#31561;&#21521;&#24322;&#36136;&#26448;&#26009;&#26377;&#38480;&#20803;&#27169;&#25311;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;El-UNet &#27604;&#20840;&#36830;&#25509;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft biological tissues often have complex mechanical properties due to variation in structural components. In this paper, we develop a novel UNet-based neural network model for inversion in elasticity (El-UNet) to infer the spatial distributions of mechanical parameters from strain maps as input images, normal stress boundary conditions, and domain physics information. We show superior performance, both in terms of accuracy and computational cost, by El-UNet compared to fully-connected physics-informed neural networks in estimating unknown parameters and stress distributions for isotropic linear elasticity. We characterize different variations of El-UNet and propose a self-adaptive spatial loss weighting approach. To validate our inversion models, we performed various finite-element simulations of isotropic domains with heterogenous distributions of material parameters to generate synthetic data. El-UNet is faster and more accurate than the fully-connected physics-informed implementat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#26174;&#33879;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01201</link><description>&lt;p&gt;
&#23398;&#20064;&#20309;&#26102;&#35828;&#35805;&#65306;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#24310;&#36831;&#19982;&#36136;&#37327;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#26174;&#33879;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#38899;&#32763;&#35793;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31163;&#32447;&#22330;&#26223;&#20013;&#65292;&#22312;&#27492;&#22330;&#26223;&#20013;&#65292;&#23436;&#25972;&#30340;&#36755;&#20837;&#35805;&#35821;&#22312;&#20219;&#20309;&#36755;&#20986;&#20043;&#21069;&#37117;&#26159;&#21487;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36825;&#24182;&#19981;&#21512;&#29702;&#12290;&#22312;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#32763;&#35793;&#24212;&#35813;&#22312;&#36755;&#20837;&#20449;&#24687;&#20986;&#29616;&#26102;&#31435;&#21363;&#21457;&#38899;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#23454;&#38469;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#36798;&#21040;&#20102;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22312;Greedy&#65288;wait-$k$&#65289;&#22522;&#32447;&#19978;&#26368;&#23567;&#21270;&#20102;&#24310;&#36831;&#22686;&#21152;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#20195;&#30721;&#21644;&#20114;&#21160;&#27979;&#35797;&#33050;&#26412;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;SimulS2ST&#30740;&#31350;&#21644;&#24212;&#29992;&#31243;&#24207;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output -- including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20266;&#35266;&#23519;&#20540;&#30340;MAE&#25351;&#26631;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01196</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#26377;&#24847;&#20041;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Effective Meaningful Way to Evaluate Survival Models. (arXiv:2306.01196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20266;&#35266;&#23519;&#20540;&#30340;MAE&#25351;&#26631;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#31181;&#30452;&#25509;&#25351;&#26631;&#26159;&#22522;&#20110;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;-&#27169;&#22411;&#39044;&#27979;&#26102;&#38388;&#19982;&#30495;&#23454;&#20107;&#20214;&#26102;&#38388;&#20043;&#38388;&#30340;&#32477;&#23545;&#24046;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#23545;&#25152;&#26377;&#20010;&#20307;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;&#65288;&#27491;&#30830;&#65289;&#34987;&#23457;&#26597;&#30340;&#20010;&#20307;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#19981;&#30693;&#36947;&#34987;&#23457;&#26597;&#20010;&#20307;&#23454;&#38469;&#32463;&#21382;&#20107;&#20214;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#21253;&#25324;&#65288;&#35768;&#22810;&#65289;&#34987;&#23457;&#26597;&#20010;&#20307;&#30340;&#29983;&#23384;&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#20272;&#35745;MAE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#21322;&#21512;&#25104;&#29983;&#23384;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#25351;&#26631;&#12290;&#22522;&#20110;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#65288;&#20351;&#29992;&#20266;&#35266;&#23519;&#27861;&#30340;MAE&#65289;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#30495;&#23454;&#30340;MAE&#38750;&#24120;&#25509;&#36817;-&#29305;&#21035;&#26159;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error (MAE) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate MAE for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric (MAE using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true MAE -- in particular, is better than several alternative methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39034;&#20174;&#39044;&#27979;&#31243;&#24207;&#19982;&#38598;&#21512;&#20540;&#30340;&#35757;&#32451;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38598;&#21512;&#20540;&#35757;&#32451;&#21644;&#26657;&#20934;&#25968;&#25454;&#30340;&#39034;&#20174;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2306.01191</link><description>&lt;p&gt;
&#26377;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#39034;&#20174;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Partially Labeled Data. (arXiv:2306.01191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39034;&#20174;&#39044;&#27979;&#31243;&#24207;&#19982;&#38598;&#21512;&#20540;&#30340;&#35757;&#32451;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38598;&#21512;&#20540;&#35757;&#32451;&#21644;&#26657;&#20934;&#25968;&#25454;&#30340;&#39034;&#20174;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39034;&#20174;&#39044;&#27979;&#20135;&#29983;&#30340;&#39044;&#27979;&#26159;&#38598;&#21512;&#20540;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#21644;&#26657;&#20934;&#30340;&#25968;&#25454;&#24212;&#35813;&#26159;&#31934;&#30830;&#30340;&#12290;&#22312;&#36229;&#38598;&#23398;&#20064;&#25110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#65292;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#21464;&#20307;&#65292;&#24773;&#20917;&#24688;&#24688;&#30456;&#21453;:&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#19981;&#31934;&#30830;&#65288;&#38598;&#21512;&#20540;&#65289;&#65292;&#20294;&#20174;&#27492;&#25968;&#25454;&#20013;&#24341;&#20986;&#30340;&#27169;&#22411;&#20135;&#29983;&#31934;&#30830;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#35774;&#32622;&#30456;&#32467;&#21512;&#65292;&#20351;&#39034;&#20174;&#39044;&#27979;&#36866;&#24212;&#38598;&#21512;&#20540;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#20174;&#39044;&#27979;&#31243;&#24207;&#30340;&#27010;&#25324;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38598;&#21512;&#20540;&#30340;&#35757;&#32451;&#21644;&#26657;&#20934;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#22312;&#20854;&#20013;&#23427;&#19982;&#33258;&#28982;&#22522;&#32447;&#30456;&#27604;&#36739;&#26377;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the predictions produced by conformal prediction are set-valued, the data used for training and calibration is supposed to be precise. In the setting of superset learning or learning from partial labels, a variant of weakly supervised learning, it is exactly the other way around: training data is possibly imprecise (set-valued), but the model induced from this data yields precise predictions. In this paper, we combine the two settings by making conformal prediction amenable to set-valued training data. We propose a generalization of the conformal prediction procedure that can be applied to set-valued training and calibration data. We prove the validity of the proposed method and present experimental studies in which it compares favorably to natural baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;SDE-RNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#65292;&#22312;&#20219;&#20309;&#26102;&#38388;&#23610;&#24230;&#19978;&#22635;&#34917;&#27979;&#37327;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#37327;&#21270;&#22635;&#34917;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01189</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;SDE-RNN&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Uncertainty Quantification via Neural SDE-RNN. (arXiv:2306.01189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;SDE-RNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38382;&#39064;&#65292;&#22312;&#20219;&#20309;&#26102;&#38388;&#23610;&#24230;&#19978;&#22635;&#34917;&#27979;&#37327;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#37327;&#21270;&#22635;&#34917;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#21407;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#33021;&#22815;&#22312;&#20219;&#20309;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#19978;&#22635;&#34917;&#27979;&#37327;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#37327;&#21270;&#22635;&#34917;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20256;&#25773;&#26102;&#38388;&#30636;&#38388;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;IEEE 37&#24052;&#22763;&#27979;&#35797;&#20998;&#24067;&#31995;&#32479;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22635;&#34917;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is a critical yet unsolved challenge for deep learning, especially for the time series imputation with irregularly sampled measurements. To tackle this problem, we propose a novel framework based on the principles of recurrent neural networks and neural stochastic differential equations for reconciling irregularly sampled measurements. We impute measurements at any arbitrary timescale and quantify the uncertainty in the imputations in a principled manner. Specifically, we derive analytical expressions for quantifying and propagating the epistemic and aleatoric uncertainty across time instants. Our experiments on the IEEE 37 bus test distribution system reveal that our framework can outperform state-of-the-art uncertainty quantification approaches for time-series data imputations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#25110;KSD&#25439;&#22833;&#26469;&#30830;&#20445;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#22312;&#28151;&#27788;&#31995;&#32479;&#19978;&#22797;&#29616;&#20854;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01187</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#20197;&#20445;&#25345;&#28151;&#27788;&#21560;&#24341;&#23376;&#30340;&#19981;&#21464;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Training neural operators to preserve invariant measures of chaotic attractors. (arXiv:2306.01187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#25110;KSD&#25439;&#22833;&#26469;&#30830;&#20445;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#22312;&#28151;&#27788;&#31995;&#32479;&#19978;&#22797;&#29616;&#20854;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#27788;&#31995;&#32479;&#20351;&#24471;&#38271;&#26102;&#38388;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#21021;&#22987;&#26465;&#20214;&#30340;&#24494;&#23567;&#25200;&#21160;&#20250;&#23548;&#33268;&#36712;&#36857;&#20197;&#25351;&#25968;&#36895;&#24230;&#21457;&#25955;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#20026;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#34429;&#28982;&#33021;&#22815;&#20934;&#30830;&#22320;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#26080;&#27861;&#20877;&#38271;&#26102;&#38388;&#20869;&#22797;&#21046;&#21160;&#21147;&#23398;&#30340;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#20135;&#29983;&#36864;&#21270;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26694;&#26550;&#65292;&#26088;&#22312;&#20445;&#25345;&#34920;&#24449;&#21160;&#24577;&#19981;&#21464;&#32479;&#35745;&#23646;&#24615;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#30340;&#19981;&#21464;&#27979;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#65288;&#27599;&#20010;&#26679;&#26412;&#36712;&#36857;&#37117;&#21463;&#30053;&#24494;&#19981;&#21516;&#21160;&#24577;&#30340;&#25511;&#21046;&#65289;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26032;&#30340;&#22788;&#29702;&#22024;&#26434;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340;&#21160;&#24577;&#19982;&#31070;&#32463;&#31639;&#23376;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#30340;&#25439;&#22833;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19987;&#23478;&#23545;&#22522;&#30784;&#29289;&#29702;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01181</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#20026;&#19968;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21327;&#21161;&#26500;&#24314;&#30456;&#20851;&#20219;&#21153;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;&#35813;&#33539;&#20363;&#22312;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#23588;&#20854;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20844;&#24320;&#30340;&#65292;&#21482;&#26377;&#24494;&#35843;&#25968;&#25454;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#29702;&#30001;&#35748;&#20026;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20173;&#28982;&#26159;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#24517;&#39035;&#20102;&#35299;&#24494;&#35843;&#27169;&#22411;&#27844;&#38706;&#26377;&#20851;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#29702;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#35775;&#38382;&#24050;&#32463;&#24494;&#35843;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#24819;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20803;&#20998;&#31867;&#22120;&#30340;&#25915;&#20987;TMI&#65292;&#23427;&#21033;&#29992;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;TMI&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20165;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20197;&#21450;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;niLES&#26041;&#27861;&#65292;&#23558;&#29702;&#24819;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#29992;&#20110;&#27169;&#25311;&#38590;&#20197;&#22788;&#29702;&#30340;&#28237;&#27969;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.01174</link><description>&lt;p&gt;
&#31070;&#32463;&#29702;&#24819;&#22823;&#28065;&#27169;&#25311;&#65306;&#29992;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations. (arXiv:2306.01174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;niLES&#26041;&#27861;&#65292;&#23558;&#29702;&#24819;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#29992;&#20110;&#27169;&#25311;&#38590;&#20197;&#22788;&#29702;&#30340;&#28237;&#27969;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#20004;&#20010;&#24378;&#22823;&#30340;&#24605;&#24819;&#65306;&#26469;&#33258;&#28237;&#27969;&#23553;&#38381;&#27169;&#22411;&#30340;&#29702;&#24819;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#21644;&#29992;&#20110;&#38543;&#26426;&#24314;&#27169;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#12290;&#29702;&#24819;LES&#36890;&#36807;&#23558;&#27599;&#20010;&#23436;&#25972;&#36712;&#36857;&#35270;&#20026;&#28508;&#22312;&#21160;&#21147;&#23398;&#30340;&#38543;&#26426;&#23454;&#29616;&#26469;&#23545;LES&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#23558;&#23567;&#23610;&#24230;&#25928;&#24212;&#36793;&#38469;&#21270;&#20197;&#33719;&#24471;LES&#29366;&#24577;&#30340;&#30830;&#23450;&#24615;&#28436;&#21270;&#12290;&#20294;&#26159;&#65292;&#29702;&#24819;LES&#22312;&#20998;&#26512;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#30340;&#31070;&#32463;SDE&#26469;&#27169;&#25311;&#38543;&#26426;&#36807;&#31243;&#30340;&#28436;&#21270;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#28508;&#22312;&#31354;&#38388;&#21644;&#25152;&#38656;&#30340;&#29702;&#24819;&#27969;&#22330;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#36825;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#38381;&#21512;&#27169;&#22411;&#31070;&#32463;&#21442;&#25968;&#21270;&#30340;&#22788;&#29702;&#26041;&#24335;&#24418;&#25104;&#20102;&#40092;&#26126;&#30340;&#23545;&#27604;&#65292;&#20854;&#20182;&#22788;&#29702;&#26041;&#24335;&#23558;&#27599;&#20010;&#36712;&#36857;&#35270;&#20026;&#21160;&#21147;&#23398;&#30340;&#30830;&#23450;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;Kolmogorov flow&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#65288;niLES - &#31070;&#32463;&#29702;&#24819;LES&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a data-driven learning framework that assimilates two powerful ideas: ideal large eddy simulation (LES) from turbulence closure modeling and neural stochastic differential equations (SDE) for stochastic modeling. The ideal LES models the LES flow by treating each full-order trajectory as a random realization of the underlying dynamics, as such, the effect of small-scales is marginalized to obtain the deterministic evolution of the LES state. However, ideal LES is analytically intractable. In our work, we use a latent neural SDE to model the evolution of the stochastic process and an encoder-decoder pair for transforming between the latent space and the desired ideal flow field. This stands in sharp contrast to other types of neural parameterization of closure models where each trajectory is treated as a deterministic realization of the dynamics. We show the effectiveness of our approach (niLES - neural ideal LES) on a challenging chaotic dynamical system: Kolmogorov flow a
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01162</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639; &#65288;Integrated Sensing-Communication-Computation&#65289; &#65288;arXiv&#65306;2306.01162v1 [cs.IT]&#65289;
&lt;/p&gt;
&lt;p&gt;
Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01162
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26159;&#23454;&#29616;&#19975;&#29289;&#26234;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#12289;&#20840;&#24687;&#25237;&#24433;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39640;&#32423;&#25216;&#26415;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#65292;&#21462;&#20915;&#20110;&#19977;&#20010;&#39640;&#24230;&#32806;&#21512;&#30340;&#36807;&#31243;&#30340;&#36136;&#37327;&#65292;&#21363;&#25968;&#25454;&#33719;&#21462;&#30340;&#24863;&#30693;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20256;&#36755;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#27169;&#22359;&#38656;&#35201;&#20026;&#22686;&#24378;&#33258;&#24049;&#30340;&#26381;&#21153;&#36136;&#37327;&#32780;&#31454;&#20105;&#32593;&#32476;&#36164;&#28304;&#12290;&#20026;&#27492;&#65292;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#65288;ISCC&#65289;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21508;&#31181; ISCC &#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20219;&#21153;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01160</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#21152;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#22788;&#29702;&#36234;&#26469;&#36234;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#65292;&#24207;&#21015;&#38271;&#24230;&#20851;&#20110;&#30340;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#21807;&#19968;&#19968;&#20010;&#19982;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#30340;&#32452;&#20214;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#26041;&#26696;&#26469;&#20351;&#33258;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#31232;&#30095;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#26696;&#36890;&#24120;&#21463;&#21040;&#23454;&#29616;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#26368;&#32456;&#24378;&#21152;&#19968;&#20010;&#31616;&#21333;&#19988;&#38745;&#24577;&#30340;&#32467;&#26500;&#22312;&#20851;&#27880;&#30697;&#38453;&#19978;&#12290;&#30456;&#21453;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#36890;&#24120;&#20250;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#26174;&#30528;&#24930;&#20110;&#20351;&#29992;Dao&#31561;&#20154;&#65288;2022&#65289;&#30340;Flash&#23454;&#29616;&#35745;&#31639;&#23436;&#25972;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;FlashAttention&#65292;&#20197;&#36866;&#24212;&#21253;&#21547;&#38190;/&#26597;&#35810;&#20002;&#24323;&#21644;&#22522;&#20110;&#21704;&#24076;&#30340;&#27880;&#24847;&#21147;&#22312;&#20869;&#30340;&#22823;&#31867;&#31232;&#30095;&#27880;&#24847;&#24615;&#27169;&#24335;&#12290;&#36825;&#23548;&#33268;&#23454;&#29616;&#27809;&#26377;&#20219;&#20309;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#21160;&#24577;&#31232;&#30095;&#27880;&#24847;&#24615;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#29992;&#20316;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#26684;&#24335;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01157</link><description>&lt;p&gt;
&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#21464;&#37327;&#19979;&#30340;Delphic&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding. (arXiv:2306.01157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#31361;&#20986;&#25361;&#25112;&#26159;&#38544;&#34255;&#30340;&#28151;&#28102;&#38382;&#39064;&#65306;&#26410;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#21487;&#33021;&#24433;&#21709;&#21040;&#26234;&#33021;&#20307;&#37319;&#21462;&#30340;&#34892;&#21160;&#21644;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#12290;&#38544;&#34255;&#30340;&#28151;&#28102;&#21487;&#33021;&#25439;&#23475;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20219;&#20309;&#22240;&#26524;&#32467;&#35770;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#26159;&#26377;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38750;&#21487;&#35782;&#21035;&#35774;&#32622;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19982;&#35266;&#23519;&#20860;&#23481;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#24046;&#24322;&#26469;&#23450;&#20041;&#30001;&#38544;&#34255;&#28151;&#28102;&#20559;&#24046;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#31216;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#26469;&#20272;&#35745;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#24754;&#35266;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#23450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#35797;&#22270;&#20943;&#23569;&#28151;&#28102;&#20559;&#24046;&#30340;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive ex
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#23545;&#20110;&#31616;&#32422;&#35299;&#26377;&#20559;&#22909;&#65292;&#38024;&#23545;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#20174;&#27491;&#20132;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#28436;&#21464;&#21482;&#20250;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#23569;&#25968;&#22855;&#24322;&#31354;&#38388;&#21521;&#37327;&#65292;&#35828;&#26126;&#23398;&#20064;&#36807;&#31243;&#20165;&#21457;&#29983;&#22312;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#19968;&#20010;&#23567;&#19981;&#21464;&#31354;&#38388;&#20869;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#23384;&#22312;&#31616;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01154</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31616;&#32422;&#23450;&#24459;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks. (arXiv:2306.01154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01154
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#23545;&#20110;&#31616;&#32422;&#35299;&#26377;&#20559;&#22909;&#65292;&#38024;&#23545;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#20174;&#27491;&#20132;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#28436;&#21464;&#21482;&#20250;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#23569;&#25968;&#22855;&#24322;&#31354;&#38388;&#21521;&#37327;&#65292;&#35828;&#26126;&#23398;&#20064;&#36807;&#31243;&#20165;&#21457;&#29983;&#22312;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#19968;&#20010;&#23567;&#19981;&#21464;&#31354;&#38388;&#20869;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#23384;&#22312;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#23545;&#20110;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#29616;&#35937;&#26159;&#26799;&#24230;&#19979;&#38477;&#23545;&#31616;&#32422;&#35299;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#26102;&#23398;&#20064;&#21160;&#24577;&#20013;&#30340;&#19968;&#20010;&#24778;&#20154;&#30340;&#8220;&#31616;&#32422;&#23450;&#24459;&#8221;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#27491;&#20132;&#21021;&#22987;&#21270;&#24320;&#22987;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#28436;&#21464;&#20165;&#24433;&#21709;&#25152;&#26377;&#26435;&#37325;&#30697;&#38453;&#30340;&#26497;&#23567;&#37096;&#20998;&#22855;&#24322;&#21521;&#37327;&#31354;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23613;&#31649;&#25152;&#26377;&#26435;&#37325;&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#20250;&#26356;&#26032;&#65292;&#20294;&#23398;&#20064;&#36807;&#31243;&#20165;&#21457;&#29983;&#22312;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#19968;&#20010;&#23567;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#20869;&#12290;&#36825;&#31181;&#23398;&#20064;&#21160;&#24577;&#30340;&#31616;&#21333;&#24615;&#21487;&#33021;&#23545;&#26377;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#32593;&#32476;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, an extensively studied phenomenon in training deep networks is the implicit bias of gradient descent towards parsimonious solutions. In this work, we investigate this phenomenon by narrowing our focus to deep linear networks. Through our analysis, we reveal a surprising "law of parsimony" in the learning dynamics when the data possesses low-dimensional structures. Specifically, we show that the evolution of gradient descent starting from orthogonal initialization only affects a minimal portion of singular vector spaces across all weight matrices. In other words, the learning process happens only within a small invariant subspace of each weight matrix, despite the fact that all weight parameters are updated throughout training. This simplicity in learning dynamics could have significant implications for both efficient training and a better understanding of deep networks. First, the analysis enables us to considerably improve training efficiency by taking advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35821;&#20041;&#28151;&#21512;&#23454;&#29616;&#20102;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#31867;&#21035;&#19982;&#38750;&#35270;&#35273;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#35821;&#20041;&#31867;&#21035;&#30340;&#23545;&#40784;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#35821;&#20041;&#21644;&#35270;&#35273;&#23545;&#40784;&#30340;&#24046;&#24322;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks. (arXiv:2306.01148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35821;&#20041;&#28151;&#21512;&#23454;&#29616;&#20102;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#31867;&#21035;&#19982;&#38750;&#35270;&#35273;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#35821;&#20041;&#31867;&#21035;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#27169;&#24335;&#12290;&#22312;&#24378;&#20581;&#30340;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#26399;&#26395;&#35270;&#35273;&#30456;&#20284;&#30340;&#31867;&#21035;&#20855;&#26377;&#30456;&#20284;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26412;&#25991;&#32771;&#34385;&#24403;&#35821;&#20041;&#31867;&#21035;&#30456;&#20284;&#32780;&#35270;&#35273;&#31867;&#21035;&#19981;&#21516;&#65292;&#20197;&#21450;&#24403;&#35270;&#35273;&#31867;&#21035;&#30456;&#20284;&#20294;&#35821;&#20041;&#31867;&#21035;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#31867;&#21035;&#19982;&#20219;&#24847;&#65288;&#38750;&#35270;&#35273;&#65289;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#20041;&#28151;&#21512;&#24037;&#20316;&#26469;&#29983;&#25104;&#20004;&#20010;&#31867;&#21035;&#30340;&#35821;&#20041;&#28151;&#21512;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#28151;&#21512;&#20307;&#20316;&#20026;&#22686;&#24378;&#25968;&#25454;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#35780;&#20272;&#35813;&#26041;&#27861;&#26159;&#21542;&#22686;&#21152;&#20102;&#35821;&#20041;&#23545;&#40784;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#25200;&#21160;&#25968;&#25454;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#21363;&#19968;&#20301;&#25915;&#20987;&#32773;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#23558;&#19968;&#20010;&#31867;&#21035;&#36716;&#25442;&#20026;&#30456;&#20284;&#34920;&#31034;&#30340;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#22686;&#21152;&#35821;&#20041;&#31867;&#21035;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of image classification, neural networks primarily rely on visual patterns. In robust networks, we would expect for visually similar classes to be represented similarly. We consider the problem of when semantically similar classes are visually dissimilar, and when visual similarity is present among non-similar classes. We propose a data augmentation technique with the goal of better aligning semantically similar classes with arbitrary (non-visual) semantic relationships. We leverage recent work in diffusion-based semantic mixing to generate semantic hybrids of two classes, and these hybrids are added to the training set as augmented data. We evaluate whether the method increases semantic alignment by evaluating model performance on adversarially perturbed data, with the idea that it should be easier for an adversary to switch one class to a similarly represented class. Results demonstrate that there is an increase in alignment of semantically similar classes when using our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#26041;&#27861;&#29983;&#25104;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.01144</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#20219;&#21153;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data. (arXiv:2306.01144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#26041;&#27861;&#29983;&#25104;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#21644;&#32852;&#21512;&#35821;&#35328;-&#35270;&#35273;&#29702;&#35299;&#27169;&#22411;&#30340;&#26174;&#30528;&#36827;&#23637;&#21644;&#24212;&#29992;&#22686;&#21152;&#20102;&#23545;&#25506;&#27979;&#20854;&#28508;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#34987;&#23398;&#26415;&#25968;&#25454;&#38598;&#28085;&#30422;&#30340;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#25910;&#38598;&#33258;&#28982;&#25968;&#25454;&#30340;&#22256;&#38590;&#21046;&#32422;&#20102;&#23545;AI&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#35206;&#30422;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20351;&#29992;&#27492;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#23613;&#31649;&#20219;&#21153;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20302;&#20110;&#29616;&#26377;&#30340;&#23398;&#26415;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#32771;&#34385;&#21512;&#25104;&#20219;&#21153;&#20197;&#35780;&#20272;&#21644;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive advances and applications of large language and joint language-and-visual understanding models has led to an increased need for methods of probing their potential reasoning capabilities. However, the difficulty of gather naturally-occurring data for complex multi-modal reasoning tasks bottlenecks the evaluation of AI methods on tasks which are not already covered by an academic dataset. In this work, we leverage recent advances in high resolution text-to-image generation to develop a framework for generating evaluation data for multi-modal reasoning tasks. We apply this framework to generate context-dependent anomaly data, creating a synthetic dataset on a challenging task which is not well covered by existing datasets. We benchmark the performance of a state-of-the-art visual question answering (VQA) model against data generated with this method, and demonstrate that while the task is tractable, the model performs significantly worse on the context-dependent anomaly det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26080;&#32447;&#33258;&#32452;&#32593;&#30340;&#25506;&#27979;&#27010;&#29575;&#65292;&#20445;&#25252;&#36890;&#20449;&#33410;&#28857;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#35813;&#26041;&#27861;&#21487;&#39044;&#27979;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#36890;&#20449;&#21306;&#22495;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01143</link><description>&lt;p&gt;
&#32852;&#37030;&#22270;&#23398;&#20064;&#24212;&#29992;&#20110;&#26080;&#32447;&#33258;&#32452;&#32593;&#20302;&#25506;&#27979;&#27010;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Graph Learning for Low Probability of Detection in Wireless Ad-Hoc Networks. (arXiv:2306.01143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26080;&#32447;&#33258;&#32452;&#32593;&#30340;&#25506;&#27979;&#27010;&#29575;&#65292;&#20445;&#25252;&#36890;&#20449;&#33410;&#28857;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#35813;&#26041;&#27861;&#21487;&#39044;&#27979;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#36890;&#20449;&#21306;&#22495;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20302;&#25506;&#27979;&#27010;&#29575;&#25216;&#26415;&#34987;&#29992;&#20110;&#25552;&#21319;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#23427;&#24182;&#38750;&#20445;&#25252;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#32780;&#26159;&#20840;&#38754;&#38544;&#34255;&#26080;&#32447;&#36890;&#20449;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#22522;&#20110;&#20302;&#25506;&#27979;&#27010;&#29575;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#20197;&#26368;&#23567;&#21270;&#25972;&#20010;&#26080;&#32447;&#33258;&#32452;&#32593;&#30340;&#25506;&#27979;&#27010;&#29575;&#24182;&#39044;&#27979;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#36890;&#20449;&#21306;&#22495;&#65292;&#20351;&#24471;&#33410;&#28857;&#22312;&#19982;&#22806;&#37096;&#26080;&#20154;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36890;&#20449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#29992;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#20013;&#20301;&#25968;&#32477;&#23545;&#35823;&#24046;&#20004;&#31181;&#24615;&#33021;&#25351;&#26631;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low probability of detection (LPD) has recently emerged as a means to enhance the privacy and security of wireless networks. Unlike existing wireless security techniques, LPD measures aim to conceal the entire existence of wireless communication instead of safeguarding the information transmitted from users. Motivated by LPD communication, in this paper, we study a privacy-preserving and distributed framework based on graph neural networks to minimise the detectability of a wireless ad-hoc network as a whole and predict an optimal communication region for each node in the wireless network, allowing them to communicate while remaining undetected from external actors. We also demonstrate the effectiveness of the proposed method in terms of two performance measures, i.e., mean absolute error and median absolute error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;(NRDE)&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#21644;&#27714;&#35299;&#36335;&#24452;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;(PPDE)&#65292;&#36890;&#36807;&#35760;&#24405;&#26631;&#35760;&#29305;&#24449;&#26469;&#32534;&#30721;&#36335;&#24452;&#20449;&#24687;&#65292;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#21033;&#29992;&#20869;&#23384;&#21644;&#38543;&#30528;&#32500;&#24230;&#30340;&#25193;&#23637;&#33021;&#21147;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34987;&#35777;&#26126;&#27604;&#25991;&#29486;&#20013;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.01123</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36335;&#24452;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;RDE&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neural RDE-based model for solving path-dependent PDEs. (arXiv:2306.01123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;(NRDE)&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#21644;&#27714;&#35299;&#36335;&#24452;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;(PPDE)&#65292;&#36890;&#36807;&#35760;&#24405;&#26631;&#35760;&#29305;&#24449;&#26469;&#32534;&#30721;&#36335;&#24452;&#20449;&#24687;&#65292;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#21033;&#29992;&#20869;&#23384;&#21644;&#38543;&#30528;&#32500;&#24230;&#30340;&#25193;&#23637;&#33021;&#21147;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34987;&#35777;&#26126;&#27604;&#25991;&#29486;&#20013;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;(PPDE)&#26368;&#21021;&#26159;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#36335;&#24452;&#20381;&#36182;&#34893;&#29983;&#21697;&#20013;&#24341;&#20837;&#30340;&#27010;&#24565;&#12290; &#32447;&#24615;&#24418;&#24335;&#21518;&#34987;&#30830;&#23450;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(BSDE)&#12290; &#19982;&#32463;&#20856;PDE&#30456;&#27604;&#65292;PPDE&#30340;&#35299;&#28041;&#21450;&#26080;&#38480;&#32500;&#31354;&#38388;&#21464;&#37327;&#65292;&#36825;&#20351;&#24471;&#23427;&#38590;&#20197;&#36924;&#36817;&#65292;&#29978;&#33267;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;(NRDE)&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;PPDE&#65292;&#36890;&#36807;&#35760;&#24405;&#26631;&#35760;&#29305;&#24449;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#36335;&#24452;&#20449;&#24687;&#65292;&#24182;&#25429;&#25417;&#22522;&#26412;&#21160;&#24577;&#12290; &#25552;&#20986;&#30340;PPDE&#35299;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#38543;&#32500;&#24230;&#25193;&#23637;&#30340;&#33021;&#21147;&#30340;&#22909;&#22788;&#12290; &#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of the path-dependent partial differential equation (PPDE) was first introduced in the context of path-dependent derivatives in financial markets. Its semilinear form was later identified as a non-Markovian backward stochastic differential equation (BSDE). Compared to the classical PDE, the solution of a PPDE involves an infinite-dimensional spatial variable, making it challenging to approximate, if not impossible. In this paper, we propose a neural rough differential equation (NRDE)-based model to learn PPDEs, which effectively encodes the path information through the log-signature feature while capturing the fundamental dynamics. The proposed continuous-time model for the PPDE solution offers the benefits of efficient memory usage and the ability to scale with dimensionality. Several numerical experiments, provided to validate the performance of the proposed model in comparison to the strong baseline in the literature, are used to demonstrate its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#24120;&#35265;&#30340;&#22352;&#26631;&#19978;&#21319;&#21464;&#20998;&#25512;&#26029;&#65288;CAVI&#65289;&#31639;&#27861;&#22312;&#20004;&#20010;&#22359;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20379;&#20102;&#35777;&#26126;&#20840;&#23616;&#25110;&#23616;&#37096;&#25351;&#25968;&#25910;&#25947;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.01122</link><description>&lt;p&gt;
&#35770;&#22352;&#26631;&#19978;&#21319;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Coordinate Ascent Variational Inference. (arXiv:2306.01122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#24120;&#35265;&#30340;&#22352;&#26631;&#19978;&#21319;&#21464;&#20998;&#25512;&#26029;&#65288;CAVI&#65289;&#31639;&#27861;&#22312;&#20004;&#20010;&#22359;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20379;&#20102;&#35777;&#26126;&#20840;&#23616;&#25110;&#23616;&#37096;&#25351;&#25968;&#25910;&#25947;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#19968;&#31181;&#35745;&#31639;&#26367;&#20195;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#21487;&#27604;&#36739;&#30340;&#21151;&#25928;&#21644;&#21331;&#36234;&#30340;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#29992;&#20110;&#36817;&#20284;&#38590;&#20197;&#35745;&#31639;&#30340;&#21518;&#39564;&#20998;&#24067;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#35777;&#26126;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;VI&#22312;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#65292;&#20026;VI&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#25454;&#65292;&#20294;&#23545;VI&#31639;&#27861;&#25910;&#25947;&#24615;&#26041;&#38754;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24120;&#35265;&#30340;&#22352;&#26631;&#19978;&#21319;&#21464;&#20998;&#25512;&#26029;&#65288;CAVI&#65289;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22343;&#20540;&#22330;&#65288;MF&#65289;VI&#65292;&#24182;&#20248;&#21270;&#25152;&#26377;&#20998;&#35299;&#20998;&#24067;&#31354;&#38388;&#19978;&#30340;KL&#25955;&#24230;&#30446;&#26631;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20989;&#25968;&#20998;&#26512;&#21644;&#20248;&#21270;&#30340;&#24191;&#27867;&#24037;&#20855;&#31665;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#22359;&#30340;&#24773;&#20917;&#65292;&#20998;&#26512;CAVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#26126;&#20840;&#23616;&#25110;&#23616;&#37096;&#25351;&#25968;&#25910;&#25947;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a computational alternative to Markov chain Monte Carlo approaches, variational inference (VI) is becoming more and more popular for approximating intractable posterior distributions in large-scale Bayesian models due to its comparable efficacy and superior efficiency. Several recent works provide theoretical justifications of VI by proving its statistical optimality for parameter estimation under various settings; meanwhile, formal analysis on the algorithmic convergence aspects of VI is still largely lacking. In this paper, we consider the common coordinate ascent variational inference (CAVI) algorithm for implementing the mean-field (MF) VI towards optimizing a Kullback--Leibler divergence objective functional over the space of all factorized distributions. Focusing on the two-block case, we analyze the convergence of CAVI by leveraging the extensive toolbox from functional analysis and optimization. We provide general conditions for certifying global or local exponential converg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.01121</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24046;&#20998;&#38544;&#31169;&#24335;&#24773;&#33410;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;(DP)&#38480;&#21046;&#19979;&#30340;&#37325;&#23614;&#22870;&#21169;&#30340;&#65288;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#31169;&#26377;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22870;&#21169;&#26469;&#33258;&#19968;&#20123;&#26377;&#30028;&#25110;&#27425;&#39640;&#26031;&#20998;&#24067;&#20197;&#30830;&#20445;DP&#30456;&#27604;&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#20998;&#24067;&#21482;&#26377;&#26377;&#38480;&#30340;$(1+v)$&#38454;&#30697;&#30340;&#24773;&#20917;&#65292;$v \in (0,1]$&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#30340;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#37325;&#23614;MDP&#30340;&#26694;&#26550;&#65292;&#21363;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;(JDP)&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;(LDP)&#27169;&#22411;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;JDP&#21644;LDP&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#65292;&#24182;&#34920;&#26126;&#20998;&#24067;&#30340;&#30697;&#21644;&#38544;&#31169;&#39044;&#31639;&#37117;&#23545;&#36951;&#25022;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#30340;&#39640;&#31934;&#24230;&#26085;&#21069;&#39044;&#27979;&#65292;&#34920;&#29616;&#20248;&#20110;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#23558;&#22826;&#38451;&#33021;&#34701;&#20837;&#30005;&#32593;&#12290;</title><link>http://arxiv.org/abs/2306.01112</link><description>&lt;p&gt;
&#22914;&#20309;&#29992;&#26102;&#31354;&#19978;&#19979;&#25991;&#20016;&#23500;&#26085;&#21069;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?. (arXiv:2306.01112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#22826;&#38451;&#36752;&#23556;&#26102;&#24207;&#30340;&#39640;&#31934;&#24230;&#26085;&#21069;&#39044;&#27979;&#65292;&#34920;&#29616;&#20248;&#20110;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#23558;&#22826;&#38451;&#33021;&#34701;&#20837;&#30005;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#33021;&#28508;&#21147;&#24040;&#22823;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;CO2&#25490;&#25918;&#20197;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22826;&#38451;&#36752;&#23556;&#30340;&#22266;&#26377;&#21464;&#24322;&#24615;&#32473;&#26080;&#32541;&#34701;&#20837;&#30005;&#32593;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#26469;&#25429;&#25417;&#26102;&#31354;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#23545;&#24403;&#22320;&#20219;&#20309;&#32473;&#23450;&#31449;&#28857;&#39640;&#31934;&#24230;&#30340;&#26085;&#21069;&#26102;&#24207;&#39044;&#27979;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#23545;&#20840;&#29699;&#27700;&#24179;&#36752;&#23556;&#65288;GHI&#65289;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#27599;&#20010;&#26102;&#38388;&#27493;&#39044;&#27979;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#39044;&#27979;&#19981;&#30830;&#23450;&#24230;&#30340;&#26377;&#20215;&#20540;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#30340;&#19977;&#20010;&#31449;&#28857;&#19978;&#20351;&#29992;&#25968;&#25454;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#26102;&#31354;&#19978;&#19979;&#25991;&#25152;&#24102;&#26469;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#32988;&#36807;&#19981;&#37319;&#29992;&#21355;&#26143;&#25968;&#25454;&#30340;&#26102;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context. In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate \textit{day-ahead} time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01110</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#20013;&#22122;&#22768;&#24433;&#21709;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31359;&#25140;&#24335;&#20581;&#24247;&#35774;&#22791;&#27491;&#22312;&#24341;&#39046;&#19968;&#31181;&#26032;&#26102;&#20195;&#30340;&#36830;&#32493;&#21644;&#38750;&#20405;&#20837;&#24615;&#36828;&#31243;&#30417;&#27979;&#12290;&#20854;&#20013;&#19968;&#39033;&#24212;&#29992;&#26159;&#29992;&#20110;&#28966;&#34385;&#26816;&#27979;&#12290;&#35768;&#22810;&#28966;&#34385;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#21457;&#29983;&#22312;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#20294;&#22122;&#22768;&#38459;&#27490;&#20102;&#36825;&#20123;&#36827;&#23637;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#24182;&#24320;&#21457;&#23545;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#26085;&#24120;&#29983;&#27963;&#20013;&#28151;&#20081;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23581;&#35797;&#30740;&#31350;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20309;&#22833;&#36133;&#65292;&#24182;&#20351;&#29992;&#21487;&#31359;&#25140;&#36127;&#33655;&#19982;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#25968;&#25454;&#38598;&#65292;&#22312;&#19977;&#31867;&#20998;&#31867;&#38382;&#39064;&#65288;&#22522;&#20934;&#20540; vs. &#21387;&#21147; vs. &#24841;&#24742;&#65289;&#20013;&#27604;&#36739;&#19981;&#21516;&#24378;&#24230;&#22122;&#22768;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#29983;&#29702;&#21796;&#37266;&#31561;&#32423;&#30340;&#24433;&#21709;&#12290;&#22312;&#24341;&#20837;&#22122;&#22768;&#20043;&#21069;&#65292;&#25105;&#20204;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;98.7&#65285;&#65292;&#32780;Schmidt 2018&#24180;&#30340;&#27169;&#22411;&#20165;&#36798;&#21040;&#20102;80.3&#65285;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ALO-VC&#65292;&#19968;&#31181;&#20219;&#24847;&#23545;&#20219;&#24847;&#20302;&#24310;&#36831;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20165;&#20026;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#20379;&#30340;&#19968;&#20010;&#35805;&#35821;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#12289;&#38899;&#39640;&#39044;&#27979;&#22120;&#21644;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#22240;&#26524;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01100</link><description>&lt;p&gt;
ALO-VC: &#20219;&#24847;&#23545;&#20219;&#24847;&#20302;&#24310;&#36831;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
ALO-VC: Any-to-any Low-latency One-shot Voice Conversion. (arXiv:2306.01100v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ALO-VC&#65292;&#19968;&#31181;&#20219;&#24847;&#23545;&#20219;&#24847;&#20302;&#24310;&#36831;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20165;&#20026;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#20379;&#30340;&#19968;&#20010;&#35805;&#35821;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#12289;&#38899;&#39640;&#39044;&#27979;&#22120;&#21644;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#22240;&#26524;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34892;&#38899;&#32032;&#21518;&#39564;&#22270;&#65288;PPG&#65289;&#30340;&#20219;&#24847;&#23545;&#20219;&#24847;&#20302;&#24310;&#36831;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#8212;&#8212;ALO-VC&#12290;&#20351;&#29992;&#21482;&#26377;47.5&#27627;&#31186;&#26410;&#26469;&#39044;&#27979;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#21333;&#20010;&#35805;&#35821;&#21363;&#21487;&#23454;&#29616;&#20219;&#24847;&#23545;&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#65292;&#39044;&#27979;&#25152;&#36716;&#25442;&#35821;&#38899;&#30340;&#38901;&#24459;&#30340;&#38899;&#39640;&#39044;&#27979;&#22120;&#20197;&#21450;&#20301;&#32622;&#32534;&#30721;&#26469;&#20256;&#36798;&#38899;&#32032;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#31995;&#32479;&#29256;&#26412;&#65306;ALO-VC-R&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;d-vector&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#65292;ALO-VC-E&#20351;&#29992;ECAPA-TDNN&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ALO-VC-R&#21644;ALO-VC-E&#22312;VCTK&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#19978;&#22343;&#21487;&#23454;&#29616;&#19982;&#38750;&#22240;&#26524;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20004;&#20010;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#26680;&#19978;&#37096;&#32626;&#65292;&#20855;&#26377;55&#27627;&#31186;&#30340;&#24310;&#36831;&#21644;0.78&#30340;&#23454;&#26102;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#22312;&#32593;&#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ALO-VC, a non-parallel low-latency one-shot phonetic posteriorgrams (PPGs) based voice conversion method. ALO-VC enables any-to-any voice conversion using only one utterance from the target speaker, with only 47.5 ms future look-ahead. The proposed hybrid signal processing and machine learning pipeline combines a pre-trained speaker encoder, a pitch predictor to predict the converted speech's prosody, and positional encoding to convey the phoneme's location information. We introduce two system versions: ALO-VC-R, which uses a pre-trained d-vector speaker encoder, and ALO-VC-E, which improves performance using the ECAPA-TDNN speaker encoder. The experimental results demonstrate both ALO-VC-R and ALO-VC-E can achieve comparable performance to non-causal baseline systems on the VCTK dataset and two out-of-domain datasets. Furthermore, both proposed systems can be deployed on a single CPU core with 55 ms latency and 0.78 real-time factor. Our demo is available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24490;&#29615;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#65288;HRED&#65289;&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#23545;&#36755;&#20837;&#23376;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#22312;&#36739;&#20302;&#39057;&#29575;&#27169;&#22411;&#20013;&#22788;&#29702;&#36825;&#20123;&#24207;&#21015;&#65292;&#24182;&#22312;&#21407;&#22987;&#25968;&#25454;&#39057;&#29575;&#19979;&#35299;&#30721;&#36755;&#20986;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01070</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Attention Encoder Decoder. (arXiv:2306.01070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24490;&#29615;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#65288;HRED&#65289;&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#23545;&#36755;&#20837;&#23376;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#22312;&#36739;&#20302;&#39057;&#29575;&#27169;&#22411;&#20013;&#22788;&#29702;&#36825;&#20123;&#24207;&#21015;&#65292;&#24182;&#22312;&#21407;&#22987;&#25968;&#25454;&#39057;&#29575;&#19979;&#35299;&#30721;&#36755;&#20986;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#33258;&#22238;&#24402;&#24314;&#27169;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#30340;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#36755;&#20986;&#65292;&#24403;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36825;&#21464;&#24471;&#32791;&#26102;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21387;&#32553;&#25968;&#25454;&#30340;&#20998;&#23618;&#33258;&#22238;&#24402;&#26041;&#27861;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#22312;&#21407;&#22987;&#25968;&#25454;&#39057;&#29575;&#19979;&#29983;&#25104;&#36755;&#20986;&#65292;&#23548;&#33268;&#27169;&#22411;&#36895;&#24230;&#24930;&#21644;&#20869;&#23384;&#21344;&#29992;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24490;&#29615;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#65288;HRED&#65289;&#26550;&#26500;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#29420;&#31435;&#22320;&#23545;&#36755;&#20837;&#23376;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#22312;&#36739;&#20302;&#39057;&#29575;&#27169;&#22411;&#20013;&#22788;&#29702;&#36825;&#20123;&#24207;&#21015;&#65292;&#24182;&#22312;&#21407;&#22987;&#25968;&#25454;&#39057;&#29575;&#19979;&#35299;&#30721;&#36755;&#20986;&#12290;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#35299;&#37322;&#20026;&#38544;&#24335;&#23450;&#20041;&#30340;&#23884;&#20837;&#30697;&#38453;&#24182;&#20351;&#29992;&#37319;&#26679;softmax&#20272;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#39640;&#39057;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38271;&#24207;&#21015;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#27169;&#25311;&#20102;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#20570;&#23545;&#27604;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30340;&#23398;&#20064;&#35268;&#24459;&#65292;&#21457;&#23637;&#20102;&#20869;&#37096;&#34920;&#24449;&#19982;&#23548;&#33322;&#31574;&#30053;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20026;&#29983;&#29289;&#23398;&#29305;&#24449;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#23450;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2306.01066</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#23548;&#33322;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Navigation Strategies in the Morris Water Maze through Deep Reinforcement Learning. (arXiv:2306.01066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#27169;&#25311;&#20102;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#32467;&#26524;&#20570;&#23545;&#27604;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30340;&#23398;&#20064;&#35268;&#24459;&#65292;&#21457;&#23637;&#20102;&#20869;&#37096;&#34920;&#24449;&#19982;&#23548;&#33322;&#31574;&#30053;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20026;&#29983;&#29289;&#23398;&#29305;&#24449;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#23450;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23548;&#33322;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#25216;&#33021;&#65292;&#20854;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#30340;&#30740;&#31350;&#21382;&#21490;&#24736;&#20037;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;2D&#30340;&#33707;&#37324;&#26031;&#27700;&#36855;&#23467;&#65292;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#65292;&#33258;&#21160;&#20998;&#31867;&#23548;&#33322;&#31574;&#30053;&#65292;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#31574;&#30053;&#20998;&#24067;&#65292;&#24182;&#19982;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#21644;&#21870;&#40831;&#31867;&#21160;&#29289;&#30456;&#20284;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#23545;&#20854;&#26377;&#29992;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26368;&#20855;&#30410;&#22788;&#30340;&#20219;&#21153;&#21487;&#33021;&#26356;&#31526;&#21512;&#30495;&#23454;&#20195;&#29702;&#20351;&#29992;&#30340;&#29983;&#29289;&#23398;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20195;&#29702;&#31243;&#24207;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20013;&#20869;&#37096;&#34920;&#24449;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#34920;&#24449;&#31867;&#20284;&#20110;&#21457;&#29616;&#20110;&#40736;&#33041;&#20013;&#30340;&#20301;&#32622;&#32454;&#32990;&#21644;&#22836;&#26041;&#21521;&#32454;&#32990;&#65292;&#20854;&#23384;&#22312;&#19982;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#30340;&#23548;&#33322;&#31574;&#30053;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigation is a complex skill with a long history of research in animals and humans. In this work, we simulate the Morris Water Maze in 2D to train deep reinforcement learning agents. We perform automatic classification of navigation strategies, analyze the distribution of strategies used by artificial agents, and compare them with experimental data to show similar learning dynamics as those seen in humans and rodents. We develop environment-specific auxiliary tasks and examine factors affecting their usefulness. We suggest that the most beneficial tasks are potentially more biologically feasible for real agents to use. Lastly, we explore the development of internal representations in the activations of artificial agent neural networks. These representations resemble place cells and head-direction cells found in mouse brains, and their presence has correlation to the navigation strategies that artificial agents employ.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23558;&#21333;&#27491;&#25968;&#25454;&#36716;&#21270;&#20026;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21333;&#27491;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#32570;&#22833;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01034</link><description>&lt;p&gt;
&#21333;&#27491;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pseudo Labels for Single Positive Multi-Label Learning. (arXiv:2306.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23558;&#21333;&#27491;&#25968;&#25454;&#36716;&#21270;&#20026;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21333;&#27491;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#32570;&#22833;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#26159;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#65306;&#24517;&#39035;&#20026;&#27599;&#20010;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#31867;&#21035;&#26631;&#35760;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#21333;&#27491;&#22810;&#26631;&#31614;&#65288;SPML&#65289;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#27599;&#20010;&#22270;&#20687;&#19978;&#35757;&#32451;&#19968;&#20010;&#27491;&#26679;&#26412;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;SPML&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22788;&#29702;&#32570;&#22833;&#30340;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#36896;&#22810;&#26631;&#31614;&#26469;&#23558;&#21333;&#27491;&#25968;&#25454;&#36716;&#21270;&#20026;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22312;&#21333;&#27491;&#26631;&#31614;&#19978;&#30340;&#25945;&#24072;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20316;&#20026;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#20197;&#22312;&#23436;&#20840;&#26631;&#35760;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#12290;&#36890;&#36807;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#29983;&#27169;&#22411;&#25152;&#36798;&#21040;&#30340;&#24615;&#33021;&#25509;&#36817;&#20351;&#29992;&#23454;&#38469;&#23436;&#20840;&#26631;&#35760;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost of data annotation is a substantial impediment for multi-label image classification: in every image, every category must be labeled as present or absent. Single positive multi-label (SPML) learning is a cost-effective solution, where models are trained on a single positive label per image. Thus, SPML is a more challenging domain, since it requires dealing with missing labels. In this work, we propose a method to turn single positive data into fully-labeled data: Pseudo Multi-Labels. Basically, a teacher network is trained on single positive labels. Then, we treat the teacher model's predictions on the training data as ground-truth labels to train a student network on fully-labeled images. With this simple approach, we show that the performance achieved by the student model approaches that of a model trained on the actual fully-labeled images.
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#22823;&#35268;&#27169;&#28151;&#26434;&#21338;&#24328;&#29615;&#22659;&#19979;&#65292;&#21363;&#20351;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#28151;&#27788;&#29616;&#35937;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2306.01032</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#19979;&#65292;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20173;&#28982;&#23384;&#22312;&#28151;&#27788;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Chaos persists in large-scale multi-agent learning despite adaptive learning rates. (arXiv:2306.01032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01032
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#22823;&#35268;&#27169;&#28151;&#26434;&#21338;&#24328;&#29615;&#22659;&#19979;&#65292;&#21363;&#20351;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#28151;&#27788;&#29616;&#35937;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30456;&#23545;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26356;&#21152;&#22256;&#38590;&#12289;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#25105;&#23545;&#24328;&#24179;&#34913;&#30340;&#25910;&#25947;&#65292;&#35768;&#22810;&#19987;&#38376;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#25216;&#26415;&#24050;&#32463;&#35774;&#35745;&#20986;&#26469;&#12290;&#20854;&#20013;&#19968;&#20010;&#22791;&#21463;&#36190;&#35465;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#21160;&#24577;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#25216;&#26415;&#24050;&#30693;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#28216;&#25103;&#20013;&#25552;&#39640;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#22312;&#26356;&#22823;&#30340;&#26234;&#33021;&#20307;&#32676;&#20307;&#20013;&#20998;&#26512;&#36825;&#20123;&#25216;&#26415;&#23601;&#22256;&#38590;&#22810;&#20102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#20004;&#20010;&#31574;&#30053;&#65292;&#20351;&#29992;&#22266;&#23450;&#23398;&#20064;&#29575;&#30340;&#23398;&#20064;&#20063;&#20250;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#36275;&#22815;&#22823;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#28151;&#27788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#26222;&#36941;&#30340;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#22823;&#37327;&#28151;&#21512;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#28151;&#27788;&#29616;&#35937;&#22312;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26102;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#25216;&#26415;&#23618;&#38754;&#19978;&#65292;&#30001;&#20110;&#31995;&#32479;&#30340;&#38750;&#33258;&#27835;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#25193;&#25955;&#20998;&#26512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent learning is intrinsically harder, more unstable and unpredictable than single agent optimization. For this reason, numerous specialized heuristics and techniques have been designed towards the goal of achieving convergence to equilibria in self-play. One such celebrated approach is the use of dynamically adaptive learning rates. Although such techniques are known to allow for improved convergence guarantees in small games, it has been much harder to analyze them in more relevant settings with large populations of agents. These settings are particularly hard as recent work has established that learning with fixed rates will become chaotic given large enough populations.In this work, we show that chaos persists in large population congestion games despite using adaptive learning rates even for the ubiquitous Multiplicative Weight Updates algorithm, even in the presence of only two strategies. At a technical level, due to the non-autonomous nature of the system, our approach g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#37325;&#35270;&#36716;&#24405;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#32654;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01031</link><description>&lt;p&gt;
&#26080;&#38656;&#31934;&#30830;&#26631;&#27880;: &#22522;&#20110;&#19981;&#23436;&#20840;&#36716;&#24405;&#30340;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts. (arXiv:2306.01031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#37325;&#35270;&#36716;&#24405;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#32654;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27169;&#22411;&#12290;&#36716;&#24405;&#19981;&#20934;&#30830;&#30340;&#35821;&#38899;&#26159;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#20250;&#38477;&#20302;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Bypass Temporal Classification(BTC)&#20316;&#20026;&#32852;&#32467;&#26102;&#24207;&#20998;&#31867;(Connectionist Temporal Classification, CTC)&#20934;&#21017;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;BTC&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#20102;&#19982;&#36716;&#24405;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#22270;&#30340;&#28789;&#27963;&#24615;&#26469;&#23454;&#29616;&#30340;&#65292;&#23427;&#34987;&#23454;&#29616;&#20026;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;(WFST)&#32452;&#21512;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#19981;&#23436;&#25972;&#36716;&#24405;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#26102;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#23558;&#25104;&#20026;&#24320;&#28304;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#25552;&#20379;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#12290;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#25353;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.01027</link><description>&lt;p&gt;
&#20351;&#29992;Tsetlin&#26426;&#22120;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;FPGA&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
An FPGA Architecture for Online Learning using the Tsetlin Machine. (arXiv:2306.01027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#25552;&#20379;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#12290;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#25353;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#19981;&#26029;&#28436;&#21270;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#26550;&#26500;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#30340;&#36816;&#34892;&#26102;&#23398;&#20064;&#31649;&#29702;&#65292;&#23454;&#29616;&#20102;&#33455;&#29255;&#20869;&#37096;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#20043;&#21069;&#36890;&#36807;&#39044;&#20998;&#31867;&#25968;&#25454;&#22312;FPGA&#19978;&#36827;&#34892;&#25353;&#38656;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a need for machine learning models to evolve in unsupervised circumstances. New classifications may be introduced, unexpected faults may occur, or the initial dataset may be small compared to the data-points presented to the system during normal operation. Implementing such a system using neural networks involves significant mathematical complexity, which is a major issue in power-critical edge applications.  This paper proposes a novel field-programmable gate-array infrastructure for online learning, implementing a low-complexity machine learning algorithm called the Tsetlin Machine. This infrastructure features a custom-designed architecture for run-time learning management, providing on-chip offline and online learning. Using this architecture, training can be carried out on-demand on the \ac{FPGA} with pre-classified data before inference takes place. Additionally, our architecture provisions online learning, where training can be interleaved with inference during operatio
&lt;/p&gt;</description></item><item><title>PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01016</link><description>&lt;p&gt;
PV2TEA&#65306;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#25277;&#21462;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01016
&lt;/p&gt;
&lt;p&gt;
PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#65288;&#20363;&#22914;&#23646;&#24615;&#20540;&#25552;&#21462;&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24314;&#27169;&#65292;&#20294;&#20165;&#22522;&#20110;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23646;&#24615;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#20687;&#30340;&#25552;&#21462;&#20013;&#21463;&#30410;&#65292;&#22914;&#39068;&#33394;&#12289;&#24418;&#29366;&#12289;&#22270;&#26696;&#31561;&#12290;&#35270;&#35273;&#27169;&#24577;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#27880;&#37322;&#30340;&#38590;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#23646;&#24615;&#20449;&#24687;&#25552;&#21462;&#22120;&#30456;&#32467;&#21512;&#12290;&#36328;&#27169;&#24577;&#38598;&#25104;&#38754;&#20020;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;C1&#65289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#22312;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#26494;&#25955;&#21305;&#37197;&#65307;&#65288;C2&#65289;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#20016;&#23500;&#30340;&#32972;&#26223;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#39044;&#27979;&#65307;&#65288;C3&#65289;&#26469;&#33258;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#21462;&#22120;&#30340;&#24369;&#30417;&#30563;&#26631;&#31614;&#23545;&#20110;&#22810;&#27169;&#24577;&#35757;&#32451;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PV2TEA&#65292;&#36825;&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37197;&#22791;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#65306;&#65288;S1&#65289;&#22686;&#24378;&#30340;&#26631;&#31614;&#24179;&#28369;&#23545;&#27604;&#65292;&#20197;&#25913;&#36827;&#26494;&#25955;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;; &#65288;S2&#65289;&#27880;&#24847;&#21147;&#21098;&#26525;&#26041;&#26696;&#29992;&#20110;&#22312;&#20445;&#30041;&#27491;&#30830;&#20449;&#24687;&#30340;&#21516;&#26102;&#28040;&#38500;&#19968;&#20123;&#19981;&#24517;&#35201;&#30340;&#32454;&#33410;&#65307;&#65288;S3&#65289;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#21487;&#37325;&#32452;&#21367;&#31215;&#33258;&#36866;&#24212;&#27169;&#22359;&#65292;&#20197;&#24110;&#21161;&#28040;&#38500;&#26469;&#33258;&#25991;&#26412;&#25552;&#21462;&#22120;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-prunin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#23545;&#20110;&#19979;&#28216;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#22270;&#24418;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.01012</link><description>&lt;p&gt;
&#26102;&#38388;&#28436;&#21270;&#22270;&#30340;&#22270;&#32423;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Graph-Level Embedding for Time-Evolving Graphs. (arXiv:2306.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#23545;&#20110;&#19979;&#28216;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#22270;&#24418;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20174;&#33410;&#28857;&#21040;&#22270;&#30340;&#31890;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#22312;&#33410;&#28857;&#32423;&#21035;&#34920;&#31034;&#26041;&#38754;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#21160;&#24577;&#25110;&#26102;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#32593;&#32476;&#20013;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#23545;&#21508;&#31181;&#19979;&#28216;&#22270;&#26816;&#32034;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#26102;&#38388;&#22270;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#26102;&#38388;&#22270;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#24314;&#31435;&#19968;&#20010;&#22810;&#23618;&#22270;&#24182;&#20351;&#29992;&#20855;&#26377;&#26102;&#38388;&#22238;&#28335;&#30340;&#20462;&#25913;&#21518;&#38543;&#26426;&#28216;&#36208;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#19978;&#35757;&#32451;&#8220;&#25991;&#26723;&#32423;&#8221;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#22270;&#32423;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph's nodes. We then train a "document-level" language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#36827;&#34892;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#27491;&#30830;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#12290;</title><link>http://arxiv.org/abs/2306.01010</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#32500;&#21333;&#20803;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model. (arXiv:2306.01010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#36827;&#34892;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#27491;&#30830;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#26469;&#39044;&#27979;&#20840;&#38034;&#28082;&#27969;&#30005;&#27744;&#30340;&#24615;&#33021;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#20854;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#20108;&#32500;&#27169;&#22411;&#21253;&#25324;6&#20010;&#25511;&#21046;&#26041;&#31243;&#21644;24&#20010;&#36793;&#30028;&#26465;&#20214;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#20869;&#21457;&#29983;&#30340;&#30005;&#21270;&#23398;&#21453;&#24212;&#12289;&#36136;&#37327;&#20256;&#36755;&#21644;&#27969;&#20307;&#21147;&#23398;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#29992;PINN&#26041;&#27861;&#35299;&#20915;2D&#27169;&#22411;&#65292;&#37319;&#29992;&#22797;&#21512;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#29289;&#31181;&#27987;&#24230;&#21644;&#30005;&#20301;&#65307;&#26681;&#25454;&#30005;&#27744;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#23545;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#23558;&#22788;&#29702;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#36793;&#30028;&#26465;&#20214;&#20808;&#32553;&#25918;&#21040;1&#30340;&#25968;&#37327;&#32423;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#37325;&#27861;&#36827;&#19968;&#27493;&#24179;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;PINN&#33021;&#22815;&#27491;&#30830;&#22320;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#65292;&#20294;&#30005;&#20301;&#30340;&#39044;&#27979;&#20855;&#26377;&#31867;&#20284;&#20110;&#24120;&#25968;&#30340;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a physics-informed neural network (PINN) approach for predicting the performance of an all-vanadium redox flow battery, with its physics constraints enforced by a two-dimensional (2D) mathematical model. The 2D model, which includes 6 governing equations and 24 boundary conditions, provides a detailed representation of the electrochemical reactions, mass transport and hydrodynamics occurring inside the redox flow battery. To solve the 2D model with the PINN approach, a composite neural network is employed to approximate species concentration and potentials; the input and output are normalized according to prior knowledge of the battery system; the governing equations and boundary conditions are first scaled to an order of magnitude around 1, and then further balanced with a self-weighting method. Our numerical results show that the PINN is able to predict cell voltage correctly, but the prediction of potentials shows a constant-like shift. To fix the shift, th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01009</link><description>&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29983;&#25104;&#21464;&#21387;&#22120;&#27169;&#22411;&#20174;&#21069;&#25552;&#20013;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#35757;&#32451;&#35774;&#32622;&#30340;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#38543;&#35268;&#27169;&#22686;&#21152;&#32780;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38500;&#20102;OpenAI GPT-3&#21644;GPT-3.5&#27169;&#22411;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#25512;&#29702;&#38142;&#30340;&#38271;&#24230;&#32780;&#20943;&#24369;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20174;1.17&#20159;&#21040;1750&#20159;&#20010;&#21442;&#25968;&#30340;&#21508;&#31181;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01007</link><description>&lt;p&gt;
&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#20844;&#24179;&#35299;&#32544;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#25353;&#26102;&#38388;&#39034;&#24207;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#25509;&#25910;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#24067;&#20551;&#35774;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#23545;&#21160;&#24577;&#36951;&#25022;&#25110;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#20005;&#26684;&#30028;&#38480;&#26469;&#23637;&#31034;&#20854;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23436;&#20840;&#24573;&#30053;&#20102;&#24102;&#26377;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#23398;&#20064;&#65292;&#20854;&#23450;&#20041;&#20026;&#36328;&#19981;&#21516;&#23376;&#26063;&#32676;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#32773;&#38656;&#35201;&#20351;&#29992;&#20840;&#23616;&#26356;&#25913;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#20302;&#25928;&#30340;&#12290;&#21463;&#21040;&#31232;&#30095;&#26426;&#21046;&#36716;&#31227;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22768;&#31216;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#21464;&#21270;&#29615;&#22659;&#21487;&#20197;&#24402;&#22240;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#37096;&#20998;&#23398;&#20064;&#21442;&#25968;&#30340;&#37096;&#20998;&#21464;&#21270;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#22312;&#20551;&#35774;&#20174;&#19981;&#21516;&#23376;&#20154;&#32676;&#25910;&#38598;&#30340;&#25968;&#25454;&#20855;&#26377;&#20844;&#24179;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27599;&#20010;&#23376;&#20154;&#32676;&#27169;&#22411;&#34920;&#31034;&#20844;&#27491;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;&#30340;&#26041;&#27861;AbODE&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#26469;&#24212;&#23545;&#34507;&#30333;&#25240;&#21472;&#12289;&#36870;&#21521;&#25240;&#21472;&#21644;&#23545;&#25509;&#31561;&#25361;&#25112;&#65292;&#24182;&#19982;&#26102;&#38388;&#32593;&#32476;&#21644;&#22270;&#21305;&#37197;&#32593;&#32476;&#20855;&#26377;&#22522;&#26412;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.01005</link><description>&lt;p&gt;
AbODE&#65306;&#20351;&#29992;&#32852;&#21512;ODE&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;
&lt;/p&gt;
&lt;p&gt;
AbODE: Ab Initio Antibody Design using Conjoined ODEs. (arXiv:2306.01005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;&#30340;&#26041;&#27861;AbODE&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#26469;&#24212;&#23545;&#34507;&#30333;&#25240;&#21472;&#12289;&#36870;&#21521;&#25240;&#21472;&#21644;&#23545;&#25509;&#31561;&#25361;&#25112;&#65292;&#24182;&#19982;&#26102;&#38388;&#32593;&#32476;&#21644;&#22270;&#21305;&#37197;&#32593;&#32476;&#20855;&#26377;&#22522;&#26412;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#19968;&#31181;Y&#24418;&#34507;&#30333;&#65292;&#21487;&#20197;&#20013;&#21644;&#30149;&#21407;&#20307;&#65292;&#24182;&#26500;&#25104;&#25105;&#20204;&#36866;&#24212;&#24615;&#20813;&#30123;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#26032;&#30340;&#25239;&#20307;&#30340;&#20174;&#22836;&#35774;&#35745;&#65292;&#20197;&#29305;&#23450;&#25239;&#21407;&#20026;&#38774;&#26631;&#65292;&#26159;&#21152;&#36895;&#30123;&#33495;&#21457;&#29616;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27688;&#22522;&#37240;&#24207;&#21015;&#21644;&#19977;&#32500;&#32467;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;&#28085;&#30422;&#24182;&#24378;&#35843;&#20102;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#19968;&#20123;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#34507;&#30333;&#25240;&#21472;&#65288;&#24207;&#21015;&#21040;&#32467;&#26500;&#65289;&#65292;&#36870;&#21521;&#25240;&#21472;&#65288;&#32467;&#26500;&#21040;&#24207;&#21015;&#65289;&#21644;&#23545;&#25509;&#65288;&#32467;&#21512;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;AbODE&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#25193;&#23637;&#20102;&#22270;&#24418;PDE&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#22806;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;AbODE&#20351;&#29992;&#19968;&#36718;&#23436;&#25972;&#30340;&#35299;&#30721;&#65292;&#24182;&#24341;&#20986;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#25239;&#20307;&#20869;&#37096;&#21644;&#25239;&#21407;&#30456;&#20851;&#30340;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;AbODE&#19982;&#26102;&#38388;&#32593;&#32476;&#20197;&#21450;&#22270;&#21305;&#37197;&#32593;&#32476;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are Y-shaped proteins that neutralize pathogens and constitute the core of our adaptive immune system. De novo generation of new antibodies that target specific antigens holds the key to accelerating vaccine discovery. However, this co-design of the amino acid sequence and the 3D structure subsumes and accentuates some central challenges from multiple tasks, including protein folding (sequence to structure), inverse folding (structure to sequence), and docking (binding). We strive to surmount these challenges with a new generative model AbODE that extends graph PDEs to accommodate both contextual information and external interactions. Unlike existing approaches, AbODE uses a single round of full-shot decoding and elicits continuous differential attention that encapsulates and evolves with latent interactions within the antibody as well as those involving the antigen. We unravel fundamental connections between AbODE and temporal networks as well as graph-matching networks. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;AGNet&#65292;&#36890;&#36807;&#36716;&#25442;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#20013;&#35782;&#21035;&#33337;&#33334;&#36752;&#23556;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01002</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#33337;&#33334;&#36752;&#23556;&#22122;&#22768;&#35782;&#21035;&#19982;&#21487;&#23398;&#20064;&#24494;&#35266;&#23567;&#27874;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform. (arXiv:2306.01002v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;AGNet&#65292;&#36890;&#36807;&#36716;&#25442;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#20013;&#35782;&#21035;&#33337;&#33334;&#36752;&#23556;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#28023;&#27915;&#22768;&#23398;&#29615;&#22659;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#32972;&#26223;&#22122;&#22768;&#21644;&#21487;&#21464;&#30340;&#20449;&#36947;&#20256;&#36755;&#29615;&#22659;&#20351;&#24471;&#20934;&#30830;&#30340;&#33337;&#33334;&#36752;&#23556;&#22122;&#22768;&#35782;&#21035;&#21464;&#24471;&#22797;&#26434;&#12290;&#29616;&#26377;&#30340;&#35782;&#21035;&#31995;&#32479;&#22312;&#22788;&#29702;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#26041;&#38754;&#36739;&#20026;&#34180;&#24369;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20196;&#20154;&#22833;&#26395;&#12290;&#20026;&#20102;&#20445;&#25345;&#35782;&#21035;&#31995;&#32479;&#22312;&#21508;&#31181;&#27700;&#19979;&#29615;&#22659;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;&#8212;&#8212;AGNet&#65288;&#33258;&#36866;&#24212;&#36890;&#29992;&#32593;&#32476;&#65289;&#12290;&#36890;&#36807;&#23558;&#22266;&#23450;&#30340;&#23567;&#27874;&#21442;&#25968;&#36716;&#25442;&#20026;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;AGNet&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#12290;&#20854;&#28789;&#27963;&#24494;&#35266;&#30340;&#35774;&#35745;&#26377;&#21161;&#20110;&#25429;&#33719;&#26356;&#22810;&#32972;&#26223;&#22768;&#23398;&#20449;&#24687;&#65288;&#20363;&#22914;&#32972;&#26223;&#22122;&#22768;&#12289;&#27700;&#19979;&#20256;&#36755;&#36890;&#36947;&#65289;&#12290;&#20026;&#20102;&#21033;&#29992;&#23567;&#27874;&#35889;&#22270;&#20013;&#30340;&#38544;&#24335;&#20449;&#24687;&#65292;AGNet&#37319;&#29992;&#24182;&#34892;&#21367;&#31215;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;Convolutional Neural Network with Parallel Convolution Attention M&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the ocean acoustic environment is a tricky task. Background noise and variable channel transmission environment make it complicated to implement accurate ship-radiated noise recognition. Existing recognition systems are weak in addressing the variable underwater environment, thus leading to disappointing performance in practical application. In order to keep the recognition system robust in various underwater environments, this work proposes an adaptive generalized recognition system - AGNet (Adaptive Generalized Network). By converting fixed wavelet parameters into fine-grained learnable parameters, AGNet learns the characteristics of underwater sound at different frequencies. Its flexible and fine-grained design is conducive to capturing more background acoustic information (e.g., background noise, underwater transmission channel). To utilize the implicit information in wavelet spectrograms, AGNet adopts the convolutional neural network with parallel convolution attention m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36827;&#34892;&#24378;&#21046;&#23545;&#40784;&#65292;&#20943;&#36731;&#20102;&#23545;&#36880;&#23383;&#36716;&#24405;&#30340;&#38656;&#27714;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00996</link><description>&lt;p&gt;
&#21033;&#29992;&#38899;&#32032;&#32423;&#24314;&#27169;&#36827;&#34892;&#22833;&#35821;&#24615;&#35328;&#35821;&#24369;&#30417;&#30563;&#24378;&#21046;&#38901;&#24459;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling. (arXiv:2306.00996v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36827;&#34892;&#24378;&#21046;&#23545;&#40784;&#65292;&#20943;&#36731;&#20102;&#23545;&#36880;&#23383;&#36716;&#24405;&#30340;&#38656;&#27714;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#30340;&#30740;&#31350;&#21487;&#20197;&#21463;&#30410;&#20110;&#26102;&#38388;&#23545;&#40784;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22833;&#35821;&#24615;&#35328;&#35821;&#20013;&#30340;&#38899;&#39057;-&#25991;&#26412;&#19981;&#21305;&#37197;&#20250;&#23548;&#33268;&#29616;&#20195;&#35821;&#38899;&#23545;&#40784;&#22120;&#30340;&#24615;&#33021;&#24555;&#36895;&#19979;&#38477;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#33258;&#21160;&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#12290;&#35813;&#24369;&#30417;&#30563;&#26041;&#27861;&#20943;&#36731;&#20102;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36880;&#23383;&#36716;&#24405;&#23454;&#29616;&#24378;&#21046;&#23545;&#40784;&#30340;&#38656;&#27714;&#12290;&#22312;&#22270;&#30340;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23545;&#24120;&#35265;&#30340;&#22833;&#35821;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#21363;&#37325;&#22797;&#21644;&#30465;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;Oracle&#38169;&#35823;&#29575;&#35780;&#20272;&#38899;&#39057;-&#25991;&#26412;&#19981;&#21305;&#37197;&#31243;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;TIMIT&#27979;&#35797;&#38598;&#21644;UCLASS&#25968;&#25454;&#38598;&#30340;&#25439;&#22351;&#29256;&#26412;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#21484;&#22238;&#29575;&#26041;&#38754;&#65292;&#22312;&#30456;&#23545;&#22522;&#32447;&#19978;&#23454;&#29616;&#20102;23-25%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.00986</link><description>&lt;p&gt;
&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusion Self-Guidance for Controllable Image Generation. (arXiv:2306.00986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#20174;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#35768;&#22810;&#26041;&#38754;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#36890;&#36807;&#25991;&#26412;&#26469;&#20256;&#36798;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#25552;&#20379;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#26356;&#22823;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#25552;&#21462;&#20986;&#23545;&#35937;&#30340;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#22806;&#35266;&#31561;&#23646;&#24615;&#24182;&#29992;&#20110;&#25351;&#23548;&#37319;&#26679;&#12290;&#33258;&#23548;&#31867;&#20284;&#20110;&#20998;&#31867;&#22120;&#24341;&#23548;&#65292;&#20294;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26412;&#36523;&#20013;&#23384;&#22312;&#30340;&#20449;&#21495;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32452;&#21512;&#19968;&#32452;&#31616;&#21333;&#30340;&#23646;&#24615;&#26469;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#20363;&#22914;&#20462;&#25913;&#23545;&#35937;&#30340;&#20301;&#32622;&#25110;&#22823;&#23567;&#65292;&#23558;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#22806;&#35266;&#19982;&#21478;&#19968;&#20010;&#22270;&#20687;&#30340;&#24067;&#23616;&#30456;&#32467;&#21512;&#65292;&#23558;&#22810;&#20010;&#22270;&#20687;&#30340;&#23545;&#35937;&#32452;&#21512;&#25104;&#19968;&#20010;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#23548;&#21487;&#20197;&#29992;&#20110;&#32534;&#36753;&#30495;&#23454;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BitE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#24211;&#32479;&#35745;&#21644;&#20803;&#25968;&#25454;&#26469;&#35843;&#25972;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#22120;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00845</link><description>&lt;p&gt;
BitE&#65306;&#22312;&#28151;&#21512;&#24037;&#20316;&#36127;&#36733;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#30340;&#26597;&#35810;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
BitE : Accelerating Learned Query Optimization in a Mixed-Workload Environment. (arXiv:2306.00845v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BitE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#24211;&#32479;&#35745;&#21644;&#20803;&#25968;&#25454;&#26469;&#35843;&#25972;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#22120;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#26597;&#35810;&#20248;&#21270;&#30340;&#21162;&#21147;&#65292;&#20294;&#26597;&#35810;&#20248;&#21270;&#22120;&#26159;&#22797;&#26434;&#30340;&#23454;&#20307;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#24037;&#20316;&#36127;&#36733;&#21644;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#20248;&#21270;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#21333;&#19968;&#24037;&#20316;&#36127;&#36733;&#30340;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#20391;&#37325;&#20110;&#25429;&#25417;&#29305;&#23450;&#24037;&#20316;&#36127;&#36733;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290; &#28982;&#32780;&#65292;&#22312;&#22810;&#20010;&#24037;&#20316;&#36127;&#36733;&#21644;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#19988;&#38656;&#35201;&#28151;&#21512;&#21644;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#38382;&#39064;&#12290; &#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; BitE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#24211;&#32479;&#35745;&#21644;&#20803;&#25968;&#25454;&#26469;&#35843;&#25972;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#22120;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290; &#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#20462;&#35746;&#29256;&#26469;&#35299;&#20915;&#20960;&#20010;&#25361;&#25112;&#65306;&#36890;&#36807;&#25193;&#23637;&#25552;&#31034;&#38598;&#25193;&#23637;&#26368;&#20248;Abstract SQL Plan&#65288;&#34920;&#31034;&#20026;&#21517;&#20026;ASP&#30340;JSON&#23545;&#35937;&#65289;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#23558;&#27169;&#22411;&#24341;&#21521;&#21487;&#33021;&#20559;&#34962;&#30340;&#40664;&#35748;&#35745;&#21010;&#20043;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the many efforts to apply deep reinforcement learning to query optimization in recent years, there remains room for improvement as query optimizers are complex entities that require hand-designed tuning of workloads and datasets. Recent research present learned query optimizations results mostly in bulks of single workloads which focus on picking up the unique traits of the specific workload. This proves to be problematic in scenarios where the different characteristics of multiple workloads and datasets are to be mixed and learned together. Henceforth, in this paper, we propose BitE, a novel ensemble learning model using database statistics and metadata to tune a learned query optimizer for enhancing performance. On the way, we introduce multiple revisions to solve several challenges: we extend the search space for the optimal Abstract SQL Plan(represented as a JSON object called ASP) by expanding hintsets, we steer the model away from the default plans that may be biased by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;Transformer&#30340;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#31995;&#32479;&#65292;&#21363;LiT-4-RSVQA&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#65292;&#24182;&#19988;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.00758</link><description>&lt;p&gt;
LiT-4-RSVQA: &#22522;&#20110;&#36731;&#37327;&#32423;Transformer&#30340;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing. (arXiv:2306.00758v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;Transformer&#30340;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#31995;&#32479;&#65292;&#21363;LiT-4-RSVQA&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#65292;&#24182;&#19988;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#27861;&#26088;&#22312;&#22238;&#31572;&#19982;&#36965;&#24863;&#22270;&#20687;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#22312;&#36965;&#24863;&#25805;&#20316;&#22330;&#26223;&#20013;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36731;&#37327;&#32423;Transformer-based VQA in RS&#65288;LiT-4-RSVQA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#36965;&#24863;&#22270;&#20687;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#21253;&#25324;&#65306;i&#65289;&#36731;&#37327;&#32423;&#25991;&#26412;&#32534;&#30721;&#22120;&#27169;&#22359;&#65307;ii&#65289;&#36731;&#37327;&#32423;&#22270;&#20687;&#32534;&#30721;&#22120;&#27169;&#22359;&#65307;iii&#65289;&#34701;&#21512;&#27169;&#22359;&#65307;&#21644;iv&#65289;&#20998;&#31867;&#27169;&#22359;&#12290;&#22312;VQA&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LiT-4-RSVQA&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;VQA&#32467;&#26524;&#65292;&#24182;&#19988;&#26174;&#33879;&#38477;&#20302;&#20102;&#25191;&#34892;&#30828;&#20214;&#30340;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312; https://git.tu-berlin.de/rsim/lit4rsvqa&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) methods in remote sensing (RS) aim to answer natural language questions with respect to an RS image. Most of the existing methods require a large amount of computational resources, which limits their application in operational scenarios in RS. To address this issue, in this paper we present an effective lightweight transformer-based VQA in RS (LiT-4-RSVQA) architecture for efficient and accurate VQA in RS. Our architecture consists of: i) a lightweight text encoder module; ii) a lightweight image encoder module; iii) a fusion module; and iv) a classification module. The experimental results obtained on a VQA benchmark dataset demonstrate that our proposed LiT-4-RSVQA architecture provides accurate VQA results while significantly reducing the computational requirements on the executing hardware. Our code is publicly available at https://git.tu-berlin.de/rsim/lit4rsvqa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#20197;&#24212;&#23545;&#21028;&#21035;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#21508;&#33258;&#32570;&#28857;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#37327;&#21270;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#24335;&#20272;&#35745;&#22120;&#32467;&#21512;&#21487;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00608</link><description>&lt;p&gt;
&#20851;&#20110;&#28151;&#21512;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Hybrid Mutual Information Estimation. (arXiv:2306.00608v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#20197;&#24212;&#23545;&#21028;&#21035;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#21508;&#33258;&#32570;&#28857;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#37327;&#21270;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#24335;&#20272;&#35745;&#22120;&#32467;&#21512;&#21487;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#20272;&#35745;&#20114;&#20449;&#24687;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#27010;&#25324;&#20102;&#21028;&#21035;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#21464;&#20998;&#30028;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20943;&#23569;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#39044;&#27979;&#37327;&#21270; (PQ) &#30340;&#31616;&#21333;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#19982;&#21028;&#21035;&#24335;&#20272;&#35745;&#22120;&#36731;&#26494;&#32467;&#21512;&#20197;&#23454;&#29616;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36890;&#36807;&#38477;&#20302;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#32780;&#20135;&#29983;&#26356;&#32039;&#30340;&#20449;&#24687;&#30028;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#21644;&#28041;&#21450;&#21463;&#22266;&#23450;&#33021;&#37327;&#26223;&#35266;&#32422;&#26463;&#30340;&#33258;&#30001;&#31890;&#23376;&#31995;&#32479;&#30340;&#38543;&#26426;&#36807;&#31243;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;&#21028;&#21035;&#24335;&#20272;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#28151;&#21512;&#26041;&#27861;&#21487;&#20197;&#25345;&#32493;&#25552;&#39640;&#20114;&#20449;&#24687;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the mutual information from samples from a joint distribution is a challenging problem in both science and engineering. In this work, we realize a variational bound that generalizes both discriminative and generative approaches. Using this bound, we propose a hybrid method to mitigate their respective shortcomings. Further, we propose Predictive Quantization (PQ): a simple generative method that can be easily combined with discriminative estimators for minimal computational overhead. Our propositions yield a tighter bound on the information thanks to the reduced variance of the estimator. We test our methods on a challenging task of correlated high-dimensional Gaussian distributions and a stochastic process involving a system of free particles subjected to a fixed energy landscape. Empirical results show that hybrid methods consistently improved mutual information estimates when compared to the corresponding discriminative counterpart.
&lt;/p&gt;</description></item><item><title>&#26426;&#26800;&#24072;&#26159;&#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;&#65292;&#33021;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#25509;&#36817;&#12289;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00144</link><description>&lt;p&gt;
Mechanic: &#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mechanic: A Learning Rate Tuner. (arXiv:2306.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00144
&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#24072;&#26159;&#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;&#65292;&#33021;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#25509;&#36817;&#12289;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26426;&#26800;&#24072;&#8221;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29616;&#31867;&#20284;&#30446;&#26631;&#30340;&#26368;&#36817;&#29702;&#35770;&#20943;&#23569;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#25209;&#37327;&#22823;&#23567;&#12289;&#35843;&#24230;&#21644;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;&#26681;&#25454;&#38382;&#39064;&#65292;&#26426;&#26800;&#24072;&#35201;&#20040;&#38750;&#24120;&#25509;&#36817;&#65292;&#35201;&#20040;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call \textsc{mechanic}. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate \textsc{mechanic} on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, \textsc{mechanic} either comes very close to, matches or even improves upon manual tuning of learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#20854;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.00133</link><description>&lt;p&gt;
&#20851;&#20110;&#37329;&#19997;&#38592;&#26333;&#20809;&#35299;&#37322;&#30340;&#19968;&#20123;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note On Interpreting Canary Exposure. (arXiv:2306.00133v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#20854;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Carlini&#31561;&#20154;&#20171;&#32461;&#30340;&#37329;&#19997;&#38592;&#26292;&#38706;&#32463;&#24120;&#34987;&#29992;&#26469;&#23454;&#35777;&#35780;&#20272;&#25110;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22521;&#35757;&#30340;&#38544;&#31169;&#12290;&#36825;&#31687;&#31508;&#35760;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20123;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Canary exposure, introduced in Carlini et al. is frequently used to empirically evaluate, or audit, the privacy of machine learning model training. The goal of this note is to provide some intuition on how to interpret canary exposure, including by relating it to membership inference attacks and differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#32773;&#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#24182;&#27604;&#36739;&#20854;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#23558;&#20154;&#31867;&#30340;&#35748;&#30693;&#22240;&#32032;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19678</link><description>&lt;p&gt;
Smooth-Trajectron++: &#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;Trajectron++&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention. (arXiv:2305.19678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#32773;&#23558;&#24179;&#28369;&#20851;&#27880;&#26426;&#21046;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#24182;&#27604;&#36739;&#20854;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#23558;&#20154;&#31867;&#30340;&#35748;&#30693;&#22240;&#32032;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#23545;&#20110;&#39044;&#27979;&#20854;&#26410;&#26469;&#36712;&#36857;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24320;&#21457;&#25552;&#20379;&#23433;&#20840;&#21487;&#38752;&#30340;&#35268;&#21010;&#31995;&#32479;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;Trajectron++&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;&#20854;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#21152;&#20837;&#24179;&#28369;&#39033;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#35813;&#20851;&#27880;&#26426;&#21046;&#27169;&#20223;&#20102;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25152;&#21551;&#21457;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#65292;&#34920;&#26126;&#20154;&#30340;&#27880;&#24847;&#21147;&#22312;&#20999;&#25442;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23558;&#20154;&#31867;&#35748;&#30693;&#27934;&#23519;&#21147;&#32435;&#20837;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding traffic participants' behaviour is crucial for predicting their future trajectories, aiding in developing safe and reliable planning systems for autonomous vehicles. Integrating cognitive processes and machine learning models has shown promise in other domains but is lacking in the trajectory forecasting of multiple traffic agents in large-scale autonomous driving datasets. This work investigates the state-of-the-art trajectory forecasting model Trajectron++ which we enhance by incorporating a smoothing term in its attention module. This attention mechanism mimics human attention inspired by cognitive science research indicating limits to attention switching. We evaluate the performance of the resulting Smooth-Trajectron++ model and compare it to the original model on various benchmarks, revealing the potential of incorporating insights from human cognition into trajectory prediction models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18901</link><description>&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#25240;&#25187;&#22870;&#21169;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21344;&#29992;&#26102;&#38388;&#30340;&#27010;&#24565;&#65288;&#29305;&#21035;&#26159;&#38024;&#23545;&#25240;&#25187;&#22870;&#21169;&#65289;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#23548;&#20986;&#24615;&#33021;&#24046;&#24322;&#21644;&#23616;&#37096;&#36924;&#36817;&#20844;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102; PG&#65288;&#31574;&#30053;&#26799;&#24230;&#65289;&#12289;TRPO&#65288;&#20449;&#20219;&#21306;&#22495;&#31574;&#30053;&#20248;&#21270;&#65289;&#21644; PPO&#65288;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#26159;&#29087;&#30693;&#21644;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#36890;&#36807;&#25968;&#23383;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18888</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. (arXiv:2305.18888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;URL&#65289;&#20855;&#22791;&#23398;&#20064;&#27867;&#21270;&#34920;&#31034;&#20197;&#21450;&#26080;&#38656;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#26631;&#31614;&#23601;&#33021;&#36866;&#29992;&#20110;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21407;&#26412;&#20026;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#35774;&#35745;&#30340;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#19988;&#20381;&#36182;&#20110;&#24378;&#20551;&#35774;&#35774;&#35745;&#23398;&#20064;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36816;&#34892;&#34920;&#29616;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27969;&#34892;&#30340;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65292;&#23398;&#20064;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25506;&#32034;&#26080;&#30417;&#30563;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#20013;&#24418;&#24577;&#29255;&#27573;&#23884;&#20837;&#30340;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;GOAT&#65292;&#32467;&#21512;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#21152;&#26435;&#27169;&#20223;&#23398;&#20064;&#27604;&#22522;&#20110;&#24754;&#35266;&#31163;&#32447;RL&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.18882</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#27867;&#21270;&#25152;&#24517;&#38656;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?. (arXiv:2305.18882v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;GOAT&#65292;&#32467;&#21512;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#21152;&#26435;&#27169;&#20223;&#23398;&#20064;&#27604;&#22522;&#20110;&#24754;&#35266;&#31163;&#32447;RL&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#23436;&#20840;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#36890;&#29992;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25968;&#25454;&#38598;&#20869;&#20445;&#23432;&#20043;&#22806;&#65292;&#23454;&#29616;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#31163;&#32447;GCRL&#30340;&#21478;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#30740;&#31350;&#20102;&#31163;&#32447;GCRL&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#20197;&#30830;&#23450;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#22312;&#19968;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21152;&#26435;&#27169;&#20223;&#23398;&#20064;&#27604;&#22522;&#20110;&#24754;&#35266;&#31163;&#32447;RL&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#12290;&#22522;&#20110;&#36825;&#20010;&#35265;&#35299;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#20851;&#20110;&#20998;&#24067;&#22806;&#27867;&#21270;&#30340;&#29702;&#35770;&#65292;&#38416;&#26126;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#65292;&#21363;&#36890;&#29992;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;RL&#65288;GOAT&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#25105;&#20204;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547; 9 &#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GOAT&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goAl-condiTioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18755</link><description>&lt;p&gt;
&#36890;&#29992;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#38477;&#32500;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction for General KDE Mode Finding. (arXiv:2305.18755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23547;&#25214;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#27169;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27169;&#20272;&#35745;&#32467;&#26524;&#36827;&#34892;&#20102;&#36890;&#29992;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#26680;&#20989;&#25968;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#36923;&#36753;&#12289;Sigmoid&#21644;&#24191;&#20041;&#39640;&#26031;&#26680;&#12290;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#65292;&#35813;&#31639;&#27861;&#21487;&#21046;&#23450;&#20986;&#26377;&#25928;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the mode of a high dimensional probability distribution $D$ is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when $D$ is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known.  In this work, we significantly generalize a result of (LeeLiMusco:2021) on mode approximation for Gaussian mixture models. We develop randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized Gaussian kernels. As in Lee et al.'s work, our dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy $(1-\epsilon)$ for any $\epsilon &gt; 0$. Moreover, when combined with gradient descent, they yield efficient practical heuristics for the problem.  In addition to our po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#65292;&#21516;&#26102;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#20197;&#21450;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.18730</link><description>&lt;p&gt;
&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#30340;&#20998;&#22359;&#38543;&#26426;&#26041;&#24046;&#32422;&#31616;&#26041;&#27861;&#21450;&#24182;&#34892;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization. (arXiv:2305.18730v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#65292;&#21516;&#26102;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#20197;&#21450;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#38750;&#20984;&#30340;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20998;&#22359;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20026;&#20102;&#36798;&#21040;&#31639;&#27861;&#30340;&#19977;&#20010;&#26399;&#26395;&#65306;&#65288;a&#65289;&#33021;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#65307;&#65288;b&#65289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#27599;&#20010;&#36845;&#20195;&#20013;&#37319;&#26679; $I$ &#22359;&#24182;&#23545;&#27599;&#20010;&#37319;&#26679;&#22359;&#37319;&#26679; $B$ &#20010;&#26679;&#26412;&#65307;&#65288;c&#65289;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#32852;&#24615;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reductio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#36335;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#21644;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#24182;&#25506;&#35752;&#20851;&#38190;&#35789;&#23545;Q/A&#20219;&#21153;&#35299;&#30721;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18679</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#37319;&#26679;&#65288;KEYS&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KEYword based Sampling (KEYS) for Large Language Models. (arXiv:2305.18679v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#36335;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#21644;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#24182;&#25506;&#35752;&#20851;&#38190;&#35789;&#23545;Q/A&#20219;&#21153;&#35299;&#30721;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;Q/A&#65289;&#20219;&#21153;&#21487;&#20197;&#34987;&#30475;&#20316;&#19968;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#38382;&#39064;&#21644;&#25991;&#31456;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31572;&#26696;&#12290;&#26368;&#36817;Q/A&#20219;&#21153;&#30340;&#36827;&#23637;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#32780;&#24456;&#23569;&#20851;&#27880;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#37319;&#26679;&#12290;&#20851;&#38190;&#35789;&#22312;&#20154;&#31867;&#35821;&#35328;&#29983;&#25104;&#20013;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#34892;&#20026;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#19988;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#24182;&#35752;&#35770;&#20851;&#38190;&#35789;&#22914;&#20309;&#24433;&#21709;Q/A&#20219;&#21153;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (Q/A) can be formulated as a generative task (Mitra, 2017) where the task is to generate an answer given the question and the passage (knowledge, if available). Recent advances in QA task is focused a lot on language model advancements and less on other areas such as sampling(Krishna et al., 2021), (Nakano et al., 2021). Keywords play very important role for humans in language generation. (Humans formulate keywords and use grammar to connect those keywords and work). In the research community, very little focus is on how humans generate answers to a question and how this behavior can be incorporated in a language model. In this paper, we want to explore these two areas combined, i.e., how sampling can be to used generate answers which are close to human-like behavior and factually correct. Hence, the type of decoding algorithm we think should be used for Q/A tasks should also depend on the keywords. These keywords can be obtained from the question, passage or interne
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.18651</link><description>&lt;p&gt;
UMD: &#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18651
&lt;/p&gt;
&lt;p&gt;
UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#65288;&#29305;&#27931;&#20234;&#65289;&#25915;&#20987;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#24120;&#35265;&#23041;&#32961;&#65292;&#20854;&#20013;&#23884;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#31867;&#21035;&#30340;&#26679;&#26412;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#23545;&#25239;&#30446;&#26631;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#20998;&#31867;&#22120;&#26159;&#21542;&#36973;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#38024;&#23545;&#21333;&#19968;&#23545;&#25239;&#30446;&#26631;&#30340;&#25915;&#20987;&#35774;&#35745;&#30340;&#65288;&#20363;&#22914;&#65292;&#20840;&#23545;&#19968;&#25915;&#20987;&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#30340;&#28304;&#31867;&#21035;&#30340;&#26356;&#26222;&#36941;&#30340;X2X&#25915;&#20987;&#65292;&#27599;&#20010;&#28304;&#31867;&#21035;&#37117;&#19982;&#20219;&#24847;&#30446;&#26631;&#31867;&#21035;&#37197;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UMD&#65292;&#31532;&#19968;&#20010;&#36890;&#36807;&#32852;&#21512;&#25512;&#26029;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#26469;&#26377;&#25928;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#37327;&#24230;&#21644;&#36873;&#25321;&#19968;&#32452;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#36873;&#25321;&#30340;&#31867;&#21035;&#23545;&#26159;&#22522;&#20110;&#32852;&#21512;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#20102;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27979;&#35797;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#19981;&#30456;&#31561;&#65292;&#23637;&#31034;&#20102;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.18473</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi. (arXiv:2305.18473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#20102;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27979;&#35797;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#19981;&#30456;&#31561;&#65292;&#23637;&#31034;&#20102;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#20998;&#26512;&#24863;&#30693;&#21387;&#21147;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;150&#20010;&#20010;&#20307;&#30340;&#24863;&#30693;&#21387;&#21147;&#27700;&#24179;&#24182;&#27979;&#37327;&#27979;&#35797;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#35813;&#27979;&#35797;&#21253;&#25324;14&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#30340;&#24471;&#20998;&#33539;&#22260;&#20026;0&#21040;4&#65292;&#24635;&#24471;&#20998;&#33539;&#22260;&#20026;0-56&#12290;&#20854;&#20013;&#65292;7&#20010;&#38382;&#39064;&#20197;&#36127;&#38754;&#26041;&#24335;&#34920;&#36848;&#24182;&#30456;&#24212;&#35780;&#20998;&#65292;&#32780;&#20854;&#20313;7&#20010;&#38382;&#39064;&#20197;&#27491;&#38754;&#26041;&#24335;&#34920;&#36848;&#24182;&#25353;&#30456;&#21453;&#26041;&#24335;&#35780;&#20998;&#12290;&#35813;&#27979;&#35797;&#36824;&#35774;&#35745;&#20026;&#35782;&#21035;&#20004;&#20010;&#23376;&#22240;&#32032;&#65306;&#24863;&#30693;&#33258;&#25105;&#25928;&#33021;&#21644;&#21387;&#21147;/&#19981;&#36866;&#24863;&#30693;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23637;&#31034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27979;&#35797;&#38382;&#39064;&#21487;&#33021;&#24182;&#19981;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#65292;&#25581;&#31034;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#31038;&#20250;&#20013;&#20986;&#29616;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#26368;&#32456;&#35777;&#26126;&#22312;&#24515;&#29702;&#19978;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#27169;&#24335;&#23384;&#22312;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#37325;&#22797;&#29616;&#26377;&#30340;&#24515;&#29702;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to reanalyze the perceived stress test using machine learning to determine the perceived stress levels of 150 individuals and measure the impact of the test questions. The test consists of 14 questions, each scored on a scale of 0 to 4, resulting in a total score range of 0-56. Out of these questions, 7 are formulated in a negative context and scored accordingly, while the remaining 7 are formulated in a positive context and scored in reverse. The test is also designed to identify two sub-factors: perceived self-efficacy and stress/discomfort perception. The main objectives of this research are to demonstrate that test questions may not have equal importance using artificial intelligence techniques, reveal which questions exhibit variations in the society using machine learning, and ultimately demonstrate the existence of distinct patterns observed psychologically. This study provides a different perspective from the existing psychology literature by repeating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.14076</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39640;&#26031;-&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#24615;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD)&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#22522;&#20110;&#31890;&#23376;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#23613;&#31649;&#20854;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29702;&#35299;SVGD&#30340;&#29702;&#35770;&#23646;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#20174;&#39640;&#26031;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#21482;&#35201;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#21452;&#32447;&#24615;&#26680;&#30340;SVGD&#21160;&#24577;&#23558;&#20445;&#25345;&#39640;&#26031;&#29366;&#24577;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#32447;&#24615;&#26680;&#23558;SVGD&#25237;&#24433;&#21040;&#39640;&#26031;&#20998;&#24067;&#26063;&#20013;&#65292;&#21363;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029; (GVI) &#19982; SVGD&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#22343;&#22330; PDE &#21644;&#31163;&#25955;&#31890;&#23376;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21576;&#29616;&#20986;&#24378;&#23545;&#25968;&#20985;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#22343;&#22330;&#39640;&#26031;-SVGD&#21160;&#24577;&#20250;&#32447;&#24615;&#25910;&#25947;&#20110;KL&#25955;&#24230;&#19979;&#26368;&#25509;&#36817;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#26377;&#38480;&#31890;&#23376;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#23545;&#22343;&#22330;&#26497;&#38480;&#30340;&#26102;&#38388;&#24494;&#27493;&#19968;&#33268;&#25910;&#25947;&#20197;&#21450;&#32447;&#24615;&#25910;&#25947;&#33267;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#24658;&#31561;&#24335;&#65292;&#35813;&#31561;&#24335;&#23558;&#30446;&#26631;&#39640;&#26031;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#19982;&#31890;&#23376;&#22343;&#21248;&#20998;&#24067;&#30340;&#36153;&#24076;&#23572;&#20449;&#24687;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#31561;&#24335;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#36879;&#35270; GVI with SVGD &#22312;&#22343;&#22330;&#21644;&#31890;&#23376;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#24615;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13035</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20351;ViT&#25104;&#24418;&#65306;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32553;&#25918;&#23450;&#24459;&#34987;&#29992;&#26469;&#25512;&#23548;&#22312;&#32473;&#23450;&#35745;&#31639;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#65288;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#21457;&#23637;&#24182;&#25913;&#36827;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20197;&#25512;&#27979;&#22914;&#23485;&#24230;&#21644;&#28145;&#24230;&#31561;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#24182;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25104;&#21151;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32463;&#36807;&#24418;&#29366;&#20248;&#21270;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#22312;&#20165;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#37327;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;SoViT-400m/14&#22312;ILSRCV2012&#19978;&#21462;&#24471;&#20102;90.3%&#30340;&#24494;&#35843;&#20934;&#30830;&#24230;&#65292;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;ViT-g/14&#65292;&#22312;&#30456;&#21516;&#35774;&#32622;&#19979;&#25509;&#36817;ViT-G/14&#65292;&#21516;&#26102;&#25512;&#26029;&#25104;&#26412;&#20063;&#19981;&#21040;&#19968;&#21322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;VQA&#21644;&#38646;-shot&#36716;&#31227;&#65292;&#22312;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24182;&#30830;&#23450;&#20102;&#20854;&#38480;&#21046;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25361;&#25112;&#20102;&#30450;&#30446;&#25193;&#22823;&#35270;&#35273;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12134</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#25110;&#30001;&#19981;&#21512;&#20316;&#23454;&#20307;&#25345;&#26377;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#12289;&#29615;&#22659;&#21644;&#20256;&#24863;&#22120;&#32423;&#21035;&#19978;&#38544;&#31169;&#23545;&#32852;&#37030;HAR&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FL&#23545;HAR&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;FL&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#65292;&#24182;&#19988;&#20027;&#35201;&#21462;&#20915;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36991;&#20813;&#25968;&#25454;&#20849;&#20139;&#24182;&#22312;&#20154;&#31867;&#25110;&#29615;&#22659;&#32423;&#21035;&#19978;&#20551;&#35774;&#38544;&#31169;&#65292;&#22914;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#20570;&#30340;&#37027;&#26679;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;5-7&#65285;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#38544;&#31169;&#24310;&#20280;&#21040;&#27169;&#24577;&#32423;&#21035;&#24182;&#20005;&#26684;&#20998;&#31163;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;19-42&#65285;&#12290;&#30001;&#20110;&#36825;&#31181;&#24418;&#24335;&#30340;&#38544;&#31169;&#26159;HAR&#20013;&#34987;&#35201;&#27714;&#30340;&#36947;&#24503;&#21033;&#29992;&#34987;&#21160;&#20256;&#24863;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#23458;&#25143;&#31471;&#30456;&#20114;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;FL&#27169;&#22411;&#21644;&#19968;&#20010;&#27599;&#31181;&#27169;&#24577;&#19968;&#20010;&#30340;&#32452;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;HAR&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>SFP&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#23376;&#32467;&#26500;&#30340;&#20462;&#21098;&#26694;&#26550;&#65292;SFP&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#23545;&#23436;&#20840;&#26292;&#38706;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#20197;&#21450;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#21516;&#26679;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#24102;&#26469;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#65292;&#21033;&#29992;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#26469;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.11615</link><description>&lt;p&gt;
SFP: &#38024;&#23545;&#20266;&#29305;&#24449;&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26080;&#20998;&#24067;&#27010;&#25324;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11615
&lt;/p&gt;
&lt;p&gt;
SFP&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#23376;&#32467;&#26500;&#30340;&#20462;&#21098;&#26694;&#26550;&#65292;SFP&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#23545;&#23436;&#20840;&#26292;&#38706;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#20197;&#21450;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#21516;&#26679;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#24102;&#26469;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#65292;&#21033;&#29992;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#26469;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#23376;&#32467;&#26500;&#23398;&#20064;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#19981;&#21464;&#30340;&#32593;&#32476;&#23376;&#32467;&#26500;&#65292;&#21487;&#20197;&#27604;&#21407;&#22987;&#30340;&#23436;&#25972;&#32467;&#26500;&#26356;&#22909;&#22320;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#27010;&#25324;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#26292;&#38706;&#30340;&#22495;&#22806;&#25968;&#25454;&#26469;&#25628;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#33021;&#24102;&#26469;&#20004;&#20010;&#32570;&#28857;&#65306;1&#65289;&#19981;&#20844;&#24179;&#65292;&#22240;&#20026;&#23436;&#20840;&#26292;&#38706;&#20986;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65307;&#21644;2&#65289;&#27425;&#20248;&#30340;OOD&#27010;&#25324;&#65292;&#30001;&#20110;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#21516;&#26679;&#30340;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#12290;&#22522;&#20110;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#26356;&#20302;&#30340;&#20307;&#39564;&#39118;&#38505;&#30340;&#24819;&#27861;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#29305;&#24449;&#23450;&#21521;&#30340;&#27169;&#22411;&#20462;&#21098;&#26694;&#26550;&#65292;&#31216;&#20026;SFP&#65292;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#19978;&#36848;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SFP&#22312;&#22521;&#35757;&#36807;&#31243;&#20013;&#20351;&#29992;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#39564;&#35777;&#30340;&#20219;&#21153;&#20002;&#22833;&#35782;&#21035;ID&#23454;&#20363;&#20013;&#30340;&#20266;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#65292;SFP&#20943;&#24369;&#20102;&#30456;&#24212;&#30340;&#29305;&#24449;&#20316;&#29992;&#65292;&#20197;&#25552;&#39640;OOD&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11033</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65306;&#26368;&#36817;&#25991;&#29486;&#20013;&#25216;&#26415;&#21644;&#24120;&#35265;&#36235;&#21183;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#38656;&#35201;&#31639;&#27861;&#22238;&#31572;&#26377;&#20851;&#29305;&#23450;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;25&#39033;&#26368;&#26032;&#30740;&#31350;&#21644;6&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#12290;&#20316;&#32773;&#28145;&#20837;&#35843;&#30740;&#20102;&#35813;&#39046;&#22495;&#30340;&#22810;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#26512;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#20809;&#29255;&#19978;&#26816;&#27979;&#20986;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#25918;&#23556;&#31185;&#21307;&#24072;&#12290;</title><link>http://arxiv.org/abs/2305.10116</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21487;&#38752;&#22320;&#35782;&#21035;&#33016;&#37096;X&#20809;&#24322;&#24120;&#27169;&#24335;&#21527;&#65311;&#19968;&#39033;&#22810;&#35835;&#32773;&#30740;&#31350;&#65292;&#26816;&#26597;AI&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#19968;&#20010;&#26376;&#30340;&#23454;&#26045;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice. (arXiv:2305.10116v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#20809;&#29255;&#19978;&#26816;&#27979;&#20986;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#25918;&#23556;&#31185;&#21307;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#26816;&#27979;&#31639;&#27861;&#65288;DLAD&#65292;Carebot AI CXR&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#20301;&#33016;&#37096;X&#32447;&#29255;&#19978;&#30340;&#19971;&#31181;&#29305;&#23450;&#25918;&#23556;&#23398;&#21457;&#29616;&#65288;&#32954;&#19981;&#24352;&#65288;ATE&#65289;&#65292;&#23454;&#21464;&#65288;CON&#65289;&#65292;&#33016;&#33108;&#31215;&#28082;&#65288;EFF&#65289;&#65292;&#32954;&#37096;&#30149;&#21464;&#65288;LES&#65289;&#65292;&#30382;&#19979;&#27668;&#32959;&#65288;SCE&#65289;&#65292;&#24515;&#33039;&#25193;&#22823;&#65288;CMG&#65289;&#65292;&#27668;&#33016;&#65288;PNO&#65289;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;956&#24352;&#33016;&#37096;X&#32447;&#29255;&#65292;&#24182;&#23558;DLAD&#30340;&#24615;&#33021;&#19982;&#22312;&#21307;&#38498;&#29615;&#22659;&#19979;&#35780;&#20272;&#22270;&#20687;&#30340;&#20845;&#21517;&#21333;&#20010;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21363;&#20351;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;DLAD&#20063;&#21462;&#24471;&#20102;&#39640;&#28789;&#25935;&#24230;&#65288;ATE 1.000&#65288;0.624-1.000&#65289;&#65292;CON 0.864&#65288;0.671-0.956&#65289;&#65292;EFF 0.953&#65288;0.887-0.983&#65289;&#65292;LES 0.905&#65288;0.715-0.978&#65289;&#65292;SCE 1.000&#65288;0.366-1.000&#65289;&#65292;CMG 0.837&#65288;0.711-0.917&#65289;&#65292;PNO 0.875&#65288;0.538-0.986&#65289;&#65289;&#65288;&#26368;&#20302;&#65306;ATE 0.000&#65288;0.000-0.376&#65289;&#65292;CON 0.182&#65288;0.070-0.382&#65289;&#65292;EFF 0.400&#65288;0.302-0.506&#65289;&#65292;LES 0.238&#65288;0.103-0.448&#65289;&#65292;SCE 0.000&#65288;0.000-0.634&#65289;&#65292;CMG 0.347&#65288;0.228-0.486&#65289;&#65292;PNO 0.375&#65288;0.134-0.691&#65289;&#65292;&#26368;&#39640;&#65306;ATE 1.000&#65288;0.624-1.000&#65289;&#65292;CON 0.864&#65288;0.671-0.956&#65289;&#65292;EFF 0.953&#65288;0.887-0.983&#65289;&#65292;LES 0.905&#65288;0.715-0.978&#65289;&#65292;SCE 1.000&#65288;0.366-1.000&#65289;&#65292;CMG 0.837&#65288;0.711-0.917&#65289;&#65292;PNO 0.875&#65288;0.538-0.986&#65289;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we developed a deep-learning-based automatic detection algorithm (DLAD, Carebot AI CXR) to detect and localize seven specific radiological findings (atelectasis (ATE), consolidation (CON), pleural effusion (EFF), pulmonary lesion (LES), subcutaneous emphysema (SCE), cardiomegaly (CMG), pneumothorax (PNO)) on chest X-rays (CXR). We collected 956 CXRs and compared the performance of the DLAD with that of six individual radiologists who assessed the images in a hospital setting. The proposed DLAD achieved high sensitivity (ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), EFF 0.953 (0.887-0.983), LES 0.905 (0.715-0.978), SCE 1.000 (0.366-1.000), CMG 0.837 (0.711-0.917), PNO 0.875 (0.538-0.986)), even when compared to the radiologists (LOWEST: ATE 0.000 (0.000-0.376), CON 0.182 (0.070-0.382), EFF 0.400 (0.302-0.506), LES 0.238 (0.103-0.448), SCE 0.000 (0.000-0.634), CMG 0.347 (0.228-0.486), PNO 0.375 (0.134-0.691), HIGHEST: ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), E
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; multi-label &#23398;&#20064;&#20013;&#24120;&#29992;&#30340; Macro-AUC &#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#21487;&#33021;&#30001;&#20110;&#23545;&#26631;&#31614;&#30340;&#19981;&#24179;&#34913;&#26356;&#25935;&#24863;&#32780;&#34920;&#29616;&#36739;&#24046;&#65292;&#36825;&#19968;&#32467;&#35770;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.05248</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;Macro-AUC&#30340;&#27867;&#21270;&#29702;&#35299;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Generalization of Macro-AUC in Multi-label Learning. (arXiv:2305.05248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; multi-label &#23398;&#20064;&#20013;&#24120;&#29992;&#30340; Macro-AUC &#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#21487;&#33021;&#30001;&#20110;&#23545;&#26631;&#31614;&#30340;&#19981;&#24179;&#34913;&#26356;&#25935;&#24863;&#32780;&#34920;&#29616;&#36739;&#24046;&#65292;&#36825;&#19968;&#32467;&#35770;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#65292;Macro-AUC&#26159;&#31867;&#20869;AUC&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#29702;&#35770;&#29702;&#35299;&#36828;&#36828;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#24212;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#34920;&#24449;&#21508;&#31181;&#23398;&#20064;&#31639;&#27861;&#30340;&#23439;AUC&#30340;&#27867;&#21270;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30830;&#23450;&#20102;&#24433;&#21709;&#27867;&#21270;&#30028;&#38480;&#30340;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#26631;&#31614;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#19981;&#24179;&#34913;&#24863;&#30693;&#35823;&#24046;&#30028;&#38480;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#27604;&#25552;&#20986;&#30340;&#22522;&#20110;&#25104;&#23545;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#31639;&#27861;&#26356;&#25935;&#24863;&#20110;&#26631;&#31614;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#65292;&#36825;&#21487;&#33021;&#24847;&#21619;&#30528;&#23427;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#32463;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#23601;&#25216;&#26415;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65288;&#26356;&#36890;&#29992;&#30340;&#65289;McDiarmid&#22411;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: \emph{the label-wise class imbalance}. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#26420;&#32032;MOO&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;&#30340;&#24330;&#31471;&#12290;</title><link>http://arxiv.org/abs/2304.13229</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#65292;&#36991;&#20813;&#20102;&#26420;&#32032;MOO&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;&#30340;&#24330;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20063;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#23545;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#25104;&#21151;&#26469;&#35828;&#65292;&#20851;&#38190;&#22240;&#32032;&#26159;&#35201;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#26576;&#20123;&#30446;&#26631;/&#30446;&#26631;&#30340;&#21512;&#26684;&#19988;&#26377;&#24046;&#24322;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#20197;&#21516;&#26102;&#25915;&#20987;&#22810;&#20010;&#27169;&#22411;&#30340;&#23545;&#25239;&#26679;&#26412;&#65289;&#12290;&#22240;&#27492;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26159;&#23454;&#29616;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#20197;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;/&#30446;&#26631;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#65292;MOO&#30340;&#26420;&#32032;&#24212;&#29992;&#24448;&#24448;&#20250;&#24179;&#31561;&#22320;&#26368;&#22823;&#21270;&#25152;&#26377;&#30446;&#26631;/&#30446;&#26631;&#65292;&#32780;&#19981;&#20851;&#24515;&#30446;&#26631;/&#30446;&#26631;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#24050;&#23454;&#29616;&#30446;&#26631;/&#30446;&#26631;&#30340;&#20219;&#21153;&#19978;&#20570;&#26080;&#29992;&#30340;&#21162;&#21147;&#65292;&#32780;&#22312;&#26410;&#23454;&#29616;&#30446;&#26631;/&#30446;&#26631;&#30340;&#20219;&#21153;&#19978;&#21017;&#25237;&#20837;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;MOO&#8221;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;...
&lt;/p&gt;
&lt;p&gt;
Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the contex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#35813;&#20808;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06333</link><description>&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Priors for symbolic regression. (arXiv:2304.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#35813;&#20808;&#39564;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#25968;&#25454;&#38598;&#36873;&#25321;&#31526;&#21495;&#27169;&#22411;&#26102;&#65292;&#20154;&#20204;&#33258;&#28982;&#20542;&#21521;&#20110;&#36873;&#25321;&#8220;&#31616;&#21333;&#8221;&#30340;&#34920;&#36798;&#24335;&#25110;&#26356;&#25509;&#36817;&#20043;&#21069;&#22312;&#31867;&#20284;&#24773;&#20917;&#19979;&#30475;&#21040;&#30340;&#26041;&#31243;&#24335;&#12290;&#36825;&#34920;&#26126;&#20989;&#25968;&#24212;&#35813;&#20855;&#26377;&#38750;&#22343;&#21248;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#32780;&#65292;&#22312;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26694;&#26550;&#20869;&#24456;&#23569;&#32771;&#34385;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#26377;&#20851;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#35814;&#32454;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;SR&#20013;&#12290;&#25105;&#20204;&#23545;&#20989;&#25968;&#32467;&#26500;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;&#21508;&#20010;&#36816;&#31639;&#31526;&#30340;&#25490;&#21015;&#26041;&#24335;&#20197;&#21450;&#27599;&#20010;&#36816;&#31639;&#31526;&#30340;&#20986;&#29616;&#39057;&#29575;&#37117;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24335;&#36125;&#21494;&#26031;&#22240;&#23376;&#30340;&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#22788;&#29702;&#25968;&#20540;&#21442;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#36125;&#21494;&#26031;&#35777;&#25454;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#65292;&#21516;&#26102;&#26126;&#30830;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#12289;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#20197;&#21450;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When choosing between competing symbolic models for a data set, a human will naturally prefer the "simpler" expression or the one which more closely resembles equations previously seen in a similar context. This suggests a non-uniform prior on functions, which is, however, rarely considered within a symbolic regression (SR) framework. In this paper we develop methods to incorporate detailed prior information on both functions and their parameters into SR. Our prior on the structure of a function is based on a $n$-gram language model, which is sensitive to the arrangement of operators relative to one another in addition to the frequency of occurrence of each operator. We also develop a formalism based on the Fractional Bayes Factor to treat numerical parameter priors in such a way that models may be fairly compared though the Bayesian evidence, and explicitly compare Bayesian, Minimum Description Length and heuristic methods for model selection. We demonstrate the performance of our pri
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#33258;&#20027;&#26041;&#27861;&#39044;&#27979;&#20102;&#22797;&#26434;&#27833;&#34255;&#30340;&#31354;&#38388;&#20998;&#24067;&#27010;&#29575;&#65292;&#21487;&#20197;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;&#21644;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2304.03048</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#27833;&#34255;&#26089;&#26399;&#22320;&#36136;&#21208;&#25506;&#20013;&#65292;&#21033;&#29992;&#20117;&#21644;&#22320;&#38663;&#25968;&#25454;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration. (arXiv:2304.03048v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#33258;&#20027;&#26041;&#27861;&#39044;&#27979;&#20102;&#22797;&#26434;&#27833;&#34255;&#30340;&#31354;&#38388;&#20998;&#24067;&#27010;&#29575;&#65292;&#21487;&#20197;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;&#21644;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#24212;&#29992;&#19968;&#31181;&#33258;&#20027;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#30740;&#31350;&#21306;&#22495;&#20869;&#27833;&#34255;&#20256;&#25773;&#30340;&#27010;&#29575;&#12290;&#33258;&#20027;&#24615;&#24847;&#21619;&#30528;&#22312;&#20934;&#22791;&#21644;&#36755;&#20837;&#22320;&#36136;&#22320;&#29699;&#29289;&#29702;&#20449;&#24687;&#20043;&#21518;&#65292;&#19987;&#23478;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#34987;&#26368;&#23567;&#21270;&#12290;&#35813;&#30740;&#31350;&#20197;&#30740;&#31350;&#21306;&#22495;&#26089;&#26399;&#21208;&#25506;&#38454;&#27573;&#30340;3D&#22320;&#38663;&#21208;&#25506;&#25968;&#25454;&#21644;&#20117;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#65292;&#20026;&#20004;&#32452;&#36755;&#20837;&#25968;&#25454;&#65306;&#22522;&#30784;&#32452;&#21644;&#21453;&#28436;&#26657;&#20934;&#21518;&#30340;&#32452;&#65292;&#39044;&#27979;&#20102;&#27833;&#34255;&#31354;&#38388;&#20998;&#24067;&#30340;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#20102;&#26631;&#23450;&#21518;&#30340;&#27010;&#29575;&#31435;&#26041;&#20307;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#22320;&#36136;&#21644;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#27010;&#21270;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#21644;&#22522;&#20110;&#27833;&#34255;&#27010;&#29575;&#34920;&#31034;&#30340;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;&#31639;&#27861;&#30340;&#21512;&#26684;&#34920;&#29616;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#27833;&#34255;&#26089;&#26399;&#22320;&#36136;&#21208;&#25506;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to develop and apply an autonomous approach for predicting the probability of hydrocarbon reservoirs spreading in the studied area. Autonomy means that after preparing and inputting geological-geophysical information, the influence of an expert on the algorithms is minimized. The study was made based on the 3D seismic survey data and well information on the early exploration stage of the studied field. As a result, a forecast of the probability of spatial distribution of reservoirs was made for two sets of input data: the base set and the set after reverse-calibration, and three-dimensional cubes of calibrated probabilities of belonging of the studied space to the identified classes were obtained. The approach presented in the paper allows for expert-independent generalization of geological and geophysical data, and to use this generalization for hypothesis testing and creating geological models based on a probabilistic representation of the reservoir. The qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11920</link><description>&lt;p&gt;
&#20013;&#38388;&#29305;&#24449;&#32852;&#30431;&#33021;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32423;&#21035;&#32467;&#26500;&#30340;&#20013;&#38388;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32423;&#21035;&#32467;&#26500;&#26159;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#65292;&#27599;&#20010;&#32423;&#21035;&#23545;&#24212;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#21363;&#29609;&#23478;&#38598;&#20998;&#21306;&#65289;&#12290;&#20174;&#21482;&#21253;&#21547;&#21333;&#20803;&#32032;&#30340;&#24179;&#20961;&#38598;&#21512;&#21040;&#21482;&#21253;&#21547;&#22823;&#32852;&#30431;&#30340;&#38598;&#21512;&#65292;&#31895;&#31961;&#24230;&#30340;&#32423;&#21035;&#36880;&#28176;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26469;&#29983;&#25104;&#25277;&#35937;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#27773;&#36710;&#27169;&#22411;&#31034;&#20363;&#21644;&#27888;&#22374;&#23612;&#20811;&#21495;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#20013;&#20013;&#38388;&#27010;&#24565;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26694;&#26550;&#21644;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.08893</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Multifidelity deep operator network approach to closure for multiscale systems. (arXiv:2303.08893v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26694;&#26550;&#21644;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25237;&#24433;&#30340;&#38477;&#38454;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#23569;&#37327;&#24191;&#20041;&#65288;&#25110;&#28508;&#22312;&#65289;&#21464;&#37327;&#26469;&#34920;&#31034;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20294;&#26159;&#30001;&#20110;&#26410;&#35299;&#26512;&#23610;&#24230;&#21644;&#24050;&#35299;&#26512;&#23610;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65288;&#31216;&#20026;&#23553;&#38381;&#38382;&#39064;&#65289;&#30340;&#19981;&#27491;&#30830;&#22788;&#29702;&#65292;&#23548;&#33268;&#38477;&#38454;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#21644;&#29978;&#33267;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#23558;&#23553;&#38381;&#38382;&#39064;&#35270;&#20026;&#22810;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONet&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21644;/&#25110;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#36824;&#37319;&#29992;&#20102;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32806;&#21512;&#25991;&#29486;&#20013;&#26368;&#36817;&#24320;&#21457;&#30340;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#32456;&#26041;&#27861;&#22312;&#19968;&#32500;&#31896;&#24615;Burgers&#26041;&#31243;&#30340;&#28608;&#27874;&#36755;&#36816;&#21644;&#20108;&#32500;Navier-Stokes&#26041;&#31243;&#30340;&#28457;&#28065;&#21512;&#24182;&#26041;&#38754;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection-based reduced order models (PROMs) have shown promise in representing the behavior of multiscale systems using a small set of generalized (or latent) variables. Despite their success, PROMs can be susceptible to inaccuracies, even instabilities, due to the improper accounting of the interaction between the resolved and unresolved scales of the multiscale system (known as the closure problem). In the current work, we interpret closure as a multifidelity problem and use a multifidelity deep operator network (DeepONet) framework to address it. In addition, to enhance the stability and/or accuracy of the multifidelity-based closure, we employ the recently developed "in-the-loop" training approach from the literature on coupling physics and machine learning models. The resulting approach is tested on shock advection for the one-dimensional viscous Burgers equation and vortex merging for the two-dimensional Navier-Stokes equations. The numerical experiments show significant improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;MNL-Bandit&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#36951;&#25022;&#24230;&#20026;$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$&#12290;&#31639;&#27861;&#22522;&#20110;&#26102;&#20195;&#31639;&#27861;&#65292;&#23545;&#30001;&#20110;&#38750;&#38745;&#24577;&#24615;&#24341;&#20837;&#30340;&#20272;&#35745;&#22120;&#20559;&#24046;&#36827;&#34892;&#20102;&#32039;&#33268;&#29305;&#24449;&#32473;&#20986;&#26032;&#30340;&#27987;&#24230;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.02504</link><description>&lt;p&gt;
&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;MNL-Bandit&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MNL-Bandit in non-stationary environments. (arXiv:2303.02504v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;MNL-Bandit&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#36951;&#25022;&#24230;&#20026;$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$&#12290;&#31639;&#27861;&#22522;&#20110;&#26102;&#20195;&#31639;&#27861;&#65292;&#23545;&#30001;&#20110;&#38750;&#38745;&#24577;&#24615;&#24341;&#20837;&#30340;&#20272;&#35745;&#22120;&#20559;&#24046;&#36827;&#34892;&#20102;&#32039;&#33268;&#29305;&#24449;&#32473;&#20986;&#26032;&#30340;&#27987;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;MNL-Bandit&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#36951;&#25022;&#24230;&#20026;$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$&#12290;&#20854;&#20013;$N$&#26159;&#33218;&#30340;&#25968;&#37327;&#65292;$L$&#26159;&#21464;&#21270;&#30340;&#25968;&#37327;&#65292;$\Delta_{\infty}^{K}$&#26159;&#26410;&#30693;&#21442;&#25968;&#30340;&#21464;&#21270;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26399;&#26395;&#36951;&#25022;&#24230;&#30340;&#21305;&#37197;&#19979;&#30028;&#65288;&#23545;&#25968;&#22240;&#23376;&#20869;&#30340;&#19979;&#30028;&#65289;&#65292;&#35828;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Agrawal&#31561;&#20154;2016&#24180;&#25552;&#20986;&#30340;&#38745;&#24577;MNL-Bandit&#30340;&#26102;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#38750;&#38745;&#24577;&#24615;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#25216;&#26415;&#21644;&#24819;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#30001;&#20110;&#38750;&#38745;&#24577;&#24615;&#24341;&#20837;&#30340;&#20272;&#35745;&#22120;&#20559;&#24046;&#30340;&#32039;&#33268;&#29305;&#24449;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#30340;&#27987;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the MNL-Bandit problem in a non-stationary environment and present an algorithm with a worst-case expected regret of $\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$. Here $N$ is the number of arms, $L$ is the number of changes and $\Delta_{\infty}^{K}$ is a variation measure of the unknown parameters. Furthermore, we show matching lower bounds on the expected regret (up to logarithmic factors), implying that our algorithm is optimal. Our approach builds upon the epoch-based algorithm for stationary MNL-Bandit in Agrawal et al. 2016. However, non-stationarity poses several challenges and we introduce new techniques and ideas to address these. In particular, we give a tight characterization for the bias introduced in the estimators due to non stationarity and derive new concentration bounds.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#25104;&#26412;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20197;&#23398;&#20064;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02262</link><description>&lt;p&gt;
&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65306;&#26377;&#20123;&#40657;&#30418;&#23376;&#26159;&#35201;&#20445;&#25345;&#23553;&#38381;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#25104;&#26412;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20197;&#23398;&#20064;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#21160;&#36866;&#24212;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#23618;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#37325;&#35201;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20294;&#26159;&#65292;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#30340;&#27493;&#39588;&#25968;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#26102;&#38388;&#28857;&#20351;&#29992;&#33258;&#36866;&#24212;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#20869;&#37096;&#25104;&#26412;&#21551;&#21457;&#24335;&#26469;&#24341;&#23548;&#35757;&#32451;&#20197;&#23398;&#20064;&#26356;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#25105;&#20204;&#22686;&#21152;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#36981;&#24490;&#32463;&#39564;&#26465;&#20214;&#24182;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We "close the black-box" and allow the use of ou
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.10893</link><description>&lt;p&gt;
&#20844;&#24179;&#25193;&#25955;&#65306;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#12290;&#20294;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#38543;&#26426;&#25277;&#21462;&#30340;&#21313;&#20159;&#32423;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#20250;&#21463;&#21040;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#21152;&#21095;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#19981;&#20165;&#25581;&#31034;&#32780;&#19988;&#23545;&#25239;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#20844;&#24179;&#25193;&#25955;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20154;&#31867;&#25351;&#23548;&#30340;&#20559;&#24046;&#36716;&#31227;&#65292;&#21487;&#22312;&#20219;&#20309;&#26041;&#21521;&#19978;&#20135;&#29983;&#20219;&#24847;&#26032;&#30340;&#27604;&#20363;&#65292;&#20363;&#22914;&#65292;&#36523;&#20221;&#32452;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#31034;&#65292;&#36825;&#31181;&#25511;&#21046;&#20351;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#33021;&#22815;&#25509;&#21463;&#25351;&#23548;&#65292;&#26080;&#38656;&#25968;&#25454;&#36807;&#28388;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;RAM&#31192;&#23494;&#20849;&#20139;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;RAMP&#20849;&#20139;&#25216;&#26415;&#23454;&#29616;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2302.09913</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064; (arXiv:2302.09913v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment. (arXiv:2302.09913v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#35745;&#31639;&#21644;&#21521;&#37327;&#25215;&#35834;&#30340;&#25308;&#21344;&#24237;&#25269;&#25239;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;RAM&#31192;&#23494;&#20849;&#20139;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;RAMP&#20849;&#20139;&#25216;&#26415;&#23454;&#29616;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#38706;&#12290;&#36825;&#31181;&#26041;&#26696;&#36890;&#36807;&#22788;&#29702;&#21333;&#20010;&#26356;&#26032;&#26469;&#31649;&#29702;&#23545;&#25239;&#34892;&#20026;&#65292;&#24182;&#22312;&#25269;&#24481;&#20018;&#36890;&#33410;&#28857;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23545;&#26356;&#26032;&#21521;&#37327;&#36827;&#34892;&#23433;&#20840;&#31192;&#23494;&#20849;&#20139;&#30340;&#36890;&#20449;&#36127;&#36733;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26412;&#22320;&#26356;&#26032;&#20998;&#21106;&#25104;&#36739;&#23567;&#23376;&#21521;&#37327;&#24182;&#20351;&#29992;RAM&#31192;&#23494;&#20849;&#20139;&#30340;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#20849;&#20139;&#26041;&#27861;&#26080;&#27861;&#36827;&#34892;&#21452;&#32447;&#24615;&#35745;&#31639;&#65292;&#20363;&#22914;&#38656;&#35201;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#25104;&#23545;&#36317;&#31163;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#20250;&#36816;&#34892;&#21478;&#19968;&#36718;RAMP&#20849;&#20139;&#65292;&#35813;&#20849;&#20139;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#23884;&#20837;&#20854;&#20013;&#12290;&#36825;&#31181;&#21463;&#32534;&#30721;&#35745;&#31639;&#24605;&#24819;&#21551;&#21457;&#30340;&#25216;&#26415;&#23454;&#29616;&#20102;&#25104;&#23545;&#36317;&#31163;&#30340;&#23433;&#20840;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an efficient secure aggregation scheme for federated learning that is protected against Byzantine attacks and privacy leakages. Processing individual updates to manage adversarial behavior, while preserving privacy of data against colluding nodes, requires some sort of secure secret sharing. However, communication load for secret sharing of long vectors of updates can be very high. To resolve this issue, in the proposed scheme, local updates are partitioned into smaller sub-vectors and shared using ramp secret sharing. However, this sharing method does not admit bi-linear computations, such as pairwise distance calculations, needed by outlier-detection algorithms. To overcome this issue, each user runs another round of ramp sharing, with different embedding of data in the sharing polynomial. This technique, motivated by ideas from coded computing, enables secure computation of pairwise distance. In addition, to maintain the integrity and privacy of the local u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;CDT&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#22312;&#25628;&#32034;&#31354;&#38388;&#20869;&#23398;&#20064;&#26368;&#20248;&#21442;&#25968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09440</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Continuous Hyperparameter Optimization for Contextual Bandits. (arXiv:2302.09440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;CDT&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#22312;&#25628;&#32034;&#31354;&#38388;&#20869;&#23398;&#20064;&#26368;&#20248;&#21442;&#25968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20174;&#26102;&#38388;&#30456;&#20851;&#34892;&#21160;&#38598;&#20013;&#20381;&#27425;&#37319;&#21462;&#34892;&#21160;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#21518;&#24724;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19968;&#26679;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#22810;&#20010;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#29702;&#35770;&#25512;&#23548;&#20986;&#30340;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#23454;&#38469;&#19978;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#19979;&#20351;&#29992;&#31163;&#32447;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#20132;&#21449;&#39564;&#35777;&#65289;&#36873;&#25321;&#36229;&#21442;&#25968;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#20915;&#31574;&#24517;&#39035;&#23454;&#26102;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#65292;&#20197;&#23398;&#20064;&#39134;&#34892;&#20013;&#30340;&#26368;&#20339;&#21442;&#25968;&#37197;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;CDT&#65288;Continuous Dynamic Tuning&#65289;&#30340;&#21452;&#23618;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#23558;&#36229;&#21442;&#25968;&#20248;&#21270;&#24418;&#24335;&#21270;&#20026;&#38750;&#24179;&#31283;&#36830;&#32493;&#27494;&#22120;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#27494;&#22120;&#20195;&#34920;&#19968;&#31181;&#36229;&#21442;&#25968;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent action set based on past experience to minimize the cumulative regret. Like many other machine learning algorithms, the performance of bandits heavily depends on their multiple hyperparameters, and theoretically derived parameter values may lead to unsatisfactory results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation to choose hyperparameters under the bandit environment, as the decisions should be made in real time. To address this challenge, we propose the first online continuous hyperparameter tuning framework for contextual bandits to learn the optimal parameter configuration within a search space on the fly. Specifically, we use a double-layer bandit framework named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization as a non-stationary continuum-armed bandit, where each arm represents a combination of hyperparameters, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05342</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Joint Representations for Reinforcement Learning with Multiple Sensors. (arXiv:2302.05342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#32467;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#65292;&#22914;&#26426;&#22120;&#20154;&#26412;&#20307;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#26412;&#20307;&#24863;&#30693;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#31639;&#27861;&#32858;&#28966;&#20110;&#30456;&#20851;&#26041;&#38754;&#65292;&#24182;&#25351;&#23548;&#20854;&#23547;&#25214;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20174;&#22810;&#20010;&#20256;&#24863;&#22120;&#20013;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32852;&#21512;&#34920;&#31034;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#26080;&#27169;&#22411;&#21644;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21253;&#21547;&#20998;&#25955;&#30340;&#35270;&#35273;&#20449;&#24687;&#25110;&#32570;&#23569;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining inputs from multiple sensor modalities effectively in reinforcement learning (RL) is an open problem. While many self-supervised representation learning approaches exist to improve performance and sample complexity for image-based RL, they usually neglect other available information, such as robot proprioception. However, using this proprioception for representation learning can help algorithms to focus on relevant aspects and guide them toward finding better representations. In this work, we systematically analyze representation learning for RL from multiple sensors by building on Recurrent State Space Models. We propose a combination of reconstruction-based and contrastive losses, which allows us to choose the most appropriate method for each sensor modality. We demonstrate the benefits of joint representations, particularly with distinct loss functions for each modality, for model-free and model-based RL on complex tasks. Those include tasks where the images contain distra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25237;&#24433;&#25439;&#22833;&#23545;&#65292;&#19982;Rademacher&#24207;&#21015;&#30456;&#20851;&#32852;&#26469;&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#35774;&#32622;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2302.02432</link><description>&lt;p&gt;
&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#26356;&#32039;&#23494;
&lt;/p&gt;
&lt;p&gt;
Tighter Information-Theoretic Generalization Bounds from Supersamples. (arXiv:2302.02432v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25237;&#24433;&#25439;&#22833;&#23545;&#65292;&#19982;Rademacher&#24207;&#21015;&#30456;&#20851;&#32852;&#26469;&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#35774;&#32622;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#23398;&#20064;&#31639;&#27861;&#30340;&#21508;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#28304;&#20110;Steinke&#65286;Zakynthinou&#65288;2020&#65289;&#30340;&#36229;&#21462;&#26679;&#35774;&#32622;-&#8220;&#26465;&#20214;&#20114;&#20449;&#24687;&#8221;&#26694;&#26550;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#24320;&#21457;&#21033;&#29992;&#23558;&#25439;&#22833;&#23545;&#65288;&#20174;&#35757;&#32451;&#23454;&#20363;&#21644;&#27979;&#35797;&#23454;&#20363;&#33719;&#24471;&#65289;&#25237;&#24433;&#21040;&#21333;&#20010;&#25968;&#23383;&#65292;&#24182;&#23558;&#25439;&#22833;&#20540;&#19982;Rademacher&#24207;&#21015;&#65288;&#21450;&#20854;&#31227;&#21160;&#21464;&#20307;&#65289;&#30456;&#20851;&#32852;&#12290;&#25152;&#21576;&#29616;&#30340;&#30028;&#38480;&#21253;&#25324;&#24179;&#26041;&#26681;&#30028;&#38480;&#65292;&#24555;&#36895;&#29575;&#30028;&#38480;&#65292;&#21253;&#25324;&#22522;&#20110;&#26041;&#24046;&#21644;&#23574;&#38160;&#24230;&#30340;&#30028;&#38480;&#20197;&#21450;&#25554;&#20540;&#31639;&#27861;&#30340;&#30028;&#38480;&#31561;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#25110;&#32463;&#39564;&#19978;&#35777;&#26126;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke &amp; Zakynthinou (2020)-the setting of the "conditional mutual information" framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02210</link><description>&lt;p&gt;
&#20302;&#20301;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26080;&#25391;&#33633;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02210
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24847;&#35782;&#35757;&#32451;&#30340;&#19968;&#20010;&#19981;&#33391;&#21103;&#20316;&#29992;&#26159;&#26435;&#37325;&#25391;&#33633;&#65292;&#20854;&#20013;&#37327;&#21270;&#26435;&#37325;&#32463;&#24120;&#22312;&#20004;&#20010;&#37327;&#21270;&#32423;&#21035;&#20043;&#38388;&#36339;&#21160;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#23376;&#20248;&#21270;&#30340;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#30340;&#27604;&#20363;&#22240;&#23376;&#8212;&#8212;&#22312;&#37327;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;$\textit{de facto}$&#35774;&#32622;&#8212;&#8212;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#19982;&#37327;&#21270;&#26435;&#37325;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20197;ViT&#20026;&#26696;&#20363;&#26469;&#35828;&#26126;&#21457;&#29616;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#37327;&#21270;&#26435;&#37325;&#30340;$\textit{query}$&#21644;$\textit{key}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#23384;&#20351;ViT&#23481;&#26131;&#21463;&#21040;&#25391;&#33633;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#32479;&#35745;&#26435;&#37325;&#37327;&#21270;&#65288;$\rm StatsQ$&#65289;&#20197;&#25913;&#21892;&#37327;&#21270;&#40065;&#26834;&#24615;&#65292;&#19982;&#26222;&#36941;&#20351;&#29992;&#30340;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#26041;&#27861;&#30456;&#27604;&#65307;&#32622;&#20449;&#24230;&#24341;&#23548;&#30340;&#36864;&#28779;&#65288;$\rm CGA$&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#20923;&#32467;&#20855;&#26377;$\textit{&#39640;&#32622;&#20449;&#24230;}$&#30340;&#26435;&#37325;&#65292;&#20197;&#20943;&#23569;&#26435;&#37325;&#25391;&#33633;&#65307;&#20197;&#21450;&#30456;&#20114;&#20381;&#36182;&#26435;&#37325;&#30340;&#22343;&#34913;&#65288;$\rm IWEqual$&#65289;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#30456;&#20114;&#20381;&#36182;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20986;&#31639;&#27861;PASCombUCB&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13393</link><description>&lt;p&gt;
&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits. (arXiv:2301.13393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20986;&#31639;&#27861;PASCombUCB&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#32447;&#20915;&#31574;&#20013;&#21487;&#33021;&#36896;&#25104;&#36807;&#24230;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#26377;&#36873;&#25321;&#20174;$L$&#20010;&#22522;&#30784;&#39033;&#20013;&#19981;&#36229;&#36807;$K$&#20010;&#36827;&#34892;&#23376;&#38598;&#30340;&#36873;&#39033;&#12290;&#27599;&#20010;&#20803;&#32032;&#37117;&#19982;&#26576;&#20010;&#24179;&#22343;&#22870;&#21169;&#21644;&#34920;&#31034;&#20854;&#39118;&#38505;&#30340;&#26041;&#24046;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20943;&#23569;&#20195;&#29702;&#20154;&#25152;&#36973;&#21463;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#35201;&#27714;&#65292;&#22312;&#25972;&#20010;&#26102;&#38388;$T$&#30340;&#26102;&#38388;&#36328;&#24230;&#19978;&#65292;&#26234;&#33021;&#20307;&#25152;&#20570;&#30340;&#27599;&#20010;&#36873;&#25321;&#37117;&#24212;&#21253;&#21547;&#20854;&#26041;&#24046;&#20043;&#21644;&#19981;&#36229;&#36807;&#26576;&#20010;&#26041;&#24046;&#39044;&#31639;&#30340;&#20803;&#32032;&#65292;&#19988;&#20854;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#28385;&#36275;&#27492;&#32422;&#26463;&#12290;&#22312;&#27492;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#31639;&#27861;PASCombUCB&#65292;&#20197;&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;&#36890;&#36807;&#24320;&#21457;&#37197;&#22871;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#38382;&#39064;&#30456;&#20851;&#21644;&#38382;&#39064;&#26080;&#20851;&#30340;&#20004;&#31181;&#33539;&#20363;&#19979;&#65292;&#31639;&#27861;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the horizon of time $T$. By developing accompanying information-theoretic lower bounds, we show that under both the problem-dependent and problem-independent paradig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#38454;&#27573;&#21160;&#24577;&#35268;&#21010;&#30340;&#28145;&#24230;&#21387;&#32553;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#21512;&#24182;&#25104;&#31561;&#25928;&#30340;&#27973;&#23618;&#21367;&#31215;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#24310;&#36831;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.12187</link><description>&lt;p&gt;
&#22522;&#20110;&#20004;&#38454;&#27573;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#21387;&#32553;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming. (arXiv:2301.12187v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#38454;&#27573;&#21160;&#24577;&#35268;&#21010;&#30340;&#28145;&#24230;&#21387;&#32553;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#21512;&#24182;&#25104;&#31561;&#25928;&#30340;&#27973;&#23618;&#21367;&#31215;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#24310;&#36831;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#32780;&#38750;&#36890;&#36947;&#21098;&#26525;&#26469;&#20943;&#23569;&#36816;&#34892;&#26102;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#36895;&#25512;&#29702;&#24310;&#36831;&#26356;&#21152;&#26377;&#25928;&#12290;&#20854;&#20013;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#21512;&#24182;&#21367;&#31215;&#23618;&#30340;&#28145;&#24230;&#21387;&#32553;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#26377;&#19968;&#20010;&#29421;&#31364;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36890;&#29992;&#21367;&#31215;&#36816;&#31639;&#30340;&#26032;&#22411;&#28145;&#24230;&#21387;&#32553;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#24658;&#31561;&#20989;&#25968;&#26367;&#25442;&#20302;&#25928;&#28608;&#27963;&#23618;&#65292;&#24182;&#23558;&#36830;&#32493;&#30340;&#21367;&#31215;&#25805;&#20316;&#20248;&#21270;&#22320;&#21512;&#24182;&#25104;&#27973;&#23618;&#31561;&#25928;&#21367;&#31215;&#25805;&#20316;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#39640;&#25928;&#25512;&#29702;&#24310;&#36831;&#12290;&#30001;&#20110;&#25152;&#25552;&#20986;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#26159;NP-hard&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#38454;&#27573;&#21160;&#24577;&#35268;&#21010;&#22312;&#20960;&#31186;&#38047;&#20869;&#31934;&#30830;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#25968;&#23398;&#29983;&#29289;&#23398;&#36807;&#24230;&#19987;&#27880;&#20110;&#20998;&#26512;&#27169;&#22411;&#65292;&#24573;&#35270;&#20102;&#21046;&#23450;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#37319;&#29992;&#24320;&#25918;/&#22810;&#20803;&#21270;&#30340;&#26041;&#27861;&#36870;&#36716;&#36825;&#19968;&#36235;&#21183;&#65292;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29616;&#35937;&#36827;&#34892;&#26080;&#38480;&#27425;&#24314;&#27169;&#65292;&#20197;&#37325;&#26032;&#21457;&#25496;&#22833;&#33853;&#30340;&#21019;&#36896;&#24615;&#25968;&#23398;&#24314;&#27169;&#33402;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.08559</link><description>&lt;p&gt;
&#25968;&#23398;&#24314;&#27169;&#30340;&#22833;&#33853;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
The Lost Art of Mathematical Modelling. (arXiv:2301.08559v2 [q-bio.OT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#25968;&#23398;&#29983;&#29289;&#23398;&#36807;&#24230;&#19987;&#27880;&#20110;&#20998;&#26512;&#27169;&#22411;&#65292;&#24573;&#35270;&#20102;&#21046;&#23450;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#37319;&#29992;&#24320;&#25918;/&#22810;&#20803;&#21270;&#30340;&#26041;&#27861;&#36870;&#36716;&#36825;&#19968;&#36235;&#21183;&#65292;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29616;&#35937;&#36827;&#34892;&#26080;&#38480;&#27425;&#24314;&#27169;&#65292;&#20197;&#37325;&#26032;&#21457;&#25496;&#22833;&#33853;&#30340;&#21019;&#36896;&#24615;&#25968;&#23398;&#24314;&#27169;&#33402;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25105;&#20204;&#23545;&#25968;&#23398;&#29983;&#29289;&#23398;&#25552;&#20986;&#20102;&#25209;&#35780;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#25968;&#23398;&#29983;&#29289;&#23398;&#20013;&#30340;&#19977;&#20010;&#27169;&#22411;&#24314;&#31435;&#27963;&#21160;&#8212;&#8212;&#65288;1&#65289;&#21046;&#23450;&#27169;&#22411;&#65292;&#65288;2&#65289;&#20998;&#26512;&#27169;&#22411;&#65292;&#65288;3&#65289;&#23558;&#27169;&#22411;&#25311;&#21512;&#25110;&#19982;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#8212;&#8212;&#30740;&#31350;&#20154;&#21592;&#30446;&#21069;&#36807;&#24230;&#19987;&#27880;&#20110;&#27963;&#21160;&#65288;2&#65289;&#65292;&#20174;&#32780;&#30095;&#24573;&#20102;&#27963;&#21160;&#65288;1&#65289;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#37319;&#21462;&#24320;&#25918;/&#22810;&#20803;&#21270;&#30340;&#26041;&#27861;&#26469;&#36870;&#36716;&#36825;&#19968;&#36235;&#21183;&#65292;&#20174;&#32780;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29616;&#35937;&#36827;&#34892;&#26080;&#38480;&#27425;&#24314;&#27169;&#12290;&#25105;&#20204;&#20197;&#40060;&#31867;&#36816;&#21160;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35299;&#37322;&#20102;&#24320;&#25918;&#26041;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;&#38459;&#30861;&#25968;&#23398;&#29983;&#29289;&#23398;&#30340;&#19968;&#20123;&#38519;&#38449;&#65288;&#26222;&#36941;&#20027;&#20041;&#12289;&#27169;&#22411;&#30340;&#27169;&#22411;&#31561;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#37325;&#26032;&#21457;&#29616;&#19968;&#31181;&#22833;&#33853;&#30340;&#33402;&#26415;&#65306;&#21019;&#36896;&#24615;&#30340;&#25968;&#23398;&#24314;&#27169;&#12290;&#26412;&#25991;&#29486;&#32473;Edmund Crampin&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a critique of mathematical biology in light of rapid developments in modern machine learning. We argue that out of the three modelling activities -- (1) formulating models; (2) analysing models; and (3) fitting or comparing models to data -- inherent to mathematical biology, researchers currently focus too much on activity (2) at the cost of (1). This trend, we propose, can be reversed by realising that any given biological phenomena can be modelled in an infinite number of different ways, through the adoption of an open/pluralistic approach. We explain the open approach using fish locomotion as a case study and illustrate some of the pitfalls -- universalism, creating models of models, etc. -- that hinder mathematical biology. We then ask how we might rediscover a lost art: that of creative mathematical modelling.  This article is dedicated to the memory of Edmund Crampin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#39044;&#27979;&#21644;&#21327;&#35843;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.13706</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#23454;&#29616;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
End-to-End Modeling Hierarchical Time Series Using Autoregressive Transformer and Conditional Normalizing Flow based Reconciliation. (arXiv:2212.13706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#39044;&#27979;&#21644;&#21327;&#35843;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#32423;&#21035;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#21327;&#35843;&#25152;&#26377;&#30340;&#39044;&#27979;&#32467;&#26524;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21363;&#39044;&#27979;&#32467;&#26524;&#24212;&#28385;&#36275;&#23618;&#27425;&#32858;&#21512;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#22522;&#20110;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#21327;&#35843;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20559;&#24046;&#20272;&#35745;&#31561;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting with hierarchical structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also reconciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#19982;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#32467;&#21512;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2212.10025</link><description>&lt;p&gt;
&#24403;&#32852;&#37030;&#23398;&#20064;&#36935;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#19982;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#32467;&#21512;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#20851;&#27880;&#30340;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#24456;&#22810;&#25991;&#29486;&#24314;&#35758;&#22312; FL &#33539;&#24335;&#20013;&#23436;&#20840;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#32553;&#23567;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411; PLMs &#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644; FL &#31995;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#36866;&#24212;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PETuning&#65289;&#26041;&#27861;&#24341;&#20837;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102; FL &#20013;&#20195;&#34920;&#24615;&#30340; PLMs &#35843;&#25972;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35206;&#30422;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#27700;&#24179;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#19981;&#21516; FL &#22330;&#26223;&#30340;&#20998;&#26512;&#12290;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#26412;&#22320;&#35843;&#25972;&#21644;&#20840;&#23616;&#32858;&#21512;&#36731;&#37327;&#32423;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24635;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing privacy concerns on data, recent studies have made significant progress using federated learning (FL) on privacy-sensitive natural language processing (NLP) tasks. Much literature suggests fully fine-tuning pre-trained language models (PLMs) in the FL paradigm can mitigate the data heterogeneity problem and close the performance gap with centralized training. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we introduce various parameter-efficient tuning (PETuning) methods into federated learning. Specifically, we provide a holistic empirical study of representative PLMs tuning methods in FL. The experimental results cover the analysis of data heterogeneity levels, data scales, and different FL scenarios. Overall communication overhead can be significantly reduced by locally tuning and globally aggregating lightweight model parameters while maintaining acceptable performance in var
&lt;/p&gt;</description></item><item><title>PATO&#26159;&#19968;&#20010;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#23454;&#29616;&#21333;&#20010;&#25805;&#20316;&#21592;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.04708</link><description>&lt;p&gt;
PATO: &#22522;&#20110;&#31574;&#30053;&#36741;&#21161;&#30340;&#21487;&#20280;&#32553;&#26426;&#22120;&#20154;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection. (arXiv:2212.04708v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04708
&lt;/p&gt;
&lt;p&gt;
PATO&#26159;&#19968;&#20010;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#23454;&#29616;&#21333;&#20010;&#25805;&#20316;&#21592;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#27491;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#20294;&#26159;&#65292;&#37319;&#38598;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#25104;&#26412;&#26356;&#39640;&#65292;&#36895;&#24230;&#26356;&#24930;&#65292;&#22240;&#20026;&#27599;&#20010;&#25805;&#20316;&#21592;&#21482;&#33021;&#21516;&#26102;&#25511;&#21046;&#19968;&#20010;&#26426;&#22120;&#20154;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#26114;&#36149;&#30340;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#39640;&#25928;&#21644;&#21487;&#20280;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65288;PATO&#65289;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#36741;&#21161;&#31574;&#30053;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#12290;PATO&#22312;&#25968;&#25454;&#37319;&#38598;&#20013;&#33258;&#21160;&#25191;&#34892;&#37325;&#22797;&#30340;&#34892;&#20026;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#27169;&#25311;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#36828;&#31243;&#25805;&#20316;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#36741;&#21161;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#33021;&#22815;&#20943;&#23569;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#21333;&#20010;&#25805;&#20316;&#21592;&#33021;&#22815;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#65292;&#36825;&#26159;&#39318;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33539;&#24335;PrefRec&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.02779</link><description>&lt;p&gt;
PrefRec&#65306;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#21152;&#24378;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement. (arXiv:2212.02779v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33539;&#24335;PrefRec&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#20248;&#21270;&#21363;&#26102;&#21442;&#19982;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26356;&#21487;&#21462;&#30340;&#32489;&#25928;&#25351;&#26631;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25552;&#39640;&#20173;&#28982;&#24456;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#38271;&#26399;&#30446;&#26631;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#20248;&#21270;&#25512;&#33616;&#20013;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26377;&#21069;&#36884;&#65292;&#20294;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#65292;&#20294;&#35774;&#35745;&#19982;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#26377;&#20851;&#30340;&#22870;&#21169;&#30456;&#24403;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#20197;&#20154;&#31867;&#20559;&#22909;&#20026;&#22522;&#30784;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#26377;&#20851;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#30340;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#20174;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#20559;&#22909;&#21487;&#20197;&#36890;&#36807;&#20247;&#21253;&#31561;&#25216;&#26415;&#36731;&#26494;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22312;&#32447;&#21253;&#35013;&#19981;&#35268;&#21017;&#19977;&#32500;&#24418;&#29366;&#30340;&#29289;&#29702;&#23454;&#29616;&#25216;&#33021;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31649;&#36947;&#24182;&#37319;&#29992;&#20505;&#36873;&#34892;&#21160;&#29983;&#25104;&#26041;&#27861;&#26469;&#20943;&#23567;&#23398;&#20064;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2212.02094</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#32447;&#21253;&#35013;&#36890;&#29992;&#19977;&#32500;&#24418;&#29366;&#30340;&#21487;&#23454;&#29616;&#29289;&#29702;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Physically Realizable Skills for Online Packing of General 3D Shapes. (arXiv:2212.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22312;&#32447;&#21253;&#35013;&#19981;&#35268;&#21017;&#19977;&#32500;&#24418;&#29366;&#30340;&#29289;&#29702;&#23454;&#29616;&#25216;&#33021;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31649;&#36947;&#24182;&#37319;&#29992;&#20505;&#36873;&#34892;&#21160;&#29983;&#25104;&#26041;&#27861;&#26469;&#20943;&#23567;&#23398;&#20064;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#22312;&#32447;&#21253;&#35013;&#19981;&#35268;&#21017;&#19977;&#32500;&#24418;&#29366;&#30340;&#25216;&#33021;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#31665;&#23376;&#35013;&#31665;&#38382;&#39064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;&#30446;&#26631;&#26159;&#36830;&#32493;&#31227;&#21160;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;3D&#23545;&#35937;&#24207;&#21015;&#21040;&#19968;&#20010;&#25351;&#23450;&#30340;&#23481;&#22120;&#20013;&#65292;&#24182;&#20165;&#35266;&#23519;&#37096;&#20998;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29289;&#29702;&#23454;&#29616;&#21487;&#33021;&#24615;&#65292;&#28041;&#21450;&#25918;&#32622;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#21644;&#32422;&#26463;&#12290;&#21253;&#35013;&#31574;&#30053;&#24212;&#20102;&#35299;&#35201;&#21253;&#35013;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#20570;&#20986;&#26377;&#25928;&#30340;&#20915;&#31574;&#26469;&#20197;&#29289;&#29702;&#21487;&#34892;&#30340;&#26041;&#24335;&#23481;&#32435;&#23427;&#20204;&#22312;&#23481;&#22120;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31649;&#36947;&#26469;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#12290;&#22797;&#26434;&#30340;&#19981;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#35937;&#25918;&#32622;&#20849;&#21516;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35299;&#31354;&#38388;&#12290;&#30452;&#25509;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#36827;&#34892;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#29702;&#35770;&#39564;&#35777;&#30340;&#20505;&#36873;&#34892;&#21160;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;RL&#30340;&#34892;&#21160;&#31354;&#38388;&#21644;&#23398;&#20064;&#36127;&#25285;&#12290;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning online packing skills for irregular 3D shapes, which is arguably the most challenging setting of bin packing problems. The goal is to consecutively move a sequence of 3D objects with arbitrary shapes into a designated container with only partial observations of the object sequence. Meanwhile, we take physical realizability into account, involving physics dynamics and constraints of a placement. The packing policy should understand the 3D geometry of the object to be packed and make effective decisions to accommodate it in the container in a physically realizable way. We propose a Reinforcement Learning (RL) pipeline to learn the policy. The complex irregular geometry and imperfect object placement together lead to huge solution space. Direct training in such space is prohibitively data intensive. We instead propose a theoretically-provable method for candidate action generation to reduce the action space of RL and the learning burden. A parameterized po
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36793;&#32536;&#20540;&#30340;&#20027;&#21160;&#23398;&#20064;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#25928;&#29575;&#19981;&#22914;&#34987;&#21160;&#23398;&#20064;&#65292;&#21363;&#20351;&#23545;&#20110;&#26080;&#22122;&#22768;&#25968;&#25454;&#21644;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#36827;&#34892;&#37319;&#26679;&#65292;&#34987;&#21160;&#23398;&#20064;&#20173;&#28982;&#26356;&#20248;&#12290;&#29305;&#21035;&#26159;&#22312;&#31867;&#20043;&#38388;&#30340;&#20998;&#31163;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#29616;&#35937;&#26356;&#21152;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2212.00772</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#22522;&#20110;&#36793;&#32536;&#37319;&#26679;&#65306;&#20027;&#21160;&#23398;&#20064;&#25928;&#29575;&#19981;&#22914;&#34987;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Margin-based sampling in high dimensions: When being active is less efficient than staying passive. (arXiv:2212.00772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00772
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#20540;&#30340;&#20027;&#21160;&#23398;&#20064;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#25928;&#29575;&#19981;&#22914;&#34987;&#21160;&#23398;&#20064;&#65292;&#21363;&#20351;&#23545;&#20110;&#26080;&#22122;&#22768;&#25968;&#25454;&#21644;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#36827;&#34892;&#37319;&#26679;&#65292;&#34987;&#21160;&#23398;&#20064;&#20173;&#28982;&#26356;&#20248;&#12290;&#29305;&#21035;&#26159;&#22312;&#31867;&#20043;&#38388;&#30340;&#20998;&#31163;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#29616;&#35937;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#35748;&#20026;&#65292;&#32473;&#23450;&#30456;&#21516;&#30340;&#26631;&#27880;&#39044;&#31639;&#65292;&#22522;&#20110;&#36793;&#32536;&#20540;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#20250;&#27604;&#34987;&#21160;&#23398;&#20064;&#31639;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#23613;&#31649;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#26368;&#36817;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#39069;&#22806;&#30340;&#25104;&#26412;&#21487;&#33021;&#26159;&#24466;&#21171;&#30340;&#65292;&#22240;&#20026;&#22522;&#20110;&#36793;&#32536;&#20540;&#30340;&#20027;&#21160;&#23398;&#20064;&#26377;&#26102;&#29978;&#33267;&#27604;&#34987;&#21160;&#23398;&#20064;&#34920;&#29616;&#26356;&#24046;&#12290;&#26412;&#25991;&#22312;&#36923;&#36753;&#22238;&#24402;&#30340;&#32972;&#26223;&#19979;&#35777;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#26080;&#22122;&#22768;&#25968;&#25454;&#21644;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#36827;&#34892;&#37319;&#26679;&#65292;&#34987;&#21160;&#23398;&#20064;&#20173;&#28982;&#20248;&#20110;&#22522;&#20110;&#36793;&#32536;&#20540;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#24471;&#30340;&#32467;&#35770;&#34920;&#26126;&#65292;&#35813;&#29616;&#35937;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#21152;&#21095;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20043;&#38388;&#30340;&#20998;&#31163;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;20&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#36827;&#34892;&#23454;&#35777;&#65292;&#35777;&#23454;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#37329;&#34701;&#21644;&#32452;&#32455;&#23398;&#21040;&#21270;&#23398;&#21644;&#35745;&#31639;&#26426;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely believed that given the same labeling budget, active learning (AL) algorithms like margin-based active learning achieve better predictive performance than passive learning (PL), albeit at a higher computational cost. Recent empirical evidence suggests that this added cost might be in vain, as margin-based AL can sometimes perform even worse than PL. While existing works offer different explanations in the low-dimensional regime, this paper shows that the underlying mechanism is entirely different in high dimensions: we prove for logistic regression that PL outperforms margin-based AL even for noiseless data and when using the Bayes optimal decision boundary for sampling. Insights from our proof indicate that this high-dimensional phenomenon is exacerbated when the separation between the classes is small. We corroborate this intuition with experiments on 20 high-dimensional datasets spanning a diverse range of applications, from finance and histology to chemistry and comput
&lt;/p&gt;</description></item><item><title>SWL-Adapt&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#26435;&#37325;&#23398;&#20064;&#26469;&#21306;&#20998;&#19981;&#21516;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#35813;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.00724</link><description>&lt;p&gt;
SWL-Adapt: &#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#26435;&#37325;&#23398;&#20064;&#30340;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SWL-Adapt: An Unsupervised Domain Adaptation Model with Sample Weight Learning for Cross-User Wearable Human Activity Recognition. (arXiv:2212.00724v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00724
&lt;/p&gt;
&lt;p&gt;
SWL-Adapt&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#26435;&#37325;&#23398;&#20064;&#26469;&#21306;&#20998;&#19981;&#21516;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#35813;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20013;&#65292;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#36890;&#24120;&#38754;&#20020;&#30528;&#36328;&#29992;&#25143;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#29992;&#25143;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25104;&#20026;&#20102;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#33258;&#28982;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#36890;&#24120;&#22312;&#39046;&#22495;&#38388;&#23545;&#40784;&#26679;&#26412;&#65292;&#20294;&#19981;&#36827;&#34892;&#26679;&#26412;&#38388;&#30340;&#21306;&#20998;&#65292;&#24573;&#30053;&#20102;&#26679;&#26412;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#26435;&#37325;&#23398;&#20064;&#30340;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;(SWL-Adapt)&#12290;SWL-Adapt&#26681;&#25454;&#32593;&#32476;&#21442;&#25968;&#35745;&#31639;&#27599;&#20010;&#26679;&#26412;&#30340;&#20998;&#31867;&#25439;&#22833;&#21644;&#39046;&#22495;&#36776;&#21035;&#25439;&#22833;&#30340;&#26679;&#26412;&#26435;&#37325;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#20803;&#20248;&#21270;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#20266;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#30340;&#20803;&#20998;&#31867;&#25439;&#22833;&#26469;&#25351;&#23548;&#36825;&#20010;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#25163;&#22836;&#30340;&#36328;&#29992;&#25143;&#21487;&#31359;&#25140;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#36866;&#24212;&#19968;&#20010;&#21152;&#26435;&#20989;&#25968;&#65292;&#36825;&#20248;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#26679;&#26412;&#32454;&#20998;&#30340;&#35268;&#21017;&#22266;&#23450;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, Wearable Human Activity Recognition (WHAR) models usually face performance degradation on the new user due to user variance. Unsupervised domain adaptation (UDA) becomes the natural solution to cross-user WHAR under annotation scarcity. Existing UDA models usually align samples across domains without differentiation, which ignores the difference among samples. In this paper, we propose an unsupervised domain adaptation model with sample weight learning (SWL-Adapt) for cross-user WHAR. SWL-Adapt calculates sample weights according to the classification loss and domain discrimination loss of each sample with a parameterized network. We introduce the meta-optimization based update rule to learn this network end-to-end, which is guided by meta-classification loss on the selected pseudo-labeled target samples. Therefore, this network can fit a weighting function according to the cross-user WHAR task at hand, which is superior to existing sample differentiation rules fixed for s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#21644;&#36991;&#20813;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.00124</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20998;&#24067;&#20559;&#31227;&#30340;&#39118;&#38505;&#35268;&#36991;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#21644;&#36991;&#20813;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36866;&#29992;&#20110;&#22312;&#32447;&#25506;&#32034;&#19981;&#21487;&#34892;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#22312;&#36825;&#31181;&#39046;&#22495;&#20013;&#65292;&#20915;&#31574;&#24212;&#32771;&#34385;&#21040;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39118;&#38505;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20915;&#31574;&#24212;&#35813;&#26159;&#39118;&#38505;&#35268;&#36991;&#30340;&#12290;&#31163;&#32447;RL&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#65292;&#21363;&#30830;&#20445;&#31574;&#30053;&#35775;&#38382;&#30340;&#29366;&#24577;-&#25805;&#20316;&#23545;&#38752;&#36817;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;-&#25805;&#20316;&#23545;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#31163;&#32447;RL&#25216;&#26415;&#65288;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#65289;&#19982;&#39118;&#38505;&#25935;&#24863;&#22411;RL&#31639;&#27861;&#65288;&#20197;&#23454;&#29616;&#39118;&#38505;&#35268;&#36991;&#65289;&#30456;&#32467;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#20316;&#20026;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27169;&#22411;&#38598;&#21512;&#26469;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#39118;&#38505;&#35268;&#36991;&#30340;&#31574;&#30053;&#65292;&#36991;&#20813;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#20026;&#12290;&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#39118;&#38505;&#35268;&#36991;&#21487;&#20197;&#38450;&#27490;&#20998;&#24067;&#20559;&#31227;&#65292;&#22240;&#20026;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#30340;&#24037;&#20316;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36830;&#32493;&#36866;&#24212;&#40654;&#26364;&#31354;&#38388;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#26354;&#29575;&#21464;&#21270;&#21644;&#23545;&#26631;&#31614;&#20381;&#36182;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.17068</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#36830;&#32493;&#36866;&#24212;&#40654;&#26364;&#31354;&#38388;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Continual Graph Learning in Adaptive Riemannian Spaces. (arXiv:2211.17068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#36830;&#32493;&#36866;&#24212;&#40654;&#26364;&#31354;&#38388;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#26354;&#29575;&#21464;&#21270;&#21644;&#23545;&#26631;&#31614;&#20381;&#36182;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#22270;&#23398;&#20064;&#22312;&#21508;&#31181;&#38656;&#35201;&#20381;&#27425;&#20986;&#29616;&#22270;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#23450;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38646;&#26354;&#29575;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#26354;&#29575;&#38543;&#21363;&#20986;&#29616;&#30340;&#22270;&#24207;&#21015;&#32780;&#21464;&#21270;&#30340;&#20107;&#23454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25991;&#29486;&#20013;&#19981;&#26029;&#30340;&#23398;&#20064;&#22120;&#20381;&#36182;&#20016;&#23500;&#30340;&#26631;&#31614;&#65292;&#20294;&#23454;&#36341;&#20013;&#26631;&#35760;&#22270;&#23588;&#20854;&#26159;&#38024;&#23545;&#19981;&#26029;&#20986;&#29616;&#30340;&#22270;&#65292;&#23545;&#23454;&#26102;&#24615;&#30340;&#35201;&#27714;&#24456;&#39640;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#33258;&#30417;&#30563;&#36830;&#32493;&#36866;&#24212;&#40654;&#26364;&#31354;&#38388;&#22270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#40654;&#26364;&#22270;&#36830;&#32493;&#23398;&#20064;&#22120;&#65288;RieGrace&#65289;&#12290;&#22312;RieGrace&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;GCN&#65288;AdaRGCN&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;GCN&#19982;&#31070;&#32463;&#26354;&#29575;&#36866;&#37197;&#22120;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#40654;&#26364;&#31354;&#38388;&#21487;&#20197;&#36866;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual graph learning routinely finds its role in a variety of real-world applications where the graph data with different tasks come sequentially. Despite the success of prior works, it still faces great challenges. On the one hand, existing methods work with the zero-curvature Euclidean space, and largely ignore the fact that curvature varies over the coming graph sequence. On the other hand, continual learners in the literature rely on abundant labels, but labeling graph in practice is particularly hard especially for the continuously emerging graphs on-the-fly. To address the aforementioned challenges, we propose to explore a challenging yet practical problem, the self-supervised continual graph learning in adaptive Riemannian spaces. In this paper, we propose a novel self-supervised Riemannian Graph Continual Learner (RieGrace). In RieGrace, we first design an Adaptive Riemannian GCN (AdaRGCN), a unified GCN coupled with a neural curvature adapter, so that Riemannian space is s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;STFT&#30456;&#20301;&#24674;&#22797;&#29983;&#25104;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21644;&#29616;&#20195;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04332</link><description>&lt;p&gt;
DiffPhase: &#22522;&#20110;&#25193;&#25955;&#30340;STFT&#30456;&#20301;&#24674;&#22797;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffPhase: Generative Diffusion-based STFT Phase Retrieval. (arXiv:2211.04332v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;STFT&#30456;&#20301;&#24674;&#22797;&#29983;&#25104;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21644;&#29616;&#20195;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#35821;&#38899;&#22686;&#24378;&#21644;&#21512;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#12290;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;&#32570;&#22833;&#30340;&#25968;&#25454;&#26159;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#29983;&#25104;&#30340;&#12290;&#30456;&#20301;&#26816;&#32034;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30456;&#20301;&#20449;&#24687;&#24517;&#39035;&#22522;&#20110;&#32473;&#23450;&#30340;&#24133;&#24230;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#35821;&#38899;&#39046;&#22495;&#20808;&#21069;&#30340;&#24037;&#20316;&#20043;&#19978;&#65292;&#19987;&#38376;&#38024;&#23545;STFT&#30456;&#20301;&#24674;&#22797;&#35843;&#25972;&#20102;&#35821;&#38899;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#12290;&#20351;&#29992;&#35821;&#38899;&#36136;&#37327;&#21644;&#21487;&#25026;&#24230;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#34920;&#26126;&#65292;&#25193;&#25955;&#26041;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#30456;&#20301;&#26816;&#32034;&#20219;&#21153;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21644;&#29616;&#20195;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been recently used in a variety of tasks, including speech enhancement and synthesis. As a generative approach, diffusion models have been shown to be especially suitable for imputation problems, where missing data is generated based on existing data. Phase retrieval is inherently an imputation problem, where phase information has to be generated based on the given magnitude. In this work we build upon previous work in the speech domain, adapting a speech enhancement diffusion model specifically for STFT phase retrieval. Evaluation using speech quality and intelligibility metrics shows the diffusion approach is well-suited to the phase retrieval task, with performance surpassing both classical and modern methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#31934;&#30830; Langevin &#31639;&#27861;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312; KL &#25955;&#24230;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#31283;&#23450;&#20559;&#24046;&#25910;&#25947;&#20445;&#35777;&#30340;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;&#30446;&#26631;&#20998;&#24067;&#28385;&#36275;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#21644;&#20998;&#25968;&#20272;&#35745;&#22120;&#23637;&#31034;&#20986;&#26377;&#30028;&#30340;&#30697;&#38453;&#29983;&#25104;&#20989;&#25968;&#35823;&#24046;&#12290;&#20316;&#32773;&#25506;&#35752;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#38752;&#30340;&#20998;&#25968;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#31616;&#21333;&#20272;&#35745;&#22120;&#28385;&#36275;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2211.01512</link><description>&lt;p&gt;
&#19981;&#31934;&#30830; Langevin &#31639;&#27861;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312; KL &#25955;&#24230;&#20013;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence. (arXiv:2211.01512v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#31934;&#30830; Langevin &#31639;&#27861;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312; KL &#25955;&#24230;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#31283;&#23450;&#20559;&#24046;&#25910;&#25947;&#20445;&#35777;&#30340;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;&#30446;&#26631;&#20998;&#24067;&#28385;&#36275;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#21644;&#20998;&#25968;&#20272;&#35745;&#22120;&#23637;&#31034;&#20986;&#26377;&#30028;&#30340;&#30697;&#38453;&#29983;&#25104;&#20989;&#25968;&#35823;&#24046;&#12290;&#20316;&#32773;&#25506;&#35752;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#38752;&#30340;&#20998;&#25968;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#31616;&#21333;&#20272;&#35745;&#22120;&#28385;&#36275;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#31934;&#30830; Langevin &#21160;&#21147;&#23398;&#65288;ILD&#65289;&#12289;&#19981;&#31934;&#30830; Langevin &#31639;&#27861;&#65288;ILA&#65289;&#21644;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#65288;SGM&#65289;&#22312;&#21033;&#29992;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#36827;&#34892;&#37319;&#26679;&#26102;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#24314;&#31435;&#20851;&#20110; Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#31283;&#23450;&#20559;&#24046;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#20445;&#35777;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;1&#65289;&#30446;&#26631;&#20998;&#24067;&#28385;&#36275;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#65288;LSI&#65289;&#65292;2&#65289;&#20998;&#25968;&#20272;&#35745;&#22120;&#23637;&#31034;&#20986;&#19968;&#20010;&#26377;&#30028;&#30340;&#30697;&#38453;&#29983;&#25104;&#20989;&#25968;&#65288;MGF&#65289;&#35823;&#24046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340; MGF &#35823;&#24046;&#20551;&#35774;&#30456;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340; $L^\infty$ &#35823;&#24046;&#20551;&#35774;&#26356;&#20026;&#23485;&#26494;&#12290;&#28982;&#32780;&#65292;&#23427;&#27604;&#26368;&#36817;&#30340;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340; $L^2$ &#35823;&#24046;&#20551;&#35774;&#26356;&#24378;&#65292;&#21518;&#32773;&#24120;&#24120;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#33719;&#24471;&#28385;&#36275; MGF &#35823;&#24046;&#20551;&#35774;&#30340;&#21487;&#38752;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#31616;&#21333;&#20272;&#35745;&#22120;&#28385;&#36275; MGF &#35823;&#24046;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Inexact Langevin Dynamics (ILD), Inexact Langevin Algorithm (ILA), and Score-based Generative Modeling (SGM) when utilizing estimated score functions for sampling. Our focus lies in establishing stable biased convergence guarantees in terms of the Kullback-Leibler (KL) divergence. To achieve these guarantees, we impose two key assumptions: 1) the target distribution satisfies the log-Sobolev inequality (LSI), and 2) the score estimator exhibits a bounded Moment Generating Function (MGF) error. Notably, the MGF error assumption we adopt is more lenient compared to the $L^\infty$ error assumption used in existing literature. However, it is stronger than the $L^2$ error assumption utilized in recent works, which often leads to unstable bounds. We explore the question of how to obtain a provably accurate score estimator that satisfies the MGF error assumption. Specifically, we demonstrate that a simple estimator based on kernel density estimation fulfills the MGF error assumpt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.16205</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31363;&#21548;&#30446;&#26631;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#24182;&#37325;&#26500;&#21463;&#23475;&#32773;&#30340;&#26412;&#22320;/&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#32463;&#20856;&#25915;&#20987;&#12290;&#26412;&#22320;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#27844;&#28431;&#27604;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#22810;&#30340;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#21033;&#29992;&#20102;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#20998;&#26512;&#19979;&#30028;&#12290;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26412;&#22320;&#37325;&#26500;&#25915;&#20987;&#23545;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#37117;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#19982;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;Stackelberg&#22343;&#34913;&#25628;&#32034;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#36827;&#34892;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.11942</link><description>&lt;p&gt;
&#39046;&#34966;&#19982;&#36861;&#38543;&#32773;&#65306;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Stackelberg&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Oracles &amp; Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;Stackelberg&#22343;&#34913;&#25628;&#32034;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#36827;&#34892;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stackelberg&#22343;&#34913;&#22312;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#28982;&#20986;&#29616;&#65292;&#20363;&#22914;&#22312;&#23433;&#20840;&#21338;&#24328;&#25110;&#38388;&#25509;&#26426;&#21046;&#35774;&#35745;&#20013;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;Stackelberg&#22343;&#34913;&#25628;&#32034;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#20854;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#65292;&#20801;&#35768;&#36827;&#34892;&#21508;&#31181;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#23558;&#20043;&#21069;&#30340;&#26041;&#27861;&#35270;&#20026;&#27492;&#26694;&#26550;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35774;&#35745;&#31354;&#38388;&#20801;&#35768;&#20351;&#29992;&#20197;&#21069;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#21033;&#29992;&#22810;&#20219;&#21153;&#21644;&#20803;-RL&#25216;&#26415;&#23454;&#29616;&#36861;&#38543;&#32773;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#22823;&#22823;&#25552;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#37319;&#29992;&#26694;&#26550;&#36793;&#30028;&#22806;&#30340;&#31639;&#27861;&#35774;&#35745;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BHMC&#30340;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23450;&#20041;&#20102;&#32422;&#26463;&#30340;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#26080;&#20559;&#37319;&#26679;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;involution checking step&#12290;</title><link>http://arxiv.org/abs/2210.11925</link><description>&lt;p&gt;
&#33258;&#20849;&#36717;&#38556;&#30861;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#27931;&#30340;&#26080;&#20559;&#32422;&#26463;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo. (arXiv:2210.11925v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BHMC&#30340;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23450;&#20041;&#20102;&#32422;&#26463;&#30340;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#26080;&#20559;&#37319;&#26679;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;involution checking step&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38556;&#30861;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;(BHMC)&#65292;&#23427;&#26159;HMC&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#20174;&#24102;&#26377;&#33258;&#20849;&#36717;&#38556;&#30861;&#24230;&#37327;&#30340;&#27969;&#24418;&#20013;&#30340;Gibbs&#20998;&#24067;&#960;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#21253;&#21547;&#24230;&#37327;&#30340;Hamiltonian&#21160;&#21147;&#23398;&#12290;&#22240;&#27492;&#65292;&#23427;&#21253;&#21547;&#23450;&#20041;&#27969;&#24418;&#30340;&#32422;&#26463;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#20854;&#24213;&#23618;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#30456;&#24212;&#30340;Hamilton&#21160;&#21147;&#23398;&#26159;&#36890;&#36807;&#19981;&#21487;&#20998;&#31163;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#26469;&#23450;&#20041;&#30340;&#65292;&#19982;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#30456;&#21453;&#12290;&#36825;&#24847;&#21619;&#30528;&#23558;HMC&#25512;&#24191;&#21040;&#40654;&#26364;&#27969;&#24418;&#20013;&#20250;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#20559;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#28388;&#27493;&#39588;&#65292;&#31216;&#20026;&#8220;involution&#26816;&#26597;&#27493;&#39588;&#8221;&#12290;&#35813;&#27493;&#39588;&#22312;&#20004;&#20010;BHMC&#29256;&#26412;&#8212;&#8212;&#36830;&#32493;BHMC(c-BHMC)&#21644;&#25968;&#20540;BHMC(n-BHMC)&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#26032;&#31639;&#27861;&#29983;&#25104;&#21487;&#36870;Markov&#38142;&#19988;&#26080;&#20559;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the HMC algorithm which aims at sampling from a Gibbs distribution $\pi$ on a manifold $\mathrm{M}$, endowed with a Hessian metric $\mathfrak{g}$ derived from a self-concordant barrier. Our method relies on Hamiltonian dynamics which comprises $\mathfrak{g}$. Therefore, it incorporates the constraints defining $\mathrm{M}$ and is able to exploit its underlying geometry. However, the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called "involution checking step", to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC) respectively. Our main results establish that these two new algorithms generate reversible Mark
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#36712;&#36857;&#65292;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20420;&#32599;&#26031;&#21943;&#23376;&#19982;&#26377;&#26426;&#29992;&#25143;&#65292;&#21487;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2210.08786</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#24449;&#21644;&#26816;&#27979;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Characterizing and Detecting State-Sponsored Troll Activity on Social Media. (arXiv:2210.08786v5 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#36712;&#36857;&#65292;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20420;&#32599;&#26031;&#21943;&#23376;&#19982;&#26377;&#26426;&#29992;&#25143;&#65292;&#21487;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22312;&#24433;&#21709;&#27963;&#21160;&#20013;&#36816;&#33829;&#30340;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#26159;&#30740;&#31350;&#31038;&#21306;&#30340;&#19968;&#20010;&#20851;&#38190;&#32780;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#20854;&#24433;&#21709;&#36229;&#20986;&#20102;&#22312;&#32447;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;AI&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#26512;&#21943;&#23376;&#30340;&#20998;&#20139;&#27963;&#21160;&#24207;&#21015;&#25110;&#36712;&#36857;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#35782;&#21035;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#24080;&#25143;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#20998;&#31867;&#22120;&#23558;&#24080;&#25143;&#30340;&#36712;&#36857;&#20998;&#31867;&#20026;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#25110;&#26377;&#26426;&#30340;&#21512;&#27861;&#29992;&#25143;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#31867;&#21518;&#30340;&#36712;&#36857;&#35745;&#31639;&#19968;&#31181;&#25351;&#26631;&#65292;&#31216;&#20026;&#8220;&#21943;&#23376;&#35780;&#20998;&#8221;&#65292;&#26469;&#37327;&#21270;&#24080;&#25143;&#30340;&#34892;&#20026;&#19982;&#22269;&#23478;&#36190;&#21161;&#30340;&#21943;&#23376;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2016&#24180;&#32654;&#22269;&#24635;&#32479;&#22823;&#36873;&#26399;&#38388;&#30340;&#20420;&#32599;&#26031;&#24178;&#39044;&#27963;&#21160;&#12290;&#25105;&#20204;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#24080;&#25143;&#36712;&#36857;&#65292;AUC&#25509;&#36817;99&#65285;&#65292;&#24182;&#20934;&#30830;&#20998;&#31867;&#20420;&#32599;&#26031;&#21943;&#23376;&#21644;&#26377;&#26426;&#29992;&#25143;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.95&#21644;0.91&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#22269;&#23478;&#36190;&#21161;&#24433;&#21709;&#27963;&#21160;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#23545;&#20445;&#25252;&#27665;&#20027;&#36827;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of state-sponsored trolls operating in influence campaigns is a critical and unsolved challenge for the research community, which has significant implications beyond the online realm. To address this challenge, we propose a new AI-based solution that identifies state-sponsored troll accounts by analyzing their sharing activity sequences, or trajectories, through a two-step process. First, we classify accounts' trajectories using an LSTM-based classifier as belonging to either a state-sponsored troll or an organic, legitimate user. Second, we utilize the classified trajectories to compute a metric, named ``Troll Score'', to quantify the extent to which an account behaves like a state-sponsored troll. To evaluate our approach, we examine the Russian interference campaign during the 2016 U.S. Presidential election. The results of our experiments show that our method can identify account trajectories with an AUC close to 99% and accurately classify Russian trolls and organic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04492</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#26102;&#33258;&#36866;&#24212;&#20248;&#21270;&#23454;&#29616;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25429;&#25417;&#21644;&#22797;&#21046;&#26377;&#23475;&#20869;&#23481;&#30340;&#24773;&#20917;&#26222;&#36941;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#27602;&#24615;&#35821;&#35328;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;&#27492;&#21069;&#22312;&#36947;&#24503;&#19978;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#37117;&#26159;&#20998;&#24320;&#35299;&#20915;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#35265;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#27602;&#24615;&#65292;&#32780;&#32463;&#36807;&#21435;&#27602;&#21270;&#30340;&#27169;&#22411;&#29978;&#33267;&#20250;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23558;&#36825;&#20004;&#20010;&#38382;&#39064;&#20316;&#20026;&#32416;&#27491;&#36755;&#20986;&#31354;&#38388;&#30340;&#20851;&#38190;&#38382;&#39064;&#32852;&#21512;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35299;&#37322;&#20026;&#23398;&#20064;&#28151;&#21512;&#21152;&#26435;&#23646;&#24615;&#30340;&#25991;&#26412;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;UDDIA&#23545;&#23569;&#37327;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20248;&#21270;&#65292;&#20174;&#32780;&#25511;&#21046;&#27599;&#20010;&#23646;&#24615;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;UDDIA&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#24182;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;&#27492;&#22806;&#65292;UDDIA&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;UDDIA&#30340;&#34892;&#20026;&#65292;&#24182;&#20026;&#26410;&#26469;&#22312;&#36947;&#24503;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: this paper contains model outputs exhibiting offensiveness and biases. Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate social biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#25439;&#22833;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#38544;&#24335;&#22320;&#30830;&#20445;&#37051;&#23621;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#25239;&#22122;&#22768;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#22235;&#20010;&#22270;&#20687;&#26816;&#32034;&#22522;&#20934;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01908</link><description>&lt;p&gt;
&#30417;&#30563;&#24335;&#24230;&#37327;&#23398;&#20064;&#20197;&#25490;&#24207;&#20026;&#30446;&#26631;&#30340;&#26816;&#32034;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supervised Metric Learning to Rank for Retrieval via Contextual Similarity Optimization. (arXiv:2210.01908v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01908
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#25439;&#22833;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#38544;&#24335;&#22320;&#30830;&#20445;&#37051;&#23621;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#25239;&#22122;&#22768;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#22235;&#20010;&#22270;&#20687;&#26816;&#32034;&#22522;&#20934;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#65292;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#24456;&#22810;&#24230;&#37327;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#37117;&#38598;&#20013;&#20110;&#23545;&#35757;&#32451;&#26679;&#26412;&#30340;&#27491;&#30830;&#25490;&#24207;&#65292;&#20294;&#26159;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#19981;&#19968;&#33268;&#30340;&#35821;&#20041;&#26631;&#31614;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#25439;&#22833;&#65292;&#23427;&#38500;&#20102;&#20248;&#21270;&#20313;&#24358;&#30456;&#20284;&#24230;&#20197;&#22806;&#65292;&#36824;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#25439;&#22833;&#33021;&#38544;&#24335;&#22320;&#30830;&#20445;&#37051;&#23621;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#24182;&#25910;&#25947;&#20110;&#27491;&#30830;&#30340;&#25490;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#23545;&#26631;&#31614;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20445;&#30041;&#22823;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#19981;&#26131;&#36807;&#25311;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#22270;&#20687;&#26816;&#32034;&#22522;&#20934;&#21644;&#22810;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#29615;&#22659;&#19979;&#20197;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#24471;&#65306;https://github.com/Chris210634/metric-learning-using-contextual-similarity&#12290;
&lt;/p&gt;
&lt;p&gt;
There is extensive interest in metric learning methods for image retrieval. Many metric learning loss functions focus on learning a correct ranking of training samples, but strongly overfit semantically inconsistent labels and require a large amount of data. To address these shortcomings, we propose a new metric learning method, called contextual loss, which optimizes contextual similarity in addition to cosine similarity. Our contextual loss implicitly enforces semantic consistency among neighbors while converging to the correct ranking. We empirically show that the proposed loss is more robust to label noise, and is less prone to overfitting even when a large portion of train data is withheld. Extensive experiments demonstrate that our method achieves a new state-of-the-art across four image retrieval benchmarks and multiple different evaluation settings. Code is available at: https://github.com/Chris210634/metric-learning-using-contextual-similarity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24102;&#23574;&#23792;&#30340;PCA&#38382;&#39064;&#65292;&#21033;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#30697;&#38453;&#25277;&#21462;&#22122;&#22768;&#27169;&#22411;&#25552;&#20986;&#20102;&#35813;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#26497;&#38480;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AMP&#31639;&#27861;&#20197;&#23454;&#29616;&#20449;&#24687;&#35770;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2210.01237</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;PCA&#20013;&#30340;&#36125;&#21494;&#26031;&#26497;&#38480;&#21450;&#20854;&#23454;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayes-optimal limits in structured PCA, and how to reach them. (arXiv:2210.01237v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#23574;&#23792;&#30340;PCA&#38382;&#39064;&#65292;&#21033;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#30697;&#38453;&#25277;&#21462;&#22122;&#22768;&#27169;&#22411;&#25552;&#20986;&#20102;&#35813;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#26497;&#38480;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AMP&#31639;&#27861;&#20197;&#23454;&#29616;&#20449;&#24687;&#35770;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#22122;&#22768;&#20013;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#22914;&#20309;&#24433;&#21709;&#39640;&#32500;&#25512;&#26029;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33539;&#20363;&#24615;&#30340;&#24102;&#23574;&#23792;&#30697;&#38453;&#27169;&#22411;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#31209;&#20026;&#19968;&#30340;&#30697;&#38453;&#34987;&#21152;&#24615;&#22122;&#22768;&#27745;&#26579;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#22122;&#22768;&#39033;&#29420;&#31435;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20174;&#20302;&#38454;&#22810;&#39033;&#24335;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#20013;&#25277;&#21462;&#22122;&#22768;&#65292;&#20135;&#29983;&#30340;&#22122;&#22768;&#30456;&#20851;&#24615;&#20351;&#24471;&#35813;&#35774;&#32622;&#23545;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#30456;&#20851;&#24615;&#20294;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#26497;&#38480;&#30340;&#34920;&#24449;&#12290;&#22914;&#26524;&#23574;&#23792;&#22312;&#26059;&#36716;&#19979;&#26159;&#19981;&#21464;&#30340;&#65292;&#21017;&#26631;&#20934;&#35889;PCA&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#19968;&#33324;&#30340;&#20808;&#39564;&#65292;&#26080;&#35770;&#26159;PCA&#36824;&#26159;&#29616;&#26377;&#30340;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#65288;AMP&#65289;&#37117;&#26080;&#27861;&#36798;&#21040;&#20449;&#24687;&#35770;&#26497;&#38480;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#37325;&#22797;&#26041;&#27861;&#35745;&#31639;&#20102;&#36825;&#19968;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AMP&#31639;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#33258;&#36866;&#24212;Thouless-Anderson-Palmer&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do statistical dependencies in measurement noise influence high-dimensional inference? To answer this, we study the paradigmatic spiked matrix model of principal components analysis (PCA), where a rank-one matrix is corrupted by additive noise. We go beyond the usual independence assumption on the noise entries, by drawing the noise from a low-order polynomial orthogonal matrix ensemble. The resulting noise correlations make the setting relevant for applications but analytically challenging. We provide the first characterization of the Bayes-optimal limits of inference in this model. If the spike is rotation-invariant, we show that standard spectral PCA is optimal. However, for more general priors, both PCA and the existing approximate message passing algorithm (AMP) fall short of achieving the information-theoretic limits, which we compute using the replica method from statistical mechanics. We thus propose a novel AMP, inspired by the theory of Adaptive Thouless-Anderson-Palmer e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#26631;&#27880;&#21161;&#25163;&#24037;&#20855;XLabel&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBM&#65289;&#21644;&#21487;&#35270;&#21270;&#23637;&#29616;&#65292;&#24110;&#21161;&#21307;&#30103;&#19987;&#23478;&#26631;&#35760;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#65288;NCDs&#65289;&#30340;&#30005;&#23376;&#30149;&#21382;&#65292;&#19982;&#20854;&#20182;&#30693;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;EBM&#30340;&#20934;&#30830;&#24615;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2209.12778</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25903;&#25345;&#30340;&#30005;&#23376;&#30149;&#21382;&#26631;&#27880;&#21487;&#35270;&#21270;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Developing A Visual-Interactive Interface for Electronic Health Record Labeling: An Explainable Machine Learning Approach. (arXiv:2209.12778v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#26631;&#27880;&#21161;&#25163;&#24037;&#20855;XLabel&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBM&#65289;&#21644;&#21487;&#35270;&#21270;&#23637;&#29616;&#65292;&#24110;&#21161;&#21307;&#30103;&#19987;&#23478;&#26631;&#35760;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#65288;NCDs&#65289;&#30340;&#30005;&#23376;&#30149;&#21382;&#65292;&#19982;&#20854;&#20182;&#30693;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;EBM&#30340;&#20934;&#30830;&#24615;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#22823;&#37327;&#30005;&#23376;&#30149;&#21382;&#30340;&#24037;&#20316;&#37327;&#22823;&#19988;&#36153;&#26102;&#65292;&#25317;&#26377;&#19968;&#20010;&#26631;&#27880;&#21161;&#25163;&#24037;&#20855;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#21307;&#30103;&#19987;&#23478;&#30340;&#24037;&#20316;&#37327;&#65292;&#20294;&#20026;&#20102;&#36194;&#24471;&#19987;&#23478;&#30340;&#20449;&#20219;&#65292;&#35813;&#24037;&#20855;&#24517;&#39035;&#33021;&#22815;&#35299;&#37322;&#20854;&#32467;&#26524;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#26631;&#27880;&#21161;&#25163;&#24037;&#20855;XLabel&#65292;&#35813;&#24037;&#20855;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBM&#65289;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21487;&#35270;&#21270;EBM&#35299;&#37322;&#30340;&#28909;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;XLabel&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24110;&#21161;&#21307;&#30103;&#19987;&#23478;&#26631;&#35760;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;(NCDs)&#30340;&#30005;&#23376;&#30149;&#21382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;XLabel&#26377;&#21161;&#20110;&#20943;&#23569;&#26631;&#27880;&#25805;&#20316;&#30340;&#27425;&#25968;&#65307;2&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;EBM&#30340;&#20934;&#30830;&#24615;&#19982;&#20854;&#20182;&#30693;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#65292;&#20248;&#20110;NCD&#19987;&#23478;&#20351;&#29992;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65307;3&#65289;&#21363;&#20351;&#36229;&#36807;40%&#30340;&#35760;&#24405;&#34987;&#26377;&#24847;&#35823;&#26631;&#65292;EBM&#20173;&#33021;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling a large number of electronic health records is expensive and time consuming, and having a labeling assistant tool can significantly reduce medical experts' workload. Nevertheless, to gain the experts' trust, the tool must be able to explain the reasons behind its outputs. Motivated by this, we introduce Explainable Labeling Assistant (XLabel) a new visual-interactive tool for data labeling. At a high level, XLabel uses Explainable Boosting Machine (EBM) to classify the labels of each data point and visualizes heatmaps of EBM's explanations. As a case study, we use XLabel to help medical experts label electronic health records with four common non-communicable diseases (NCDs). Our experiments show that 1) XLabel helps reduce the number of labeling actions, 2) EBM as an explainable classifier is as accurate as other well-known machine learning models outperforms a rule-based model used by NCD experts, and 3) even when more than 40% of the records were intentionally mislabeled, E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;MaxWeight&#35843;&#24230;&#31574;&#30053;&#19982;&#25240;&#25187;&#19978;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#23398;&#20064;&#32479;&#35745;&#20449;&#24687;&#21644;&#23558;&#20316;&#19994;&#35843;&#24230;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#22312;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#23454;&#29616;&#26368;&#22823;&#21270;&#21033;&#29992;&#26381;&#21153;&#22120;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.01126</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#32479;&#35745;&#20449;&#24687;&#30340;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#23398;&#20064;&#35843;&#24230;:&#32467;&#21512;&#25240;&#25187;&#19978;&#32622;&#20449;&#24230;&#19982;MaxWeight
&lt;/p&gt;
&lt;p&gt;
Learning While Scheduling in Multi-Server Systems with Unknown Statistics: MaxWeight with Discounted UCB. (arXiv:2209.01126v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;MaxWeight&#35843;&#24230;&#31574;&#30053;&#19982;&#25240;&#25187;&#19978;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#23398;&#20064;&#32479;&#35745;&#20449;&#24687;&#21644;&#23558;&#20316;&#19994;&#35843;&#24230;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#22312;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#23454;&#29616;&#26368;&#22823;&#21270;&#21033;&#29992;&#26381;&#21153;&#22120;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26381;&#21153;&#22120;&#38431;&#21015;&#31995;&#32479;&#26159;&#26426;&#22120;&#23398;&#20064;&#12289;&#26080;&#32447;&#32593;&#32476;&#12289;&#20247;&#21253;&#21644;&#21307;&#30103;&#31995;&#32479;&#20013;&#20316;&#19994;&#35843;&#24230;&#30340;&#24191;&#27867;&#27169;&#22411;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#26381;&#21153;&#22120;&#22810;&#20316;&#19994;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#19981;&#30693;&#36947;&#22788;&#29702;&#26102;&#38388;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#30446;&#26631;&#26159;&#22312;&#19981;&#30693;&#36947;&#22788;&#29702;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26381;&#21153;&#22120;&#19978;&#23433;&#25490;&#20316;&#19994;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#26381;&#21153;&#22120;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#33267;&#23569;&#38656;&#35201;&#23398;&#20064;&#19981;&#21516;&#31867;&#22411;&#20316;&#19994;&#22312;&#19981;&#21516;&#26381;&#21153;&#22120;&#19978;&#30340;&#26381;&#21153;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;MaxWeight&#35843;&#24230;&#31574;&#30053;&#19982;&#25240;&#25187;&#19978;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#23398;&#20064;&#32479;&#35745;&#20449;&#24687;&#21644;&#23558;&#20316;&#19994;&#35843;&#24230;&#21040;&#26381;&#21153;&#22120;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#31639;&#27861;&#19979;&#65292;&#28176;&#36817;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-server queueing systems are widely used models for job scheduling in machine learning, wireless networks, crowdsourcing, and healthcare systems. This paper considers a multi-server system with multiple servers and multiple types of jobs, where different job types require different amounts of processing time at different servers. The goal is to schedule jobs on servers without knowing the statistics of the processing times. To fully utilize the processing power of the servers, it is known that one has to at least learn the service rates of different job types on different servers. Prior works on this topic decouple the learning and scheduling phases which leads to either excessive exploration or extremely large job delays. We propose a new algorithm, which combines the MaxWeight scheduling policy with discounted upper confidence bound (UCB), to simultaneously learn the statistics and schedule jobs to servers. We prove that under our algorithm the asymptotic average queue length is
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26377;&#21161;&#20110;&#25913;&#36827;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#27169;&#25311;&#20013;&#23384;&#22312;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2208.13825</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#29992;&#20110;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentiable Programming for Earth System Modeling. (arXiv:2208.13825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13825
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26377;&#21161;&#20110;&#25913;&#36827;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#27169;&#25311;&#20013;&#23384;&#22312;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#26159;&#30740;&#31350;&#26410;&#26469;&#20960;&#21313;&#24180;&#21040;&#20960;&#20010;&#19990;&#32426;&#30340;&#22320;&#29699;&#31995;&#32479;&#29366;&#24577;&#30340;&#20027;&#35201;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#28201;&#23460;&#27668;&#20307;&#37322;&#25918;&#30340;&#24433;&#21709;&#19979;&#12290;&#26368;&#20808;&#36827;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#21487;&#20197;&#37325;&#29616;&#36807;&#21435;150&#24180;&#30340;&#35266;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#20851;&#20110;(i)&#27668;&#20505;&#25935;&#24863;&#24230;&#20272;&#35745;&#30340;&#22823;&#33539;&#22260;&#24046;&#24322;&#65292;&#21363;&#28201;&#23460;&#27668;&#20307;&#22686;&#21152;&#30340;&#28201;&#24230;&#21709;&#24212;&#65307;(ii)&#20851;&#38190;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#21644;&#38477;&#27700;&#65289;&#30340;&#31354;&#38388;&#27169;&#24335;&#65307;(iii)&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#34920;&#31034;&#65307;&#20197;&#21450;(iv)&#22810;&#31283;&#24577;&#22320;&#29699;&#31995;&#32479;&#32452;&#20214;&#30340;&#34920;&#31034;&#21450;&#20854;&#39044;&#27979;&#30456;&#20851;&#31361;&#21464;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20351;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#33258;&#21160;&#21487;&#24494;&#20998;&#20855;&#26377;&#25512;&#21160;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#20851;&#38190;&#32570;&#38519;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#33258;&#21160;&#21487;&#24494;&#20998;&#21487;&#20197;&#20801;&#35768;&#23458;&#35266;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth System Models (ESMs) are the primary tools for investigating future Earth system states at time scales from decades to centuries, especially in response to anthropogenic greenhouse gas release. State-of-the-art ESMs can reproduce the observational global mean temperature anomalies of the last 150 years. Nevertheless, ESMs need further improvements, most importantly regarding (i) the large spread in their estimates of climate sensitivity, i.e., the temperature response to increases in atmospheric greenhouse gases, (ii) the modeled spatial patterns of key variables such as temperature and precipitation, (iii) their representation of extreme weather events, and (iv) their representation of multistable Earth system components and their ability to predict associated abrupt transitions. Here, we argue that making ESMs automatically differentiable has huge potential to advance ESMs, especially with respect to these key shortcomings. First, automatic differentiability would allow objecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38598;&#21512;&#32534;&#30721;&#26041;&#27861;UMBC&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#38750;MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2208.12401</link><description>&lt;p&gt;
&#20855;&#26377;&#36890;&#29992;&#36855;&#20320;&#25209;&#37327;&#19968;&#33268;&#24615;&#21644;&#26080;&#20559;&#23436;&#20840;&#38598;&#21512;&#26799;&#24230;&#36817;&#20284;&#30340;&#21487;&#25193;&#23637;&#38598;&#21512;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation. (arXiv:2208.12401v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38598;&#21512;&#32534;&#30721;&#26041;&#27861;UMBC&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#38750;MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20851;&#20110;&#38598;&#21512;&#20989;&#25968;&#30340;&#23567;&#25209;&#37327;&#19968;&#33268;&#24615;(MBC)&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#20445;&#35777;&#23558;&#19968;&#20010;&#20998;&#21106;&#30340;&#38598;&#21512;&#30340;&#37096;&#20998;&#39034;&#24207;&#22788;&#29702;&#21644;&#32858;&#21512;&#65292;&#32780;&#20445;&#35777;&#25152;&#26377;&#20998;&#21106;&#30340;&#36755;&#20986;&#30456;&#21516;&#30340;&#38656;&#27714;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MBC&#26550;&#26500;&#30340;&#38480;&#21046;&#23548;&#33268;&#20102;&#20855;&#26377;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#35299;&#20915;&#22312;&#38656;&#35201;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#22788;&#29702;&#35757;&#32451;&#20013;&#30340;&#22823;&#22411;&#38598;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#20219;&#24847;&#38750;-MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;MBC (UMBC) &#31867;&#38598;&#21512;&#20989;&#25968;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65292;&#20351;&#24471;MBC&#35774;&#32622;&#20013;&#21487;&#20197;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#21151;&#33021;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#23436;&#25104;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#26080;&#30417;&#30563;&#32858;&#31867;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on mini-batch consistency (MBC) for set functions has brought attention to the need for sequentially processing and aggregating chunks of a partitioned set while guaranteeing the same output for all partitions. However, existing constraints on MBC architectures lead to models with limited expressive power. Additionally, prior work has not addressed how to deal with large sets during training when the full set gradient is required. To address these issues, we propose a Universally MBC (UMBC) class of set functions which can be used in conjunction with arbitrary non-MBC components while still satisfying MBC, enabling a wider range of function classes to be used in MBC settings. Furthermore, we propose an efficient MBC training algorithm which gives an unbiased approximation of the full set gradient and has a constant memory overhead for any set size for both train- and test-time. We conduct extensive experiments including image completion, text classification, unsupervised cl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#31639;&#27861;&#65292;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#12290;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#21464;&#37327;&#26679;&#26412;&#20351;&#24471;&#36807;&#21435;&#30340;&#20915;&#31574;&#22312;&#22238;&#28335;&#20013;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#65292;&#22312;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.06272</link><description>&lt;p&gt;
&#24102;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#30340;&#36861;&#28335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight Learning for MDPs with Exogenous Inputs. (arXiv:2207.06272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06272
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#31639;&#27861;&#65292;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#12290;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#21464;&#37327;&#26679;&#26412;&#20351;&#24471;&#36807;&#21435;&#30340;&#20915;&#31574;&#22312;&#22238;&#28335;&#20013;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#65292;&#22312;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20570;&#20986;&#36845;&#20195;&#20915;&#31574;&#65292;&#20854;&#20013;&#24433;&#21709;&#20915;&#31574;&#32467;&#26524;&#30340;&#21807;&#19968;&#19981;&#30830;&#23450;&#24615;&#26159;&#20915;&#31574;&#32773;&#25511;&#21046;&#20043;&#22806;&#30340;&#22806;&#37096;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#22806;&#37096;&#36755;&#20837;&#30340;MDPs&#65288;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31867;&#21517;&#20026;&#36861;&#28335;&#23398;&#20064;&#65288;HL&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;HL&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20851;&#38190;&#27934;&#35265;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#65306;&#36890;&#36807;&#22806;&#37096;&#21464;&#37327;&#30340;&#26679;&#26412;&#65292;&#36807;&#21435;&#30340;&#20915;&#31574;&#21487;&#20197;&#22312;&#22238;&#28335;&#20013;&#37325;&#26032;&#23457;&#35270;&#65292;&#20197;&#25512;&#26029;&#20986;&#21487;&#20197;&#21152;&#36895;&#31574;&#30053;&#25913;&#36827;&#30340;&#21453;&#20107;&#23454;&#21518;&#26524;&#12290;&#25105;&#20204;&#23558;HL&#19982;&#22810;&#20010;&#22522;&#32447;&#31639;&#27861;&#22312;&#22810;&#20010;&#27979;&#35797;&#26696;&#20363;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;&#22810;&#31192;&#20070;&#21644;&#33322;&#31354;&#20844;&#21496;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#19994;&#21153;&#20851;&#38190;&#30340;&#20113;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#8212;&#8212;&#23558;&#34394;&#25311;&#26426;&#65288;VM&#65289;&#20998;&#37197;&#21040;&#29289;&#29702;&#26426;&#22120;&#19978;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#22823;&#22411;&#20844;&#20849;&#20113;&#25552;&#20379;&#21830;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#27169;&#25311;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;HL&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting the decision outcomes are exogenous variables outside the control of the decision-maker. We model these problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms for them termed Hindsight Learning (HL). Our HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We compare HL against classic baselines in the multi-secretary and airline revenue management problems. We also scale our algorithms to a business-critical cloud resource management problem -- allocating Virtual Machines (VMs) to physical machines, and simulate their performance with real datasets from a large public cloud provider. We find that HL algorithms outperform d
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#34987;&#21098;&#26525;&#30340;&#20887;&#20313;&#26631;&#35760;&#30001;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#36880;&#27493;&#21160;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2207.01580</link><description>&lt;p&gt;
&#21160;&#24577;&#31354;&#38388;&#31232;&#30095;&#21270;&#65306;&#39640;&#25928;&#35270;&#35273;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01580
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#34987;&#21098;&#26525;&#30340;&#20887;&#20313;&#26631;&#35760;&#30001;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#36880;&#27493;&#21160;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#21152;&#36895;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#26368;&#32456;&#39044;&#27979;&#20165;&#22522;&#20110;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#26368;&#20016;&#23500;&#30340;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#30340;&#22270;&#20687;&#35782;&#21035;&#36275;&#22815;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#26681;&#25454;&#36755;&#20837;&#36880;&#27493;&#21644;&#21160;&#24577;&#22320;&#21098;&#26525;&#20887;&#20313;&#26631;&#35760;&#20197;&#21152;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#65292;&#20197;&#26681;&#25454;&#24403;&#21069;&#29305;&#24449;&#26469;&#20272;&#35745;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#27169;&#22359;&#34987;&#28155;&#21152;&#21040;&#19981;&#21516;&#23618;&#20013;&#20197;&#20998;&#23618;&#22320;&#21098;&#26525;&#20887;&#20313;&#26631;&#35760;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#26469;&#28304;&#20110;&#25105;&#20204;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#31232;&#30095;&#27880;&#24847;&#21147;&#30340;&#35266;&#23519;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#33258;&#36866;&#24212;&#21644;&#38750;&#23545;&#31216;&#35745;&#31639;&#30340;&#24605;&#24819;&#21487;&#20197;&#25104;&#20026;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#19968;&#33324;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#35270;&#35273;&#21464;&#21387;&#22120;&#22312;&#20869;&#30340;&#20998;&#23618;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers 
&lt;/p&gt;</description></item><item><title>D-Struct&#26159;&#19968;&#31181;&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#32467;&#26500;&#21487;&#20197;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#20256;&#36755;&#65292;&#27604;NOTEARS&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.06354</link><description>&lt;p&gt;
&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentiable and Transportable Structure Learning. (arXiv:2206.06354v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06354
&lt;/p&gt;
&lt;p&gt;
D-Struct&#26159;&#19968;&#31181;&#21487;&#24494;&#21644;&#21487;&#20256;&#36755;&#30340;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#32467;&#26500;&#21487;&#20197;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#20256;&#36755;&#65292;&#27604;NOTEARS&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#22312;&#23427;&#20204;&#30340;&#32467;&#26500;&#20013;&#32534;&#30721;&#20102;&#20851;&#20110;&#29305;&#23450;&#20998;&#24067;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#36825;&#20123;&#32467;&#26500;&#25152;&#38656;&#30340;&#35745;&#31639;&#36890;&#24120;&#26159;&#21464;&#37327;&#25968;&#30340;&#36229;&#25351;&#25968;&#65292;&#22240;&#20026;&#25512;&#26029;&#38656;&#35201;&#25195;&#25551;&#19968;&#20010;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#30340;&#28508;&#22312;&#32467;&#26500;&#31354;&#38388;&#12290;&#30452;&#21040;&#26368;&#36817;&#30340;&#36827;&#23637;&#25165;&#20351;&#24471;&#20351;&#29992;&#21487;&#24494;&#24230;&#37327;&#25628;&#32034;&#36825;&#20010;&#31354;&#38388;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25628;&#32034;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;D-Struct&#65292;&#23427;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#24674;&#22797;&#20102;&#21457;&#29616;&#32467;&#26500;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#20256;&#36755;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#23436;&#20840;&#21487;&#24494;&#12290;&#22240;&#20026;D-Struct&#20173;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#21487;&#24494;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed acyclic graphs (DAGs) encode a lot of information about a particular distribution in their structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique -- named NOTEARS -- is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. We introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function while remaining fully differentiable. Because D-Struct remains differentiable, our method can be easily adopted in existing different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#21453;&#24605;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20986;&#30340;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#24182;&#35797;&#22270;&#23558;&#20854;&#19982;&#36947;&#24503;&#21644;&#25919;&#27835;&#21746;&#23398;&#20013;&#30340;&#35770;&#28857;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#32771;&#34385;&#21040;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#25152;&#28041;&#21450;&#30340;&#26174;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#30340;&#24046;&#24322;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#22270;&#65292;&#23427;&#21253;&#25324;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12289;&#39044;&#27979;&#32467;&#26524;&#21644;&#24341;&#21457;&#24433;&#21709;&#30340;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#35843;&#26597;&#30340;&#38544;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04101</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#29616;&#29366;&#12289;&#21453;&#24605;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
What-is and How-to for Fairness in Machine Learning: A Survey, Reflection, and Perspective. (arXiv:2206.04101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#21453;&#24605;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20986;&#30340;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#24182;&#35797;&#22270;&#23558;&#20854;&#19982;&#36947;&#24503;&#21644;&#25919;&#27835;&#21746;&#23398;&#20013;&#30340;&#35770;&#28857;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#32771;&#34385;&#21040;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#25152;&#28041;&#21450;&#30340;&#26174;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#30340;&#24046;&#24322;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#22270;&#65292;&#23427;&#21253;&#25324;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12289;&#39044;&#27979;&#32467;&#26524;&#21644;&#24341;&#21457;&#24433;&#21709;&#30340;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#35843;&#26597;&#30340;&#38544;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#28165;&#26224;&#30340;&#35299;&#20915;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#21644;&#21453;&#24605;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20986;&#30340;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#24182;&#35797;&#22270;&#23558;&#20854;&#19982;&#36947;&#24503;&#21644;&#25919;&#27835;&#21746;&#23398;&#20013;&#30340;&#35770;&#28857;&#32852;&#31995;&#36215;&#26469;&#65292;&#29305;&#21035;&#26159;&#20844;&#27491;&#29702;&#35770;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#20174;&#21160;&#24577;&#30340;&#35282;&#24230;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#32771;&#34385;&#24403;&#21069;&#39044;&#27979;&#21644;&#20915;&#31574;&#25152;&#24341;&#21457;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#21040;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#25152;&#28041;&#21450;&#30340;&#26174;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#30340;&#24046;&#24322;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#22270;&#65292;&#23427;&#21253;&#25324;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12289;&#39044;&#27979;&#32467;&#26524;&#21644;&#24341;&#21457;&#24433;&#21709;&#30340;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#35843;&#26597;&#30340;&#38544;&#24615;&#20551;&#35774;&#21644;&#39044;&#26399;&#32467;&#26524;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21305;&#37197;&#20351;&#21629;&#65288;&#21363;&#24076;&#26395;&#23454;&#29616;&#21738;&#31181;&#20844;&#24179;&#24615;&#65289;&#19982;&#20844;&#24179;&#24615;&#25514;&#26045;&#20043;&#38388;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness has attracted increasing attention in the machine learning community. Various definitions are proposed in the literature, but the differences and connections among them are not clearly addressed. In this paper, we review and reflect on various fairness notions previously proposed in machine learning literature, and make an attempt to draw connections to arguments in moral and political philosophy, especially theories of justice. We also consider fairness inquiries from a dynamic perspective, and further consider the long-term impact that is induced by current prediction and decision. In light of the differences in the characterized fairness, we present a flowchart that encompasses implicit assumptions and expected outcomes of different types of fairness inquiries on the data generating process, on the predicted outcome, and on the induced impact, respectively. This paper demonstrates the importance of matching the mission (which kind of fairness one would like to e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.03861</link><description>&lt;p&gt;
&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Regularized Learning Over Random Time-Varying Graphs. (arXiv:2206.03861v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#37117;&#36816;&#34892;&#19968;&#20010;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#21019;&#26032;&#39033;&#65288;&#22788;&#29702;&#33258;&#36523;&#26032;&#27979;&#37327;&#20540;&#65289;&#12289;&#20849;&#35782;&#39033;&#65288;&#21152;&#26435;&#24179;&#22343;&#33258;&#36523;&#21450;&#20854;&#37051;&#23621;&#30340;&#20272;&#35745;&#65292;&#24102;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#36890;&#20449;&#22122;&#22768;&#65289;&#21644;&#27491;&#21017;&#21270;&#39033;&#65288;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65289;&#12290;&#19981;&#35201;&#27714;&#22238;&#24402;&#30697;&#38453;&#21644;&#22270;&#28385;&#36275;&#29305;&#27530;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#22914;&#30456;&#20114;&#29420;&#31435;&#12289;&#26102;&#31354;&#29420;&#31435;&#25110;&#24179;&#31283;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#22914;&#26524;&#31639;&#27861;&#22686;&#30410;&#12289;&#22270;&#21644;&#22238;&#24402;&#30697;&#38453;&#20849;&#21516;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#22320;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#22686;&#30410;&#65292;&#35813;&#26465;&#20214;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the decentralized online regularized linear regression algorithm over random time-varying graphs. At each time step, every node runs an online estimation algorithm consisting of an innovation term processing its own new measurement, a consensus term taking a weighted sum of estimations of its own and its neighbors with additive and multiplicative communication noises and a regularization term preventing over-fitting. It is not required that the regression matrices and graphs satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity. We develop the nonnegative supermartingale inequality of the estimation error, and prove that the estimations of all nodes converge to the unknown true parameter vector almost surely if the algorithm gains, graphs and regression matrices jointly satisfy the sample path spatio-temporal persistence of excitation condition. Especially, this condition holds by choosing appropriate algorithm gains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.03317</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#20307;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Subject Membership Inference Attacks in Federated Learning. (arXiv:2206.03317v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#25915;&#20987;&#36890;&#24120;&#30528;&#37325;&#20110;&#25512;&#35770;&#35757;&#32451;&#25968;&#25454;&#20013;&#29305;&#23450;&#25968;&#25454;&#28857;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#30495;&#27491;&#24819;&#30693;&#36947;&#30340;&#26159;&#29305;&#23450;&#20010;&#20307;&#30340;&#65288;&#20027;&#20307;&#30340;&#65289;&#25968;&#25454;&#26159;&#21542;&#21253;&#21547;&#22312;&#35757;&#32451;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#26356;&#21487;&#33021;&#25317;&#26377;&#29305;&#23450;&#20027;&#20307;&#20998;&#24067;&#32780;&#38750;&#23454;&#38469;&#35760;&#24405;&#12290;&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy attacks on Machine Learning (ML) models often focus on inferring the existence of particular data points in the training data. However, what the adversary really wants to know is if a particular individual's (subject's) data was included during training. In such scenarios, the adversary is more likely to have access to the distribution of a particular subject than actual records. Furthermore, in settings like cross-silo Federated Learning (FL), a subject's data can be embodied by multiple data records that are spread across multiple organizations. Nearly all of the existing private FL literature is dedicated to studying privacy at two granularities -- item-level (individual data records), and user-level (participating user in the federation), neither of which apply to data subjects in cross-silo FL. This insight motivates us to shift our attention from the privacy of data records to the privacy of data subjects, also known as subject-level privacy. We propose two novel black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#38750;&#32447;&#24615;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#20248;&#38597;&#30340;&#22522;&#20110;&#20960;&#20309;&#30340;&#20844;&#24335;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#24605;&#24819;&#20197;&#21450;&#26377;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#27493;&#39588;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.14977</link><description>&lt;p&gt;
&#24555;&#36895;&#38750;&#32447;&#24615;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Fast Nonlinear Vector Quantile Regression. (arXiv:2205.14977v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#38750;&#32447;&#24615;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#20248;&#38597;&#30340;&#22522;&#20110;&#20960;&#20309;&#30340;&#20844;&#24335;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#24605;&#24819;&#20197;&#21450;&#26377;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#27493;&#39588;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20301;&#25968;&#22238;&#24402;&#26159;&#20272;&#35745;&#32473;&#23450;&#35299;&#37322;&#29305;&#24449;X&#30340;&#30446;&#26631;&#21464;&#37327;Y&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;QR&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#65292;&#30001;&#20110;&#20854;&#30446;&#26631;&#20989;&#25968;&#30340;&#21046;&#23450;&#65292;&#20165;&#23545;&#26631;&#37327;&#30446;&#26631;&#21464;&#37327;&#36827;&#34892;&#23450;&#20041;&#65292;&#30001;&#20110;&#37327;&#23376;&#30340;&#27010;&#24565;&#22312;&#22810;&#20803;&#20998;&#24067;&#20013;&#27809;&#26377;&#26631;&#20934;&#23450;&#20041;&#12290;&#26368;&#36817;&#65292;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;(VQR)&#34987;&#25552;&#20986;&#20316;&#20026;QR&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#29992;&#20110;&#21521;&#37327;&#20540;&#30446;&#26631;&#21464;&#37327;&#65292;&#24471;&#30410;&#20110;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23545;&#22810;&#20803;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#27010;&#24565;&#30340;&#26377;&#24847;&#20041;&#30340;&#27010;&#25324;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#38750;&#32447;&#24615;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;(Fast-NVQR)&#65292;&#19968;&#31181;&#22522;&#20110;&#37327;&#20301;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;Fast-NVQR&#20445;&#30041;&#20102;VQR&#30340;&#20248;&#38597;&#30340;&#22522;&#20110;&#20960;&#20309;&#30340;&#20844;&#24335;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#24605;&#24819;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21644;&#25191;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Fast-NVQR&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\mathrm{Y}$ given explanatory features $\boldsymbol{\mathrm{X}}$. A limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions. Recently, vector quantile regression (VQR) was proposed as an extension of QR for vector-valued target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions via optimal transport. Despite its elegance, VQR is arguably not applicable in practice due to several limitations: (i) it assumes a linear model for the quantiles of the target $\boldsymbol{\mathrm{Y}}$ given the features $\boldsymbol{\mathrm{X}}$; (ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#19982;&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;SNE&#65289;&#30340;&#32852;&#31995;&#65292;SSCL&#23454;&#38469;&#19978;&#26159;SNE&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36890;&#36807;SNE&#30340;&#35270;&#35282;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39046;&#22495;&#26080;&#20851;&#22686;&#24191;&#12289;&#38544;&#24335;&#20559;&#24046;&#21644;&#23398;&#21040;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#30340;&#26032;&#20998;&#26512;&#21644;&#23454;&#29992;&#25351;&#21335;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;SSCL&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2205.14814</link><description>&lt;p&gt;
&#24744;&#30340;&#23545;&#27604;&#23398;&#20064;&#20854;&#23454;&#26159;&#22312;&#20570;&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. (arXiv:2205.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#19982;&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;SNE&#65289;&#30340;&#32852;&#31995;&#65292;SSCL&#23454;&#38469;&#19978;&#26159;SNE&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36890;&#36807;SNE&#30340;&#35270;&#35282;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39046;&#22495;&#26080;&#20851;&#22686;&#24191;&#12289;&#38544;&#24335;&#20559;&#24046;&#21644;&#23398;&#21040;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#30340;&#26032;&#20998;&#26512;&#21644;&#23454;&#29992;&#25351;&#21335;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;SSCL&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SSCL&#65289;&#22312;&#20174;&#38750;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;SSCL&#21644;&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;SNE&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21363;&#20174;&#20445;&#30041;&#37051;&#36817;&#20449;&#24687;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;SSCL&#21487;&#20197;&#34987;&#35270;&#20026;SNE&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#20854;&#36755;&#20837;&#31354;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#30001;&#25968;&#25454;&#22686;&#24378;&#25351;&#23450;&#12290;&#36890;&#36807;SNE&#30340;&#35270;&#35282;&#65292;&#23545;&#39046;&#22495;&#26080;&#20851;&#22686;&#24191;&#12289;&#38544;&#24335;&#20559;&#24046;&#21644;&#23398;&#21040;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29992;&#25351;&#21335;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;SNE&#21040;t-SNE&#30340;&#20462;&#25913;&#23545;SSCL&#20855;&#26377;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#65292;&#19982;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13602</link><description>&lt;p&gt;
&#21033;&#29992;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;&#36827;&#34892;&#27969;&#34892;&#30149;&#20998;&#21306;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#19988;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods. (arXiv:2205.13602v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13602
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#65292;&#19982;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#27969;&#34892;&#30149;&#23398;&#25512;&#29702;&#21521;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#27169;&#22411;&#30340;&#25193;&#23637;&#38590;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27850;&#26494;&#36817;&#20284;&#20284;&#28982;(PAL)&#26041;&#27861;&#12290;&#19982;&#37319;&#29992;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#26469;&#28608;&#21169;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;ODE&#26041;&#27861;&#19981;&#21516;&#65292;PAL&#26159;&#20174;&#26377;&#38480;&#20154;&#21475;&#25968;&#37327;&#30340;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#30340;&#36817;&#20284;&#28388;&#27874;&#26041;&#31243;&#23548;&#20986;&#30340;&#65292;&#24182;&#19988;&#22823;&#20154;&#21475;&#25968;&#37327;&#38480;&#21046;&#25512;&#21160;&#26368;&#22823;PAL&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20284;&#20046;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#31867;&#21035;&#30340;&#37096;&#20998;&#35266;&#23519;&#38543;&#26426;&#20998;&#21306;&#27169;&#22411;&#21644;&#35299;&#20915;&#22823;&#20154;&#21475;&#25968;&#37327;&#23616;&#38480;&#24615;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#21442;&#25968;&#20272;&#35745;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;PALs&#23454;&#29616;&#31616;&#21333;&#65292;&#20165;&#28041;&#21450;&#22522;&#26412;&#31639;&#26415;&#25805;&#20316;&#21644;&#27809;&#26377;&#35843;&#25972;&#21442;&#25968;&#65292;&#19988;&#35780;&#20272;&#36895;&#24230;&#24555;&#65292;&#19981;&#38656;&#35201;&#20174;&#27169;&#22411;&#36827;&#34892;&#27169;&#25311;&#65292;&#35745;&#31639;&#25104;&#26412;&#19982;&#20154;&#21475;&#35268;&#27169;&#26080;&#20851;&#12290;&#36890;&#36807;&#31034;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;PALs
&lt;/p&gt;
&lt;p&gt;
Addressing the challenge of scaling-up epidemiological inference to complex and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL) methods. In contrast to the popular ODE approach to compartmental modelling, in which a large population limit is used to motivate a deterministic model, PALs are derived from approximate filtering equations for finite-population, stochastic compartmental models, and the large population limit drives consistency of maximum PAL estimators. Our theoretical results appear to be the first likelihood-based parameter estimation consistency results which apply to a broad class of partially observed stochastic compartmental models and address the large population limit. PALs are simple to implement, involving only elementary arithmetic operations and no tuning parameters, and fast to evaluate, requiring no simulation from the model and having computational cost independent of population size. Through examples we demonstrate how PALs can be used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#27169;&#22411;&#21644;Schur&#30697;&#38453;&#29305;&#24615;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#34920;&#26684;&#22411;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#65292;&#33021;&#22815;&#32479;&#19968;&#22320;&#35206;&#30422;&#22522;&#20110;&#31574;&#30053;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2204.10479</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65306;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of Temporal Difference Learning: Discrete-Time Linear System Perspective. (arXiv:2204.10479v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#27169;&#22411;&#21644;Schur&#30697;&#38453;&#29305;&#24615;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#34920;&#26684;&#22411;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#65292;&#33021;&#22815;&#32479;&#19968;&#22320;&#35206;&#30422;&#22522;&#20110;&#31574;&#30053;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD-learning&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#31181;&#22522;&#26412;&#30340;&#31639;&#27861;&#65292;&#23427;&#34987;&#29992;&#20110;&#36890;&#36807;&#20272;&#35745;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#30456;&#24212;&#20215;&#20540;&#20989;&#25968;&#26469;&#35780;&#20272;&#32473;&#23450;&#31574;&#30053;&#12290;&#23613;&#31649;TD-learning&#30340;&#29702;&#35770;&#20998;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#23450;&#26469;&#25581;&#31034;&#20102;&#20854;&#32479;&#35745;&#25928;&#29575;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#22411;&#26102;&#24207;&#24046;&#20998;(TD)&#23398;&#20064;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#30452;&#25509;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#27169;&#22411;&#24182;&#21033;&#29992;Schur&#30697;&#38453;&#29305;&#24615;&#65292;&#20026;&#29616;&#26377;&#30693;&#35782;&#36129;&#29486;&#26032;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#32479;&#19968;&#22320;&#35206;&#30422;&#22522;&#20110;&#31574;&#30053;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#12289;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#19981;&#20165;&#21487;&#20197;&#36827;&#19968;&#27493;&#38416;&#26126;TD-learning&#21644;&#30456;&#20851;RL&#31639;&#27861;&#30340;&#20998;&#26512;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
TD-learning is a fundamental algorithm in the field of reinforcement learning (RL), that is employed to evaluate a given policy by estimating the corresponding value function for a Markov decision process. While significant progress has been made in the theoretical analysis of TD-learning, recent research has uncovered guarantees concerning its statistical efficiency by developing finite-time error bounds. This paper aims to contribute to the existing body of knowledge by presenting a novel finite-time analysis of tabular temporal difference (TD) learning, which makes direct and effective use of discrete-time stochastic linear system models and leverages Schur matrix properties. The proposed analysis can cover both on-policy and off-policy settings in a unified manner. By adopting this approach, we hope to offer new and straightforward templates that not only shed further light on the analysis of TD-learning and related RL algorithms but also provide valuable insights for future resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Object Preference Adaptation (OPA)&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#38024;&#23545;&#29305;&#23450;&#29289;&#20307;&#30340;&#21453;&#39304;&#65292;&#22312;&#21333;&#27425;&#24178;&#39044;&#21518;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.04951</link><description>&lt;p&gt;
&#20174;&#20154;&#30340;&#29289;&#29702;&#21453;&#39304;&#20013;&#23398;&#20064;&#65306;&#19968;&#31181;&#38754;&#21521;&#29289;&#20307;&#30340;&#21333;&#27425;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Object Preference Adaptation (OPA)&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#38024;&#23545;&#29305;&#23450;&#29289;&#20307;&#30340;&#21453;&#39304;&#65292;&#22312;&#21333;&#27425;&#24178;&#39044;&#21518;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#22312;&#24178;&#39044;&#36807;&#31243;&#20013;&#34920;&#36798;&#30340;&#21453;&#39304;&#12290;&#36825;&#21487;&#20197;&#32416;&#27491;&#19981;&#33391;&#34892;&#20026;&#25110;&#25351;&#20986;&#20854;&#20182;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#22797;&#30340;&#20132;&#20114;&#22238;&#21512;&#65292;&#35201;&#20040;&#20551;&#23450;&#20808;&#21069;&#24050;&#30693;&#30340;&#22870;&#21169;&#29305;&#24449;&#65292;&#36825;&#26159;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#24182;&#19988;&#24456;&#38590;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26415;&#35821;&#19978;&#23558;&#20154;&#31867;&#20219;&#21153;&#25551;&#36848;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#29289;&#29702;&#24178;&#39044;&#35299;&#37322;&#20026;&#19982;&#29305;&#23450;&#29289;&#20307;&#30340;&#20851;&#31995;&#26469;&#25918;&#26494;&#36825;&#20123;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Object Preference Adaptation (OPA)&#65292;&#30001;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#32452;&#25104;&#65306;1&#65289;&#39044;&#35757;&#32451;&#22522;&#30784;&#31574;&#30053;&#20197;&#20135;&#29983;&#21508;&#31181;&#34892;&#20026;&#65292;&#20197;&#21450;2&#65289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#22312;&#32447;&#26356;&#26032;&#12290;&#25105;&#20204;&#24555;&#36895;&#32780;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#30340;&#20851;&#38190;&#22312;&#20110;&#22266;&#23450;&#20102;&#20195;&#29702;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#19968;&#33324;&#20132;&#20114;&#21160;&#24577;&#65292;&#21482;&#26356;&#26032;&#29305;&#23450;&#20110;&#29289;&#20307;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#22312;&#32447;&#36827;&#34892;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve
&lt;/p&gt;</description></item><item><title>&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#28789;&#27963;&#12289;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#35268;&#33539;&#21644;&#29983;&#25104;&#65292;&#21487;&#24110;&#21161;&#25913;&#36827;RL&#20013;&#30340;&#38646;-shot&#27867;&#21270;&#65292;&#38656;&#35201;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.04500</link><description>&lt;p&gt;
&#24773;&#22659;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;--&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contextualize Me -- The Case for Context in Reinforcement Learning. (arXiv:2202.04500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04500
&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#28789;&#27963;&#12289;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#35268;&#33539;&#21644;&#29983;&#25104;&#65292;&#21487;&#24110;&#21161;&#25913;&#36827;RL&#20013;&#30340;&#38646;-shot&#27867;&#21270;&#65292;&#38656;&#35201;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#35768;&#22810;&#31639;&#27861;&#20173;&#28982;&#23545;&#21363;&#20351;&#24494;&#23567;&#30340;&#29615;&#22659;&#21464;&#21270;&#20063;&#38750;&#24120;&#33030;&#24369;&#12290;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#65288;cRL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28789;&#27963;&#12289;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#35268;&#33539;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;cRL&#26694;&#26550;&#22914;&#20309;&#36890;&#36807;&#26377;&#24847;&#20041;&#30340;&#22522;&#20934;&#21644;&#20851;&#20110;&#27867;&#21270;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#20026;&#25913;&#36827;RL&#20013;&#30340;&#38646;-shot&#27867;&#21270;&#36129;&#29486;&#12290;&#25105;&#20204;&#30830;&#35748;&#22312;cRL&#20013;&#26368;&#20248;&#34892;&#20026;&#38656;&#35201;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#27934;&#23519;&#21147;&#65292;&#23601;&#20687;&#20854;&#20182;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#39046;&#22495;&#19968;&#26679;&#12290;&#20026;&#20102;&#22312;cRL&#26694;&#26550;&#20013;&#20174;&#23454;&#35777;&#19978;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24120;&#35265;RL&#29615;&#22659;&#30340;&#22810;&#31181;&#24773;&#22659;&#25193;&#23637;&#29256;&#26412;&#12290;&#23427;&#20204;&#26159;&#39318;&#20010;&#22522;&#20934;&#24211;CARL&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#24211;&#26088;&#22312;&#22522;&#20110;cRL&#25193;&#23637;&#30340;&#26222;&#36941;&#22522;&#20934;&#36827;&#34892;&#27867;&#21270;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Reinforcement Learning ( RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26368;&#22823;&#29109;&#21644;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;MEEP&#65289;&#65292;&#36890;&#36807;&#24809;&#32602;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#26469;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.12218</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#26368;&#22823;&#29109;(MEEP)&#65306;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation. (arXiv:2112.12218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26368;&#22823;&#29109;&#21644;&#38169;&#35823;&#39044;&#27979;&#19978;&#20351;&#29992;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;MEEP&#65289;&#65292;&#36890;&#36807;&#24809;&#32602;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#26469;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#29978;&#33267;&#22312;&#39640;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#23548;&#33268;&#27169;&#22411;&#26657;&#20934;&#19981;&#33391;&#12289;&#19981;&#21487;&#38752;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38169;&#35823;&#39044;&#27979;&#65288;MEEP&#65289;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#20998;&#21106;&#32593;&#32476;&#65292;&#23427;&#26377;&#36873;&#25321;&#22320;&#23545;&#33258;&#20449;&#24230;&#36807;&#39640;&#30340;&#39044;&#27979;&#36827;&#34892;&#24809;&#32602;&#65292;&#20165;&#20851;&#27880;&#38169;&#35823;&#20998;&#31867;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#31070;&#32463;&#32467;&#26500;&#19981;&#21152;&#20559;&#35265;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#21487;&#19982;&#22810;&#20010;&#20998;&#21106;&#25439;&#22833;&#20989;&#25968;&#37197;&#23545;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65306;&#33041;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#30340;&#30333;&#36136;&#39640;&#20449;&#21495;&#30149;&#21464;&#21644;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#30340;&#24515;&#25151;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;MEEP&#19982;&#26631;&#20934;&#20998;&#21106;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#22312;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#65292;&#32780;&#19988;&#22312;&#20998;&#21106;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#30456;&#27604;&#20110;&#22522;&#32447;&#27169;&#22411;&#37117;&#20250;&#24102;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;YouTube&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;</title><link>http://arxiv.org/abs/2111.01908</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#23545;YouTube&#35780;&#35770;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying YouTube Comments Based on Sentiment and Type of Sentence. (arXiv:2111.01908v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;YouTube&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;YouTube&#39057;&#36947;&#30340;&#22686;&#38271;&#65292;&#27599;&#20010;&#35270;&#39057;&#37117;&#21487;&#33021;&#25910;&#38598;&#22823;&#37327;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#26159;&#29702;&#35299;&#35266;&#20247;&#26399;&#26395;&#21644;&#25913;&#21892;&#39057;&#36947;&#21442;&#19982;&#24230;&#30340;&#20027;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#35770;&#21482;&#20195;&#34920;&#29992;&#25143;&#20851;&#20110;&#39057;&#36947;&#21644;&#20869;&#23481;&#30340;&#19968;&#33324;&#35266;&#28857;&#12290;&#35768;&#22810;&#35780;&#35770;&#26500;&#36896;&#36739;&#24046;&#65292;&#29712;&#30862;&#24182;&#19988;&#23384;&#22312;&#25340;&#20889;&#21644;&#35821;&#27861;&#38169;&#35823;&#65292;&#22240;&#27492;&#65292;&#35782;&#21035;&#26368;&#33021;&#21560;&#24341;&#20869;&#23481;&#21019;&#20316;&#32773;&#30340;&#35780;&#35770;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#23558;&#21407;&#22987;&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a YouTube channel grows, each video can potentially collect enormous amounts of comments that provide direct feedback from the viewers. These comments are a major means of understanding viewer expectations and improving channel engagement. However, the comments only represent a general collection of user opinions about the channel and the content. Many comments are poorly constructed, trivial, and have improper spellings and grammatical errors. As a result, it is a tedious job to identify the comments that best interest the content creators. In this paper, we extract and classify the raw comments into different categories based on both sentiment and sentence types that will help YouTubers find relevant comments for growing their viewership. Existing studies have focused either on sentiment analysis (positive and negative) or classification of sub-types within the same sentence types (e.g., types of questions) on a text corpus. These have limited application on non-traditional text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22312;&#32447; 3D &#26080;&#24207;&#35013;&#31665;&#38382;&#39064;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#21472;&#25918;&#26641;&#30340;&#35013;&#31665;&#31283;&#23450;&#24615;&#22312;&#32447;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#32500;&#24230;&#20998;&#31163;&#30340;&#35013;&#31665;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;</title><link>http://arxiv.org/abs/2108.13680</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#34892;&#30340;&#22312;&#32447; 3D &#26080;&#24207;&#35013;&#31665;&#31574;&#30053;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Practically Feasible Policies for Online 3D Bin Packing. (arXiv:2108.13680v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.13680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#32447; 3D &#26080;&#24207;&#35013;&#31665;&#38382;&#39064;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#21472;&#25918;&#26641;&#30340;&#35013;&#31665;&#31283;&#23450;&#24615;&#22312;&#32447;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#32500;&#24230;&#20998;&#31163;&#30340;&#35013;&#31665;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447; 3D &#26080;&#24207;&#35013;&#31665;&#38382;&#39064;&#65292;&#36825;&#26159;&#32463;&#20856;&#35013;&#31665;&#38382;&#39064;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#29992;&#20215;&#20540;&#30340;&#21464;&#31181;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#29289;&#21697;&#34987;&#36882;&#36865;&#21040;&#20195;&#29702;&#20154;&#22788;&#65292;&#20294;&#19981;&#25552;&#20379;&#23436;&#25972;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#20195;&#29702;&#20154;&#24517;&#39035;&#23558;&#36825;&#20123;&#29289;&#21697;&#30452;&#25509;&#31283;&#23450;&#22320;&#35013;&#20837;&#30446;&#26631;&#35013;&#31665;&#20013;&#65292;&#32780;&#19981;&#33021;&#25913;&#21464;&#23427;&#20204;&#30340;&#21040;&#36798;&#39034;&#24207;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#22312;&#32447; 3D &#26080;&#24207;&#35013;&#31665;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#31574;&#30053;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#29992;&#21463;&#38480;&#21160;&#20316;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#20010;MDP &#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#23454;&#29992;&#30340;&#21487;&#34892;&#30340;&#35013;&#31665;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#21472;&#25918;&#26641;&#30340;&#35013;&#31665;&#31283;&#23450;&#24615;&#22312;&#32447;&#20998;&#26512;&#26041;&#27861;&#12290;&#23427;&#23454;&#29616;&#20102;&#39640;&#24230;&#30340;&#20998;&#26512;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174; O(N^2) &#38477;&#20302;&#21040; O(NlogN)&#65292;&#38750;&#24120;&#36866;&#21512;&#20110; RL &#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#32500;&#24230;&#20998;&#31163;&#30340;&#35013;&#31665;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
We tackle the Online 3D Bin Packing Problem, a challenging yet practically useful variant of the classical Bin Packing Problem. In this problem, the items are delivered to the agent without informing the full sequence information. Agent must directly pack these items into the target bin stably without changing their arrival order, and no further adjustment is permitted. Online 3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt deep reinforcement learning, in particular, the on-policy actor-critic framework, to solve this MDP with constrained action space. To learn a practically feasible packing policy, we propose three critical designs. First, we propose an online analysis of packing stability based on a novel stacking tree. It attains a high analysis accuracy while reducing the computational complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL training. Second, we propose a decoupled packing policy learning for different dimensions o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;XAI&#21487;&#29992;&#20110;&#35777;&#26126;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2106.14186</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#29992;XAI&#26041;&#27861;&#26816;&#27979;DCIS&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An XAI Approach to Deep Learning Models in the Detection of DCIS. (arXiv:2106.14186v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;XAI&#21487;&#29992;&#20110;&#35777;&#26126;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#30830;&#23454;&#21487;&#20197;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#23545;&#20020;&#24202;&#31038;&#21306;&#20869;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23454;&#26045;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The results showed that XAI could indeed be used as a proof of concept to begin discussions on the implementation of assistive AI systems within the clinical community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#34920;&#36798;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.00661</link><description>&lt;p&gt;
&#22870;&#21169;&#36275;&#20197;&#22788;&#29702;&#20984;&#24615;MDP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reward is enough for convex MDPs. (arXiv:2106.00661v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#34920;&#36798;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDP) &#20013;&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#21644;&#24179;&#31283;&#30340;&#32047;&#31215;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25429;&#25417;&#21040;&#35768;&#22810;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#30446;&#26631;&#37117;&#33021;&#20197;&#27492;&#26041;&#24335;&#25429;&#33719;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;MDPs&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20316;&#20026;&#38745;&#24577;&#20998;&#24067;&#30340;&#20984;&#20989;&#25968;&#34920;&#36798;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#26469;&#34920;&#36798;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;&#20984;&#24615;MDP&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#25919;&#31574;&#21644;&#20195;&#20215;(&#36127;&#22870;&#21169;)&#8220;&#29609;&#23478;&#8221;&#30340;&#26368;&#23567;&#26368;&#22823;&#21338;&#24328;&#65292;&#21033;&#29992; Fenchel &#23545;&#20598;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#20803;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#35768;&#22810;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximising a cumulative reward function that is Markov and stationary, i.e., defined over state-action pairs and independent of time, is sufficient to capture many kinds of goals in a Markov decision process (MDP). However, not all goals can be captured in this manner. In this paper we study convex MDPs in which goals are expressed as convex functions of the stationary distribution and show that they cannot be formulated using stationary reward functions. Convex MDPs generalize the standard reinforcement learning (RL) problem formulation to a larger framework that includes many supervised and unsupervised RL problems, such as apprenticeship learning, constrained MDPs, and so-called `pure exploration'. Our approach is to reformulate the convex MDP problem as a min-max game involving policy and cost (negative reward) `players', using Fenchel duality. We propose a meta-algorithm for solving this problem and show that it unifies many existing algorithms in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25512;&#36827;&#27169;&#22411;&#30340;&#26080;&#23494;&#24230;&#31163;&#32447;&#31639;&#27861; GAC&#65292;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#29109;&#27491;&#21017;&#21270;&#22120;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26426;&#21046;&#25552;&#39640;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.03733</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;: &#20351;&#29992;&#25512;&#36827;&#27169;&#22411;&#30340;&#31163;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25512;&#36827;&#27169;&#22411;&#30340;&#26080;&#23494;&#24230;&#31163;&#32447;&#31639;&#27861; GAC&#65292;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#29109;&#27491;&#21017;&#21270;&#22120;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26426;&#21046;&#25552;&#39640;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#20855;&#26377;&#39640;&#26031;&#20998;&#24067;&#30340;&#31574;&#30053;&#23548;&#33268;&#29615;&#22659;&#30340;&#25506;&#32034;&#20302;&#25928;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#31639;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23494;&#24230;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;Generative Actor-Critic&#65292;GAC&#65289;&#65292;&#20351;&#29992;&#25512;&#36827;&#27169;&#22411;&#26469;&#22686;&#21152;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#21253;&#25324;&#19968;&#31181;&#29109;&#31867;&#25216;&#26415;&#65292;&#31216;&#20026;&#26368;&#22823;&#22343;&#20540;&#24046;&#65288;Maximum Mean Discrepancy&#65292;MMD&#65289;&#29109;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#33258;&#21160;&#32553;&#25918;&#36825;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;GAC&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#20855;&#26377;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23485;&#24120;&#29992;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#30340;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19977;&#31181;&#31574;&#30053;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#20351;&#29992;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2104.06022</link><description>&lt;p&gt;
Transformer&#23618;&#38388;&#21442;&#25968;&#20849;&#20139;&#30340;&#25945;&#35757;&#65288;arXiv&#65306;2104.06022v4 [cs.CL]&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.06022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23485;&#24120;&#29992;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#30340;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19977;&#31181;&#31574;&#30053;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#20351;&#29992;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65288;Vaswani&#31561;&#20154;&#65292;2017&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25918;&#23485;&#20102;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21363;&#23558;&#19968;&#20010;&#23618;&#30340;&#21442;&#25968;&#19982;&#25152;&#26377;&#23618;&#20849;&#20139;&#65292;&#22914;&#36890;&#29992;Transformer&#65288;Dehghani&#31561;&#20154;&#65292;2019&#65289;&#65292;&#20197;&#22686;&#21152;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65306;&#24207;&#21015;&#12289;&#24490;&#29615;&#21644;&#24490;&#29615;&#65288;&#21453;&#21521;&#65289;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#20351;&#29992;&#35768;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#20063;&#26159;&#26377;&#25928;&#30340;&#65292;&#20363;&#22914;&#26368;&#36817;&#30340;WMT&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#31867;&#22411;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;&#65292;&#24674;&#22797;&#39044;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOIL2+QCBA&#30456;&#23545;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#32780;&#35328;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/1711.10166</link><description>&lt;p&gt;
QCBA: &#36890;&#36807;&#24674;&#22797;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation. (arXiv:1711.10166v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1711.10166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#31867;&#22411;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;&#65292;&#24674;&#22797;&#39044;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOIL2+QCBA&#30456;&#23545;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#32780;&#35328;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#23646;&#24615;&#30340;&#39044;&#31163;&#25955;&#21270;&#26159;&#26576;&#20123;&#35268;&#21017;&#23398;&#20064;&#31639;&#27861;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20294;&#26159;&#20250;&#23548;&#33268;&#19968;&#20123;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#35268;&#21017;&#35843;&#25972;&#27493;&#39588;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#35268;&#21017;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#20102;&#23545;&#22522;&#20110;&#20851;&#32852;&#24615;&#20998;&#31867;&#65288;CBA&#65289;&#31639;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#20013;&#30340;&#23450;&#37327;&#23646;&#24615;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20294;&#20063;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20116;&#20010;&#20851;&#32852;&#35268;&#21017;&#20998;&#31867;&#31639;&#27861;&#65288;CBA&#12289;CMAR&#12289;CPAR&#12289;IDS&#12289;SBRL&#65289;&#21644;&#20004;&#20010;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#22120;&#65288;FOIL2&#21644;PRM&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#21518;&#22788;&#29702;&#25928;&#26524;&#65292;&#20351;&#29992;UCI&#20179;&#24211;&#30340;22&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;FOIL2+QCBA&#30456;&#27604;&#20110;&#25152;&#26377;&#19971;&#20010;&#22522;&#32447;&#30340;&#22823;&#23567;&#26356;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20339;&#30340;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#12290;&#21518;&#20248;&#21270;&#30340;CBA&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prediscretisation of numerical attributes which is required by some rule learning algorithms is a source of inefficiencies. This paper describes new rule tuning steps that aim to recover lost information in the discretisation and new pruning techniques that may further reduce the size of rule models and improve their accuracy. The proposed QCBA method was initially developed to postprocess quantitative attributes in models generated by the Classification based on associations (CBA) algorithm, but it can also be applied to the results of other rule learning approaches. We demonstrate the effectiveness on the postprocessing of models generated by five association rule classification algorithms (CBA, CMAR, CPAR, IDS, SBRL) and two first-order logic rule learners (FOIL2 and PRM). Benchmarks on 22 datasets from the UCI repository show smaller size and the overall best predictive performance for FOIL2+QCBA compared to all seven baselines. Postoptimised CBA models have a better predictive p
&lt;/p&gt;</description></item></channel></rss>