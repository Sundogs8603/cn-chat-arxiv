<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01442</link><description>&lt;p&gt;
&#28145;&#24230;&#21452;&#19979;&#38477;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#36991;&#20813;&#27169;&#22411;&#27424;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#22312;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#27169;&#22411;&#26550;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21363;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20960;&#20010;Transformer&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;epoch-wise&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#20197;&#21450;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#21487;&#20197;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#22312;72&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#36817;70%&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#35757;&#32451;&#27169;&#24335;&#20462;&#25913;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation
&lt;/p&gt;</description></item><item><title>&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16121</link><description>&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#65306;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics. (arXiv:2310.16121v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16121
&lt;/p&gt;
&lt;p&gt;
&#21482;&#38656;19&#20010;&#21442;&#25968;&#30340;&#24494;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31890;&#23376;&#21152;&#36895;&#22120;&#22686;&#21152;&#30896;&#25758;&#36895;&#29575;&#21644;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#24471;&#21040;&#35777;&#23454;&#65292;&#23545;&#20110;&#20302;&#24310;&#36831;&#20219;&#21153;&#65288;&#22914;&#35302;&#21457;&#22120;&#65289;&#65292;&#38656;&#35201;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#27931;&#20262;&#20857;&#21644;&#32622;&#25442;&#23545;&#31216;&#26550;&#26500;PELICAN&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#30340;&#20960;&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#20165;&#20026;19&#20010;&#65292;&#22312;&#39030;&#22840;&#20811;&#21943;&#27880;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24615;&#33021;&#36229;&#36807;&#20102;&#25968;&#20197;&#19975;&#35745;&#21442;&#25968;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16111</link><description>&lt;p&gt;
&#20351;&#29992;&#38646;&#38454;&#25552;&#31034;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#38544;&#31169;&#20445;&#25252;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-Prompt&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#26469;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#24403;DP-Prompt&#19982;&#20687;ChatGPT&#65288;gpt-3.5&#65289;&#36825;&#26679;&#30340;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21435;&#21311;&#21517;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#23613;&#31649;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#65292;&#20294;&#23427;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24456;&#22823;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#22312;IMDB&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;DP-Prompt&#65288;&#20351;&#29992;ChatGPT&#65289;&#23436;&#20840;&#24674;&#22797;&#20102;&#28165;&#27905;&#30340;&#24773;&#24863;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#38745;&#24577;&#25915;&#20987;&#32773;&#30340;&#20316;&#32773;&#35782;&#21035;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;46&#65285;&#30340;&#38477;&#20302;&#21644;26&#65285;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.09485</link><description>&lt;p&gt;
&#24212;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#22312;&#30149;&#27602;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#26377;&#21161;&#20110;&#33719;&#24471;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#37325;&#22609;&#21307;&#30103;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#26080;&#20215;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#35786;&#26029;&#65292;&#20174;&#32780;&#20943;&#36731;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#34892;&#19994;&#37117;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#27169;&#24335;&#35782;&#21035;&#65292;&#36825;&#20123;&#23545;&#20154;&#31867;&#25110;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#35828;&#26412;&#26469;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#23558;&#23574;&#31471;&#30149;&#27602;&#20998;&#26512;&#24102;&#32473;&#20840;&#29699;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25968;&#25454;&#32452;&#32455;&#26041;&#38754;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20005;&#37325;&#31243;&#24230;&#25351;&#25968;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#24739;&#32773;&#25252;&#29702;&#38656;&#27714;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#31526;&#21512;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#26356;&#24191;&#27867;&#20998;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03915</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#24490;&#29615;&#36830;&#25509;&#36827;&#34892;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24320;&#21457;&#33021;&#22815;&#19982;&#21464;&#21270;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#33258;&#20027;&#20195;&#29702;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#31283;&#20581;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#20195;&#29702;&#36890;&#24120;&#26159;&#22312;&#19987;&#23478;&#31034;&#33539;&#20013;&#36827;&#34892;&#31163;&#32447;&#25311;&#21512;&#65292;&#20294;&#22312;&#22312;&#32447;&#37096;&#32626;&#26102;&#24517;&#39035;&#33021;&#22815;&#25512;&#24191;&#21040;&#29615;&#22659;&#20869;&#30340;&#38381;&#29615;&#21453;&#39304;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#36825;&#31867;&#20219;&#21153;&#65292;&#24182;&#20102;&#35299;&#20854;&#24490;&#29615;&#36830;&#25509;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#38381;&#29615;&#35774;&#32622;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24490;&#29615;&#36830;&#25509;&#34920;&#31034;&#20026;&#31209;&#21644;&#31232;&#30095;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#23637;&#31034;&#35843;&#33410;&#36825;&#20004;&#20010;&#21464;&#37327;&#23545;&#32593;&#32476;&#21160;&#24577;&#30340;&#26377;&#30410;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#20026;&#32593;&#32476;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#20808;&#39564;&#65292;&#23545;&#20110;&#19968;&#31867;&#31216;&#20026;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#65288;CfCs&#65289;&#30340;&#27169;&#22411;&#26368;&#20026;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;CfCs&#21487;&#20197;&#36229;&#36807;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can ou
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>OpenMM 8&#26159;&#19968;&#20010;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#21151;&#33021;&#12289;&#20248;&#21270;&#35745;&#31639;&#36895;&#24230;&#21644;&#25552;&#20379;&#39640;&#32423;&#25509;&#21475;&#65292;&#20351;&#24471;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#25104;&#20026;&#19968;&#31181;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03121</link><description>&lt;p&gt;
OpenMM 8&#65306;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials. (arXiv:2310.03121v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03121
&lt;/p&gt;
&lt;p&gt;
OpenMM 8&#26159;&#19968;&#20010;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#21151;&#33021;&#12289;&#20248;&#21270;&#35745;&#31639;&#36895;&#24230;&#21644;&#25552;&#20379;&#39640;&#32423;&#25509;&#21475;&#65292;&#20351;&#24471;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#25104;&#20026;&#19968;&#31181;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#35282;&#33394;&#12290;OpenMM&#20998;&#23376;&#21160;&#21147;&#23398;&#24037;&#20855;&#21253;&#30340;&#26368;&#26032;&#29256;&#26412;&#24341;&#20837;&#20102;&#26032;&#21151;&#33021;&#65292;&#20197;&#25903;&#25345;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#12290;&#20219;&#24847;&#30340;PyTorch&#27169;&#22411;&#21487;&#28155;&#21152;&#21040;&#27169;&#25311;&#20013;&#65292;&#24182;&#21487;&#29992;&#20110;&#35745;&#31639;&#21147;&#21644;&#33021;&#37327;&#12290;&#26356;&#39640;&#32423;&#30340;&#25509;&#21475;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#20351;&#29992;&#36890;&#29992;&#39044;&#35757;&#32451;&#21183;&#20989;&#25968;&#23545;&#24863;&#20852;&#36259;&#30340;&#20998;&#23376;&#36827;&#34892;&#24314;&#27169;&#12290;&#20248;&#21270;&#30340;CUDA&#26680;&#20989;&#25968;&#21644;&#33258;&#23450;&#20041;&#30340;PyTorch&#25805;&#20316;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;&#27700;&#20013;&#27169;&#25311;&#20102;CDK8&#21644;&#32511;&#33394;&#33639;&#20809;&#34507;&#30333;(GFP)&#33394;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#29305;&#24615;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#21464;&#24471;&#23454;&#38469;&#65292;&#24182;&#21482;&#26377;&#36866;&#24230;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning plays an important and growing role in molecular simulation. The newest version of the OpenMM molecular dynamics toolkit introduces new features to support the use of machine learning potentials. Arbitrary PyTorch models can be added to a simulation and used to compute forces and energy. A higher-level interface allows users to easily model their molecules of interest with general purpose, pretrained potential functions. A collection of optimized CUDA kernels and custom PyTorch operations greatly improves the speed of simulations. We demonstrate these features on simulations of cyclin-dependent kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water. Taken together, these features make it practical to use machine learning to improve the accuracy of simulations at only a modest increase in cost.
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.12673</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20316;&#20026;&#29616;&#20195; Hopfield &#27169;&#22411;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#19982;&#20854;&#23494;&#38598;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20855;&#22791;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#20854;&#19968;&#27493;&#36817;&#20284;&#23545;&#24212;&#20110;&#31232;&#30095;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#31232;&#30095;&#29109;&#27491;&#21017;&#21270;&#22120;&#30340;&#20984;&#20849;&#36717;&#23548;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#31232;&#30095; Hopfield &#33021;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20174;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#19968;&#27493;&#36817;&#20284;&#31561;&#20215;&#20110;&#31232;&#30095;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#31232;&#30095;&#24230;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#35777;&#26126;&#19978;&#35201;&#27604;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#26356;&#32039;&#20945;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#31232;&#30095;&#20248;&#21183;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#24555;&#36895;&#30340;&#22266;&#23450;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12114</link><description>&lt;p&gt;
&#31616;&#32422;&#22810;&#20219;&#21153;&#27169;&#22411;-&#20351;&#29992;&#32467;&#26500;&#31232;&#30095;&#24615;&#23454;&#29616;&#31616;&#27905;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32452;&#31232;&#30095;&#24615;&#40723;&#21169;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#27963;&#36291;&#21442;&#25968;&#32452;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#32467;&#26500;&#21270;&#32452;&#31232;&#30095;&#24615;&#32435;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#30340;&#20849;&#20139;&#21442;&#25968;&#65292;&#24320;&#21457;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#65292;&#32780;&#20445;&#25345;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#26377;&#21161;&#20110;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#12290;&#27492;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#65292;&#36824;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#20174;&#32780;&#22686;&#24378;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25968;&#25454;&#38598;NYU-v2&#21644;CelebAMask-HQ&#19978;&#27604;&#36739;&#20102;&#32452;&#31232;&#30095;&#24615;&#19979;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09952</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#23547;&#25214;&#20986;&#29616;&#65306;&#21551;&#21457;&#20110;&#22240;&#26524;&#20986;&#29616;&#30340;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding emergence in data: causal emergence inspired dynamics learning. (arXiv:2308.09952v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24494;&#35266;&#23618;&#38754;&#30340;&#35266;&#27979;&#25968;&#25454;&#26080;&#27861;&#30452;&#25509;&#25429;&#25417;&#21040;&#20986;&#29616;&#34892;&#20026;&#21644;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23439;&#35266;&#23618;&#38754;&#20986;&#29616;&#21160;&#21147;&#23398;&#24182;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#37327;&#21270;&#20986;&#29616;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21253;&#21547;&#20986;&#29616;&#28508;&#22312;&#31354;&#38388;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#20449;&#24687;&#65288;EI&#65289;&#26469;&#33719;&#24471;&#19968;&#20010;&#20855;&#26377;&#26356;&#24378;&#22240;&#26524;&#25928;&#26524;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#23545;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#19981;&#20165;&#25104;&#21151;&#25429;&#25417;&#21040;&#20986;&#29616;&#27169;&#24335;&#65292;&#36824;&#23398;&#20064;&#20102;&#31895;&#31890;&#21270;&#31574;&#30053;&#24182;&#37327;&#21270;&#20102;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20986;&#29616;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#19981;&#21516;&#29615;&#22659;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26694;&#26550;&#22312;&#24314;&#27169;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09835</link><description>&lt;p&gt;
&#36890;&#36807;&#28857;&#21644;&#24418;&#29366;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#21512;&#25104;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#38656;&#35201;&#23494;&#38598;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#25104;&#26412;&#39640;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#19982;&#23436;&#25972;&#26631;&#27880;&#25152;&#25551;&#36848;&#30340;&#23545;&#35937;&#30340;&#23436;&#25972;&#36718;&#24275;&#30456;&#27604;&#65292;&#28857;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#23545;&#35937;&#36136;&#24515;&#65292;&#26356;&#23481;&#26131;&#33719;&#21462;&#65292;&#24182;&#19988;&#20173;&#28982;&#20026;&#21518;&#32493;&#20998;&#21106;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#28857;&#27880;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#20351;&#29992;&#24418;&#29366;&#20808;&#39564;&#32422;&#26463;&#37319;&#26679;&#19968;&#20010;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20197;&#38750;&#37197;&#23545;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#65307;&#65288;3&#65289;&#20266;&#25513;&#30721;&#21644;&#21512;&#25104;&#22270;&#20687;&#20849;&#21516;&#26500;&#25104;&#20102;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#21644;&#24314;&#27169;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#25928;&#29575;&#21644;&#29983;&#25104;&#26080;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.04051</link><description>&lt;p&gt;
&#24418;&#29366;&#20248;&#21270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#38477;&#20302;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization. (arXiv:2308.04051v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#21644;&#24314;&#27169;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#25928;&#29575;&#21644;&#29983;&#25104;&#26080;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20004;&#20010;&#30446;&#26631;&#26159;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#29983;&#25104;&#27809;&#26377;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;&#36890;&#36807;&#20943;&#23569;&#23450;&#20041;&#26032;&#30340;&#20943;&#23569;&#23376;&#31354;&#38388;&#30340;&#21407;&#22987;&#35774;&#35745;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#32447;&#24615;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#24314;&#27169;&#25968;&#25454;&#30340;&#24213;&#23618;&#29983;&#25104;&#36807;&#31243;&#65292;&#22914;&#22240;&#23376;&#20998;&#26512;&#21644;&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24418;&#29366;&#20462;&#25913;&#26041;&#27861;&#26159;&#32447;&#24615;&#30340;&#19988;&#35774;&#35745;&#21464;&#37327;&#22312;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#26102;&#65292;&#25968;&#25454;&#36817;&#20284;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#26159;&#30001;&#20110;&#30452;&#25509;&#24212;&#29992;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#21033;&#29992;&#39532;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#35770;&#25991;&#35777;&#26126;&#24322;&#24120;&#35774;&#35745;&#24448;&#24448;&#20855;&#26377;&#36739;&#39640;&#30340;&#35813;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>TimeGNN&#26159;&#19968;&#31181;&#23398;&#20064;&#21160;&#24577;&#26102;&#38388;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23427;&#22312;&#25512;&#26029;&#26102;&#38388;&#19978;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14680</link><description>&lt;p&gt;
TimeGNN: &#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting. (arXiv:2307.14680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14680
&lt;/p&gt;
&lt;p&gt;
TimeGNN&#26159;&#19968;&#31181;&#23398;&#20064;&#21160;&#24577;&#26102;&#38388;&#22270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23427;&#22312;&#25512;&#26029;&#26102;&#38388;&#19978;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#37325;&#35201;&#23454;&#38469;&#24212;&#29992;&#20013;&#22788;&#20110;&#26680;&#24515;&#20301;&#32622;&#12290;&#22823;&#37327;&#21253;&#21547;&#22797;&#26434;&#27169;&#24335;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#23384;&#22312;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22522;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21407;&#22987;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#22270;&#32467;&#26500;&#26469;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeGNN&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#21160;&#24577;&#30340;&#26102;&#38388;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#27169;&#24335;&#30340;&#28436;&#21464;&#20197;&#21450;&#22810;&#20010;&#31995;&#21015;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;TimeGNN&#30340;&#25512;&#26029;&#26102;&#38388;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24555;4&#21040;80&#20493;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.15220</link><description>&lt;p&gt;
S-TLLR: &#21463;&#21040;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;STDP&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks. (arXiv:2306.15220v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15220
&lt;/p&gt;
&lt;p&gt;
S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#21487;&#29992;&#20110;&#36793;&#32536;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#38754;&#20020;&#30528;&#31934;&#30830;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#29992;&#20998;&#37197;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;BPTT&#31639;&#27861;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23427;&#20135;&#29983;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BPTT&#21450;&#20854;&#36817;&#20284;&#20165;&#21033;&#29992;&#20174;&#33033;&#20914;&#27963;&#21160;&#20013;&#23548;&#20986;&#30340;&#22240;&#26524;&#20449;&#24687;&#26469;&#35745;&#31639;&#31361;&#35302;&#26356;&#26032;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S-TLLR&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;Spike-Timing Dependent Plasticity&#65288;STDP&#65289;&#26426;&#21046;&#21551;&#21457;&#30340;&#26032;&#22411;&#19977;&#22240;&#32032;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26088;&#22312;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;SNN&#35757;&#32451;&#12290;S-TLLR&#21516;&#26102;&#32771;&#34385;&#20102;&#21069;&#21518;&#31361;&#35302;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for the deployment for energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses a significant challenge due to the necessity for precise temporal and spatial credit assignment. Back-propagation through time (BPTT) algorithm, whilst being the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. Moreover, BPTT and its approximations solely utilize causal information derived from the spiking activity to compute the synaptic updates, thus neglecting non-causal relationships. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training SNNs on event-based learning tasks. S-TLLR considers both causal and non-causal relationships between pre and post-syn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12230</link><description>&lt;p&gt;
&#22855;&#22937;&#30340;&#26435;&#37325;&#21450;&#20854;&#26597;&#25214;&#26041;&#27861;&#65306;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20248;&#21270;&#20854;&#31232;&#30095;&#21021;&#22987;&#21270;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;DST&#33021;&#22815;&#32988;&#36807;&#23494;&#38598;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21098;&#26525;&#21644;&#29983;&#38271;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#21453;&#22797;&#24212;&#29992;&#20197;&#35843;&#25972;&#32593;&#32476;&#30340;&#31232;&#30095;&#36830;&#25509;&#12290;&#34429;&#28982;&#29983;&#38271;&#26631;&#20934;&#23545;DST&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#22909;&#22320;&#30740;&#31350;&#20102;&#65292;&#20294;&#21098;&#26525;&#26631;&#20934;&#30340;&#24433;&#21709;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23545;&#21508;&#31181;&#21098;&#26525;&#26631;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#23545; DST &#35299;&#20915;&#26041;&#26696;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#30740;&#31350;&#26041;&#27861;&#37117;&#20135;&#29983;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#26469;&#20811;&#26381;&#29616;&#26377;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#26159;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00210</link><description>&lt;p&gt;
PERFOGRAPH&#65306;&#19968;&#31181;&#25968;&#20540;&#24863;&#30693;&#30340;&#31243;&#24207;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#24615;&#33021;&#20248;&#21270;&#21644;&#31243;&#24207;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis. (arXiv:2306.00210v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00210
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#26469;&#20811;&#26381;&#29616;&#26377;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#26159;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#33879;&#22686;&#38271;&#21644;&#37325;&#35201;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#32534;&#31243;&#35821;&#35328;&#21644;&#31243;&#24207;&#20998;&#26512;&#20013;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#36825;&#30452;&#25509;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20851;&#20110;&#31243;&#24207;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20197;&#24448;&#30340;&#34920;&#31034;&#20316;&#21697;&#20013;&#32570;&#20047;&#25968;&#20540;&#24847;&#35782;&#12289;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#20449;&#24687;&#21644;&#21464;&#37327;&#34920;&#31034;&#19981;&#24403;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#24403;&#21069;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#12290; PERFOGRAPH&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#26032;&#33410;&#28857;&#21644;&#36793;&#26469;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#23884;&#20837;&#26041;&#27861;&#26469;&#21512;&#24182;&#25968;&#20540;&#24847;&#35782;&#12290;&#36825;&#20123;&#22686;&#24378;&#20351;PERFOGRAPH&#25104;&#20026;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages, which directly impacts the ability of machine learning methods to reason about programs. The absence of numerical awareness, composite data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the composite data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness. These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16526</link><description>&lt;p&gt;
&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Extending Explainable Boosting Machines to Scientific Image Data. (arXiv:2305.16526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#21307;&#23398;&#25110;&#31185;&#23398;&#31561;&#37325;&#35201;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#31995;&#32479;&#36755;&#20986;&#32467;&#26524;&#30340;&#35299;&#37322;&#38656;&#27714;&#24050;&#25104;&#20026;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#20351;&#24471;&#20174;&#35299;&#37322;&#35282;&#24230;&#20351;&#29992;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30446;&#21069;&#35299;&#37322;&#36825;&#20123;&#19981;&#36879;&#26126;&#27169;&#22411;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#24182;&#21463;&#21040;&#20005;&#37325;&#25209;&#35780;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#26159;&#19968;&#31867;&#26131;&#20110;&#35299;&#37322;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19981;&#36874;&#20110;&#26368;&#20339;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;&#22312;&#31185;&#23398;&#30028;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#24212;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#20197;&#25903;&#25745;&#37327;&#23376;&#25216;&#26415;&#21457;&#23637;&#30340;&#37325;&#35201;&#24212;&#29992;&#20919;&#21407;&#23376;&#23396;&#23376;&#22270;&#20687;&#25968;&#25454;&#20026;&#20363;&#65292;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#26426;&#22120;&#31639;&#27861;&#36827;&#34892;&#25506;&#31350;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#20854;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deployment of computer vision technology becomes increasingly common in applications of consequence such as medicine or science, the need for explanations of the system output has become a focus of great concern. Unfortunately, many state-of-the-art computer vision models are opaque, making their use challenging from an explanation standpoint, and current approaches to explaining these opaque models have stark limitations and have been the subject of serious criticism. In contrast, Explainable Boosting Machines (EBMs) are a class of models that are easy to interpret and achieve performance on par with the very best-performing models, however, to date EBMs have been limited solely to tabular data. Driven by the pressing need for interpretable models in science, we propose the use of EBMs for scientific image data. Inspired by an important application underpinning the development of quantum technologies, we apply EBMs to cold-atom soliton image data, and, in doing so, demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15613</link><description>&lt;p&gt;
&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;
&lt;/p&gt;
&lt;p&gt;
Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;nD&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#28857;&#20113;&#20998;&#26512;&#20013;&#31561;&#21464;&#20110;&#27491;&#20132;&#36716;&#25442;&#65292;&#21033;&#29992;&#20102;&#36229;&#29699;&#20307;&#21644;&#24120;&#35268;n&#21333;&#24418;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36817;&#26399;&#21457;&#23637;&#30340;&#21487;&#25805;&#32437;3D&#29699;&#24418;&#31070;&#32463;&#20803;&#29702;&#35770;--&#22522;&#20110;&#29699;&#24418;&#20915;&#31574;&#38754;&#30340;SO&#65288;3&#65289;-&#31561;&#21464;&#28388;&#27874;&#22120;&#32452;&#65292;&#23558;&#35813;&#31070;&#32463;&#20803;&#25193;&#23637;&#21040;&#20102;nD&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22534;&#21472;&#22312;&#22810;&#23618;&#20013;&#12290;&#21033;&#29992;ModelNet40&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#28508;&#22312;&#23454;&#29992;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#24230;&#37327;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#36798;&#21040;&#29702;&#24819;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14595</link><description>&lt;p&gt;
&#23454;&#29616;&#21453;&#20107;&#23454;&#24230;&#37327;&#65306;&#28608;&#21169;&#12289;&#25490;&#21517;&#21644;&#20449;&#24687;&#19981;&#23545;&#31216;
&lt;/p&gt;
&lt;p&gt;
Operationalizing Counterfactual Metrics: Incentives, Ranking, and Information Asymmetry. (arXiv:2305.14595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14595
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#22914;&#26524;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#24230;&#37327;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#36798;&#21040;&#29702;&#24819;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31038;&#20250;&#31185;&#23398;&#21040;&#26426;&#22120;&#23398;&#20064;&#65292;&#25991;&#29486;&#24050;&#32463;&#20805;&#20998;&#35777;&#26126;&#35201;&#20248;&#21270;&#30340;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#31038;&#20250;&#31119;&#21033;&#19968;&#33268;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#65292;Dranove&#31561;&#20154;&#34920;&#26126;&#65292;&#21457;&#24067;&#25163;&#26415;&#27515;&#20129;&#29575;&#25351;&#26631;&#23454;&#38469;&#19978;&#36890;&#36807;&#22686;&#21152;&#25552;&#20379;&#32773;&#36873;&#25321;&#34892;&#20026;&#65292;&#21361;&#23475;&#20102;&#26356;&#30149;&#37325;&#24739;&#32773;&#30340;&#31119;&#21033;&#12290;&#21033;&#29992;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#30452;&#25509;&#30740;&#31350;&#20102;&#20174;&#36825;&#31181;&#24179;&#22343;&#27835;&#30103;&#32467;&#26524;&#24230;&#37327;&#20013;&#20135;&#29983;&#30340;&#28608;&#21169;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#26524;&#24230;&#37327;&#65288;i&#65289;&#32771;&#34385;&#26410;&#27835;&#30103;&#24773;&#20917;&#21644;&#65288;ii&#65289;&#32771;&#34385;&#25972;&#20307;&#31119;&#21033;&#32780;&#19981;&#26159;&#34987;&#27835;&#30103;&#32773;&#30340;&#24179;&#22343;&#31119;&#21033;&#65292;&#37027;&#20040;&#39537;&#21160;&#27835;&#30103;&#20915;&#31574;&#30340;&#28608;&#21169;&#23558;&#19982;&#26368;&#22823;&#21270;&#24739;&#32773;&#24635;&#31119;&#21033;&#19968;&#33268;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20462;&#25913;&#21453;&#20107;&#23454;&#24230;&#37327;&#20197;&#28385;&#36275;&#25490;&#21517;&#26102;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#22312;&#23558;&#36825;&#20010;&#26041;&#27861;&#25512;&#24191;&#21040;&#25552;&#20379;&#32773;&#35266;&#23519;&#21040;&#30340;&#20851;&#20110;&#24739;&#32773;&#36229;&#36234;&#30417;&#31649;&#26426;&#26500;&#30340;&#20449;&#24687;&#26356;&#22810;&#23454;&#38469;&#24773;&#20917;&#26102;&#65292;&#25105;&#20204;&#38480;&#21046;&#19981;&#23545;&#31216;&#20449;&#24687;&#31243;&#24230;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20849;&#20139;&#26041;&#26696;&#26469;&#25552;&#39640;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the social sciences to machine learning, it has been well documented that metrics to be optimized are not always aligned with social welfare. In healthcare, Dranove et al. [12] showed that publishing surgery mortality metrics actually harmed the welfare of sicker patients by increasing provider selection behavior. Using a principal-agent model, we directly study the incentive misalignments that arise from such average treated outcome metrics, and show that the incentives driving treatment decisions would align with maximizing total patient welfare if the metrics (i) accounted for counterfactual untreated outcomes and (ii) considered total welfare instead of average welfare among treated patients. Operationalizing this, we show how counterfactual metrics can be modified to satisfy desirable properties when used for ranking. Extending to realistic settings when the providers observe more about patients than the regulatory agencies do, we bound the decay in performance by the degree 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.11164</link><description>&lt;p&gt;
&#25506;&#32034;&#25265;&#25265;&#33080;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65306;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study. (arXiv:2305.11164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#30340;&#23835;&#36215;&#21152;&#21095;&#20102;&#23427;&#20204;&#30340;&#30899;&#36275;&#36857;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22686;&#21152;&#30340;&#33021;&#21147;&#21644;&#27169;&#22411;&#22823;&#23567;&#25152;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#22914;&#20309;&#23454;&#38469;&#27979;&#37327;&#12289;&#25253;&#21578;&#21644;&#35780;&#20272;&#30340;&#35748;&#35782;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#20998;&#26512;&#22312;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;Hugging Face&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#39044;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#23384;&#20648;&#24211;&#12290;&#30446;&#26631;&#26159;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;Hugging Face Hub API&#19978;&#26377;&#20851;&#30899;&#25490;&#25918;&#30340;&#31532;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;(1) ML&#27169;&#22411;&#30340;&#21019;&#24314;&#32773;&#22914;&#20309;&#22312;Hugging Face Hub&#19978;&#27979;&#37327;&#21644;&#25253;&#21578;&#30899;&#25490;&#25918;&#65311;(2) &#21738;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#30899;&#25490;&#25918;&#65311;&#35813;&#30740;&#31350;&#24471;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;&#20854;&#20013;&#21253;&#25324;&#30899;&#25490;&#25918;&#25253;&#21578;&#27169;&#24335;&#27604;&#20363;&#30340;&#36880;&#27493;&#19979;&#38477;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a decreasing proportion of carbon emissions-reporting mode
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#20132;&#20114;&#29615;&#22659;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#34987;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#22823;&#37327;&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#30830;&#23450;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#24517;&#35201;&#25110;&#34987;&#36807;&#24230;&#20351;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#25506;&#32034;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21487;&#25509;&#21463;&#30340;&#21160;&#20316;&#30340;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#20165;&#20174;&#28216;&#25103;&#35266;&#23519;&#20013;&#29983;&#25104;&#25991;&#26412;&#21629;&#20196;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#12290;&#22312; Jericho &#30340; 10 &#31181;&#28216;&#25103;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20984;&#26174;&#20986;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#26356;&#36731;&#37327;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21033;&#29992;&#29615;&#22659;&#20449;&#24687;&#30340;&#26032;&#35270;&#35282;&#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $ \epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#20110;&#26368;&#24378;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.10253</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#35270;&#35282;&#30340;&#25193;&#25955;&#27169;&#22411;&#19982;&#26816;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A data augmentation perspective on diffusion models and retrieval. (arXiv:2304.10253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#20110;&#26368;&#24378;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#25991;&#26412;&#26597;&#35810;&#20013;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#29983;&#25104;&#33021;&#21147;&#26469;&#22686;&#24378;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26159;&#22312;&#22823;&#22411;&#22024;&#26434;&#30340;&#30417;&#30563;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22686;&#24378;&#36807;&#31243;&#20013;&#20351;&#29992;&#38468;&#21152;&#25968;&#25454;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#30740;&#31350;&#26032;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#30410;&#12290;&#34429;&#28982;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#30446;&#26631;&#25968;&#25454;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#20248;&#20110;&#26356;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#20063;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#36817;&#37051;&#26816;&#32034;&#36807;&#31243;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#24378;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at generating photorealistic images from text-queries. Naturally, many approaches have been proposed to use these generative abilities to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large noisily supervised, but nonetheless, annotated datasets. It is an open question whether the generalization capabilities of diffusion models beyond using the additional data of the pre-training process for augmentation lead to improved downstream performance. We perform a systematic evaluation of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. While we find that personalizing diffusion models towards the target data outperforms simpler prompting strategies, we also show that using the training data of the diffusion model alone, via a simple nearest neighbor retrieval procedure, leads to even stronger downstream performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.10886</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#30340;&#19968;&#20123;&#22522;&#26412;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#65292;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#22312;Lipschitz&#24120;&#25968;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#20013;&#35782;&#21035;&#20986;&#20102;&#26126;&#26174;&#30340;&#21452;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36830;&#32493;&#24615;&#26159;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#20851;&#38190;&#30340;&#21151;&#33021;&#24615;&#36136;&#65292;&#23427;&#22788;&#20110;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20989;&#25968;&#30340;Lipschitz&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#32791;&#23613;&#26368;&#31616;&#21333;&#21644;&#26368;&#19968;&#33324;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#26497;&#38480;&#65292;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65288;&#21363;&#65292;&#20307;&#31995;&#32467;&#26500;&#12289;&#25439;&#22833;&#12289;&#20248;&#21270;&#22120;&#12289;&#26631;&#31614;&#22122;&#38899;&#31561;&#65289;&#65292;&#34429;&#28982;&#36825;&#19968;&#36873;&#25321;&#20027;&#35201;&#26159;&#21463;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#30340;&#39537;&#21160;&#65292;&#20294;&#23427;&#20063;&#38750;&#24120;&#20016;&#23500;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20960;&#20010;&#22522;&#26412;&#21644;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#34917;&#20805;&#20102;&#36866;&#24403;&#30340;&#29702;&#35770;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2302.08893</link><description>&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08893
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#25968;&#25454;&#20165;&#20197;&#26410;&#26631;&#35760;&#24418;&#24335;&#21487;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#26368;&#23567;&#21270;&#19982;&#25910;&#38598;&#26631;&#35760;&#35266;&#27979;&#30456;&#20851;&#30340;&#25104;&#26412;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26631;&#27880;&#27599;&#20010;&#35266;&#27979;&#21487;&#20197;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#20351;&#24471;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#25552;&#20986;&#65292;&#26088;&#22312;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35266;&#27979;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#22320;&#20998;&#20026;&#20004;&#31867;&#65306;&#38745;&#24577;&#22522;&#20110;&#27744;&#30340;&#21644;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;&#28041;&#21450;&#20174;&#23553;&#38381;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;&#35266;&#27979;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#35843;&#26597;&#21644;&#25991;&#29486;&#32508;&#36848;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#27169;&#22411;&#36866;&#24212;&#26032;&#36827;&#25968;&#25454;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#20363;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2209.03077</link><description>&lt;p&gt;
&#20851;&#20110;ELBO&#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19979;&#30028;&#65288;&#21448;&#31216;ELBO&#25110;&#33258;&#30001;&#33021;&#65289;&#26159;&#35768;&#22810;&#32463;&#20856;&#21644;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#21464;&#20998;&#19979;&#30028;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#23398;&#20064;&#36827;&#34892;&#21040;&#21442;&#25968;&#25910;&#25947;&#21040;&#25509;&#36817;&#23398;&#20064;&#21160;&#24577;&#30340;&#31283;&#23450;&#28857;&#20540;&#12290;&#22312;&#26412;&#25991;&#30340;&#29702;&#35770;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;&#23545;&#20110;&#19968;&#31867;&#38750;&#24120;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65289;&#65292;&#21464;&#20998;&#19979;&#30028;&#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#22343;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#12290;&#23545;&#20110;&#20855;&#26377;&#19968;&#32452;&#28508;&#22312;&#21464;&#37327;&#21644;&#19968;&#32452;&#35266;&#27979;&#21464;&#37327;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20010;&#21644;&#21253;&#25324;&#19977;&#20010;&#29109;: (A) &#21464;&#20998;&#20998;&#24067;&#30340;&#29109;&#65288;&#24179;&#22343;&#29109;&#65289;&#65292;(B) &#27169;&#22411;&#20808;&#39564;&#20998;&#24067;&#30340;&#36127;&#29109;&#21644; (C) &#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#65288;&#26399;&#26395;&#65289;&#36127;&#29109;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#65306;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#22312;&#23398;&#20064;&#30340;&#20219;&#24847;&#38454;&#27573;&#21644;&#21508;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#30495;&#23454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#65292;&#26159;&#23545;&#20248;&#21270;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26080;&#27169;&#22411;&#22320;&#37325;&#26032;&#35774;&#35745;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#23454;&#39564;&#27425;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#20808;&#35774;&#35745;&#32447;&#24615;&#25511;&#21046;&#24459;&#36798;&#21040;&#21021;&#27493;&#25511;&#21046;&#65292;&#20877;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2103.03808</link><description>&lt;p&gt;
&#20004;&#27493;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#38750;&#32447;&#24615;&#26368;&#20248;&#35843;&#33410;&#22120;&#26080;&#27169;&#22411;&#37325;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator. (arXiv:2103.03808v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03808
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26080;&#27169;&#22411;&#22320;&#37325;&#26032;&#35774;&#35745;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#23454;&#39564;&#27425;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#20808;&#35774;&#35745;&#32447;&#24615;&#25511;&#21046;&#24459;&#36798;&#21040;&#21021;&#27493;&#25511;&#21046;&#65292;&#20877;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#25511;&#21046;&#24212;&#29992;&#20013;&#65292;&#38381;&#29615;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;&#24037;&#21378;&#29305;&#24615;&#30340;&#25913;&#21464;&#32780;&#38543;&#26102;&#38388;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#37325;&#26032;&#35774;&#35745;&#25511;&#21046;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#24314;&#27169;&#36807;&#31243;&#65292;&#36825;&#23545;&#20110;&#38381;&#29615;&#31995;&#32479;&#26469;&#35828;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#26368;&#20248;&#25511;&#21046;&#22120;&#21487;&#20197;&#20165;&#22522;&#20110;&#38381;&#29615;&#31995;&#32479;&#27979;&#37327;&#25968;&#25454;&#37325;&#26032;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;RL&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#25511;&#21046;&#19981;&#33391;&#30340;&#31995;&#32479;&#36827;&#34892;&#30456;&#24403;&#25968;&#37327;&#30340;&#35797;&#38169;&#23454;&#39564;&#65292;&#36825;&#21487;&#33021;&#20250;&#21152;&#36895;&#24037;&#21378;&#30340;&#30952;&#25439;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#20004;&#27493;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#25552;&#39640;&#20102;RL&#22312;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#35843;&#33410;&#22120;&#37325;&#26032;&#35774;&#35745;&#38382;&#39064;&#20013;&#30340;&#26242;&#24577;&#23398;&#20064;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#25511;&#21046;&#24459;&#65292;&#20197;&#23454;&#29616;&#19968;&#23450;&#31243;&#24230;&#30340;&#25511;&#21046;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many practical control applications, the performance level of a closed-loop system degrades over time due to the change of plant characteristics. Thus, there is a strong need for redesigning a controller without going through the system modeling process, which is often difficult for closed-loop systems. Reinforcement learning (RL) is one of the promising approaches that enable model-free redesign of optimal controllers for nonlinear dynamical systems based only on the measurement of the closed-loop system. However, the learning process of RL usually requires a considerable number of trial-and-error experiments using the poorly controlled system that may accumulate wear on the plant. To overcome this limitation, we propose a model-free two-step design approach that improves the transient learning performance of RL in an optimal regulator redesign problem for unknown nonlinear systems. Specifically, we first design a linear control law that attains some degree of control performance i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#12290;</title><link>http://arxiv.org/abs/2009.09213</link><description>&lt;p&gt;
&#38544;&#24335;&#31354;&#38388;&#22495;&#38519;&#27874;&#28388;&#27874;&#22120;&#65306;&#36530;&#36991;DeepFake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.09213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;DeepFake&#22270;&#20687;&#30340;&#39640;&#20445;&#30495;&#29983;&#25104;&#21644;&#39640;&#31934;&#24230;&#26816;&#27979;&#27491;&#22788;&#20110;&#19968;&#22330;&#20891;&#22791;&#31454;&#36187;&#20013;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#8220;&#26816;&#27979;&#38590;&#20197;&#25417;&#25720;&#8221;&#30340;DeepFakes&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;DeepFake&#26816;&#27979;&#33021;&#21147;&#30340;&#25552;&#39640;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#36827;&#34892;&#38544;&#24335;&#31354;&#38388;&#22495;&#38519;&#27874;&#28388;&#27874;&#26469;&#20943;&#23569;&#20266;&#36896;&#22270;&#20687;&#30340;&#20266;&#24433;&#32441;&#29702;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#22270;&#20687;&#36136;&#37327;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#31354;&#38388;&#22495;&#36827;&#34892;&#21608;&#26399;&#24615;&#22122;&#22768;&#30340;&#39057;&#22495;&#38519;&#27874;&#34429;&#28982;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#38519;&#27874;&#22120;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#19981;&#36866;&#21512;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#37325;&#29616;&#38519;&#27874;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22686;&#21152;&#21387;&#20498;&#24615;&#31354;&#38388;&#22122;&#22768;&#26469;&#25171;&#30772;&#21608;&#26399;&#22122;&#22768;&#27169;&#24335;&#65292;&#20877;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#28388;&#27874;&#26469;&#37325;&#26500;&#26080;&#22122;&#22768;DeepFakes&#12290;
&lt;/p&gt;
&lt;p&gt;
The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and 'detection evasive' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to the manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fa
&lt;/p&gt;</description></item></channel></rss>