<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Matcha-TTS&#26159;&#19968;&#31181;&#24555;&#36895;TTS&#26550;&#26500;&#65292;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#35757;&#32451;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#36755;&#20986;&#21644;&#24555;&#36895;&#21512;&#25104;&#27493;&#39588;&#12290;&#23427;&#19981;&#38656;&#35201;&#22806;&#37096;&#23545;&#40784;&#65292;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#22312;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.03199</link><description>&lt;p&gt;
Matcha-TTS: &#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#27969;&#21305;&#37197;&#30340;&#24555;&#36895;TTS&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Matcha-TTS: A fast TTS architecture with conditional flow matching. (arXiv:2309.03199v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03199
&lt;/p&gt;
&lt;p&gt;
Matcha-TTS&#26159;&#19968;&#31181;&#24555;&#36895;TTS&#26550;&#26500;&#65292;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#35757;&#32451;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#36755;&#20986;&#21644;&#24555;&#36895;&#21512;&#25104;&#27493;&#39588;&#12290;&#23427;&#19981;&#38656;&#35201;&#22806;&#37096;&#23545;&#40784;&#65292;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#22312;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Matcha-TTS&#65292;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#36895;TTS&#22768;&#23398;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;OT-CFM&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#22522;&#20110;ODE&#30340;&#35299;&#30721;&#22120;&#33021;&#22815;&#22312;&#27604;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23569;&#30340;&#21512;&#25104;&#27493;&#39588;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#36873;&#25321;&#30830;&#20445;&#27599;&#20010;&#21512;&#25104;&#27493;&#39588;&#30340;&#36816;&#34892;&#36895;&#24230;&#24555;&#12290;&#35813;&#26041;&#27861;&#26159;&#27010;&#29575;&#30340;&#12289;&#38750;&#33258;&#22238;&#24402;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#35828;&#35805;&#65292;&#26080;&#38656;&#22806;&#37096;&#23545;&#40784;&#12290;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;Matcha-TTS&#31995;&#32479;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#19982;&#26368;&#24555;&#27169;&#22411;&#22312;&#38271;&#35821;&#38899;&#29255;&#27573;&#19978;&#30340;&#36895;&#24230;&#30456;&#24403;&#65292;&#24182;&#22312;&#19968;&#39033;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;&#35831;&#35775;&#38382;https://shivammehta25.github.io/Matcha-TTS/ &#26597;&#30475;&#38899;&#39057;&#31034;&#20363;&#12289;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#38142;&#25509;&#27010;&#29575;&#25512;&#26029;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#25552;&#20986;&#30340;LDP&#26426;&#21046;&#26377;&#20004;&#20010;&#21464;&#20307;&#65292;&#22312;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03190</link><description>&lt;p&gt;
Blink: &#20351;&#29992;&#36125;&#21494;&#26031;&#20272;&#35745;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#38142;&#25509;&#27010;&#29575;&#25512;&#26029;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#25552;&#20986;&#30340;LDP&#26426;&#21046;&#26377;&#20004;&#20010;&#21464;&#20307;&#65292;&#22312;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30001;&#20110;&#22312;&#21508;&#31181;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#35757;&#32451;&#23427;&#20204;&#21487;&#33021;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26469;&#36827;&#34892;&#20998;&#25955;&#33410;&#28857;&#30340;&#21327;&#20316;&#65292;&#20351;&#24471;GNNs&#21487;&#20197;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#36827;&#34892;&#35757;&#32451;&#32780;&#19981;&#27844;&#38706;&#20219;&#20309;&#38142;&#25509;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#26356;&#22909;&#22320;&#21435;&#22122;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;GNNs&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#38480;&#21046;&#20174;&#25512;&#26029;&#20986;&#30340;&#38142;&#25509;&#27010;&#29575;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#30340;LDP&#26426;&#21046;&#30340;&#21464;&#20307;&#20043;&#19968;&#65292;&#20854;&#20013;&#22312;&#36739;&#20302;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#20272;&#35745;&#36739;&#23569;&#30340;&#38142;&#25509;&#65292;&#20197;&#36991;&#20813;&#24403;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#26102;&#20986;&#29616;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is hig
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.03179</link><description>&lt;p&gt;
SLiMe: &#20687;&#25105;&#19968;&#26679;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03179
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;SD&#65289;&#65292;&#22312;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#23545;&#24212;&#21644;3D&#24418;&#29366;&#29983;&#25104;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20986;SLiMe&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#23545;&#22270;&#20687;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;SLiMe&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24352;&#35757;&#32451;&#22270;&#20687;&#21450;&#20854;&#20998;&#21106;&#25513;&#33180;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SD&#20808;&#39564;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#21152;&#26435;&#32047;&#31215;&#33258;&#27880;&#24847;&#21147;&#22270;&#8221;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20248;&#21270;&#31283;&#23450;&#25193;&#25955;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#20351;&#24471;&#27599;&#20010;&#23884;&#20837;&#21482;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20998;&#21106;&#21306;&#22495;&#12290;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#28982;&#21518;&#22312;&#27880;&#24847;&#21147;&#22270;&#20013;&#31361;&#20986;&#26174;&#31034;&#20998;&#21106;&#21306;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#36825;&#20351;&#24471;SLiMe&#21487;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#25311;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#21644;&#22270;&#20687;&#20687;&#32032;&#35823;&#24046;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#28210;&#26579;&#26469;&#20248;&#21270;&#29289;&#20307;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#22330;&#26223;&#20013;&#30340;&#20301;&#32622;&#12290;&#20351;&#29992;&#31532;&#20108;&#31181;&#27169;&#24577;&#65288;&#28608;&#20809;&#38647;&#36798;&#65289;&#33021;&#22815;&#21152;&#24555;&#25910;&#25947;&#65292;&#36825;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#29992;&#36884;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;</title><link>http://arxiv.org/abs/2309.03177</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19977;&#32500;&#29289;&#20307;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
3D Object Positioning Using Differentiable Multimodal Learning. (arXiv:2309.03177v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#25311;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#21644;&#22270;&#20687;&#20687;&#32032;&#35823;&#24046;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#28210;&#26579;&#26469;&#20248;&#21270;&#29289;&#20307;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#22330;&#26223;&#20013;&#30340;&#20301;&#32622;&#12290;&#20351;&#29992;&#31532;&#20108;&#31181;&#27169;&#24577;&#65288;&#28608;&#20809;&#38647;&#36798;&#65289;&#33021;&#22815;&#21152;&#24555;&#25910;&#25947;&#65292;&#36825;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#29992;&#36884;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#25311;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#21644;&#22270;&#20687;&#20687;&#32032;&#35823;&#24046;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#28210;&#26579;&#26469;&#20248;&#21270;&#29289;&#20307;&#30456;&#23545;&#20110;&#35266;&#23519;&#32773;&#25110;&#26576;&#20010;&#21442;&#32771;&#29289;&#20307;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#22330;&#26223;&#20013;&#30340;&#20301;&#32622;&#12290;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23436;&#25104;&#29289;&#20307;&#20301;&#32622;&#20248;&#21270;&#65292;&#25439;&#22833;&#20989;&#25968;&#21463;&#21040;&#20004;&#31181;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#36890;&#24120;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#20165;&#20351;&#29992;&#22270;&#20687;&#20687;&#32032;&#35823;&#24046;&#26469;&#36827;&#34892;&#29289;&#20307;&#25918;&#32622;&#20248;&#21270;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#31532;&#20108;&#31181;&#27169;&#24577;&#65288;&#28608;&#20809;&#38647;&#36798;&#65289;&#33021;&#22815;&#21152;&#24555;&#25910;&#25947;&#12290;&#34701;&#21512;&#20256;&#24863;&#22120;&#36755;&#20837;&#30340;&#36825;&#31181;&#26041;&#27861;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#29992;&#36884;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#22330;&#26223;&#20013;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#22810;&#31181;&#25968;&#25454;&#27169;&#25311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes a multi-modal method using simulated Lidar data via ray tracing and image pixel loss with differentiable rendering to optimize an object's position with respect to an observer or some referential objects in a computer graphics scene. Object position optimization is completed using gradient descent with the loss function being influenced by both modalities. Typical object placement optimization is done using image pixel loss with differentiable rendering only, this work shows the use of a second modality (Lidar) leads to faster convergence. This method of fusing sensor input presents a potential usefulness for autonomous vehicles, as these methods can be used to establish the locations of multiple actors in a scene. This article also presents a method for the simulation of multiple types of data to be used in the training of autonomous vehicles.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03169</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#33616;&#31995;&#32479;&#20174;&#38544;&#24335;&#21453;&#39304;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#20294;&#24448;&#24448;&#20250;&#24573;&#30053;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#22810;&#34892;&#20026;&#20114;&#21160;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#35201;&#20040;&#23558;&#25152;&#26377;&#34892;&#20026;&#65292;&#22914;&#8220;&#21360;&#35937;&#8221;&#65288;&#20197;&#21069;&#31216;&#20026;&#8220;&#27983;&#35272;&#8221;&#65289;&#12289;&#8220;&#28155;&#21152;&#21040;&#36141;&#29289;&#36710;&#8221;&#21644;&#8220;&#36141;&#20080;&#8221;&#65292;&#24402;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#8220;&#20114;&#21160;&#8221;&#26631;&#31614;&#65292;&#35201;&#20040;&#20165;&#20248;&#20808;&#32771;&#34385;&#30446;&#26631;&#34892;&#20026;&#65292;&#36890;&#24120;&#26159;&#8220;&#36141;&#20080;&#8221;&#34892;&#20026;&#65292;&#24182;&#20002;&#24323;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#20449;&#21495;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#35797;&#22270;&#35299;&#20915;&#36825;&#31181;&#31616;&#21270;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#20110;&#20248;&#21270;&#30446;&#26631;&#34892;&#20026;&#65292;&#19982;&#25968;&#25454;&#31232;&#32570;&#20316;&#26007;&#20105;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#32469;&#36807;&#20102;&#19982;&#34892;&#20026;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#26377;&#20851;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;H&#8221;ierarchical &#8220;M&#8221;ulti-behavior &#8220;G&#8221;raph Attention &#8220;N&#8221;etwork&#65288;HMGN&#65289;&#12290;&#36825;&#20010;&#24320;&#21019;&#24615;&#30340;&#26694;&#26550;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;
&lt;/p&gt;
&lt;p&gt;
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#21160;&#21253;&#21547;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.03167</link><description>&lt;p&gt;
&#20998;&#21106;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Split-Boost Neural Networks. (arXiv:2309.03167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#21160;&#21253;&#21547;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#35757;&#32451;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20851;&#38190;&#38556;&#30861;&#26159;&#38656;&#35201;&#36873;&#25321;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21069;&#39304;&#26550;&#26500;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#65288;split-boost&#65289;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#24182;&#33258;&#21160;&#21253;&#21547;&#19968;&#31181;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#22320;&#24314;&#27169;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26368;&#32456;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#20813;&#26174;&#24335;&#24314;&#27169;&#27491;&#21017;&#21270;&#39033;&#65292;&#20943;&#23569;&#24635;&#30340;&#36229;&#21442;&#25968;&#25968;&#37327;&#24182;&#21152;&#36895;&#35843;&#20248;&#38454;&#27573;&#12290;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#21311;&#21517;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24212;&#29992;&#20110;&#22522;&#20934;&#21307;&#30103;&#20445;&#38505;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The calibration and training of a neural network is a complex and time-consuming procedure that requires significant computational resources to achieve satisfactory results. Key obstacles are a large number of hyperparameters to select and the onset of overfitting in the face of a small amount of data. In this framework, we propose an innovative training strategy for feed-forward architectures - called split-boost - that improves performance and automatically includes a regularizing behaviour without modeling it explicitly. Such a novel approach ultimately allows us to avoid explicitly modeling the regularization term, decreasing the total number of hyperparameters and speeding up the tuning phase. The proposed strategy is tested on a real-world (anonymized) dataset within a benchmark medical insurance design problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#22312;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#20248;&#21270;&#35206;&#30422;&#36712;&#36857;&#65292;&#24182;&#37319;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03157</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#65306;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#22312;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#20248;&#21270;&#35206;&#30422;&#36712;&#36857;&#65292;&#24182;&#37319;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#26377;&#25928;&#30340;&#36335;&#24452;&#65292;&#35206;&#30422;&#20852;&#36259;&#21306;&#22495;&#20013;&#30340;&#27599;&#19968;&#20010;&#28857;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20805;&#30005;&#26377;&#38480;&#30340;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#30005;&#21147;&#38480;&#21046;CPP&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#23558;&#20805;&#30005;&#26053;&#31243;&#25972;&#21512;&#21040;&#25972;&#20307;&#35206;&#30422;&#31574;&#30053;&#20013;&#24102;&#26469;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#31361;&#20986;&#20102;&#21046;&#23450;&#25112;&#30053;&#24615;&#12289;&#38271;&#26399;&#24615;&#20915;&#31574;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#36816;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#26469;&#20248;&#21270;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#30340;&#35206;&#30422;&#36712;&#36857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20301;&#32622;&#21382;&#21490;&#35760;&#24405;&#32473;&#26234;&#33021;&#20307;&#65292;&#20197;&#22788;&#29702;&#20805;&#30005;&#33021;&#21147;&#24341;&#36215;&#30340;&#26032;&#20986;&#29616;&#30340;&#29366;&#24577;&#24490;&#29615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#23545;&#26410;&#30693;&#22320;&#22270;&#30340;&#27867;&#21270;&#24615;&#33021;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#30340;&#20449;&#36947;&#30340;&#26497;&#21270;&#30721;&#12290;&#36890;&#36807;&#26367;&#20195;&#21407;&#22987;&#30340;&#36830;&#32493;&#21462;&#28040;&#35299;&#30721;&#22120;&#30340;&#26680;&#24515;&#20803;&#32032;&#65292;&#21363;&#26816;&#39564;&#33410;&#28857;&#12289;&#27604;&#29305;&#33410;&#28857;&#21644;&#36719;&#21028;&#20915;&#65292;&#35774;&#35745;&#20102;&#31070;&#32463;&#36830;&#32493;&#21462;&#28040;&#35299;&#30721;&#22120;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03148</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#26497;&#21270;&#30721;&#29992;&#20110;&#26410;&#30693;&#24102;&#21644;&#19981;&#24102;&#35760;&#24518;&#30340;&#20449;&#36947;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory. (arXiv:2309.03148v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#30340;&#20449;&#36947;&#30340;&#26497;&#21270;&#30721;&#12290;&#36890;&#36807;&#26367;&#20195;&#21407;&#22987;&#30340;&#36830;&#32493;&#21462;&#28040;&#35299;&#30721;&#22120;&#30340;&#26680;&#24515;&#20803;&#32032;&#65292;&#21363;&#26816;&#39564;&#33410;&#28857;&#12289;&#27604;&#29305;&#33410;&#28857;&#21644;&#36719;&#21028;&#20915;&#65292;&#35774;&#35745;&#20102;&#31070;&#32463;&#36830;&#32493;&#21462;&#28040;&#35299;&#30721;&#22120;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#30340;&#20449;&#36947;&#30340;&#26497;&#21270;&#30721;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20449;&#36947;&#20197;&#8220;&#40657;&#30418;&#8221;&#30340;&#24418;&#24335;&#32473;&#23450;&#65292;&#35774;&#35745;&#32773;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#35266;&#27979;&#26469;&#33719;&#21462;&#20449;&#36947;&#30340;&#20449;&#24687;&#65292;&#20294;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#26174;&#24335;&#30340;&#20449;&#36947;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#21462;&#28040;&#65288;SC&#65289;&#35299;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#31070;&#32463;&#36830;&#32493;&#21462;&#28040;&#65288;NSC&#65289;&#35299;&#30721;&#22120;&#12290;NSC&#35299;&#30721;&#22120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#26367;&#20195;&#21407;&#22987;SC&#35299;&#30721;&#22120;&#30340;&#26680;&#24515;&#20803;&#32032;&#65292;&#21363;&#26816;&#39564;&#33410;&#28857;&#12289;&#27604;&#29305;&#33410;&#28857;&#21644;&#36719;&#21028;&#20915;&#12290;&#38500;&#20102;NSC&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20854;&#20182;&#23884;&#20837;&#20449;&#36947;&#36755;&#20986;&#21040;SC&#35299;&#30721;&#22120;&#36755;&#20837;&#31354;&#38388;&#30340;NN&#12290;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#21253;&#25324;NSC&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;NSC&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19981;&#38543;&#20449;&#36947;&#35760;&#24518;&#22823;&#23567;&#22686;&#38271;&#65292;&#36825;&#26159;&#20854;&#19982;&#36830;&#32493;&#21462;&#28040;&#30340;&#20027;&#35201;&#20248;&#21183;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a novel data-driven methodology for designing polar codes for channels with and without memory is proposed. The methodology is suitable for the case where the channel is given as a "black-box" and the designer has access to the channel for generating observations of its inputs and outputs, but does not have access to the explicit channel model. The proposed method leverages the structure of the successive cancellation (SC) decoder to devise a neural SC (NSC) decoder. The NSC decoder uses neural networks (NNs) to replace the core elements of the original SC decoder, the check-node, the bit-node and the soft decision. Along with the NSC, we devise additional NN that embeds the channel outputs into the input space of the SC decoder. The proposed method is supported by theoretical guarantees that include the consistency of the NSC. Also, the NSC has computational complexity that does not grow with the channel memory size. This sets its main advantage over successive cancellat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03145</link><description>&lt;p&gt;
&#26368;&#20339;&#33218;&#36530;&#36991;&#65306;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits. (arXiv:2309.03145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MABs&#65289;&#30340;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#65306;&#20219;&#20309;&#20351;&#29992;&#23376;&#32447;&#24615;&#20869;&#23384;&#30340;&#27969;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992; $O(\frac{n}{\Delta^2})$ &#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#38656;&#35201; $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ &#20010;&#36890;&#36947;&#12290;&#36825;&#37324;&#65292;$n$ &#26159;&#33218;&#30340;&#25968;&#37327;&#65292;$\Delta$ &#26159;&#26368;&#20339;&#33218;&#21644;&#27425;&#20339;&#33218;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;Jin&#31561;&#20154;[ICML'21]&#30340; $O(\log(\frac{1}{\Delta}))$ &#36890;&#36947;&#31639;&#27861;&#30456;&#21305;&#37197;&#65288;&#38500;&#20102;&#20302;&#38454;&#39033;&#65289;&#65292;&#35813;&#31639;&#27861;&#20165;&#20351;&#29992; $O(1)$ &#20869;&#23384;&#65292;&#24182;&#22238;&#31572;&#20102;Assadi&#21644;Wang[STOC'20]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#20010;&#31561;&#21464;&#21521;&#37327;&#12290;&#22810;&#36890;&#36947;EGNN&#22312;&#22810;&#20010;&#29289;&#29702;&#31995;&#32479;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#30340;&#21333;&#36890;&#36947;EGNN&#65292;&#32780;&#19988;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#36816;&#34892;&#26102;&#38388;&#25110;&#21442;&#25968;&#25968;&#37327;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.03139</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20010;&#21521;&#37327;&#36890;&#36947;&#25913;&#21892;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks. (arXiv:2309.03139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#20010;&#31561;&#21464;&#21521;&#37327;&#12290;&#22810;&#36890;&#36947;EGNN&#22312;&#22810;&#20010;&#29289;&#29702;&#31995;&#32479;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#30340;&#21333;&#36890;&#36947;EGNN&#65292;&#32780;&#19988;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#36816;&#34892;&#26102;&#38388;&#25110;&#21442;&#25968;&#25968;&#37327;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#20010;&#31561;&#21464;&#21521;&#37327;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#36825;&#20010;&#25193;&#23637;&#65292;&#24182;&#26174;&#31034;&#23427;&#22312;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#22522;&#20934;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#36816;&#34892;&#26102;&#38388;&#25110;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;&#25552;&#20986;&#30340;&#22810;&#36890;&#36947;EGNN&#22312;N&#20307;&#24102;&#30005;&#31890;&#23376;&#21160;&#21147;&#23398;&#12289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#39044;&#27979;&#22826;&#38451;&#31995;&#22825;&#20307;&#36712;&#36857;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#21333;&#36890;&#36947;EGNN&#12290;&#37492;&#20110;&#22810;&#36890;&#36947;EGNN&#30340;&#39069;&#22806;&#20248;&#21183;&#21644;&#26368;&#23567;&#30340;&#38468;&#21152;&#25104;&#26412;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25193;&#23637;&#21487;&#33021;&#23545;&#20174;&#20107;&#29289;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#26377;&#23454;&#38469;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a natural extension to E(n)-equivariant graph neural networks that uses multiple equivariant vectors per node. We formulate the extension and show that it improves performance across different physical systems benchmark tasks, with minimal differences in runtime or number of parameters. The proposed multichannel EGNN outperforms the standard singlechannel EGNN on N-body charged particle dynamics, molecular property predictions, and predicting the trajectories of solar system bodies. Given the additional benefits and minimal additional cost of multi-channel EGNN, we suggest that this extension may be of practical use to researchers working in machine learning for the physical sciences
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;PCB&#21046;&#36896;&#20013;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#65292;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2309.03113</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#19978;&#26816;&#27979;PCB&#21046;&#36896;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features. (arXiv:2309.03113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03113
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;PCB&#21046;&#36896;&#20013;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#65292;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#65288;SPI&#65289;&#21644;&#33258;&#21160;&#20809;&#23398;&#26816;&#26597;&#65288;AOI&#65289;&#26426;&#22120;&#33258;&#21160;&#26816;&#27979;&#21360;&#21046;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#21046;&#36896;&#20013;&#30340;&#32570;&#38519;&#21487;&#20197;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#65292;&#26174;&#33879;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#20174;SPI&#25552;&#21462;&#30340;600&#19975;&#20010;&#24341;&#33050;&#30340;&#29305;&#24449;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20197;&#22312;PCB&#21046;&#36896;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#12290;&#36825;600&#19975;&#20010;PCB&#24341;&#33050;&#23545;&#24212;&#20110;&#23646;&#20110;15,387&#20010;PCB&#30340;200&#19975;&#20010;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#30340;ML&#27169;&#22411;&#65292;&#36845;&#20195;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#20214;&#21644;PCB ID&#32452;&#21512;&#30340;&#24341;&#33050;&#32423;SPI&#29305;&#24449;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#32452;&#20214;&#21644;PCB&#32423;&#21035;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#36825;&#20351;&#24471;ML&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#22312;&#24341;&#33050;&#32423;&#21035;&#21487;&#33021;&#19981;&#26126;&#26174;&#30340;&#24341;&#33050;&#38388;&#12289;&#32452;&#20214;&#38388;&#25110;&#31354;&#38388;&#25928;&#24212;&#12290;&#27169;&#22411;&#22312;&#24341;&#33050;&#12289;&#32452;&#20214;&#21644;PCB&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#31995;&#25968;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.03107</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#22810;&#23610;&#24230;&#26925;&#22278;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving multiscale elliptic problems by sparse radial basis function neural networks. (arXiv:2309.03107v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#31995;&#25968;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#23610;&#24230;&#31995;&#25968;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#21463;&#28145;&#24230;&#28151;&#21512;&#27531;&#24046;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20108;&#38454;&#38382;&#39064;&#37325;&#20889;&#20026;&#19968;&#38454;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22810;&#20010;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;RBFNNs&#65289;&#26469;&#36924;&#36817;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;RBFNN&#30340;&#31616;&#21333;&#24615;&#32780;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22240;&#27492;&#65292;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20004;&#37096;&#20998;&#65306;&#29992;&#20110;&#19968;&#38454;&#31995;&#32479;&#21644;&#36793;&#30028;&#26465;&#20214;&#27531;&#24046;&#30340;L2&#25439;&#22833;&#65292;&#20197;&#21450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBFs&#65289;&#30340;&#26435;&#37325;&#30340;L1&#27491;&#21017;&#21270;&#39033;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#26469;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has been successfully applied to various fields of scientific computing in recent years. In this work, we propose a sparse radial basis function neural network method to solve elliptic partial differential equations (PDEs) with multiscale coefficients. Inspired by the deep mixed residual method, we rewrite the second-order problem into a first-order system and employ multiple radial basis function neural networks (RBFNNs) to approximate unknown functions in the system. To aviod the overfitting due to the simplicity of RBFNN, an additional regularization is introduced in the loss function. Thus the loss function contains two parts: the $L_2$ loss for the residual of the first-order system and boundary conditions, and the $\ell_1$ regularization term for the weights of radial basis functions (RBFs). An algorithm for optimizing the specific loss function is introduced to accelerate the training process. The accuracy and effectiveness of the proposed method are demonstrate
&lt;/p&gt;</description></item><item><title>ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03103</link><description>&lt;p&gt;
ContrastWSD: &#20351;&#29992;&#35789;&#20041;&#28040;&#23696;&#21152;&#24378;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03103
&lt;/p&gt;
&lt;p&gt;
ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ContrastWSD&#65292;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;(MIP)&#21644;&#35789;&#20041;&#28040;&#23696;(WSD)&#26469;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#30830;&#23450;&#23427;&#22312;&#21477;&#23376;&#20013;&#26159;&#21542;&#20197;&#38544;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;WSD&#27169;&#22411;&#24471;&#20986;&#30340;&#21333;&#35789;&#35789;&#20041;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22686;&#24378;&#20102;&#38544;&#21947;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#36229;&#36807;&#20102;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#20165;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#20854;&#20182;&#22806;&#37096;&#30693;&#35782;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#24378;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#25512;&#36827;&#38544;&#21947;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
&lt;/p&gt;</description></item><item><title>PeptideBERT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32957;&#30340;&#28342;&#34880;&#24615;&#12289;&#28342;&#35299;&#24615;&#21644;&#38459;&#22434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03099</link><description>&lt;p&gt;
PeptideBERT: &#22522;&#20110;Transformer&#30340;&#32957;&#24615;&#36136;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction. (arXiv:2309.03099v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03099
&lt;/p&gt;
&lt;p&gt;
PeptideBERT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32957;&#30340;&#28342;&#34880;&#24615;&#12289;&#28342;&#35299;&#24615;&#21644;&#38459;&#22434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#34507;&#30333;&#36136;&#24314;&#27169;&#39046;&#22495;&#25317;&#26377;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#34507;&#30333;&#24207;&#21015;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21033;&#29992;Transformer&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26174;&#24335;&#32467;&#26500;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24207;&#21015;&#21040;&#24615;&#36136;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PeptideBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#32957;&#30340;&#19977;&#20010;&#20851;&#38190;&#24615;&#36136;&#65288;&#28342;&#34880;&#24615;&#12289;&#28342;&#35299;&#24615;&#21644;&#38459;&#22434;&#24615;&#65289;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;PeptideBert&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#20855;&#26377;12&#20010;&#27880;&#24847;&#21147;&#22836;&#21644;12&#20010;&#38544;&#34255;&#23618;&#30340;ProtBERT Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#23545;&#36825;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#20197;&#36866;&#24212;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#28342;&#34880;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#22312;&#39044;&#27979;&#32957;&#30340;&#25239;&#38750;&#29305;&#24322;&#24615;&#30456;&#20114;&#20316;&#29992;&#33021;&#21147;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non-fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide's potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide's capacity to resist non-specific interactions. This m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21517;&#20026;SIAD&#65292;&#23427;&#22312;&#23384;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03094</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20984;&#24809;&#32602;&#30340;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#24179;&#28369;ADMM
&lt;/p&gt;
&lt;p&gt;
Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties. (arXiv:2309.03094v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21517;&#20026;SIAD&#65292;&#23427;&#22312;&#23384;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#22914;&#26368;&#23567;&#26368;&#22823;&#20985;&#24809;&#32602;&#65288;MCP&#65289;&#21644;&#24179;&#28369;&#21098;&#20999;&#32477;&#23545;&#20559;&#24046;&#65288;SCAD&#65289;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#29305;&#24615;&#32463;&#24120;&#23548;&#33268;&#35768;&#22810;&#31639;&#27861;&#30340;&#25910;&#25947;&#22256;&#38590;&#12290;&#34429;&#28982;&#36845;&#20195;&#25216;&#26415;&#22914;&#22352;&#26631;&#19979;&#38477;&#21644;&#23616;&#37096;&#32447;&#24615;&#36817;&#20284;&#21487;&#20197;&#20419;&#36827;&#25910;&#25947;&#65292;&#20294;&#36807;&#31243;&#36890;&#24120;&#24456;&#24930;&#12290;&#36825;&#31181;&#32531;&#24930;&#30340;&#36895;&#24230;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#22312;&#27599;&#19968;&#27493;&#36816;&#34892;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#30452;&#21040;&#23436;&#20840;&#25910;&#25947;&#65292;&#36825;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;\emph{&#20108;&#27425;&#25910;&#25947;&#36845;&#20195;}&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#65288;ADMM&#65289;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#36882;&#22686;&#24809;&#32602;&#21442;&#25968;&#30340;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;SIAD&#65292;&#19987;&#38376;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;SIAD&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and est
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.03092</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Markov Equivalence in Cyclic Directed Graphs. (arXiv:2309.03092v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#21487;&#33021;&#21253;&#21547;&#24490;&#29615;&#30340;&#26377;&#21521;&#22270;&#19978;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Thomas Richardson&#22312;90&#24180;&#20195;&#20013;&#26399;&#20851;&#20110;&#24490;&#29615;&#27169;&#22411;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#30340;&#24490;&#29615;&#31561;&#20215;&#23450;&#29702;(CET)&#65292;&#20294;&#26159;&#29616;&#22312;&#20174;&#19968;&#20010;&#31062;&#20808;&#30340;&#35282;&#24230;&#37325;&#26032;&#34920;&#36848;&#12290;&#24471;&#21040;&#30340;&#29305;&#24449;&#23548;&#33268;&#20102;&#19968;&#31181;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#27010;&#24565;&#19978;&#31616;&#21270;&#30340;&#29305;&#24449;&#21487;&#33021;&#26377;&#21161;&#20110;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#37325;&#26032;&#28608;&#21457;&#23545;&#24490;&#29615;&#21457;&#29616;&#30340;&#29702;&#35770;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;&#35813;&#29256;&#26412;&#21253;&#25324;&#20102;&#23450;&#29702;1&#20013;&#35268;&#21017;(iv)&#30340;&#20462;&#27491;&#65292;&#20197;&#21450;&#31639;&#27861;2&#31532;2&#37096;&#20998;&#30340;&#30456;&#20851;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new, efficient procedure to establish Markov equivalence between directed graphs that may or may not contain cycles under the \textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem (CET) in the seminal works on cyclic models by Thomas Richardson in the mid '90s, but now rephrased from an ancestral perspective. The resulting characterization leads to a procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to a significantly reduced algorithmic complexity. The conceptually simplified characterization may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders. This version includes a correction to rule (iv) in Theorem 1, and the subsequent adjustment in part 2 of Algorithm 2.
&lt;/p&gt;</description></item><item><title>LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03086</link><description>&lt;p&gt;
LieDetect: &#20174;&#28857;&#20113;&#20013;&#26816;&#27979;&#32039;&#33268;Lie&#32676;&#30340;&#34920;&#31034;&#36712;&#36947;
&lt;/p&gt;
&lt;p&gt;
LieDetect: Detection of representation orbits of compact Lie groups from point clouds. (arXiv:2309.03086v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03086
&lt;/p&gt;
&lt;p&gt;
LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#25253;&#36947;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#20316;&#20026;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#30452;&#21644;&#12290;&#32780;&#19988;&#65292;&#23545;&#34920;&#31034;&#31867;&#22411;&#30340;&#20102;&#35299;&#21487;&#20197;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#20294;&#21482;&#32771;&#34385;&#20102;SO(2), T^d, SU(2)&#21644;SO(3)&#30340;&#23454;&#20363;&#21270;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;Hausdorff&#21644;Wasserstein&#36317;&#31163;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#26469;&#33258;&#20110;&#20960;&#20309;&#27979;&#24230;&#29702;&#35770;&#65292;&#35745;&#31639;&#20960;&#20309;&#21644;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#12290;&#31639;&#27861;&#22312;&#39640;&#36798;16&#32500;&#30340;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#22270;&#20687;&#20998;&#26512;&#65292;&#35856;&#27874;&#20998;&#26512;&#21644;&#32463;&#20856;&#21147;&#23398;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful to identify the Lie group that generates the action. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2) and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 16, as well as real-life applications in image analysis, harmonic analysis, and classical mechanics systems, achieving very accurate results.
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>ORL-AUDITOR&#26159;&#19968;&#31181;&#29992;&#20110;&#23457;&#26680;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#21644;&#38450;&#27490;&#28389;&#29992;&#25110;&#20405;&#26435;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.03081</link><description>&lt;p&gt;
ORL-AUDITOR&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#38598;&#23457;&#26680;
&lt;/p&gt;
&lt;p&gt;
ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning. (arXiv:2309.03081v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03081
&lt;/p&gt;
&lt;p&gt;
ORL-AUDITOR&#26159;&#19968;&#31181;&#29992;&#20110;&#23457;&#26680;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#21644;&#38450;&#27490;&#28389;&#29992;&#25110;&#20405;&#26435;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#39046;&#22495;&#65292;&#25968;&#25454;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36164;&#20135;&#65292;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;&#31163;&#32447;DRL&#65289;&#32463;&#24120;&#29992;&#20110;&#22312;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#19982;&#30495;&#23454;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#22312;&#32447;DRL&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#35768;&#22810;&#26426;&#26500;&#20197;&#24320;&#28304;&#35768;&#21487;&#30340;&#24418;&#24335;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#23384;&#22312;&#28508;&#22312;&#30340;&#28389;&#29992;&#25110;&#20405;&#26435;&#39118;&#38505;&#12290;&#21521;&#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#27700;&#21360;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#24050;&#32463;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#19988;&#21518;&#32493;&#20462;&#25913;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20854;&#20182;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#25968;&#25454;&#38598;&#25512;&#26029;&#21644;&#25104;&#21592;&#25512;&#26029;&#65292;&#22312;&#31163;&#32447;DRL&#22330;&#26223;&#19979;&#30001;&#20110;&#27169;&#22411;&#34892;&#20026;&#29305;&#24449;&#22810;&#26679;&#21644;&#31163;&#32447;&#29615;&#22659;&#38480;&#21046;&#32780;&#19981;&#36215;&#20316;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#32929;&#31080;&#24066;&#22330;&#19978;&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#65292;&#29983;&#25104;&#27934;&#23519;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#30456;&#23545;&#26631;&#26222;500&#25351;&#25968;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2309.03079</link><description>&lt;p&gt;
GPT-InvestAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#32929;&#31080;&#25237;&#36164;&#31574;&#30053;&#36890;&#36807;&#24180;&#24230;&#25253;&#21578;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models. (arXiv:2309.03079v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#32929;&#31080;&#24066;&#22330;&#19978;&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#65292;&#29983;&#25104;&#27934;&#23519;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#30456;&#23545;&#26631;&#26222;500&#25351;&#25968;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#21253;&#21547;&#20102;&#20851;&#20110;&#20854;&#36130;&#21153;&#29366;&#20917;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#20854;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#36825;&#20123;&#25253;&#21578;&#30340;&#20869;&#23481;&#38750;&#24120;&#20840;&#38754;&#65292;&#26377;&#26102;&#29978;&#33267;&#36229;&#36807;100&#39029;&#12290;&#21363;&#20351;&#23545;&#20110;&#19968;&#20010;&#20844;&#21496;&#26469;&#35828;&#65292;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#20063;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#24037;&#20316;&#65292;&#26356;&#19981;&#29992;&#35828;&#25972;&#20010;&#20844;&#21496;&#32676;&#20307;&#20102;&#12290;&#22810;&#24180;&#26469;&#65292;&#37329;&#34701;&#19987;&#23478;&#24050;&#32463;&#33021;&#22815;&#30456;&#23545;&#24555;&#36895;&#22320;&#20174;&#36825;&#20123;&#25991;&#20214;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22810;&#24180;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#31616;&#21270;&#23545;&#25152;&#26377;&#20844;&#21496;&#24180;&#24230;&#25253;&#21578;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;LLM&#29983;&#25104;&#30340;&#27934;&#23519;&#21147;&#34987;&#27719;&#32534;&#22312;&#19968;&#20010;&#37327;&#21270;&#39118;&#26684;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#36827;&#34892;&#34917;&#20805;&#12290;&#28982;&#21518;&#20351;&#29992;LLM&#36755;&#20986;&#20316;&#20026;&#29305;&#24449;&#35757;&#32451;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21069;&#21521;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#26631;&#26222;500&#25351;&#25968;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;&#26412;&#25991;&#26088;&#22312;
&lt;/p&gt;
&lt;p&gt;
Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&amp;P500 returns. This paper intends
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#31995;&#22806;&#34892;&#26143;&#22823;&#27668;&#21387;&#21147;-&#28201;&#24230;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23545;&#20998;&#24067;&#21151;&#33021;&#24418;&#24335;&#30340;&#26126;&#30830;&#20551;&#35774;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#33021;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2309.03075</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#31995;&#22806;&#34892;&#26143;&#22823;&#27668;&#21387;&#21147;-&#28201;&#24230;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks. (arXiv:2309.03075v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#31995;&#22806;&#34892;&#26143;&#22823;&#27668;&#21387;&#21147;-&#28201;&#24230;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23545;&#20998;&#24067;&#21151;&#33021;&#24418;&#24335;&#30340;&#26126;&#30830;&#20551;&#35774;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#33021;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#22806;&#34892;&#26143;&#22823;&#27668;&#26816;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#25216;&#26415;&#21644;&#21069;&#21521;&#27169;&#25311;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#20272;&#31639;&#35266;&#27979;&#20809;&#35889;&#26469;&#25512;&#26029;&#22823;&#27668;&#24615;&#36136;&#12290;&#27169;&#25311;&#20809;&#35889;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21387;&#21147;-&#28201;&#24230;&#65288;PT&#65289;&#20998;&#24067;&#65292;&#23427;&#25551;&#36848;&#20102;&#22823;&#27668;&#30340;&#28909;&#32467;&#26500;&#12290;&#30446;&#21069;&#24120;&#29992;&#30340;&#22823;&#27668;&#26816;&#27979;&#27969;&#31243;&#36890;&#24120;&#20351;&#29992;&#20102;&#20154;&#20026;&#25311;&#21512;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#23558;&#25512;&#27979;&#30340;PT&#20998;&#24067;&#38480;&#21046;&#20026;&#31616;&#21333;&#30340;&#36817;&#20284;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#30456;&#23545;&#36739;&#22810;&#30340;&#21442;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#19978;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;PT&#20998;&#24067;&#65292;&#23427;&#19981;&#38656;&#35201;&#23545;PT&#20998;&#24067;&#30340;&#21151;&#33021;&#24418;&#24335;&#20570;&#20986;&#26126;&#30830;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#23427;&#23398;&#20064;&#20102;&#20989;&#25968;&#65288;PT&#20998;&#24067;&#65289;&#30340;&#20998;&#24067;&#12290;&#27599;&#20010;&#20998;&#24067;&#30001;&#19968;&#20010;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#26465;&#20214;&#21270;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atmospheric retrievals (AR) of exoplanets typically rely on a combination of a Bayesian inference technique and a forward simulator to estimate atmospheric properties from an observed spectrum. A key component in simulating spectra is the pressure-temperature (PT) profile, which describes the thermal structure of the atmosphere. Current AR pipelines commonly use ad hoc fitting functions here that limit the retrieved PT profiles to simple approximations, but still use a relatively large number of parameters. In this work, we introduce a conceptually new, data-driven parameterization scheme for physically consistent PT profiles that does not require explicit assumptions about the functional form of the PT profiles and uses fewer parameters than existing methods. Our approach consists of a latent variable model (based on a neural network) that learns a distribution over functions (PT profiles). Each profile is represented by a low-dimensional vector that can be used to condition a decoder
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#25163;&#20889;&#23383;&#31526;&#20998;&#21106;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#23558;&#20998;&#21106;&#19982;&#35782;&#21035;&#35299;&#32806;&#65292;&#36890;&#36807;&#23558;&#20854;&#35270;&#20026;&#26679;&#28857;&#21644;&#25991;&#26412;&#20013;&#23383;&#31526;&#20043;&#38388;&#30340;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;Transformer&#35299;&#30721;&#22120;&#22359;&#20013;&#23398;&#20064;&#21040;&#30340;&#23383;&#31526;&#26597;&#35810;&#26469;&#24418;&#25104;&#27599;&#20010;&#32858;&#31867;&#12290;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#22312;&#32447;&#25163;&#20889;&#23383;&#36857;&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2309.03072</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#32447;&#25163;&#20889;&#23383;&#31526;&#20998;&#21106;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation. (arXiv:2309.03072v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#25163;&#20889;&#23383;&#31526;&#20998;&#21106;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#23558;&#20998;&#21106;&#19982;&#35782;&#21035;&#35299;&#32806;&#65292;&#36890;&#36807;&#23558;&#20854;&#35270;&#20026;&#26679;&#28857;&#21644;&#25991;&#26412;&#20013;&#23383;&#31526;&#20043;&#38388;&#30340;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;Transformer&#35299;&#30721;&#22120;&#22359;&#20013;&#23398;&#20064;&#21040;&#30340;&#23383;&#31526;&#26597;&#35810;&#26469;&#24418;&#25104;&#27599;&#20010;&#32858;&#31867;&#12290;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#22312;&#32447;&#25163;&#20889;&#23383;&#36857;&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25163;&#20889;&#23383;&#31526;&#20998;&#21106;&#36890;&#24120;&#19982;&#25163;&#20889;&#35782;&#21035;&#30456;&#20851;&#32852;&#65292;&#21363;&#20351;&#35782;&#21035;&#27169;&#22411;&#22312;&#35782;&#21035;&#36807;&#31243;&#20013;&#21253;&#25324;&#23450;&#20301;&#30456;&#20851;&#20301;&#32622;&#30340;&#26426;&#21046;&#65292;&#36890;&#24120;&#20063;&#26080;&#27861;&#20135;&#29983;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#23558;&#20998;&#21106;&#19982;&#35782;&#21035;&#35299;&#32806;&#21518;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992;&#35782;&#21035;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30340;&#24773;&#26223;&#26159;&#22312;&#20107;&#20808;&#24050;&#30693;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#65292;&#27492;&#26102;&#23383;&#31526;&#20998;&#21106;&#21464;&#25104;&#20102;&#26679;&#28857;&#21644;&#25991;&#26412;&#20013;&#23383;&#31526;&#20043;&#38388;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;&#21463;&#21040;K-means&#32858;&#31867;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#32858;&#31867;&#20998;&#37197;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;&#32858;&#31867;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#22359;&#20013;&#23398;&#20064;&#21040;&#30340;&#23383;&#31526;&#26597;&#35810;&#24418;&#25104;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#24120;&#35265;&#30340;&#22312;&#32447;&#25163;&#20889;&#23383;&#36857;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#23383;&#31526;&#20998;&#21106;&#30340;&#20934;&#30830;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-line handwritten character segmentation is often associated with handwriting recognition and even though recognition models include mechanisms to locate relevant positions during the recognition process, it is typically insufficient to produce a precise segmentation. Decoupling the segmentation from the recognition unlocks the potential to further utilize the result of the recognition. We specifically focus on the scenario where the transcription is known beforehand, in which case the character segmentation becomes an assignment problem between sampling points of the stylus trajectory and characters in the text. Inspired by the $k$-means clustering algorithm, we view it from the perspective of cluster assignment and present a Transformer-based architecture where each cluster is formed based on a learned character query in the Transformer decoder block. In order to assess the quality of our approach, we create character segmentation ground truths for two popular on-line handwriting d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#26368;&#26174;&#33879;&#24433;&#21709;&#30340;&#21442;&#25968;&#26041;&#21521;&#26500;&#24314;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30001;&#20110;&#21442;&#25968;&#31354;&#38388;&#39640;&#32500;&#24230;&#32780;&#24102;&#26469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#26174;&#33879;&#20943;&#23569;&#30340;&#20027;&#21160;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#25110;&#21464;&#20998;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.03061</link><description>&lt;p&gt;
&#23398;&#20064;&#20027;&#21160;&#23376;&#31354;&#38388;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks. (arXiv:2309.03061v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#26368;&#26174;&#33879;&#24433;&#21709;&#30340;&#21442;&#25968;&#26041;&#21521;&#26500;&#24314;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30001;&#20110;&#21442;&#25968;&#31354;&#38388;&#39640;&#32500;&#24230;&#32780;&#24102;&#26469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#26174;&#33879;&#20943;&#23569;&#30340;&#20027;&#21160;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#25110;&#21464;&#20998;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20855;&#26377;&#25552;&#20379;&#20855;&#26377;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#30001;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#32780;&#36896;&#25104;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#26368;&#26174;&#33879;&#24433;&#21709;&#30340;&#21442;&#25968;&#26041;&#21521;&#65292;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#65292;&#21363;&#20027;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#20027;&#21160;&#23376;&#31354;&#38388;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;&#37319;&#26679;&#26041;&#27861;&#65288;&#21542;&#21017;&#38590;&#20197;&#35745;&#31639;&#65289;&#25110;&#21464;&#20998;&#25512;&#29702;&#23454;&#29616;&#20102;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#20174;&#23454;&#35777;&#19978;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#21508;&#31181;&#22238;&#24402;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference for neural networks, or Bayesian deep learning, has the potential to provide well-calibrated predictions with quantified uncertainty and robustness. However, the main hurdle for Bayesian deep learning is its computational complexity due to the high dimensionality of the parameter space. In this work, we propose a novel scheme that addresses this limitation by constructing a low-dimensional subspace of the neural network parameters-referred to as an active subspace-by identifying the parameter directions that have the most significant influence on the output of the neural network. We demonstrate that the significantly reduced active subspace enables effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods, otherwise computationally intractable, or variational inference. Empirically, our approach provides reliable predictions with robust uncertainty estimates for various regression tasks.
&lt;/p&gt;</description></item><item><title>CoLA&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#21644;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#65292;&#33258;&#21160;&#26500;&#24314;&#20102;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2309.03060</link><description>&lt;p&gt;
CoLA: &#28145;&#20837;&#21033;&#29992;&#32452;&#21512;&#32467;&#26500;&#23454;&#29616;&#33258;&#21160;&#21644;&#39640;&#25928;&#30340;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra. (arXiv:2309.03060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03060
&lt;/p&gt;
&lt;p&gt;
CoLA&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#21644;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#65292;&#33258;&#21160;&#26500;&#24314;&#20102;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#39046;&#22495;&#28041;&#21450;&#21040;&#22823;&#35268;&#27169;&#30340;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#65292;&#22914;&#29305;&#24449;&#20998;&#35299;&#12289;&#35299;&#32447;&#24615;&#31995;&#32479;&#12289;&#35745;&#31639;&#30697;&#38453;&#25351;&#25968;&#21644;&#36857;&#20272;&#35745;&#31561;&#12290;&#28041;&#21450;&#30340;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;Krondor&#12289;&#21367;&#31215;&#12289;&#22359;&#23545;&#35282;&#12289;&#27714;&#21644;&#25110;&#20056;&#31215;&#31561;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#22823;&#35268;&#27169;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;CoLA&#65288;&#32452;&#21512;&#32447;&#24615;&#20195;&#25968;&#65289;&#12290;&#36890;&#36807;&#23558;&#32447;&#24615;&#25805;&#20316;&#31526;&#25277;&#35937;&#19982;&#32452;&#21512;&#35843;&#24230;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;CoLA&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#39640;&#25928;&#30340;&#25968;&#20540;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;CoLA&#36824;&#25552;&#20379;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#12289;&#20302;&#31934;&#24230;&#35745;&#31639;&#21644;JAX&#21644;PyTorch&#20013;&#30340;GPU&#21152;&#36895;&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#36890;&#36807;&#22810;&#37325;&#35843;&#24230;&#36866;&#24212;&#19979;&#28216;&#36719;&#20214;&#21253;&#20013;&#30340;&#26032;&#23545;&#35937;&#12289;&#25805;&#20316;&#21644;&#35268;&#21017;&#12290;CoLA&#21487;&#20197;&#21152;&#36895;&#35768;&#22810;&#20195;&#25968;&#25805;&#20316;&#65292;&#21516;&#26102;&#20063;&#20415;&#20110;&#21407;&#22411;&#21270;&#30697;&#38453;&#32467;&#26500;&#21644;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#30340;&#38477;&#20302;-
&lt;/p&gt;
&lt;p&gt;
Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named CoLA (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;CVE&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23041;&#32961;&#20248;&#20808;&#32423;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#23427;&#35299;&#20915;&#20102;CVE&#25551;&#36848;&#30340;&#32570;&#38519;&#65292;&#36890;&#36807;&#25552;&#20379;&#25915;&#20987;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;CVE&#30340;&#32508;&#21512;&#24369;&#28857;&#29305;&#24449;&#21644;&#23041;&#32961;&#24433;&#21709;&#24471;&#20197;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;CVE&#28431;&#27934;&#20998;&#26512;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.03040</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;CVE&#20998;&#26512;&#29992;&#20110;&#23041;&#32961;&#20248;&#20808;&#32423;&#21644;&#24433;&#21709;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated CVE Analysis for Threat Prioritization and Impact Prediction. (arXiv:2309.03040v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;CVE&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#23041;&#32961;&#20248;&#20808;&#32423;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#23427;&#35299;&#20915;&#20102;CVE&#25551;&#36848;&#30340;&#32570;&#38519;&#65292;&#36890;&#36807;&#25552;&#20379;&#25915;&#20987;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;CVE&#30340;&#32508;&#21512;&#24369;&#28857;&#29305;&#24449;&#21644;&#23041;&#32961;&#24433;&#21709;&#24471;&#20197;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;CVE&#28431;&#27934;&#20998;&#26512;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#28431;&#27934;&#21644;&#26292;&#38706;&#65288;CVE&#65289;&#26159;&#20027;&#21160;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#21253;&#25324;&#26381;&#21153;&#34917;&#19969;&#12289;&#23433;&#20840;&#21152;&#22266;&#31561;&#12290;&#28982;&#32780;&#65292;CVE&#36890;&#24120;&#25552;&#20379;&#22522;&#20110;&#20135;&#21697;&#30340;&#20302;&#32423;&#25551;&#36848;&#65292;&#20844;&#24320;&#25259;&#38706;&#30340;&#32593;&#32476;&#23433;&#20840;&#28431;&#27934;&#32570;&#20047;&#32508;&#21512;&#24369;&#28857;&#29305;&#24449;&#21644;&#23041;&#32961;&#24433;&#21709;&#20272;&#35745;&#25152;&#38656;&#30340;&#25915;&#20987;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#23545;&#20110;CVE&#20248;&#20808;&#32423;&#21644;&#28508;&#22312;&#23545;&#31574;&#30340;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#37327;CVE&#26102;&#12290;&#30446;&#21069;&#30340;&#34892;&#19994;&#23454;&#36341;&#28041;&#21450;&#23545;CVE&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#20351;&#29992;&#36890;&#29992;&#28431;&#27934;&#35780;&#20998;&#31995;&#32479;&#65288;CVSS&#65289;&#35780;&#20272;&#25915;&#20987;&#20005;&#37325;&#24615;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#36890;&#29992;&#24369;&#28857;&#26522;&#20030;&#65288;CWE&#65289;&#20197;&#36827;&#34892;&#28508;&#22312;&#32531;&#35299;&#30340;&#35782;&#21035;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#25163;&#21160;&#20998;&#26512;&#22312;&#28431;&#27934;&#20998;&#26512;&#36807;&#31243;&#20013;&#24418;&#25104;&#20102;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#65292;&#23548;&#33268;&#20027;&#21160;&#32593;&#32476;&#23433;&#20840;&#24037;&#20316;&#30340;&#25918;&#32531;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Common Vulnerabilities and Exposures (CVE) are pivotal information for proactive cybersecurity measures, including service patching, security hardening, and more. However, CVEs typically offer low-level, product-oriented descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking the essential attack semantic information required for comprehensive weakness characterization and threat impact estimation. This critical insight is essential for CVE prioritization and the identification of potential countermeasures, particularly when dealing with a large number of CVEs. Current industry practices involve manual evaluation of CVEs to assess their attack severities using the Common Vulnerability Scoring System (CVSS) and mapping them to Common Weakness Enumeration (CWE) for potential mitigation identification. Unfortunately, this manual analysis presents a major bottleneck in the vulnerability analysis process, leading to slowdowns in proactive cybersecurity efforts an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.03033</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#22218;&#32958;&#30149;&#20013;&#30340;&#24212;&#29992;: &#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis. (arXiv:2309.03033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22522;&#22240;&#34920;&#36798;&#20998;&#26512;&#23454;&#29616;&#23545;&#24739;&#32773;&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#30340;&#20934;&#30830;&#21644;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22218;&#32958;&#30149;&#65288;PKD&#65289;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#32958;&#33039;&#20013;&#22218;&#32959;&#30340;&#24418;&#25104;&#65292;&#36827;&#32780;&#23548;&#33268;&#33268;&#21629;&#30340;&#24182;&#21457;&#30151;&#65292;&#22240;&#27492;&#26089;&#26399;&#26816;&#27979;PKD&#23545;&#20110;&#26377;&#25928;&#31649;&#29702;&#35813;&#30149;&#24773;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#20013;&#28041;&#21450;&#30340;&#21508;&#31181;&#24739;&#32773;&#29305;&#23450;&#22240;&#32032;&#20351;&#20854;&#23545;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38590;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#30142;&#30149;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23545;&#24739;&#32773;&#21487;&#33021;&#30340;PKD&#36827;&#34892;&#20934;&#30830;&#19988;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#23558;&#20855;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#23884;&#20837;&#26041;&#27861;&#30340;&#24418;&#24335;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03023</link><description>&lt;p&gt;
&#29992;&#20110;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#25991;&#23383;&#23884;&#20837;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;
&lt;/p&gt;
&lt;p&gt;
Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals. (arXiv:2309.03023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#23558;&#20855;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#23884;&#20837;&#26041;&#27861;&#30340;&#24418;&#24335;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#23494;&#38598;&#25968;&#20540;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#20851;&#27880;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#36739;&#23569;&#26377;&#26041;&#27861;&#23558;&#25991;&#23383;&#25551;&#36848;&#25110;&#25968;&#20540;&#20449;&#24687;&#31561;&#30693;&#35782;&#20063;&#32771;&#34385;&#22312;&#20869;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#25991;&#23383;&#31867;&#22411;&#21644;&#23884;&#20837;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#21487;&#29992;&#20110;&#23558;&#24102;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#26041;&#27861;&#36827;&#34892;&#23884;&#20837;&#30340;&#24418;&#24335;&#12290;&#22312;kgbench&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embeddings are dense numerical representations of entities in a knowledge graph (KG). While the majority of approaches concentrate only on relational information, i.e., relations between entities, fewer approaches exist which also take information about literal values (e.g., textual descriptions or numerical information) into account. Those which exist are typically tailored towards a particular modality of literal and a particular embedding method. In this paper, we propose a set of universal preprocessing operators which can be used to transform KGs with literals for numerical, temporal, textual, and image information, so that the transformed KGs can be embedded with any method. The results on the kgbench dataset with three different embedding methods show promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#27010;&#29575;&#20803;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.03018</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#38144;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Amortised Inference in Bayesian Neural Networks. (arXiv:2309.03018v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#27010;&#29575;&#20803;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#32452;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#27979;&#35797;&#26102;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#36817;&#24180;&#26469;&#65292;&#27010;&#29575;&#20803;&#23398;&#20064;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#30340;&#27010;&#29575;&#20803;&#27169;&#22411;&#23384;&#22312;&#19968;&#20010;&#20849;&#24615;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#25165;&#33021;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#39044;&#27979;&#21644;&#33391;&#22909;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#24456;&#38590;&#33719;&#21462;&#36825;&#20040;&#22810;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#24341;&#20837;&#20102;&#25674;&#38144;&#20266;&#35266;&#27979;&#21464;&#20998;&#25512;&#29702;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;APOVI-BNN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#27010;&#29575;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#25674;&#38144;&#26041;&#26696;&#19979;&#33719;&#21462;&#30340;&#36817;&#20284;&#21518;&#39564;&#19982;&#20256;&#32479;&#21464;&#20998;&#25512;&#29702;&#33719;&#21462;&#30340;&#36817;&#20284;&#21518;&#39564;&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.  In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional vari
&lt;/p&gt;</description></item><item><title>SymED &#26159;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#21644;&#20998;&#24067;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#25968;&#25454;&#30340;&#31526;&#21495;&#21270;&#34920;&#31034;&#12290;&#23427;&#35299;&#20915;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#21270;&#34920;&#31034;&#26469;&#36827;&#34892;&#21508;&#31181;&#36793;&#32536;&#24212;&#29992;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.03014</link><description>&lt;p&gt;
SymED: &#22312;&#36793;&#32536;&#19978;&#33258;&#36866;&#24212;&#21644;&#22312;&#32447;&#31526;&#21495;&#21270;&#34920;&#31034;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
SymED: Adaptive and Online Symbolic Representation of Data on the Edge. (arXiv:2309.03014v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03014
&lt;/p&gt;
&lt;p&gt;
SymED &#26159;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#21644;&#20998;&#24067;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#25968;&#25454;&#30340;&#31526;&#21495;&#21270;&#34920;&#31034;&#12290;&#23427;&#35299;&#20915;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#21270;&#34920;&#31034;&#26469;&#36827;&#34892;&#21508;&#31181;&#36793;&#32536;&#24212;&#29992;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#33539;&#20363;&#26377;&#21161;&#20110;&#22788;&#29702;&#29289;&#32852;&#32593;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#22312;&#28304;&#22836;&#38468;&#36817;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#20256;&#36755;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#36825;&#20123;&#36805;&#36895;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#23384;&#22312;&#25361;&#25112;&#12290;&#31526;&#21495;&#21270;&#34920;&#31034;&#31639;&#27861;&#26159;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#20801;&#35768;&#23545;&#31526;&#21495;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65288;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#21644;&#36235;&#21183;&#39044;&#27979;&#65289;&#65292;&#20174;&#32780;&#21463;&#30410;&#20110;&#21508;&#31181;&#36793;&#32536;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31526;&#21495;&#21270;&#34920;&#31034;&#31639;&#27861;&#22312;&#35774;&#35745;&#19978;&#26159;&#38598;&#20013;&#21270;&#30340;&#65292;&#24182;&#19988;&#31163;&#32447;&#22788;&#29702;&#25209;&#37327;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#23454;&#26102;&#24212;&#29992;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SymED Symbolic Edge Data&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;&#19968;&#31181;&#22312;&#36793;&#32536;&#19978;&#36827;&#34892;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;SymED&#22522;&#20110;&#33258;&#36866;&#24212;&#24067;&#26391;&#26725;&#32858;&#21512;&#65288;ABBA&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#20551;&#35774;&#20302;&#21151;&#32791;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20808;&#23545;&#25968;&#25454;&#36827;&#34892;&#21021;&#27493;&#21387;&#32553;&#65288;&#21457;&#36865;&#32773;&#65289;&#65292;&#26356;&#24378;&#22823;&#30340;&#36793;&#32536;&#35774;&#22791;&#23545;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The edge computing paradigm helps handle the Internet of Things (IoT) generated data in proximity to its source. Challenges occur in transferring, storing, and processing this rapidly growing amount of data on resource-constrained edge devices. Symbolic Representation (SR) algorithms are promising solutions to reduce the data size by converting actual raw data into symbols. Also, they allow data analytics (e.g., anomaly detection and trend prediction) directly on symbols, benefiting large classes of edge applications. However, existing SR algorithms are centralized in design and work offline with batch data, which is infeasible for real-time cases. We propose SymED Symbolic Edge Data representation method, i.e., an online, adaptive, and distributed approach for symbolic representation of data on edge. SymED is based on the Adaptive Brownian Bridge-based Aggregation (ABBA), where we assume low-powered IoT devices do initial data compression (senders) and the more robust edge devices d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22374;&#26497;&#23567;&#20540;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#35299;&#37322;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;MLP&#23618;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#23454;&#35777;&#35266;&#23519;&#20026;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24402;&#22240;&#20110;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20294;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#20165;&#38480;&#20110;&#27973;&#23618;&#32593;&#32476;&#12289;&#23567;&#35757;&#32451;&#27493;&#38271;&#20197;&#21450;&#20462;&#25913;&#30340;&#35757;&#32451;&#65292;&#23613;&#31649;&#36825;&#31181;&#31232;&#30095;&#24615;&#24050;&#22312;&#36890;&#36807;vanilla&#21327;&#35758;&#36827;&#34892;&#22823;&#27493;&#39588;&#35757;&#32451;&#30340;&#28145;&#23618;&#27169;&#22411;&#20013;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19977;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#31232;&#30095;&#24615;&#30340;&#27010;&#24565;&#20316;&#20026;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#35299;&#37322;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;&#36825;&#20010;&#29702;&#35770;&#36866;&#29992;&#20110;&#32463;&#36807;LayerNorm&#26631;&#20934;&#35757;&#32451;&#30340;&#32431;MLP&#65292;&#24182;&#19988;&#22914;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#26435;&#37325;&#28155;&#21152;&#22122;&#22768;&#65292;&#36824;&#36866;&#29992;&#20110;Transformers&#25110;&#20854;&#20182;&#26550;&#26500;&#12290;&#20026;&#20102;&#28040;&#38500;&#20854;&#20182;&#26469;&#28304;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#24230;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#12290;</title><link>http://arxiv.org/abs/2309.02976</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#39640;&#32500;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#23454;&#29616;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#65292;&#26080;&#38656;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#24230;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#29615;&#22659;&#20013;&#20197;&#31283;&#20581;&#30340;&#21452;&#36275;&#34892;&#36208;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#20182;&#20204;&#20805;&#20998;&#35843;&#25972;&#29983;&#29289;&#21147;&#23398;&#32908;&#32905;&#21160;&#21147;&#23398;&#21644;&#31070;&#32463;&#20449;&#21495;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#22312;&#22320;&#38754;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#20445;&#25345;&#31283;&#20581;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#22914;&#20309;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#32771;&#34385;&#31283;&#23450;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#65292;&#33021;&#37327;&#26368;&#23567;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#36712;&#36857;&#20248;&#21270;&#25110;&#22522;&#20110;&#21453;&#23556;&#30340;&#25511;&#21046;&#26041;&#27861;&#20013;&#37325;&#29616;&#20102;&#33258;&#28982;&#34892;&#36208;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19968;&#27425;&#21482;&#20851;&#27880;&#29305;&#23450;&#30340;&#36816;&#21160;&#65292;&#24182;&#19988;&#22312;&#34917;&#20607;&#24178;&#25200;&#26102;&#65292;&#25152;&#20135;&#29983;&#30340;&#25511;&#21046;&#22120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#22312;&#22235;&#36275;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#39640;&#24230;&#31283;&#23450;&#65288;&#21644;&#39640;&#25928;&#65289;&#30340;&#36816;&#21160;&#65292;&#20294;&#35201;&#20351;&#29992;&#21452;&#36275;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#34892;&#36208;&#30340;&#34892;&#36208;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39282;&#26009;&#25104;&#26412;&#39118;&#38505;&#23545;&#27700;&#20135;&#20859;&#27542;&#20272;&#20540;&#21644;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20915;&#31574;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25913;&#36827;&#20256;&#32479;&#26041;&#27861;&#30340;&#22238;&#24402;&#21644;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.02970</link><description>&lt;p&gt;
&#20851;&#20110;&#39282;&#26009;&#25104;&#26412;&#39118;&#38505;&#23545;&#27700;&#20135;&#20859;&#27542;&#20272;&#20540;&#21644;&#20915;&#31574;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making. (arXiv:2309.02970v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39282;&#26009;&#25104;&#26412;&#39118;&#38505;&#23545;&#27700;&#20135;&#20859;&#27542;&#20272;&#20540;&#21644;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20915;&#31574;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25913;&#36827;&#20256;&#32479;&#26041;&#27861;&#30340;&#22238;&#24402;&#21644;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39282;&#26009;&#25104;&#26412;&#30340;&#38543;&#26426;&#24615;&#23545;&#20197;&#21160;&#29289;&#20026;&#22522;&#30784;&#30340;&#21830;&#21697;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#27700;&#20135;&#20859;&#27542;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#35910;&#26399;&#36135;&#26469;&#25512;&#26029;&#40081;&#40060;&#39282;&#26009;&#30340;&#38543;&#26426;&#34892;&#20026;&#65292;&#25105;&#20204;&#20551;&#35774;&#20854;&#36981;&#24490;Schwartz-2&#22240;&#23376;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20197;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#39282;&#26009;&#25104;&#26412;&#65288;&#21363;&#21253;&#25324;&#39282;&#26009;&#25104;&#26412;&#39118;&#38505;&#65289;&#20570;&#20986;&#25910;&#33719;&#40081;&#40060;&#30340;&#20915;&#31574;&#35268;&#21017;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#32771;&#34385;&#38543;&#26426;&#39282;&#26009;&#25104;&#26412;&#26102;&#20250;&#26377;&#26126;&#26174;&#25913;&#21892;&#30340;&#24773;&#20917;&#65292;&#20063;&#26377;&#30830;&#23450;&#24615;&#39282;&#26009;&#25104;&#26412;&#36275;&#22815;&#22909;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26032;&#25512;&#23548;&#20986;&#30340;&#35268;&#21017;&#22343;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#38468;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#20174;&#26041;&#27861;&#35770;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#26029;&#20915;&#31574;&#36793;&#30028;&#65292;&#20197;&#30830;&#23450;&#25910;&#33719;&#25110;&#24310;&#32493;&#65292;&#25913;&#36827;&#20102;&#26356;&#20256;&#32479;&#30340;&#22238;&#24402;&#21644;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
We study the effect of stochastic feeding costs on animal-based commodities with particular focus on aquaculture. More specifically, we use soybean futures to infer on the stochastic behaviour of salmon feed, which we assume to follow a Schwartz-2-factor model. We compare the decision of harvesting salmon using a decision rule assuming either deterministic or stochastic feeding costs, i.e. including feeding cost risk. We identify cases, where accounting for stochastic feeding costs leads to significant improvements as well as cases where deterministic feeding costs are a good enough proxy. Nevertheless, in all of these cases, the newly derived rules show superior performance, while the additional computational costs are negligible. From a methodological point of view, we demonstrate how to use Deep-Neural-Networks to infer on the decision boundary that determines harvesting or continuation, improving on more classical regression-based and curve-fitting methods. To achieve this we use a
&lt;/p&gt;</description></item><item><title>CR-VAE&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#26469;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02968</link><description>&lt;p&gt;
CR-VAE:&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#27604;&#27491;&#21017;&#21270;&#20197;&#38450;&#27490;&#21518;&#39564;&#22349;&#22604;
&lt;/p&gt;
&lt;p&gt;
CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. (arXiv:2309.02968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02968
&lt;/p&gt;
&lt;p&gt;
CR-VAE&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#26469;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#24050;&#30693;&#23384;&#22312;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#19982;&#36755;&#20837;&#20043;&#38388;&#21464;&#24471;&#29420;&#31435;&#12290;&#36825;&#23548;&#33268;&#36755;&#20837;&#30340;&#34920;&#31034;&#36864;&#21270;&#65292;&#36825;&#24402;&#22240;&#20110;VAE&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CR-VAE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22312;&#21407;&#22987;VAE&#20013;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#65292;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#31181;&#31574;&#30053;&#30830;&#20445;&#20102;&#36755;&#20837;&#19982;&#20854;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26368;&#22823;&#21270;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#21518;&#39564;&#22349;&#22604;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;CR-VAE&#22312;&#38450;&#27490;&#21518;&#39564;&#22349;&#22604;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
&lt;/p&gt;</description></item><item><title>M3D-NCA&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;3D&#20998;&#21106;&#26041;&#27861;&#65292;&#20855;&#26377;&#20869;&#24314;&#36136;&#37327;&#25511;&#21046;&#65292;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;Neural Cellular Automata&#65288;NCA&#65289;&#20998;&#21106;&#21644;&#26032;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;M3D-NCA&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#21015;&#33146;&#20998;&#21106;&#20013;&#36229;&#36234;&#20102;&#22823;&#35268;&#27169;UNet&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02954</link><description>&lt;p&gt;
M3D-NCA: &#20855;&#26377;&#20869;&#24314;&#36136;&#37327;&#25511;&#21046;&#30340;&#40065;&#26834;&#30340;3D&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
M3D-NCA: Robust 3D Segmentation with Built-in Quality Control. (arXiv:2309.02954v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02954
&lt;/p&gt;
&lt;p&gt;
M3D-NCA&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;3D&#20998;&#21106;&#26041;&#27861;&#65292;&#20855;&#26377;&#20869;&#24314;&#36136;&#37327;&#25511;&#21046;&#65292;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;Neural Cellular Automata&#65288;NCA&#65289;&#20998;&#21106;&#21644;&#26032;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;M3D-NCA&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#21015;&#33146;&#20998;&#21106;&#20013;&#36229;&#36234;&#20102;&#22823;&#35268;&#27169;UNet&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20363;&#22914;&#22522;&#20110;UNet&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#39640;&#35745;&#31639;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#65288;&#22914;&#21021;&#32423;&#20445;&#20581;&#35774;&#26045;&#21644;&#20914;&#31361;&#22320;&#21306;&#65289;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;&#39046;&#22495;&#30340;&#21464;&#21270;&#21487;&#33021;&#20351;&#36825;&#20123;&#27169;&#22411;&#22833;&#25928;&#65292;&#29978;&#33267;&#22312;&#26410;&#21457;&#29616;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#21361;&#21450;&#24739;&#32773;&#23433;&#20840;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;M3D-NCA&#65292;&#23427;&#21033;&#29992;Neural Cellular Automata&#65288;NCA&#65289;&#20998;&#21106;&#36827;&#34892;3D&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#65292;&#37319;&#29992;n&#32423;&#34917;&#19969;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;M3D-NCA&#20013;&#30340;&#24046;&#24322;&#26469;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;NCA&#20998;&#21106;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;M3D-NCA&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20004;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;UNet&#27169;&#22411;&#65292;&#20854;Dice&#31995;&#25968;&#25552;&#39640;2&#65285;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Raspberry Pi 4 Model B&#65288;2GB RAM&#65289;&#19978;&#36816;&#34892;&#12290;&#36825;&#20984;&#26174;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation relies heavily on large-scale deep learning models, such as UNet-based architectures. However, the real-world utility of such models is limited by their high computational requirements, which makes them impractical for resource-constrained environments such as primary care facilities and conflict zones. Furthermore, shifts in the imaging domain can render these models ineffective and even compromise patient safety if such errors go undetected. To address these challenges, we propose M3D-NCA, a novel methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D medical images using n-level patchification. Moreover, we exploit the variance in M3D-NCA to develop a novel quality metric which can automatically detect errors in the segmentation process of NCAs. M3D-NCA outperforms the two magnitudes larger UNet models in hippocampus and prostate segmentation by 2% Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#23454;&#29616;&#32447;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.02935</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#20197;&#25903;&#25345;&#27844;&#28431;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Estimating irregular water demands with physics-informed machine learning to inform leakage detection. (arXiv:2309.02935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#23454;&#29616;&#32447;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39278;&#29992;&#27700;&#20379;&#24212;&#32593;&#32476;&#20013;&#30340;&#28431;&#27700;&#38382;&#39064;&#32473;&#27700;&#21153;&#20844;&#21496;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#22522;&#30784;&#35774;&#26045;&#25925;&#38556;&#12289;&#36816;&#33829;&#20013;&#26029;&#12289;&#29615;&#22659;&#39118;&#38505;&#12289;&#36130;&#20135;&#25439;&#22833;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#21450;&#26102;&#35782;&#21035;&#21644;&#20934;&#30830;&#23450;&#20301;&#36825;&#20123;&#27844;&#28431;&#23545;&#20110;&#27700;&#21153;&#20844;&#21496;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#27844;&#28431;&#26816;&#27979;&#31639;&#27861;&#30340;&#23454;&#26045;&#22312;&#23454;&#36341;&#20013;&#21463;&#21040;&#27700;&#21147;&#27169;&#22411;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21033;&#29992;&#27700;&#21147;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#26368;&#32456;&#21033;&#29992;&#20271;&#21162;&#21033;&#26041;&#31243;&#26377;&#25928;&#32447;&#24615;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;L-Town&#22522;&#20934;&#32593;&#32476;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Leakages in drinking water distribution networks pose significant challenges to water utilities, leading to infrastructure failure, operational disruptions, environmental hazards, property damage, and economic losses. The timely identification and accurate localisation of such leakages is paramount for utilities to mitigate these unwanted effects. However, implementation of algorithms for leakage detection is limited in practice by requirements of either hydraulic models or large amounts of training data. Physics-informed machine learning can utilise hydraulic information thereby circumventing both limitations. In this work, we present a physics-informed machine learning algorithm that analyses pressure data and therefrom estimates unknown irregular water demands via a fully connected neural network, ultimately leveraging the Bernoulli equation and effectively linearising the leakage detection problem. Our algorithm is tested on data from the L-Town benchmark network, and results indic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GroupEnc&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#32676;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#23454;&#29616;&#27604;&#20256;&#32479;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26356;&#23569;&#30340;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#21644;&#26550;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02917</link><description>&lt;p&gt;
GroupEnc: &#20855;&#26377;&#32676;&#25439;&#22833;&#30340;&#32534;&#30721;&#22120;&#20197;&#23454;&#29616;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
GroupEnc: encoder with group loss for global structure preservation. (arXiv:2309.02917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GroupEnc&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#32676;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#23454;&#29616;&#27604;&#20256;&#32479;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26356;&#23569;&#30340;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#21644;&#26550;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38477;&#32500;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#20934;&#30830;&#30340;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;&#38500;&#20102;&#21487;&#35270;&#21270;&#30446;&#30340;&#22806;&#65292;&#36825;&#20123;&#23884;&#20837;&#36824;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#22788;&#29702;&#65292;&#21253;&#25324;&#25209;&#27425;&#25928;&#24212;&#26631;&#20934;&#21270;&#12289;&#32858;&#31867;&#12289;&#31038;&#21306;&#26816;&#27979;&#25110;&#36712;&#36857;&#25512;&#26029;&#12290;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#30340;&#32467;&#26500;&#20445;&#25345;&#27010;&#24565;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21644;SQuadMDS&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#22235;&#20998;&#20301;&#25439;&#22833;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#21517;&#20026;GroupEnc&#65292;&#20351;&#29992;&#8220;&#32676;&#25439;&#22833;&#8221;&#20989;&#25968;&#21019;&#24314;&#23884;&#20837;&#65292;&#30456;&#27604;&#20110;VAE&#65292;&#33021;&#22815;&#20943;&#23569;&#20840;&#23616;&#32467;&#26500;&#30072;&#21464;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#21644;&#26550;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#29983;&#29289;&#23398;&#21333;&#32454;&#32990;&#36716;&#24405;&#32452;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;RNX&#26354;&#32447;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in dimensionality reduction have achieved more accurate lower-dimensional embeddings of high-dimensional data. In addition to visualisation purposes, these embeddings can be used for downstream processing, including batch effect normalisation, clustering, community detection or trajectory inference. We use the notion of structure preservation at both local and global levels to create a deep learning model, based on a variational autoencoder (VAE) and the stochastic quartet loss from the SQuadMDS algorithm. Our encoder model, called GroupEnc, uses a 'group loss' function to create embeddings with less global structure distortion than VAEs do, while keeping the model parametric and the architecture flexible. We validate our approach using publicly available biological single-cell transcriptomic datasets, employing RNX curves for evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.02915</link><description>&lt;p&gt;
&#12298;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#65292;&#20195;&#30721;&#28151;&#21512;&#21644;&#33050;&#26412;&#28151;&#21512;&#38750;&#24120;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#20110;&#20195;&#30721;&#28151;&#21512;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#24403;&#22320;&#29615;&#22659;&#65292;&#32780;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#26102;&#22823;&#22810;&#24573;&#35270;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24320;&#21457;&#19968;&#31181;&#20154;&#29289;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#29983;&#25104;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#65288;PARADOX&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#30340;&#26465;&#20214;&#19979;&#23545;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#40784;&#27169;&#22359;&#65292;&#23545;&#29983;&#25104;&#30340;&#24207;&#21015;&#36827;&#34892;&#37325;&#26032;&#26657;&#20934;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;PARADOX&#29983;&#25104;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;EDNN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#20572;&#30041;&#20301;&#32622;&#21644;&#35774;&#22791;&#36873;&#25321;&#27010;&#29575;&#65292;&#20943;&#23569;&#20102;&#35774;&#22791;&#20043;&#38388;&#30340;&#39044;&#26399;&#26102;&#24310;&#20449;&#24687;&#65288;AoI&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#38477;&#20302;&#39044;&#26399;AoI&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#23454;&#29616;29.5%&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.02913</link><description>&lt;p&gt;
&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#26102;&#24310;&#20449;&#24687;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks. (arXiv:2309.02913v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;EDNN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#20572;&#30041;&#20301;&#32622;&#21644;&#35774;&#22791;&#36873;&#25321;&#27010;&#29575;&#65292;&#20943;&#23569;&#20102;&#35774;&#22791;&#20043;&#38388;&#30340;&#39044;&#26399;&#26102;&#24310;&#20449;&#24687;&#65288;AoI&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#38477;&#20302;&#39044;&#26399;AoI&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#23454;&#29616;29.5%&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#20154;&#26426;&#36741;&#21161;&#32593;&#32476;&#20013;&#26102;&#24310;&#20449;&#24687;&#65288;AoI&#65289;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#20572;&#30041;&#20301;&#32622;&#21644;&#35774;&#22791;&#36873;&#25321;&#27010;&#29575;&#26469;&#26368;&#23567;&#21270;&#35774;&#22791;&#20043;&#38388;&#30340;&#39044;&#26399;AoI&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35774;&#22791;&#36873;&#25321;&#27010;&#29575;&#30340;&#39044;&#26399;AoI&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#38750;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#28155;&#21152;&#20102;&#26381;&#21153;&#36136;&#37327;&#32422;&#26463;&#12290;&#30001;&#20110;&#36825;&#20010;&#38382;&#39064;&#24456;&#38590;&#35299;&#20915;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;EDNN&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#30740;&#31350;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38598;&#25104;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36890;&#36807;&#20351;&#29992;&#25152;&#30740;&#31350;&#38382;&#39064;&#30340;Lagrangian&#20989;&#25968;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;EDNN&#26041;&#27861;&#22312;&#20943;&#23569;&#39044;&#26399;AoI&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;DNNs&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;29.5%&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of Age-of-Information (AoI) in UAV-assisted networks. Our objective is to minimize the expected AoI across devices by optimizing UAVs' stopping locations and device selection probabilities. To tackle this problem, we first derive a closed-form expression of the expected AoI that involves the probabilities of selection of devices. Then, we formulate the problem as a non-convex minimization subject to quality of service constraints. Since the problem is challenging to solve, we propose an Ensemble Deep Neural Network (EDNN) based approach which takes advantage of the dual formulation of the studied problem. Specifically, the Deep Neural Networks (DNNs) in the ensemble are trained in an unsupervised manner using the Lagrangian function of the studied problem. Our experiments show that the proposed EDNN method outperforms traditional DNNs in reducing the expected AoI, achieving a remarkable reduction of $29.5\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#20854;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65307;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.02911</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32508;&#21512;&#19977;&#32500;&#30719;&#20135;&#21069;&#26223;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#20197;&#21450;&#32852;&#21512;&#23398;&#20064;&#30340;&#32467;&#26500;-&#27969;&#20307;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships. (arXiv:2309.02911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#20854;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65307;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#36825;&#23545;&#20110;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#19977;&#32500;&#30719;&#20135;&#21069;&#26223;&#26144;&#23556;&#65288;3D MPM&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#26377;&#25928;&#22320;&#25972;&#21512;&#32467;&#26500;&#21644;&#27969;&#20307;&#20449;&#24687;&#12290;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26469;&#23545;&#40784;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#23545;&#33014;&#33014;&#37329;&#30719;&#24202;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21306;&#20998;&#21547;&#30719;&#23454;&#20363;&#21644;&#39044;&#27979;&#30719;&#20135;&#21069;&#26223;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32467;&#26524;&#20998;&#26512;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#32852;&#21512;&#29305;&#24449;&#21033;&#29992;&#21644;CCA&#34701;&#21512;&#30340;&#30410;&#22788;&#12290;&#26412;&#30740;&#31350;&#19981;&#20165;&#25512;&#36827;&#20102;&#30719;&#20135;&#21069;&#26223;&#24314;&#27169;&#65292;&#36824;&#24378;&#35843;&#20102;&#25968;&#25454;&#25972;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#22312;&#22686;&#24378;&#21208;&#25506;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel multimodal fusion model for three-dimensional mineral prospectivity mapping (3D MPM), effectively integrating structural and fluid information through a deep network architecture. Leveraging Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs canonical correlation analysis (CCA) to align and fuse multimodal features. Rigorous evaluation on the Jiaojia gold deposit dataset demonstrates the model's superior performance in distinguishing ore-bearing instances and predicting mineral prospectivity, outperforming other models in result analyses. Ablation studies further reveal the benefits of joint feature utilization and CCA incorporation. This research not only advances mineral prospectivity modeling but also highlights the pivotal role of data integration and feature alignment for enhanced exploration decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02908</link><description>&lt;p&gt;
DECODE: &#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#25968;&#25454;&#39537;&#21160;&#33021;&#32791;&#39044;&#27979;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#30340;&#33021;&#32791;&#39044;&#27979;&#22312;&#26377;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#20248;&#21270;&#30340;&#33021;&#32791;&#21644;&#30005;&#32593;&#20998;&#37197;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#33021;&#28304;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;LSTM&#27169;&#22411;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#33021;&#32791;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;LSTM&#27169;&#22411;&#19982;&#32447;&#24615;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#24050;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#25552;&#20986;&#30340;LSTM&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;R2&#24471;&#20998;&#20026;0.97&#65292;&#26368;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.007&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#26500;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#22312;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#24110;&#21161;&#19979;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#24182;&#23398;&#20064;&#21040;&#23545;&#31216;&#24615;&#12290;&#22312;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02898</link><description>&lt;p&gt;
&#21457;&#29616;&#31163;&#25955;&#23545;&#31216;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Discovering Discrete Symmetries. (arXiv:2309.02898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#26500;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#22312;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#24110;&#21161;&#19979;&#65292;&#39640;&#25928;&#22320;&#20248;&#21270;&#24182;&#23398;&#20064;&#21040;&#23545;&#31216;&#24615;&#12290;&#22312;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#19968;&#31867;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#19968;&#20010;&#31526;&#21512;&#23545;&#31216;&#24615;&#30340;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#21457;&#29616;&#21253;&#25324;&#23616;&#37096;&#23545;&#31216;&#12289;&#20108;&#38754;&#35282;&#21644;&#24490;&#29615;&#23376;&#32676;&#22312;&#20869;&#30340;&#24191;&#27867;&#23376;&#32676;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#30001;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#32452;&#25104;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#34920;&#36798;&#36825;&#20123;&#23376;&#32676;&#19981;&#21464;&#30340;&#20989;&#25968;&#12290;&#26550;&#26500;&#30340;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#21644;&#26799;&#24230;&#19979;&#38477;&#20998;&#21035;&#39640;&#25928;&#20248;&#21270;&#32447;&#24615;&#21644;&#24352;&#37327;&#20540;&#20989;&#25968;&#65292;&#24182;&#25512;&#26029;&#20986;&#26368;&#32456;&#23398;&#20064;&#21040;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26550;&#26500;&#20013;&#24352;&#37327;&#20540;&#20989;&#25968;&#30340;&#24517;&#35201;&#24615;&#12290;&#23545;&#22270;&#20687;&#25968;&#23383;&#27714;&#21644;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#20013;&#29699;&#30340;&#27010;&#24565;&#31867;&#20013;&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.02876</link><description>&lt;p&gt;
&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#22312;&#22270;&#20013;&#29699;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-Clashing Teaching Maps for Balls in Graphs. (arXiv:2309.02876v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#20013;&#29699;&#30340;&#27010;&#24565;&#31867;&#20013;&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Kirkpatrick&#31561;&#20154;[ALT 2019]&#21644;Fallat&#31561;&#20154;[JMLR 2023]&#24341;&#20837;&#20102;&#38750;&#20914;&#31361;&#25945;&#23398;&#65292;&#24182;&#34920;&#26126;&#23427;&#26159;&#28385;&#36275;Goldman&#21644;Mathias&#25552;&#20986;&#30340;&#38450;&#27490;&#21246;&#32467;&#22522;&#20934;&#30340;&#26368;&#39640;&#25928;&#30340;&#26426;&#22120;&#25945;&#23398;&#27169;&#22411;&#12290;&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867;$\cal{C}$&#26469;&#35828;&#65292;&#25945;&#23398;&#22270;$T$&#23558;&#19968;&#20010;&#65288;&#25945;&#23398;&#65289;&#38598;&#21512;$T(C)$&#20998;&#37197;&#32473;&#27599;&#20010;&#27010;&#24565;$C \in \cal{C}$&#12290;&#22914;&#26524;&#27809;&#26377;&#19968;&#23545;&#27010;&#24565;&#19982;&#23427;&#20204;&#30340;&#25945;&#23398;&#38598;&#21512;&#30340;&#24182;&#19968;&#33268;&#65292;&#21017;&#25945;&#23398;&#22270;&#26159;&#38750;&#20914;&#31361;&#30340;&#12290;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65288;NCTM&#65289;$T$&#30340;&#22823;&#23567;&#26159;$T(C)$&#20013;&#30340;&#26368;&#22823;&#22823;&#23567;&#65292;&#20854;&#20013;$C \in \cal{C}$&#12290;&#27010;&#24565;&#31867;$\mathcal{B}(G)$&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#32500;&#24230;NCTD$(\cal{C})$&#26159;$\cal{C}$&#30340;&#19968;&#20010;NCTM&#30340;&#26368;&#23567;&#22823;&#23567;&#12290;&#31867;&#20284;&#22320;&#65292;NCTM$^+$&#21644;NCTD$^+(\cal{C})$&#30340;&#23450;&#20041;&#26159;&#31867;&#20284;&#30340;&#65292;&#21482;&#26159;&#25945;&#24072;&#21482;&#33021;&#20351;&#29992;&#27491;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#22270;$G$&#30340;&#25152;&#26377;&#29699;&#32452;&#25104;&#30340;&#27010;&#24565;&#31867;$\mathcal{B}(G)$&#30340;NCTMs&#21644;NCTM$^+$s&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;NCTD$^+$&#30340;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\cal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \in \cal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a $T(C)$, $C \in \cal{C}$. The non-clashing teaching dimension NCTD$(\cal{C})$ of $\cal{C}$ is the minimum size of an NCTM for $\cal{C}$. NCTM$^+$ and NCTD$^+(\cal{C})$ are defined analogously, except the teacher may only use positive examples.  We study NCTMs and NCTM$^+$s for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in spl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#25311;&#22120;&#26356;&#26032;&#28508;&#22312;&#29366;&#24577;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#39044;&#27979;&#24182;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.02873</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#22791;&#27169;&#25311;&#22120;&#20449;&#24687;&#30340;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Dynamics Models With Simulator-Informed Latent States. (arXiv:2309.02873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#25311;&#22120;&#26356;&#26032;&#28508;&#22312;&#29366;&#24577;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#39044;&#27979;&#24182;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#30340;&#20219;&#21153;&#26159;&#20174;&#27979;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#26410;&#30693;&#30340;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#30340;&#34892;&#20026;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#35757;&#32451;&#36882;&#24402;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#36890;&#24120;&#22312;&#29289;&#29702;&#24847;&#20041;&#19978;&#19981;&#21512;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32047;&#31215;&#35823;&#24046;&#30340;&#23384;&#22312;&#65292;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#20250;&#24694;&#21270;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#30340;&#27169;&#25311;&#22120;&#26159;&#29289;&#29702;&#24847;&#20041;&#19978;&#21512;&#29702;&#30340;&#12290;&#28982;&#32780;&#65292;&#24314;&#27169;&#31616;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#28151;&#21512;&#24314;&#27169;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#65292;&#26088;&#22312;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#40657;&#30418;&#27169;&#25311;&#22120;&#23558;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#27169;&#25311;&#22120;&#26469;&#25511;&#21046;&#39044;&#27979;&#65292;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02870</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Momentum Knowledge Distillation in Online Continual Learning. (arXiv:2309.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#20986;&#29616;&#12290;&#19982;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;OCL&#20013;&#21482;&#33021;&#30475;&#21040;&#25968;&#25454;&#19968;&#27425;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#31574;&#30053;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#23427;&#20204;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#22312;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;OCL&#20013;&#20173;&#28982;&#26410;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23558;KD&#24212;&#29992;&#20110;OCL&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#65288;MKD&#65289;&#24212;&#29992;&#20110;&#35768;&#22810;&#26071;&#33328;OCL&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#23558;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ImageNet100&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#33391;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#26469;&#24809;&#32602;&#31995;&#32479;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#19968;&#26694;&#26550;&#22312;&#20445;&#25345;&#21331;&#36234;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#33391;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.02869</link><description>&lt;p&gt;
&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#33391;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#26469;&#24809;&#32602;&#31995;&#32479;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#19968;&#26694;&#26550;&#22312;&#20445;&#25345;&#21331;&#36234;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#33391;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#22823;&#37327;&#24212;&#29992;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#26497;&#22823;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#25104;&#21151;&#30340;&#22522;&#20110;DRL&#30340;&#36719;&#20214;&#20063;&#21487;&#33021;&#34920;&#29616;&#20986;&#26497;&#20854;&#19981;&#33391;&#30340;&#34892;&#20026;&#12290;&#36825;&#26159;&#22240;&#20026;DRL&#35757;&#32451;&#26159;&#22522;&#20110;&#26368;&#22823;&#21270;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#24120;&#33021;&#25429;&#25417;&#21040;&#19968;&#33324;&#36235;&#21183;&#65292;&#20294;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#25110;&#25490;&#38500;&#31995;&#32479;&#30340;&#26576;&#20123;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22823;&#24133;&#38477;&#20302;&#22522;&#20110;DRL&#30340;&#36719;&#20214;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#24037;&#31243;&#24072;&#23545;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#36827;&#34892;&#21487;&#29702;&#35299;&#30340;&#34920;&#24449;&#12290;&#22312;&#24213;&#23618;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20915;&#31574;&#26641;&#25972;&#21512;&#21040;DRL&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#24403;&#31995;&#32479;&#21457;&#29983;&#38169;&#35823;&#26102;&#23545;&#20854;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#26041;&#27861;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#29616;&#65292;&#24182;&#29992;&#23427;&#26469;&#35780;&#20272;&#35813;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#24182;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#25429;&#25417;&#21160;&#24577;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#22312;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02868</link><description>&lt;p&gt;
&#20511;&#21161;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Enhancing Event Sequence Modeling with Contrastive Relational Inference. (arXiv:2309.02868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#24182;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#25429;&#25417;&#21160;&#24577;&#27169;&#24335;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#22312;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26102;&#24207;&#28857;&#36807;&#31243;&#65288;TPPs&#65289;&#22312;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25429;&#25417;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#25191;&#34892;&#39044;&#27979;&#31561;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;TPP&#27169;&#22411;&#20391;&#37325;&#20110;&#21442;&#25968;&#21270;&#26410;&#26469;&#20107;&#20214;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20294;&#38590;&#20197;&#24314;&#27169;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#65288;NRI&#65289;&#26469;&#23398;&#20064;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21516;&#26102;&#23398;&#20064;&#21160;&#24577;&#27169;&#24335;&#21644;&#25512;&#26029;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23545;&#27604;&#20851;&#31995;&#25512;&#29702;&#39537;&#21160;&#30340;Hawkes&#36807;&#31243;&#65288;CRIHP&#65289;&#65292;&#22312;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#19979;&#25512;&#29702;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23427;&#21033;&#29992;&#22522;&#20110;&#24378;&#24230;&#30340;&#23398;&#20064;&#26469;&#25628;&#32034;&#23545;&#27604;&#20851;&#31995;&#32422;&#26463;&#30340;&#21407;&#22411;&#36335;&#24452;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#25429;&#25417;&#20107;&#20214;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural temporal point processes(TPPs) have shown promise for modeling continuous-time event sequences. However, capturing the interactions between events is challenging yet critical for performing inference tasks like forecasting on event sequence data. Existing TPP models have focused on parameterizing the conditional distribution of future events but struggle to model event interactions. In this paper, we propose a novel approach that leverages Neural Relational Inference (NRI) to learn a relation graph that infers interactions while simultaneously learning the dynamics patterns from observational data. Our approach, the Contrastive Relational Inference-based Hawkes Process (CRIHP), reasons about event interactions under a variational inference framework. It utilizes intensity-based learning to search for prototype paths to contrast relationship constraints. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model in capturing event interactions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#20316;&#20026;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;GEMINI&#22312;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#20854;&#21487;&#20197;&#36873;&#25321;&#21512;&#36866;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.02858</link><description>&lt;p&gt;
&#36890;&#29992;&#20114;&#20449;&#24687;&#65306;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generalised Mutual Information: a Framework for Discriminative Clustering. (arXiv:2309.02858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#20316;&#20026;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;GEMINI&#22312;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#20854;&#21487;&#20197;&#36873;&#25321;&#21512;&#36866;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#25104;&#26524;&#20027;&#35201;&#28041;&#21450;&#20316;&#20026;&#26080;&#30417;&#30563;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#23458;&#35266;&#20989;&#25968;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#24182;&#22686;&#21152;&#20102;&#27491;&#21017;&#39033;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;&#30340;&#36136;&#37327;&#24050;&#32463;&#34987;&#24191;&#27867;&#35752;&#35770;&#20197;&#36827;&#34892;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;MI&#20316;&#20026;&#32858;&#31867;&#30446;&#26631;&#30340;&#30456;&#20851;&#24615;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#24378;&#35843;&#20102;&#26368;&#22823;&#21270;MI&#24182;&#19981;&#33021;&#24471;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#25955;&#24230;&#26159;&#36825;&#19968;&#34892;&#20026;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#20854;&#26680;&#24515;&#24046;&#24322;&#65292;&#24341;&#20837;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#26469;&#25512;&#24191;&#20114;&#20449;&#24687;&#65306;&#19968;&#32452;&#29992;&#20110;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#19982;MI&#19981;&#21516;&#30340;&#26159;&#65292;&#19968;&#20123;GEMINI&#22312;&#35757;&#32451;&#26102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20855;&#26377;&#20960;&#20309;&#24847;&#35782;&#30340;&#36317;&#31163;&#25110;&#26680;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;GEMINI&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#25968;&#37327;&#30340;&#32858;&#31867;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, recent successes in deep clustering majorly involved the Mutual Information (MI) as an unsupervised objective for training neural networks with increasing regularisations. While the quality of the regularisations have been largely discussed for improvements, little attention has been dedicated to the relevance of MI as a clustering objective. In this paper, we first highlight how the maximisation of MI does not lead to satisfying clusters. We identified the Kullback-Leibler divergence as the main reason of this behaviour. Hence, we generalise the mutual information by changing its core distance, introducing the Generalised Mutual Information (GEMINI): a set of metrics for unsupervised neural network training. Unlike MI, some GEMINIs do not require regularisations when training as they are geometry-aware thanks to distances or kernels in the data space. Finally, we highlight that GEMINIs can automatically select a relevant number of clusters, a property that has been
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26085;&#24535;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#30340;&#34920;&#29616;&#24418;&#24335;&#21644;&#20854;&#26816;&#27979;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.02854</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#24120;&#35265;&#26085;&#24535;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques. (arXiv:2309.02854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26085;&#24535;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#30340;&#34920;&#29616;&#24418;&#24335;&#21644;&#20854;&#26816;&#27979;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#23384;&#20648;&#19982;&#31995;&#32479;&#25110;&#24212;&#29992;&#31243;&#24207;&#30340;&#24213;&#23618;&#24037;&#20316;&#27969;&#30456;&#23545;&#24212;&#30340;&#20107;&#20214;&#25191;&#34892;&#27169;&#24335;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#26085;&#24535;&#20855;&#26377;&#20449;&#24687;&#24615;&#65292;&#20294;&#26085;&#24535;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#25351;&#31034;&#25925;&#38556;&#25110;&#20107;&#25925;&#30340;&#30165;&#36857;&#12290;&#22240;&#27492;&#65292;&#26085;&#24535;&#25968;&#25454;&#24120;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#33258;&#21160;&#25581;&#31034;&#24847;&#22806;&#25110;&#20854;&#20182;&#30456;&#20851;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#36234;&#26469;&#36234;&#20851;&#27880;&#20197;&#24207;&#21015;&#27169;&#24335;&#30340;&#21464;&#21270;&#20026;&#29305;&#24449;&#30340;&#24322;&#24120;&#65292;&#22312;&#19968;&#33324;&#20107;&#20214;&#36861;&#36394;&#20013;&#12290;&#19968;&#20123;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;HDFS&#65292;BGL&#65292;Thunderbird&#65292;OpenStack&#21644;Hadoop&#65292;&#24050;&#25104;&#20026;&#35780;&#20272;&#36825;&#20123;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#30340;&#26631;&#20934;&#65292;&#28982;&#32780;&#65292;&#36807;&#21435;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#36866;&#29992;&#24615;&#24182;&#26410;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26085;&#24535;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#30340;&#34920;&#29616;&#24418;&#24335;&#21644;&#20854;&#26816;&#27979;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Log data store event execution patterns that correspond to underlying workflows of systems or applications. While most logs are informative, log data also include artifacts that indicate failures or incidents. Accordingly, log data are often used to evaluate anomaly detection techniques that aim to automatically disclose unexpected or otherwise relevant system behavior patterns. Recently, detection approaches leveraging deep learning have increasingly focused on anomalies that manifest as changes of sequential patterns within otherwise normal event traces. Several publicly available data sets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become standards for evaluating these anomaly detection techniques, however, the appropriateness of these data sets has not been closely investigated in the past. In this paper we therefore analyze six publicly available log data sets with focus on the manifestations of anomalies and simple techniques for their detection. Our findi
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.02842</link><description>&lt;p&gt;
&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random postprocessing for combinatorial Bayesian optimization. (arXiv:2309.02842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02842
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#39034;&#24207;&#26041;&#27861;&#29992;&#20110;&#31163;&#25955;&#30340;&#8220;&#40657;&#30418;&#8221;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#24120;&#20250;&#23545;&#32473;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#35775;&#38382;&#22810;&#27425;&#30456;&#21516;&#30340;&#28857;&#65292;&#23548;&#33268;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#25165;&#33021;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20005;&#26684;&#31105;&#27490;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#22797;&#26679;&#26412;&#12290;&#25105;&#20204;&#21457;&#29616;&#21518;&#22788;&#29702;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#25152;&#38656;&#30340;&#39034;&#24207;&#27493;&#39588;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#37319;&#26679;&#20989;&#25968;&#26159;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#26102;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20013;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#20462;&#25913;&#26368;&#23567;&#20108;&#20056;GAN&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SAN&#21487;&#20197;&#25913;&#21892;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02836</link><description>&lt;p&gt;
BigVSAN: &#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network. (arXiv:2309.02836v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#20462;&#25913;&#26368;&#23567;&#20108;&#20056;GAN&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;SAN&#21487;&#20197;&#25913;&#21892;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#22768;&#30721;&#22120;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#27604;&#23454;&#26102;&#26356;&#24555;&#22320;&#21512;&#25104;&#39640;&#20445;&#30495;&#38899;&#39057;&#27874;&#24418;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30740;&#31350;&#25253;&#21578;&#21457;&#29616;&#22823;&#22810;&#25968;GAN&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26080;&#27861;&#33719;&#24471;&#21306;&#20998;&#30495;&#20551;&#25968;&#25454;&#30340;&#26368;&#20339;&#25237;&#24433;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#30340;GAN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAN&#22312;&#22768;&#30721;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20462;&#25913;&#20102;&#22823;&#22810;&#25968;&#22522;&#20110;GAN&#30340;&#22768;&#30721;&#22120;&#25152;&#37319;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;GAN&#65292;&#20351;&#20854;&#25439;&#22833;&#20989;&#25968;&#28385;&#36275;SAN&#30340;&#35201;&#27714;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SAN&#21487;&#20197;&#36890;&#36807;&#23567;&#30340;&#20462;&#25913;&#25913;&#21892;&#21253;&#25324;BigVGAN&#22312;&#20869;&#30340;&#22522;&#20110;GAN&#30340;&#22768;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/sony/bigvsan&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
&lt;/p&gt;</description></item><item><title>Roulette&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02820</link><description>&lt;p&gt;
Roulette&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks. (arXiv:2309.02820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02820
&lt;/p&gt;
&lt;p&gt;
Roulette&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#20316;&#20026;&#22312;&#29289;&#32852;&#32593;&#21644;5G/6G&#32593;&#32476;&#20013;&#25512;&#24191;&#20854;&#24212;&#29992;&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#21644;&#38544;&#31169;&#27844;&#38706;&#26041;&#38754;&#37117;&#23384;&#22312;&#31934;&#24230;&#19979;&#38477;&#38382;&#39064;&#12290;&#23545;&#20110;&#31934;&#24230;&#19979;&#38477;&#65292;&#30452;&#25509;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#25104;&#26412;&#36739;&#39640;&#65292;&#38544;&#31169;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#23545;&#20110;&#38544;&#31169;&#27844;&#38706;&#65292;&#22522;&#20110;&#23494;&#30721;&#23398;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#12290;&#20854;&#20182;&#36731;&#37327;&#32423;&#26041;&#27861;&#20551;&#35774;&#30495;&#23454;&#26631;&#31614;&#26159;&#38750;&#25935;&#24863;&#30340;&#24182;&#19988;&#21487;&#20197;&#34987;&#20844;&#24320;&#12290;&#20294;&#26159;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#30495;&#23454;&#26631;&#31614;&#26159;&#29992;&#25143;&#30340;&#20851;&#38190;&#25935;&#24863;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Roulette&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#12290;&#38500;&#20102;&#36755;&#20837;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#31169;&#23494;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#32467;&#26524;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradig
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#29992;&#20110;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#20248;&#21270;&#35774;&#35745;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.02818</link><description>&lt;p&gt;
&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#22686;&#24378;&#24037;&#19994;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization. (arXiv:2309.02818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02818
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#29992;&#20110;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#20248;&#21270;&#35774;&#35745;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#24212;&#29992;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#65292;&#30001;&#20110;&#35813;&#36807;&#31243;&#19979;&#38754;&#30340;&#20998;&#26512;&#26041;&#31243;&#65292;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#12290;&#34429;&#28982;&#22238;&#24402;&#26367;&#20195;&#27169;&#22411;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36825;&#20010;&#36807;&#31243;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#25112;&#30053;&#24615;&#22320;&#21033;&#29992;&#26631;&#35760;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#21487;&#37096;&#32626;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#29615;&#22659;&#20013;&#23558;&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#21387;&#32553;&#26426;&#27169;&#22411;&#65288;&#21363;&#25105;&#20204;&#20869;&#37096;&#30340;&#21387;&#32553;&#26426;&#35774;&#35745;&#36719;&#20214;&#65289;&#19982;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26367;&#20195;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#20854;&#25193;&#23637;&#21040;&#22312;&#32447;AL&#26694;&#26550;&#65292;&#20854;&#20013;&#19982;&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#21387;&#32553;&#26426;&#27169;&#22411;&#30340;&#23454;&#26102;&#20132;&#20114;&#20801;&#35768;&#22312;&#29983;&#20135;&#20013;&#37096;&#32626;&#12290;ActiveCompDesign&#22312;&#26367;&#20195;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design process of centrifugal compressors requires applying an optimization process which is computationally expensive due to complex analytical equations underlying the compressor's dynamical equations. Although the regression surrogate models could drastically reduce the computational cost of such a process, the major challenge is the scarcity of data for training the surrogate model. Aiming to strategically exploit the labeled samples, we propose the Active-CompDesign framework in which we combine a thermodynamics-based compressor model (i.e., our internal software for compressor design) and Gaussian Process-based surrogate model within a deployable Active Learning (AL) setting. We first conduct experiments in an offline setting and further, extend it to an online AL framework where a real-time interaction with the thermodynamics-based compressor's model allows the deployment in production. ActiveCompDesign shows a significant performance improvement in surrogate modeling by lev
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#28909;&#21147;&#23398;&#20449;&#24687;&#30340;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#32467;&#21512;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26041;&#27861;&#21644;&#22788;&#29702;&#23454;&#39564;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02805</link><description>&lt;p&gt;
&#24341;&#20837;&#28909;&#21147;&#23398;&#20449;&#24687;&#30340;&#31526;&#21495;&#22238;&#24402;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#24320;&#21457;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Introducing Thermodynamics-Informed Symbolic Regression -- A Tool for Thermodynamic Equations of State Development. (arXiv:2309.02805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02805
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#28909;&#21147;&#23398;&#20449;&#24687;&#30340;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#32467;&#21512;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26041;&#27861;&#21644;&#22788;&#29702;&#23454;&#39564;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#21147;&#23398;&#29366;&#24577;&#26041;&#31243;&#65288;EOS&#65289;&#23545;&#35768;&#22810;&#34892;&#19994;&#20197;&#21450;&#23398;&#26415;&#30028;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#21363;&#20351;&#19981;&#32771;&#34385;&#29992;&#20110;&#25968;&#25454;&#37319;&#38598;&#30340;&#26114;&#36149;&#32780;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;EOS&#30340;&#24320;&#21457;&#20173;&#26159;&#19968;&#20010;&#38750;&#24120;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#21644;&#36845;&#20195;&#24494;&#35843;&#12290;&#20026;&#20102;&#25913;&#36827;&#21644;&#21152;&#24555;EOS&#24320;&#21457;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28909;&#21147;&#23398;&#20449;&#24687;&#30340;&#31526;&#21495;&#22238;&#24402;&#65288;TiSR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#28909;&#21147;&#23398;EOS&#24314;&#27169;&#30340;&#31526;&#21495;&#22238;&#24402;&#24037;&#20855;&#12290;TiSR&#24050;&#32463;&#26159;&#19968;&#20010;&#21151;&#33021;&#24378;&#22823;&#30340;&#31526;&#21495;&#22238;&#24402;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;https://doi.org/10.1007/s10765-023-03197-z&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#23427;&#26088;&#22312;&#23558;&#22522;&#20110;SR&#30340;&#26041;&#27861;&#19982;&#22788;&#29702;&#36890;&#24120;&#24378;&#28872;&#20998;&#25955;&#30340;&#23454;&#39564;&#25968;&#25454;&#25152;&#38656;&#30340;&#25193;&#23637;&#12289;&#19981;&#21516;&#30340;&#27531;&#24046;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#36873;&#39033;&#20197;&#21450;&#32771;&#34385;&#28909;&#21147;&#23398;EOS&#24320;&#21457;&#25152;&#38656;&#30340;&#20854;&#20182;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#23613;&#31649;TiSR&#23578;&#26410;&#20934;&#22791;&#22909;&#20379;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#25253;&#21578;&#20854;&#24403;&#21069;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thermodynamic equations of state (EOS) are essential for many industries as well as in academia. Even leaving aside the expensive and extensive measurement campaigns required for the data acquisition, the development of EOS is an intensely time-consuming process, which does often still heavily rely on expert knowledge and iterative fine-tuning. To improve upon and accelerate the EOS development process, we introduce thermodynamics-informed symbolic regression (TiSR), a symbolic regression (SR) tool aimed at thermodynamic EOS modeling. TiSR is already a capable SR tool, which was used in the research of https://doi.org/10.1007/s10765-023-03197-z. It aims to combine an SR base with the extensions required to work with often strongly scattered experimental data, different residual pre- and post-processing options, and additional features required to consider thermodynamic EOS development. Although TiSR is not ready for end users yet, this paper is intended to report on its current state, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#21160;&#24577;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#20998;&#31163;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#23454;&#29616;&#20256;&#36755;&#36164;&#28304;&#28040;&#32791;&#21644;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02787</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#20998;&#31163;&#23398;&#20064;&#30340;&#21160;&#24577;&#32534;&#30721;&#21644;&#35299;&#30721;: &#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory. (arXiv:2309.02787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#21160;&#24577;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#20998;&#31163;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#23454;&#29616;&#20256;&#36755;&#36164;&#28304;&#28040;&#32791;&#21644;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31163;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#34987;&#20998;&#20026;&#20004;&#37096;&#20998;&#65288;&#21363;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65289;&#12290;&#32534;&#30721;&#22120;&#20849;&#20139;&#25152;&#35859;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#31163;&#23398;&#20064;&#26469;&#35757;&#32451;&#32593;&#32476;&#21151;&#33021;&#65288;&#22914;&#27969;&#37327;&#39044;&#27979;&#65289;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20301;&#20110;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#65292;&#35299;&#30721;&#22120;&#20301;&#20110;&#36793;&#32536;&#32593;&#32476;&#20013;&#12290;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#21644;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#21644;&#35757;&#32451;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#20256;&#36755;&#36164;&#28304;&#28040;&#32791;&#19982;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#30340;&#20449;&#24687;&#24615;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#65292;&#36825;&#30452;&#25509;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26426;&#21046;&#25552;&#20379;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#31181;&#22797;&#26434;&#24615;-&#30456;&#20851;&#24615;&#26435;&#34913;&#30340;&#27169;&#24335;&#65292;&#23454;&#29616;&#21487;&#35843;&#33410;&#30340;&#24615;&#33021;&#12290;&#36866;&#24212;&#24615;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate vary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21644;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;CVE&#39537;&#21160;&#25915;&#20987;&#25216;&#26415;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;CVE&#25551;&#36848;&#24182;&#25512;&#26029;&#20986;&#30001;CVE&#21033;&#29992;&#23548;&#33268;&#30340;&#21487;&#33021;&#30340;TTP&#25915;&#20987;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;CVE&#21644;TTP&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20026;&#23454;&#26102;&#28431;&#27934;&#24773;&#25253;&#20998;&#26512;&#21644;&#23545;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.02785</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21644;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;CVE&#39537;&#21160;&#25915;&#20987;&#25216;&#26415;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model. (arXiv:2309.02785v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21644;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;CVE&#39537;&#21160;&#25915;&#20987;&#25216;&#26415;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;CVE&#25551;&#36848;&#24182;&#25512;&#26029;&#20986;&#30001;CVE&#21033;&#29992;&#23548;&#33268;&#30340;&#21487;&#33021;&#30340;TTP&#25915;&#20987;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;CVE&#21644;TTP&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20026;&#23454;&#26102;&#28431;&#27934;&#24773;&#25253;&#20998;&#26512;&#21644;&#23545;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;CVE&#65288;&#24120;&#35265;&#28431;&#27934;&#21644;&#26292;&#38706;&#65289;&#25152;&#20195;&#34920;&#30340;&#28431;&#27934;&#20449;&#24687;&#19982;&#23548;&#33268;&#30340;&#32593;&#32476;&#25915;&#20987;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;CVE&#25552;&#20379;&#20102;&#28431;&#27934;&#30340;&#27934;&#23519;&#21147;&#65292;&#20294;&#24120;&#24120;&#32570;&#20047;ATT&#65286;CK&#26694;&#26550;&#20013;&#28508;&#22312;&#23041;&#32961;&#34892;&#20026;&#65288;&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65289;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#36825;&#31181;&#24046;&#36317;&#22952;&#30861;&#20102;&#20934;&#30830;&#30340;CVE&#20998;&#31867;&#21644;&#20027;&#21160;&#30340;&#23545;&#31574;&#25514;&#26045;&#21551;&#21160;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;TTPpredictor&#24037;&#20855;&#65292;&#23427;&#21033;&#29992;&#21019;&#26032;&#25216;&#26415;&#20998;&#26512;CVE&#25551;&#36848;&#65292;&#24182;&#25512;&#26029;&#20986;&#30001;CVE&#21033;&#29992;&#23548;&#33268;&#30340;&#21487;&#33021;&#30340;TTP&#25915;&#20987;&#12290;TTPpredictor&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21644;CVE&#19982;TTP&#25551;&#36848;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#23427;&#39318;&#20808;&#20351;&#29992;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#25216;&#26415;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#32593;&#32476;&#23041;&#32961;&#25253;&#21578;&#20013;&#25552;&#21462;&#23041;&#32961;&#34892;&#20026;&#12290;&#36825;&#20123;&#34892;&#20026;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#19982;MITRE&#30340;&#25915;&#20987;&#21151;&#33021;&#31867;&#21035;&#36827;&#34892;&#20851;&#32852;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;CVE&#21644;TTP&#20043;&#38388;&#30340;&#36830;&#25509;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#26102;&#28431;&#27934;&#24773;&#25253;&#20998;&#26512;&#21644;&#23545;&#31574;&#21046;&#23450;&#20013;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a critical challenge in cybersecurity: the gap between vulnerability information represented by Common Vulnerabilities and Exposures (CVEs) and the resulting cyberattack actions. CVEs provide insights into vulnerabilities, but often lack details on potential threat actions (tactics, techniques, and procedures, or TTPs) within the ATT&amp;CK framework. This gap hinders accurate CVE categorization and proactive countermeasure initiation. The paper introduces the TTPpredictor tool, which uses innovative techniques to analyze CVE descriptions and infer plausible TTP attacks resulting from CVE exploitation. TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It initially extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques. These actions, along with their contextual attributes, are correlated with MITRE's attack functionality classes. This automated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#33539;&#22260;&#23457;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65292;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.02783</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#25552;&#39640;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review. (arXiv:2309.02783v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#33539;&#22260;&#23457;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65292;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#21644;&#30284;&#30151;&#25104;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#21253;&#25324;&#32954;&#30284;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35786;&#26029;&#21644;&#39044;&#27979;&#32954;&#30284;&#12290;&#26412;&#33539;&#22260;&#23457;&#26597;&#26088;&#22312;&#30830;&#23450;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23558;&#35270;&#35273;&#21464;&#25442;&#22120;&#19982;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32954;&#30284;&#24615;&#33021;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#23457;&#26597;&#36824;&#30830;&#23450;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#21457;&#23637;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26816;&#32034;&#21040;&#30340;314&#39033;&#30740;&#31350;&#20013;&#65292;&#26412;&#23457;&#26597;&#21253;&#25324;&#20102;&#20174;2020&#24180;&#21040;2022&#24180;&#21457;&#34920;&#30340;34&#39033;&#30740;&#31350;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#23545;&#32954;&#30284;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#32954;&#40158;&#29366;&#32454;&#32990;&#30284;&#19982;&#32954;&#33146;&#30284;&#65292;&#24182;&#37492;&#21035;&#33391;&#24615;&#19982;&#24694;&#24615;&#32954;&#32467;&#33410;&#12290;&#20854;&#20182;&#24212;&#29992;&#21253;&#25324;&#32954;&#30284;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer imaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI methods for lung cancer diagnosis and prognosis. This scoping review aims to identify the recent developments on vision transformer-based AI methods for lung cancer imaging applications. It provides key insights into how vision transformers complemented the performance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets that contributed to advancing the field. Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most commonly addressed task in these studies was the classification of lung cancer types, such as lung squamous cell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other applications included survival prediction of lung can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#35823;&#24046;&#23545;&#22810;&#20445;&#30495;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#29616;&#26377;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.02771</link><description>&lt;p&gt;
&#20851;&#20110;&#24322;&#36136;&#35823;&#24046;&#23545;&#22810;&#20445;&#30495;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization. (arXiv:2309.02771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#35823;&#24046;&#23545;&#22810;&#20445;&#30495;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#29616;&#26377;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#36830;&#32493;&#20248;&#21270;&#31574;&#30053;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#26448;&#26009;&#35774;&#35745;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#29289;&#29702;&#23454;&#39564;&#25110;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#33719;&#21462;&#39640;&#20445;&#30495;&#24230;&#65288;HF&#65289;&#25968;&#25454;&#26159;BO&#30340;&#20027;&#35201;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#29942;&#39048;&#65292;&#37319;&#29992;&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#19982;HF&#26679;&#26412;&#30456;&#20851;&#30340;&#24265;&#20215;&#20302;&#20445;&#30495;&#24230;&#65288;LF&#65289;&#25968;&#25454;&#28304;&#65292;&#20943;&#23569;&#25277;&#26679;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#20445;&#30495;&#24230;BO&#65288;MFBO&#65289;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20551;&#35774;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#23569;&#25104;&#31435;&#65306;&#65288;1&#65289;LF&#25968;&#25454;&#28304;&#22312;&#20840;&#23616;&#33539;&#22260;&#20869;&#19982;HF&#25968;&#25454;&#21576;&#33391;&#22909;&#30456;&#20851;&#65292;&#65288;2&#65289;&#19968;&#20010;&#38543;&#26426;&#36807;&#31243;&#21487;&#20197;&#27169;&#25311;&#34701;&#21512;&#25968;&#25454;&#30340;&#22122;&#22768;&#12290;&#36825;&#20123;&#20551;&#35774;&#22312;LF&#25968;&#25454;&#28304;&#20165;&#22312;&#23616;&#37096;&#19982;HF&#28304;&#30456;&#20851;&#25110;&#22122;&#22768;&#26041;&#24046;&#22312;&#19981;&#21516;&#22320;&#26041;&#21464;&#21270;&#26102;&#65292;&#20250;&#26174;&#33879;&#38477;&#20302;MFBO&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a sequential optimization strategy that is increasingly employed in a wide range of areas including materials design. In real world applications, acquiring high-fidelity (HF) data through physical experiments or HF simulations is the major cost component of BO. To alleviate this bottleneck, multi-fidelity (MF) methods are used to forgo the sole reliance on the expensive HF data and reduce the sampling costs by querying inexpensive low-fidelity (LF) sources whose data are correlated with HF samples. However, existing multi-fidelity BO (MFBO) methods operate under the following two assumptions that rarely hold in practical applications: (1) LF sources provide data that are well correlated with the HF data on a global scale, and (2) a single random process can model the noise in the fused data. These assumptions dramatically reduce the performance of MFBO when LF sources are only locally correlated with the HF source or when the noise variance varies across t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33410;&#28857;&#29305;&#24449;&#28165;&#26224;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#28909;&#26680;&#28388;&#27874;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#25512;&#24191;&#20026;G-MHKG&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#35745;&#31639;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#22312;&#22686;&#24378;GNNs&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02769</link><description>&lt;p&gt;
&#32479;&#19968;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#65306;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#26041;&#27861;&#21644;&#26356;&#22810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond. (arXiv:2309.02769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33410;&#28857;&#29305;&#24449;&#28165;&#26224;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#28909;&#26680;&#28388;&#27874;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#25512;&#24191;&#20026;G-MHKG&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#35745;&#31639;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#22312;&#22686;&#24378;GNNs&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39046;&#20808;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;GNNs&#20173;&#28982;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#31561;&#20851;&#38190;&#35745;&#31639;&#25361;&#25112;&#65292;&#36825;&#20123;&#38382;&#39064;&#20250;&#24433;&#21709;GNNs&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#20856;&#21644;&#37327;&#23376;&#29289;&#29702;&#20013;&#24120;&#29992;&#30340;&#26102;&#38388;&#21453;&#28436;&#21407;&#29702;&#24471;&#21040;&#21551;&#21457;&#65292;&#23558;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#36827;&#34892;&#21453;&#36716;&#65292;&#24471;&#21040;&#20102;&#19968;&#31867;&#39640;&#36890;&#28388;&#27874;&#20989;&#25968;&#65292;&#21487;&#20197;&#22686;&#24378;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#28165;&#26224;&#24230;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#23610;&#24230;&#28909;&#26680;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MHKG&#65289;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#31181;&#28388;&#27874;&#20989;&#25968;&#23545;&#33410;&#28857;&#29305;&#24449;&#30340;&#24433;&#21709;&#26469;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#25506;&#32034;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;MHKG&#25512;&#24191;&#21040;&#19968;&#20010;&#31216;&#20026;G-MHKG&#30340;&#27169;&#22411;&#65292;&#24182;&#35814;&#32454;&#23637;&#31034;&#20102;&#27599;&#20010;&#20803;&#32032;&#22312;&#25511;&#21046;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#25152;&#26377;&#30340;&#35266;&#27979;&#21487;&#36798;&#19968;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all afo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#22270;&#34917;&#20840;&#23398;&#20064;&#65288;GCL&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20851;&#31995;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02762</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#29305;&#24449;&#21644;&#32467;&#26500;&#32570;&#22833;&#30340;&#22270;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#22270;&#34917;&#20840;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing. (arXiv:2309.02762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#22270;&#34917;&#20840;&#23398;&#20064;&#65288;GCL&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20851;&#31995;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#19981;&#21487;&#39044;&#27979;&#30340;&#22240;&#32032;&#65292;&#24403;&#25910;&#38598;&#21040;&#30340;&#33410;&#28857;&#29305;&#24449;&#25110;&#32467;&#26500;&#20851;&#31995;&#37096;&#20998;&#32570;&#22833;&#26102;&#65292;GNN&#30340;&#20248;&#36234;&#24615;&#33021;&#23558;&#21463;&#21040;&#20005;&#37325;&#25439;&#23475;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22270;&#34917;&#20840;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#26088;&#22312;&#22312;&#29305;&#23450;&#30340;&#30417;&#30563;&#20219;&#21153;&#24341;&#23548;&#19979;&#37325;&#24314;&#32570;&#22833;&#30340;&#33410;&#28857;&#29305;&#24449;&#25110;&#32467;&#26500;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20123;&#25552;&#20986;&#30340;GCL&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#37325;&#24314;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20851;&#31995;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;GCL&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#21516;&#26102;&#37096;&#20998;&#32570;&#22833;&#25910;&#38598;&#21040;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#32467;&#26500;&#20851;&#31995;&#26102;&#20173;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;GCL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNN) have achieved significant developments in a variety of graph analytical tasks. Nevertheless, GNN's superior performance will suffer from serious damage when the collected node features or structure relationships are partially missing owning to numerous unpredictable factors. Recently emerged graph completion learning (GCL) has received increasing attention, which aims to reconstruct the missing node features or structure relationships under the guidance of a specifically supervised task. Although these proposed GCL methods have made great success, they still exist the following problems: the reliance on labels, the bias of the reconstructed node features and structure relationships. Besides, the generalization ability of the existing GCL still faces a huge challenge when both collected node features and structure relationships are partially missing at the same time. To solve the above issues, we propose a more general GCL framework with the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;SWAP&#65292;&#36890;&#36807;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#26469;&#23454;&#29616;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ASR&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02752</link><description>&lt;p&gt;
SWAP:&#21033;&#29992;&#27425;&#32423;logits&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series. (arXiv:2309.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;SWAP&#65292;&#36890;&#36807;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#26469;&#23454;&#29616;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ASR&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;TSC&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#23545;&#25239;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#30528;&#36229;&#21442;&#25968;&#21270;&#25110;&#38543;&#26426;logit&#25200;&#21160;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#26356;&#22810;&#30340;&#22122;&#22768;&#65292;&#20351;&#24471;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;TSC&#27169;&#22411;&#30340;&#25915;&#20987;&#26041;&#27861;SWAP&#12290;SWAP&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#12290;&#36825;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;logit&#20998;&#24067;&#21644;&#39044;&#27979;logit&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SWAP&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;ASR&#36798;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) has emerged as a critical task in various domains, and deep neural models have shown superior performance in TSC tasks. However, these models are vulnerable to adversarial attacks, where subtle perturbations can significantly impact the prediction results. Existing adversarial methods often suffer from over-parameterization or random logit perturbation, hindering their effectiveness. Additionally, increasing the attack success rate (ASR) typically involves generating more noise, making the attack more easily detectable. To address these limitations, we propose SWAP, a novel attacking method for TSC models. SWAP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits. This is achieved by minimizing the Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution. Experimental results demonstrate that SWAP achieves state-of-the-art performance, with an ASR
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02724</link><description>&lt;p&gt;
&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#21450;BERT&#27169;&#22411;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20398;&#36785;&#24615;&#35821;&#35328;&#26816;&#27979;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;(&#22914;&#24076;&#20271;&#26469;&#35821;)&#20013;&#20173;&#26377;&#25152;&#28382;&#21518;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#20174;Twitter&#19978;&#25910;&#38598;&#20102;15881&#26465;&#25512;&#25991;&#12290;&#27599;&#26465;&#25512;&#25991;&#37117;&#30001;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#20154;&#22763;&#26631;&#35760;&#20026;&#20116;&#20010;&#31867;&#21035;(&#36785;&#39554;&#12289;&#20167;&#24680;&#12289;&#26292;&#21147;&#12289;&#33394;&#24773;&#25110;&#38750;&#20398;&#36785;&#24615;)&#12290;&#26631;&#27880;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#37117;&#38656;&#35201;&#29087;&#24713;&#20197;&#33394;&#21015;&#30340;&#25991;&#21270;&#12289;&#25919;&#27835;&#21644;&#23454;&#36341;&#65292;&#20197;&#29702;&#35299;&#27599;&#26465;&#25512;&#25991;&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#19968;&#20010;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#23545;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;(HeBERT&#21644;AlephBERT)&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#19982;D_OLaH&#32467;&#21512;&#21518;&#65292;&#25552;&#39640;&#20102;HeBERT&#27169;&#22411;2%&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#23545;AlephBERT&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;D_OLaH&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;69%&#65292;&#32780;&#22312;D_OLaH&#19978;&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#20934;&#30830;&#29575;&#20026;57%&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02712</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#27839;&#65306;&#22609;&#36896;&#22810;&#20010;&#39046;&#22495;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the frontiers of deep learning: innovations shaping diverse domains. (arXiv:2309.02712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20351;&#24471;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#12289;&#20248;&#21270;&#12289;&#25913;&#36827;&#21644;&#39044;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;DL&#24050;&#32463;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#22788;&#29702;&#12289;&#20892;&#19994;&#12289;&#20132;&#36890;&#39044;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#29983;&#29289;&#21307;&#23398;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29983;&#24577;&#23398;&#12290;&#20026;&#20102;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#28508;&#22312;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24191;&#27867;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#27491;&#22914;&#25991;&#29486;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;DL&#22312;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#24182;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.02711</link><description>&lt;p&gt;
&#35299;&#20915;&#19981;&#23436;&#20840;&#23545;&#31216;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension. (arXiv:2309.02711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#29702;&#35299;&#25105;&#20204;&#30340;&#29615;&#22659;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20294;&#24448;&#24448;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#36807;&#20110;&#31616;&#21270;&#20102;&#29616;&#23454;&#12290;&#20154;&#31867;&#26159;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#65292;&#22806;&#35980;&#21644;&#35748;&#30693;&#20559;&#35265;&#65288;&#20363;&#22914;&#26377;&#19968;&#21482;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#25163;&#65289;&#37117;&#19981;&#23436;&#32654;&#22320;&#20559;&#31163;&#20102;&#23545;&#31216;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#22823;&#33041;&#24456;&#23481;&#26131;&#20811;&#26381;&#36825;&#20123;&#19981;&#23436;&#32654;&#24182;&#39640;&#25928;&#22320;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#30340;&#39537;&#21160;&#21160;&#26426;&#22312;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25429;&#25417;&#36825;&#31181;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;-&#19968;&#31181;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#19968;&#20010;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#19968;&#20010;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#25152;&#26377;&#29366;&#24577;&#20013;&#24378;&#21046;&#23454;&#26045;&#20849;&#21516;&#30340;&#23545;&#31216;&#20851;&#31995;&#65292;&#24182;&#36866;&#24212;&#20102;&#25152;&#23398;&#31574;&#30053;&#12290;&#23558;ASL&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#23545;&#31216;&#22686;&#24378;&#26041;&#27861;&#22312;&#19968;&#20010;&#28041;&#21450;&#22235;&#36275;&#34434;&#34433;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for mul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;$k$-means&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#40065;&#26834;&#12290;&#31639;&#27861;&#22312;&#22797;&#26434;&#24230;&#20302;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#32473;&#23450;&#25968;&#25454;&#30340;&#26377;&#25928;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.02710</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;$k$-means&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Outlier Robust Seeding for k-means. (arXiv:2309.02710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;$k$-means&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#40065;&#26834;&#12290;&#31639;&#27861;&#22312;&#22797;&#26434;&#24230;&#20302;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#32473;&#23450;&#25968;&#25454;&#30340;&#26377;&#25928;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-means&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#65292;&#20294;&#23427;&#23545;&#24322;&#24120;&#20540;&#38750;&#24120;&#25935;&#24863;&#12290;&#29616;&#26377;&#30340;$k$-means++&#21021;&#22987;&#21270;&#26041;&#27861;&#20351;&#29992;$D^2$&#37319;&#26679;&#24182;&#20855;&#26377;$O(\log k)$&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#22122;&#22768;&#25110;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;$D^2$&#37319;&#26679;&#26356;&#26377;&#21487;&#33021;&#36873;&#25321;&#36828;&#31163;&#38598;&#32676;&#30340;&#24322;&#24120;&#20540;&#20316;&#20026;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#65292;&#22240;&#27492;&#20854;&#20851;&#20110;$k$-means&#22312;&#20869;&#37096;&#25968;&#25454;&#19978;&#30340;&#36817;&#20284;&#20445;&#35777;&#19981;&#20877;&#25104;&#31435;&#12290;&#25105;&#20204;&#20551;&#35774;&#24322;&#24120;&#20540;&#26500;&#25104;&#32473;&#23450;&#25968;&#25454;&#30340;&#19968;&#20010;&#24120;&#25968;&#20998;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$D^2$&#37319;&#26679;&#20998;&#24067;&#20013;&#20351;&#20854;&#40065;&#26834;&#24615;&#26356;&#22909;&#30340;&#31616;&#21333;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;$O(ndk)$&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#36755;&#20986;$O(k)$&#20010;&#32858;&#31867;&#65292;&#27604;&#26368;&#20248;&#24322;&#24120;&#20540;&#20010;&#25968;&#22810;&#20002;&#24323;&#19968;&#20123;&#25968;&#25454;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;$O(1)$&#36817;&#20284;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#20462;&#25913;&#20026;&#36755;&#20986;&#24688;&#22909;$k$&#20010;&#32858;&#31867;&#32780;&#19981;&#26159;$O(k)$&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$ approximation guarantee \cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \textit{w.r.t.} $k$-means solution on inliers, does not hold.  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, whil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31574;&#30053;&#30340;&#31163;&#32447;&#20540;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#32447;&#33829;&#38144;&#27963;&#21160;&#20013;&#30340;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31574;&#30053;&#25928;&#29575;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.02669</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#32422;&#26463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33829;&#38144;&#39044;&#31639;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning. (arXiv:2309.02669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02669
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31574;&#30053;&#30340;&#31163;&#32447;&#20540;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#32447;&#33829;&#38144;&#27963;&#21160;&#20013;&#30340;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31574;&#30053;&#25928;&#29575;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#30340;&#22312;&#32447;&#33829;&#38144;&#27963;&#21160;&#20013;&#30340;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#33829;&#38144;&#39044;&#31639;&#20998;&#37197;&#20915;&#31574;&#30340;&#38271;&#26399;&#25928;&#24212;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28151;&#21512;&#31574;&#30053;&#30340;&#21338;&#24328;&#35770;&#31163;&#32447;&#20215;&#20540;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20197;&#21069;&#30340;&#26041;&#27861;&#20013;&#23384;&#20648;&#26080;&#25968;&#31574;&#30053;&#20943;&#23569;&#21040;&#21482;&#26377;&#24658;&#23450;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31574;&#30053;&#25928;&#29575;&#65292;&#20351;&#20854;&#23545;&#24037;&#19994;&#20351;&#29992;&#23454;&#38469;&#21644;&#26377;&#21033;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#26159;&#20197;&#21069;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#33829;&#38144;&#39044;&#31639;&#20998;&#37197;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#33829;&#38144;&#27963;&#21160;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28041;&#21450;&#25968;&#21315;&#19975;&#29992;&#25143;&#21644;&#36229;&#36807;&#21313;&#20159;&#30340;&#39044;&#31639;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the budget allocation problem in online marketing campaigns that utilize previously collected offline data. We first discuss the long-term effect of optimizing marketing budget allocation decisions in the offline setting. To overcome the challenge, we propose a novel game-theoretic offline value-based reinforcement learning method using mixed policies. The proposed method reduces the need to store infinitely many policies in previous methods to only constantly many policies, which achieves nearly optimal policy efficiency, making it practical and favorable for industrial usage. We further show that this method is guaranteed to converge to the optimal policy, which cannot be achieved by previous value-based reinforcement learning methods for marketing budget allocation. Our experiments on a large-scale marketing campaign with tens-of-millions users and more than one billion budget verify the theoretical results and show that the proposed method outperforms various baseline meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#30446;&#21069;&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#26680;&#36817;&#20284;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#23558;&#39640;&#32500;&#36755;&#20837;&#36716;&#21270;&#20026;&#20302;&#32500;&#29305;&#24449;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19978;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.02651</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#26680;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning as Kernel Approximation. (arXiv:2309.02651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#30446;&#21069;&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#26680;&#36817;&#20284;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#23558;&#39640;&#32500;&#36755;&#20837;&#36716;&#21270;&#20026;&#20302;&#32500;&#29305;&#24449;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19978;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38656;&#35201;&#20026;&#25968;&#25454;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#25552;&#20379;&#26631;&#31614;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#21407;&#22987;&#25968;&#25454;&#24456;&#23481;&#26131;&#33719;&#24471;&#65292;&#20294;&#25163;&#21160;&#26631;&#27880;&#36825;&#20123;&#25968;&#25454;&#30340;&#25104;&#26412;&#22826;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#22411;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#39640;&#32500;&#36755;&#20837;&#30340;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#29305;&#24449;&#65289;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#24378;&#21046;&#35201;&#27714;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#30456;&#20284;&#30340;&#36755;&#20837;&#20855;&#26377;&#36739;&#39640;&#30340;&#20869;&#31215;&#65292;&#32780;&#19981;&#30456;&#20284;&#30340;&#36755;&#20837;&#20855;&#26377;&#36739;&#20302;&#30340;&#20869;&#31215;&#12290;&#19982;&#36880;&#20010;&#27880;&#37322;&#27599;&#20010;&#36755;&#20837;&#19981;&#21516;&#65292;&#21482;&#38656;&#23450;&#20041;&#19968;&#31181;&#37319;&#26679;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#36755;&#20837;&#23545;&#30340;&#26041;&#27861;&#21363;&#21487;&#12290;&#28982;&#21518;&#65292;&#23545;&#27604;&#29305;&#24449;&#21487;&#20197;&#20316;&#20026;&#36755;&#20837;&#25552;&#20379;&#32473;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#22312;&#24863;&#20852;&#36259;&#30340;&#26368;&#32456;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.  The goal of this thesis is to provide an overview of the current theoretical understanding of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#26085;&#24535;&#21644;&#25552;&#20379;&#39044;&#27979;&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.02641</link><description>&lt;p&gt;
TFBEST: &#20855;&#26377;&#21487;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction. (arXiv:2309.02641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#26085;&#24535;&#21644;&#25552;&#20379;&#39044;&#27979;&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#30828;&#30424;&#25925;&#38556;&#26159;&#26114;&#36149;&#30340; - &#20174;&#28798;&#38590;&#24615;&#30340;&#25968;&#25454;&#20002;&#22833;&#21040;&#21830;&#19994;&#20449;&#35465;&#30340;&#38382;&#39064;&#65292;&#21033;&#30410;&#30456;&#20851;&#32773;&#24076;&#26395;&#20687;&#30239;&#30123;&#19968;&#26679;&#36991;&#20813;&#23427;&#12290;&#31215;&#26497;&#30417;&#27979;&#30828;&#30424;&#25925;&#38556;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#21450;&#26102;&#20272;&#35745;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#20026;&#27492;&#65292;&#30828;&#30424;&#39537;&#21160;&#22120;&#20869;&#37096;&#20351;&#29992;&#30340;&#33258;&#25105;&#30417;&#27979;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#25216;&#26415;&#65288;S.M.A.R.T.&#65289;&#20026;&#36825;&#20123;&#37325;&#35201;&#25968;&#25454;&#23384;&#20648;&#35774;&#22791;&#30340;&#38271;&#26399;&#32500;&#25252;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#26085;&#24535;&#12290;&#36807;&#21435;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#36825;&#20123;S.M.A.R.T.&#26085;&#24535;&#21644;&#22522;&#20110;CNN/RNN&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#39044;&#27979;&#30340;RUL&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#20197;&#21450;&#22788;&#29702;&#38750;&#24120;&#38271;&#30340;&#26085;&#24535;&#24207;&#21015;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20351;&#29992;LSTM&#31561;&#26041;&#27861;&#30340;&#30740;&#31350;&#22312;&#35757;&#32451;&#36895;&#24230;&#19978;&#24456;&#24930;&#65292;&#24182;&#19988;&#38656;&#35201;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#30340;&#21452;&#37325;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic data loss to a question of goodwill, stakeholders want to avoid it like the plague. An important tool in proactively monitoring against HDD failure is timely estimation of the Remaining Useful Life (RUL). To this end, the Self-Monitoring, Analysis and Reporting Technology employed within HDDs (S.M.A.R.T.) provide critical logs for long-term maintenance of the security and dependability of these essential data storage devices. Data-driven predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based architectures heavily. However, they have suffered significantly in providing a confidence interval around the predicted RUL values as well as in processing very long sequences of logs. In addition, some of these approaches, such as those based on LSTMs, are inherently slow to train and have tedious feature engineering overheads. To overcome these challenges, in this work we propose a novel transfo
&lt;/p&gt;</description></item><item><title>Epi-Curriculum&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#38598;&#35757;&#32451;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02640</link><description>&lt;p&gt;
Epi-Curriculum: &#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20998;&#38598;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation. (arXiv:2309.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02640
&lt;/p&gt;
&lt;p&gt;
Epi-Curriculum&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#38598;&#35757;&#32451;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#38480;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26032;&#39046;&#22495;&#30340;&#32763;&#35793;&#26102;&#65292;&#20854;&#24615;&#33021;&#20173;&#28982;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Epi-Curriculum&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#23427;&#21253;&#21547;&#19968;&#20010;&#26032;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#36890;&#36807;&#21608;&#26399;&#24615;&#22320;&#23558;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26292;&#38706;&#32473;&#32463;&#39564;&#19981;&#36275;&#30340;&#35299;&#30721;&#22120;/&#32534;&#30721;&#22120;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#20174;&#31616;&#21333;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#33521;&#24503;&#21644;&#33521;&#32599;&#39532;&#23612;&#20122;&#32763;&#35793;&#26041;&#21521;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;i&#65289;Epi-Curriculum&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#22686;&#24378;&#20102;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) models have become successful, but their performance remains poor when translating on new domains with a limited number of data. In this paper, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Experiments on English-German and English-Romanian translation show that: (i) Epi-Curriculum improves both model's robustness and adaptability in seen and unseen domains; (ii) Our episodic training framework enhances the encoder and decoder's robustness to domain shift.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26102;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#22343;&#20540;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#27169;&#22411;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02636</link><description>&lt;p&gt;
&#22810;&#31867;&#21035;&#23545;&#32593;&#32476;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36827;&#34892;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Multiclass Alignment of Confidence and Certainty for Network Calibration. (arXiv:2309.02636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26102;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#22343;&#20540;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#27169;&#22411;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#23481;&#26131;&#20570;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36825;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#25972;&#20307;&#21487;&#20449;&#24230;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#26089;&#26399;&#24037;&#20316;&#37319;&#29992;&#20102;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#20445;&#30041;&#25968;&#25454;&#38598;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#35757;&#32451;&#26102;&#26657;&#20934;&#26041;&#27861;&#65292;&#22312;&#28041;&#21450;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#32988;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26102;&#26657;&#20934;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#31616;&#21333;&#30340;&#12289;&#21363;&#25554;&#21363;&#29992;&#30340;&#36741;&#21161;&#25439;&#22833;&#65292;&#31216;&#20026;&#22810;&#31867;&#21035;&#23545;&#39044;&#27979;&#22343;&#20540;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36827;&#34892;&#23545;&#40784;&#65288;MACC&#65289;&#12290;&#23427;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#27169;&#22411;&#30340;&#22833;&#26657;&#20934;&#30452;&#25509;&#19982;&#20854;&#30830;&#23450;&#24615;&#30456;&#20851;&#65292;&#22240;&#27492;&#22343;&#20540;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#36739;&#22823;&#24046;&#36317;&#24847;&#21619;&#30528;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#37117;&#23384;&#22312;&#24456;&#24046;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have made great strides in pushing the state-of-the-art in several challenging domains. Recent studies reveal that they are prone to making overconfident predictions. This greatly reduces the overall trust in model predictions, especially in safety-critical applications. Early work in improving model calibration employs post-processing techniques which rely on limited parameters and require a hold-out set. Some recent train-time calibration methods, which involve all model parameters, can outperform the postprocessing methods. To this end, we propose a new train-time calibration method, which features a simple, plug-and-play auxiliary loss known as multi-class alignment of predictive mean confidence and predictive certainty (MACC). It is based on the observation that a model miscalibration is directly related to its predictive certainty, so a higher gap between the mean confidence and certainty amounts to a poor calibration both for in-distribution and out-o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#32479;&#35745;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;&#12290;&#31639;&#27861;&#20855;&#26377;&#19977;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#34920;&#31034;&#25968;&#25454;&#38598;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#12289;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#22823;&#23567;&#20197;&#21450;&#23558;&#31751;&#32452;&#21512;&#20026;&#36229;&#31751;&#12290;&#31639;&#27861;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21512;&#36866;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#26469;&#30830;&#23450;&#26368;&#20339;&#36229;&#31751;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.02623</link><description>&lt;p&gt;
&#36890;&#36807;&#25214;&#21040;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Superclustering by finding statistically significant separable groups of optimal gaussian clusters. (arXiv:2309.02623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#32479;&#35745;&#26174;&#33879;&#21487;&#20998;&#31163;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#30340;&#20998;&#32452;&#65292;&#23454;&#29616;&#36229;&#32858;&#31867;&#12290;&#31639;&#27861;&#20855;&#26377;&#19977;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#34920;&#31034;&#25968;&#25454;&#38598;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#12289;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#22823;&#23567;&#20197;&#21450;&#23558;&#31751;&#32452;&#21512;&#20026;&#36229;&#31751;&#12290;&#31639;&#27861;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21512;&#36866;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#26469;&#30830;&#23450;&#26368;&#20339;&#36229;&#31751;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#39640;&#26031;&#31751;&#20998;&#32452;&#25104;&#32479;&#35745;&#21487;&#20998;&#31163;&#30340;&#36229;&#31751;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#23558;&#25968;&#25454;&#38598;&#34920;&#31034;&#20026;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;-&#31751;&#65292;&#20854;&#25968;&#37327;&#22522;&#20110;BIC&#20934;&#21017;&#30340;&#26368;&#23567;&#20540;&#30830;&#23450;&#65307;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#20272;&#35745;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#31751;&#30340;&#22823;&#23567;&#65307;&#20351;&#29992;DBSCAN&#26041;&#27861;&#23558;&#24471;&#21040;&#30340;&#31751;&#32452;&#21512;&#25104;&#36229;&#31751;&#65292;&#36890;&#36807;&#25214;&#21040;&#20854;&#36229;&#21442;&#25968;&#65288;&#26368;&#22823;&#36317;&#31163;&#65289;&#22312;&#26368;&#22823;&#25968;&#37327;&#30340;&#36229;&#31751;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#24341;&#20837;&#30340;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#30340;&#26368;&#22823;&#20540;&#12290;&#30697;&#38453;&#36136;&#37327;&#20934;&#21017;&#23545;&#24212;&#20110;&#25152;&#26377;&#21457;&#29616;&#30340;&#36229;&#31751;&#20013;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#20998;&#31163;&#30340;&#36229;&#31751;&#25152;&#21344;&#27604;&#20363;&#12290;&#35813;&#31639;&#27861;&#21482;&#26377;&#19968;&#20010;&#36229;&#21442;&#25968;-&#32479;&#35745;&#26174;&#33879;&#24615;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#21160;&#20026;&#20854;&#36873;&#25321;&#21512;&#36866;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.  The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.  The algorithm has only one hyperparameter - statistical significance level, and automatically d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#23558;&#35270;&#35273;Transformer&#24212;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#65292;&#22914;&#26080;&#20154;&#26426;&#12290;&#36825;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.02617</link><description>&lt;p&gt;
&#21387;&#32553;&#35270;&#35273;Transformer&#20197;&#23454;&#29616;&#20302;&#36164;&#28304;&#30340;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compressing Vision Transformers for Low-Resource Visual Learning. (arXiv:2309.02617v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#23558;&#35270;&#35273;Transformer&#24212;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#65292;&#22914;&#26080;&#20154;&#26426;&#12290;&#36825;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#21450;&#20854;&#21464;&#31181;&#24050;&#32463;&#22312;&#35270;&#35273;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#20851;&#27880;&#35270;&#35273;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#21644;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#24222;&#22823;&#19988;&#35745;&#31639;&#37327;&#22823;&#12290;&#20363;&#22914;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;ViT-B&#27169;&#22411;&#26377;86M&#20010;&#21442;&#25968;&#65292;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#31227;&#21160;&#21644;&#36793;&#32536;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#21033;&#29992;&#27969;&#34892;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;&#33976;&#39311;&#12289;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#20026;&#23558;&#35270;&#35273;Transformer&#24341;&#20837;&#36793;&#32536;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36873;&#25321;&#30340;&#24212;&#29992;&#29615;&#22659;&#26159;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#65292;&#23427;&#20197;&#30005;&#27744;&#20379;&#30005;&#24182;&#19988;&#20869;&#23384;&#21463;&#38480;&#65292;&#25658;&#24102;&#20102;&#19968;&#22359;&#23610;&#23544;&#19982;NVIDIA Jetson Nano&#30456;&#24403;&#30340;&#21333;&#26495;&#35745;&#31639;&#26426;&#65292;&#20869;&#23384;&#20026;4GB&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.  Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;AI&#36741;&#21161;&#30340;&#26080;&#35757;&#32451;&#23433;&#20840;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#31934;&#30830;&#20869;&#23481;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2309.02616</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;AI&#36741;&#21161;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26080;&#35757;&#32451;&#23433;&#20840;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts. (arXiv:2309.02616v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;AI&#36741;&#21161;&#30340;&#26080;&#35757;&#32451;&#23433;&#20840;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#31934;&#30830;&#20869;&#23481;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#22312;&#23454;&#29616;&#36890;&#20449;&#30446;&#26631;&#30340;&#21516;&#26102;&#20943;&#23569;&#32593;&#32476;&#36164;&#28304;&#28040;&#32791;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20197;&#21450;&#22312;&#32593;&#32476;&#35774;&#22791;&#20013;&#30340;&#37096;&#32626;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#24320;&#38144;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#29983;&#25104;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;GAI&#27169;&#22411;&#30340;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#34920;&#26126;&#65292;&#22312;&#20165;&#21033;&#29992;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#65288;&#22914;&#25552;&#31034;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#35299;&#30721;&#22120;&#21487;&#20197;&#37325;&#26500;&#21407;&#22987;&#28040;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#35821;&#20041;&#32534;&#30721;&#22120;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#25361;&#25112;&#26159;GAI&#22810;&#26679;&#30340;&#29983;&#25104;&#33021;&#21147;&#24341;&#20837;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#22312;&#36755;&#20986;&#20013;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#38480;&#21046;&#20102;GAI&#22312;&#38656;&#35201;&#31934;&#30830;&#28040;&#24687;&#24674;&#22797;&#30340;&#22330;&#26223;&#65288;&#22914;&#20154;&#33080;&#22270;&#20687;&#20256;&#36755;&#65289;&#20013;&#30340;&#30452;&#25509;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAI&#36741;&#21161;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;SemCom&#31995;&#32479;&#65292;&#29992;&#20110;&#31934;&#30830;&#20869;&#23481;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decodi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#21355;&#26143;&#25968;&#25454;&#19982;&#29289;&#29702;&#27169;&#22411;&#34701;&#21512;&#65292;&#21033;&#29992;&#28779;&#28798;&#21040;&#36798;&#26102;&#38388;&#20316;&#20026;&#28779;&#28798;&#21382;&#21490;&#30340;&#31616;&#26126;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#28779;&#28798;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02615</link><description>&lt;p&gt;
&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#19982;&#29289;&#29702;&#27169;&#22411;&#34701;&#21512;&#30340;&#29983;&#25104;&#31639;&#27861;&#21021;&#22987;&#21270;&#28779;&#28798;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts. (arXiv:2309.02615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#21355;&#26143;&#25968;&#25454;&#19982;&#29289;&#29702;&#27169;&#22411;&#34701;&#21512;&#65292;&#21033;&#29992;&#28779;&#28798;&#21040;&#36798;&#26102;&#38388;&#20316;&#20026;&#28779;&#28798;&#21382;&#21490;&#30340;&#31616;&#26126;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#28779;&#28798;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37326;&#28779;&#27963;&#21160;&#21644;&#20854;&#24102;&#26469;&#30340;&#24433;&#21709;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#24320;&#21457;&#39640;&#20998;&#36776;&#29575;&#30340;&#28779;&#28798;&#34892;&#20026;&#27169;&#22411;&#20197;&#39044;&#27979;&#28779;&#28798;&#34067;&#24310;&#12290;&#26368;&#36817;&#65292;&#22312;&#20351;&#29992;&#21355;&#26143;&#26816;&#27979;&#28779;&#28798;&#20301;&#32622;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20351;&#29992;&#27979;&#37327;&#25968;&#25454;&#36890;&#36807;&#25968;&#25454;&#21516;&#21270;&#26469;&#25913;&#36827;&#25968;&#20540;&#27169;&#22411;&#39044;&#27979;&#28779;&#28798;&#34067;&#24310;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#21355;&#26143;&#27979;&#37327;&#20013;&#25512;&#26029;&#28779;&#28798;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#20026;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#21021;&#22987;&#21270;&#32806;&#21512;&#22823;&#27668;-&#28779;&#28798;&#27169;&#22411;&#25552;&#20379;&#25152;&#38656;&#20449;&#24687;&#12290;&#28779;&#28798;&#21040;&#36798;&#26102;&#38388;&#65292;&#21363;&#28779;&#28798;&#21040;&#36798;&#32473;&#23450;&#30340;&#31354;&#38388;&#20301;&#32622;&#30340;&#26102;&#38388;&#65292;&#20316;&#20026;&#28779;&#28798;&#21382;&#21490;&#30340;&#31616;&#26126;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#32463;WRF-SFIRE&#27169;&#25311;&#35757;&#32451;&#30340;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cWGAN&#65289;&#26469;&#20174;&#21355;&#26143;&#27963;&#21160;&#28779;&#28798;&#25968;&#25454;&#20013;&#25512;&#26029;&#28779;&#28798;&#21040;&#36798;&#26102;&#38388;&#12290;cWGAN&#29992;&#20110;&#29983;&#25104;&#21487;&#33021;&#30340;&#28779;&#28798;&#21040;&#36798;&#26102;&#38388;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GANs&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.02614</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#31283;&#23450;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds. (arXiv:2309.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GANs&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#29289;&#29702;&#22522;&#30784;&#25340;&#22270;&#28216;&#25103;&#24868;&#24594;&#30340;&#23567;&#40479;&#20013;&#31283;&#23450;&#32467;&#26500;&#30340;&#36866;&#29992;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#23545;&#20110;&#20851;&#21345;&#29983;&#25104;&#30340;GANs&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#22522;&#20110;&#29926;&#29255;&#30340;&#34920;&#31034;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#21019;&#24314;&#30001;&#22810;&#20010;&#36739;&#23567;&#22359;&#32452;&#25104;&#30340;&#31283;&#23450;&#32467;&#26500;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#21253;&#25324;&#20102;&#35814;&#32454;&#30340;&#32534;&#30721;/&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#24868;&#24594;&#30340;&#23567;&#40479;&#20851;&#21345;&#25551;&#36848;&#36716;&#25442;&#20026;&#36866;&#21512;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GAN&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#30340;&#32467;&#26500;&#35774;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GANs&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#21508;&#31181;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-SaS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27969;&#25968;&#25454;&#20013;&#20986;&#29616;&#31361;&#28982;&#20998;&#24067;&#36716;&#21464;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#21010;&#20998;&#19981;&#21516;&#21046;&#24230;&#24182;&#25429;&#25417;&#21040;&#21464;&#21270;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.02610</link><description>&lt;p&gt;
T-SaS: &#38754;&#21521;&#27969;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data. (arXiv:2309.02610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-SaS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27969;&#25968;&#25454;&#20013;&#20986;&#29616;&#31361;&#28982;&#20998;&#24067;&#36716;&#21464;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#21010;&#20998;&#19981;&#21516;&#21046;&#24230;&#24182;&#25429;&#25417;&#21040;&#21464;&#21270;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27969;&#25968;&#25454;&#22312;&#26102;&#38388;&#27493;&#39588;&#20013;&#23384;&#22312;&#20998;&#24067;&#36716;&#21464;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#24207;&#21015;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23637;&#31034;&#25345;&#32493;&#21160;&#24577;&#30340;&#19981;&#21516;&#21046;&#24230;&#12290;&#21457;&#29616;&#27969;&#25968;&#25454;&#20013;&#30340;&#36716;&#21464;&#34892;&#20026;&#21644;&#28436;&#21270;&#27169;&#24335;&#23545;&#20110;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#40065;&#26834;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#20998;&#24067;&#30340;&#28436;&#21270;&#25968;&#25454;&#65292;&#25110;&#32773;&#36890;&#36807;&#26126;&#30830;&#32473;&#23450;&#30340;&#21046;&#24230;&#36793;&#30028;&#23545;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25968;&#25454;&#27969;&#20013;&#30340;&#36716;&#21464;&#21487;&#33021;&#20250;&#31361;&#28982;&#32780;&#24613;&#21095;&#22320;&#21457;&#29983;&#65292;&#27809;&#26377;&#21069;&#20806;&#12290;&#20998;&#24067;&#36716;&#21464;&#30340;&#36793;&#30028;&#36890;&#24120;&#26159;&#26080;&#27861;&#24471;&#21040;&#30340;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20026;&#25152;&#26377;&#39046;&#22495;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#21464;&#21270;&#30340;&#27169;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#27809;&#26377;&#20219;&#20309;&#21069;&#20806;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#31361;&#28982;&#20998;&#24067;&#36716;&#21464;&#30340;&#36807;&#31243;&#25968;&#25454;&#24314;&#27169;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21517;&#20026;T-SaS&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#21464;&#37327;&#12289;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#20108;&#36827;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.02606</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#22312;&#32447;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Variational Inference for Online Supervised Learning. (arXiv:2309.02606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#21464;&#37327;&#12289;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#20108;&#36827;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#24320;&#21457;&#39640;&#25928;&#30340;&#25512;&#26029;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#19979;&#19968;&#20195;&#23450;&#20301;&#12289;&#36319;&#36394;&#21644;&#22320;&#22270;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#12289;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#27010;&#29575;&#21644;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#27010;&#29575;&#25512;&#26029;&#31639;&#27861;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#25191;&#34892;&#36817;&#20284;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#22522;&#26412;&#25216;&#26415;&#65292;&#20854;&#20013;&#23558;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#23494;&#24230;&#29992;&#21442;&#25968;&#21270;&#23494;&#24230;&#26469;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20998;&#31163;&#30340;&#36739;&#20302;&#19979;&#30028;&#65292;&#29992;&#20110;&#38598;&#20013;&#24335;&#20272;&#35745;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#19968;&#36339;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35777;&#25454;&#36739;&#20302;&#19979;&#30028; (DELBO) &#21253;&#25324;&#35266;&#27979;&#20284;&#28982;&#21644;&#36317;&#31163;&#20808;&#39564;&#23494;&#24230;&#30340;&#24046;&#20540;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#19982;&#27979;&#37327;&#35777;&#25454;&#30340;&#24046;&#36317;&#26159;&#30001;&#20110;&#20849;&#35782;&#21644;&#24314;&#27169;&#35823;&#24046;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary 
&lt;/p&gt;</description></item><item><title>TriNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#25351;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25346;&#21495;&#22788;&#33258;&#21160;&#31579;&#26597;&#20986;&#38656;&#35201;&#19979;&#28216;&#27979;&#35797;&#36827;&#34892;&#35786;&#26029;&#30830;&#35748;&#30340;&#30149;&#30151;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#30340;&#20020;&#24202;&#22522;&#20934;&#65292;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#21307;&#30103;&#25351;&#23548;&#21487;&#20197;&#25552;&#20379;&#26080;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#31579;&#26597;&#12290;</title><link>http://arxiv.org/abs/2309.02604</link><description>&lt;p&gt;
&#22312;&#38376;&#35786;&#25346;&#21495;&#22788;&#20351;&#29992;TriNet&#36827;&#34892;&#32954;&#28814;&#21644;&#23615;&#36335;&#24863;&#26579;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet. (arXiv:2309.02604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02604
&lt;/p&gt;
&lt;p&gt;
TriNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#25351;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25346;&#21495;&#22788;&#33258;&#21160;&#31579;&#26597;&#20986;&#38656;&#35201;&#19979;&#28216;&#27979;&#35797;&#36827;&#34892;&#35786;&#26029;&#30830;&#35748;&#30340;&#30149;&#30151;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#30340;&#20020;&#24202;&#22522;&#20934;&#65292;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#21307;&#30103;&#25351;&#23548;&#21487;&#20197;&#25552;&#20379;&#26080;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#21475;&#32467;&#26500;&#21644;&#23551;&#21629;&#30340;&#31283;&#23450;&#22686;&#38271;&#65292;&#21271;&#32654;&#30340;&#24613;&#35786;&#31185;&#23601;&#35786;&#20154;&#25968;&#19981;&#26029;&#22686;&#21152;&#12290;&#38543;&#30528;&#26356;&#22810;&#24739;&#32773;&#23601;&#35786;&#65292;&#20256;&#32479;&#30340;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#21464;&#24471;&#36127;&#33655;&#36807;&#37325;&#21644;&#20302;&#25928;&#65292;&#23548;&#33268;&#31561;&#24453;&#26102;&#38388;&#24310;&#38271;&#21644;&#21307;&#30103;&#36136;&#37327;&#38477;&#20302;&#12290;&#20854;&#20013;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#26159;&#25346;&#21495;&#21307;&#30103;&#25351;&#23548;&#65292;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#20154;&#21147;&#24037;&#20316;&#37327;&#12289;&#19981;&#20934;&#30830;&#30340;&#35786;&#26029;&#21644;&#20405;&#20837;&#24615;&#30340;&#36807;&#24230;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriNet&#65306;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#25351;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25346;&#21495;&#22788;&#33258;&#21160;&#31579;&#26597;&#20986;&#38656;&#35201;&#19979;&#28216;&#27979;&#35797;&#36827;&#34892;&#35786;&#26029;&#30830;&#35748;&#30340;&#30149;&#30151;&#12290;&#20026;&#20102;&#39564;&#35777;&#31579;&#26597;&#28508;&#21147;&#65292;TriNet&#22312;&#21307;&#38498;&#25346;&#21495;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#26816;&#27979;&#32954;&#28814;&#65288;0.86&#65289;&#21644;&#23615;&#36335;&#24863;&#26579;&#65288;0.93&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#30340;&#38451;&#24615;&#39044;&#27979;&#20540;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#30340;&#20020;&#24202;&#22522;&#20934;&#65292;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#21307;&#30103;&#25351;&#23548;&#21487;&#20197;&#25552;&#20379;&#26080;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32954;&#37096;&#36229;&#22768;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#24494;&#35843;&#26102;&#34920;&#29616;&#20986;&#25913;&#21892;&#30340;&#24615;&#33021;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#36229;&#36807;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02596</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#22810;&#39033;&#32954;&#37096;&#36229;&#22768;&#35299;&#35835;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks. (arXiv:2309.02596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#32954;&#37096;&#36229;&#22768;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#24494;&#35843;&#26102;&#34920;&#29616;&#20986;&#25913;&#21892;&#30340;&#24615;&#33021;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#36229;&#36807;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#22815;&#20135;&#29983;&#19968;&#20010;&#36866;&#29992;&#20110;B&#22411;&#32954;&#37096;&#36229;&#22768;&#20998;&#26512;&#20013;&#30340;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#22312;&#19977;&#20010;&#32954;&#37096;&#36229;&#22768;&#20219;&#21153;&#19978;&#24494;&#35843;&#26102;&#65292;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23616;&#37096;&#21644;&#22806;&#37096;&#27979;&#35797;&#38598;&#19978;&#30340;&#24179;&#22343;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20998;&#21035;&#25552;&#39640;&#20102;0.032&#21644;0.061&#12290;&#22312;&#21333;&#19968;&#39044;&#35757;&#32451;&#27169;&#22411;&#36755;&#20986;&#30340;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#32039;&#20945;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#24182;&#27809;&#26377;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#25552;&#39640;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#25512;&#29702;&#26102;&#38388;&#19982;&#20998;&#24320;&#24494;&#35843;&#27169;&#22411;&#30340;&#20018;&#34892;&#25191;&#34892;&#30456;&#27604;&#20943;&#23569;&#20102;49%&#12290;&#24403;&#20351;&#29992;&#21487;&#29992;&#26631;&#31614;&#30340;1&#65285;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#35266;&#23519;&#20998;&#31867;&#20219;&#21153;&#20013;&#27979;&#35797;AUC&#30340;&#26368;&#22823;&#22686;&#21152;&#20026;0.396&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#20110;&#20135;&#29983;&#32954;&#37096;&#36229;&#22768;&#20998;&#31867;&#20219;&#21153;&#30340;&#21021;&#22987;&#26435;&#37325;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound cl
&lt;/p&gt;</description></item><item><title>CM3Leon&#26159;&#19968;&#20010;&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#36798;&#21040;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.02591</link><description>&lt;p&gt;
&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;: &#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning. (arXiv:2309.02591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02591
&lt;/p&gt;
&lt;p&gt;
CM3Leon&#26159;&#19968;&#20010;&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#36798;&#21040;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CM3Leon&#65288;&#21457;&#38899;&#20026;"Chameleon"&#65289;&#65292;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#30340;&#22522;&#20110;&#20196;&#29260;&#30340;&#35299;&#30721;&#22120;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21644;&#22635;&#20805;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;CM3Leon&#20351;&#29992;&#20102;CM3&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#22312;&#26356;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#39118;&#26684;&#25968;&#25454;&#19978;&#30340;&#25193;&#23637;&#21644;&#35843;&#25972;&#30340;&#24040;&#22823;&#20248;&#21183;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#20174;&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#32534;&#30340;&#37197;&#26041;&#36827;&#34892;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#26816;&#32034;&#22686;&#24378;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#31532;&#20108;&#20010;&#22810;&#20219;&#21153;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#23427;&#20063;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#33258;&#21253;&#21547;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#37197;&#26041;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#12290;CM3Leon&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35757;&#32451;&#35745;&#31639;&#37327;&#27604;&#31867;&#20284;&#26041;&#27861;&#23569;5&#20493;&#65288;&#38646;&#26679;&#26412;MS-COCO FID&#65289;
&lt;/p&gt;
&lt;p&gt;
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20799;&#31461;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#33041;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#30315;&#30187;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.02580</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25581;&#31034;&#38590;&#27835;&#24615;&#30315;&#30187;&#33041;&#32593;&#32476;&#65306;&#19968;&#31181;&#38024;&#23545;&#20799;&#31461;&#24739;&#32773;&#21333;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#30315;&#30187;&#39044;&#27979;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients. (arXiv:2309.02580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20799;&#31461;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#33041;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#30315;&#30187;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#20840;&#29699;&#26377;5000&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#65292;&#32654;&#22269;&#26377;120&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#12290;&#23384;&#22312;&#30528;&#22823;&#37327;&#20799;&#31461;&#24739;&#32773;&#24739;&#26377;&#38590;&#27835;&#24615;&#30315;&#30187;&#65292;&#21363;&#30315;&#30187;&#21457;&#20316;&#26080;&#27861;&#24471;&#21040;&#25511;&#21046;&#12290;&#30315;&#30187;&#21457;&#20316;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#12289;&#36855;&#22833;&#26041;&#21521;&#12289;&#22833;&#21435;&#24847;&#35782;&#65292;&#20197;&#21450;&#20854;&#20182;&#21487;&#33021;&#22952;&#30861;&#20799;&#31461;&#21442;&#19982;&#26085;&#24120;&#27963;&#21160;&#30340;&#30151;&#29366;&#12290;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#21487;&#20197;&#24110;&#21161;&#23478;&#38271;&#21644;&#21307;&#25252;&#20154;&#21592;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#65292;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;&#65292;&#24182;&#35753;&#20799;&#31461;&#22312;&#38754;&#23545;&#30315;&#30187;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#20943;&#23569;&#28966;&#34385;&#21644;&#32039;&#24352;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20197;&#33041;&#30005;&#22270;&#20449;&#21495;&#20026;&#22522;&#30784;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#23398;&#39537;&#21160;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30149;&#29702;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#26631;&#27880;&#30340;&#35299;&#21078;&#21306;&#22495;&#36793;&#30028;&#26694;&#20316;&#20026;&#30149;&#29702;&#30340;&#20195;&#29702;&#65292;&#21462;&#24471;&#20102;&#27604;&#24369;&#30417;&#30563;&#26041;&#27861;&#21644;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02578</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#21078;&#23398;&#39537;&#21160;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30149;&#29702;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anatomy-Driven Pathology Detection on Chest X-rays. (arXiv:2309.02578v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#23398;&#39537;&#21160;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30149;&#29702;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#26631;&#27880;&#30340;&#35299;&#21078;&#21306;&#22495;&#36793;&#30028;&#26694;&#20316;&#20026;&#30149;&#29702;&#30340;&#20195;&#29702;&#65292;&#21462;&#24471;&#20102;&#27604;&#24369;&#30417;&#30563;&#26041;&#27861;&#21644;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#29702;&#26816;&#27979;&#21644;&#20934;&#30830;&#23450;&#20301;&#33021;&#22815;&#23454;&#29616;&#23545;&#21307;&#23398;&#25195;&#25551;&#65288;&#22914;&#33016;&#37096;X&#23556;&#32447;&#65289;&#30340;&#33258;&#21160;&#35299;&#37322;&#65292;&#24182;&#25552;&#20379;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25903;&#25345;&#25918;&#23556;&#31185;&#21307;&#29983;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#30149;&#29702;&#33539;&#22260;&#30340;&#24037;&#20316;&#32791;&#26102;&#65292;&#22240;&#27492;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#24456;&#23569;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#22240;&#32570;&#20047;&#36793;&#30028;&#26694;&#30417;&#30563;&#32780;&#22312;&#24615;&#33021;&#19978;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20174;&#22270;&#20687;&#32423;&#21035;&#27880;&#37322;&#20013;&#23398;&#20064;&#30149;&#29702;&#30340;&#65288;&#31895;&#30053;&#65289;&#23450;&#20301;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35299;&#21078;&#23398;&#39537;&#21160;&#30340;&#30149;&#29702;&#26816;&#27979;&#65288;ADPD&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#26131;&#20110;&#27880;&#37322;&#30340;&#35299;&#21078;&#21306;&#22495;&#36793;&#30028;&#26694;&#20316;&#20026;&#30149;&#29702;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#35757;&#32451;&#26041;&#27861;&#65306;&#20351;&#29992;&#35299;&#21078;&#32423;&#21035;&#30149;&#29702;&#26631;&#31614;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#21644;&#20351;&#29992;&#22270;&#20687;&#32423;&#30149;&#29702;&#26631;&#31614;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#35299;&#21078;&#32423;&#21035;&#35757;&#32451;&#26041;&#27861;&#20248;&#20110;&#24369;&#30417;&#30563;&#26041;&#27861;&#21644;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#33258;&#21160;&#21270;&#32954;&#27668;&#32959;&#20122;&#22411;&#21010;&#20998;&#21644;&#20005;&#37325;&#31243;&#24230;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;COPDGene&#30740;&#31350;&#20013;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#35780;&#20272;&#65292;&#35813;&#31639;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;&#32954;&#27668;&#32959;&#30340;&#21487;&#35270;&#35780;&#20998;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02576</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#23545;&#32954;&#27668;&#32959;&#36827;&#34892;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks. (arXiv:2309.02576v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#33258;&#21160;&#21270;&#32954;&#27668;&#32959;&#20122;&#22411;&#21010;&#20998;&#21644;&#20005;&#37325;&#31243;&#24230;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;COPDGene&#30740;&#31350;&#20013;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#35780;&#20272;&#65292;&#35813;&#31639;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;&#32954;&#27668;&#32959;&#30340;&#21487;&#35270;&#35780;&#20998;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37492;&#23450;&#32954;&#27668;&#32959;&#30340;&#20122;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#30340;&#31649;&#29702;&#21644;&#30142;&#30149;&#24322;&#36136;&#24615;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#32954;&#27668;&#32959;&#20122;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#25163;&#21160;&#20998;&#26512;&#26082;&#32321;&#29712;&#21448;&#20027;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;Fleischner&#23398;&#20250;&#23545;&#32954;&#27668;&#32959;&#20122;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#36827;&#34892;&#21487;&#35270;&#21270;&#35780;&#20998;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;COPDGene&#30740;&#31350;&#20013;&#30340;9650&#20010;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;&#31639;&#27861;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;52&#65285;&#65292;&#20248;&#20110;&#20197;&#21069;&#21457;&#24067;&#30340;&#26041;&#27861;&#30340;45&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39044;&#27979;&#24471;&#20998;&#19982;&#21487;&#35270;&#35780;&#20998;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#22909;&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#33719;&#24471;&#20102;&#20013;&#31561;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22238;&#24402;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#20998;&#31867;&#26631;&#31614;&#65292;&#21516;&#26102;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#30340;&#23616;&#37096;&#28608;&#27963;&#22270;&#20197;&#21487;&#35270;&#21270;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate identification of emphysema subtypes and severity is crucial for effective management of COPD and the study of disease heterogeneity. Manual analysis of emphysema subtypes and severity is laborious and subjective. To address this challenge, we present a deep learning-based approach for automating the Fleischner Society's visual score system for emphysema subtyping and severity analysis. We trained and evaluated our algorithm using 9650 subjects from the COPDGene study. Our algorithm achieved the predictive accuracy at 52\%, outperforming a previously published method's accuracy of 45\%. In addition, the agreement between the predicted scores of our method and the visual scores was good, where the previous method obtained only moderate agreement. Our approach employs a regression training strategy to generate categorical labels while simultaneously producing high-resolution localized activation maps for visualizing the network predictions. By leveraging these dense activation m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.02571</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#24674;&#22797;&#65306;&#22522;&#20110;FFT&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach. (arXiv:2309.02571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#20013;&#19968;&#20010;&#22522;&#30784;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#22240;&#26524;&#20851;&#31995;&#26159;&#38745;&#24577;&#30340;&#26102;&#20505;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#36328;&#26102;&#38388;&#28857;&#23454;&#20307;&#20043;&#38388;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#27604;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#21521;&#37327;&#33258;&#22238;&#24402; (VAR) &#27169;&#22411;&#24674;&#22797;&#22240;&#26524;&#32467;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026; $O(Tn^3N^2)$&#65292;&#20854;&#20013; $n$ &#26159;&#33410;&#28857;&#25968;&#65292;$T$ &#26159;&#26679;&#26412;&#25968;&#65292;$N$ &#26159;&#23454;&#20307;&#20043;&#38388;&#30340;&#26368;&#22823;&#26102;&#38388;&#28382;&#21518;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#20026; $O(Tn^3\log N)$ &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#22240;&#26524;&#32467;&#26500;&#20197;&#33719;&#24471;&#39057;&#22495; (FD) &#34920;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#30001;&#20110;FFT&#23558;&#25152;&#26377;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#31215;&#32047;&#22312;&#27599;&#20010;&#39057;&#29575;&#19978;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by consider
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;Diffusion+&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#24494;&#36719;365&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02564</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#24494;&#36719;365&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Time Series Data Imputation for Microsoft 365. (arXiv:2309.02564v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;Diffusion+&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#24494;&#36719;365&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#23545;&#20110;&#20687;&#24494;&#36719;365&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#20113;&#25925;&#38556;&#65288;&#22914;&#30913;&#30424;&#25925;&#38556;&#12289;&#33410;&#28857;&#25925;&#38556;&#31561;&#65289;&#23041;&#32961;&#21040;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#22312;&#32447;&#26381;&#21153;&#20013;&#26029;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#39044;&#27979;&#20113;&#25925;&#38556;&#24182;&#22312;&#25925;&#38556;&#21457;&#29983;&#20043;&#21069;&#37319;&#21462;&#31215;&#26497;&#34892;&#21160;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#39044;&#27979;&#20013;&#23384;&#22312;&#25968;&#25454;&#32570;&#22833;&#31561;&#25968;&#25454;&#36136;&#37327;&#24046;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;Diffusion+&#65288;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#24212;&#29992;&#23454;&#36341;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is extremely important for large-scale cloud systems like Microsoft 365. Cloud failures such as disk failure, node failure, etc. threaten service reliability, resulting in online service interruptions and economic loss. Existing works focus on predicting cloud failures and proactively taking action before failures happen. However, they suffer from poor data quality like data missing in model training and prediction, which limits the performance. In this paper, we focus on enhancing data quality through data imputation by the proposed Diffusion+, a sample-efficient diffusion model, to impute the missing data efficiently based on the observed data. Our experiments and application practice show that our model contributes to improving the performance of the downstream failure prediction task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31232;&#30095;&#20013;&#24515;&#23545;&#35937;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#19981;&#23545;&#31216;&#24773;&#20917;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#20108;&#27425;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#38382;&#39064;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02557</link><description>&lt;p&gt;
&#31232;&#30095;&#20013;&#24515;&#23545;&#35937;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Sparse Partitioning Around Medoids. (arXiv:2309.02557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31232;&#30095;&#20013;&#24515;&#23545;&#35937;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#19981;&#23545;&#31216;&#24773;&#20917;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#20108;&#27425;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#38382;&#39064;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#23545;&#35937;&#20998;&#21306;&#65288;PAM&#65292;k-Medoids&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#36317;&#31163;&#20989;&#25968;&#25110;&#30456;&#20284;&#24230;&#65292;&#27599;&#20010;&#32858;&#31867;&#30001;&#20854;&#26368;&#20013;&#24515;&#30340;&#23545;&#35937;&#65288;&#31216;&#20026;medoid&#25110;&#31163;&#25955;&#20013;&#20540;&#65289;&#20195;&#34920;&#12290;&#22312;&#36816;&#33829;&#30740;&#31350;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#20063;&#34987;&#31216;&#20026;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#65288;FLP&#65289;&#12290;FastPAM&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36739;&#22823;k&#30340;&#21152;&#36895;&#25216;&#26415;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#20294;&#35813;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20173;&#20026;N&#30340;&#24179;&#26041;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#31232;&#30095;&#21644;&#19981;&#23545;&#31216;&#21464;&#31181;&#65292;&#20363;&#22914;&#29992;&#20110;&#36947;&#36335;&#32593;&#32476;&#31561;&#22270;&#24418;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#20108;&#27425;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#24182;&#20351;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#21482;&#35201;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#36275;&#22815;&#36830;&#25509;&#30340;&#23567;&#22270;&#24418;&#26469;&#36827;&#34892;&#23616;&#37096;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19981;&#23545;&#31216;&#24773;&#20917;&#65292;&#20854;&#20013;medoid&#30340;&#38598;&#21512;&#19982;&#35201;&#35206;&#30422;&#30340;&#28857;&#30340;&#38598;&#21512;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;Transformer(ViT)&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26102;&#20063;&#33021;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02556</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26377;&#25928;&#24494;&#35843;&#35270;&#35273;Transformer&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;Transformer(ViT)&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26102;&#20063;&#33021;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#36716;&#25442;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#34987;&#24212;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#12289;&#35775;&#38382;&#25511;&#21046;&#21644;&#23545;&#25239;&#38450;&#24481;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36716;&#25442;&#25968;&#25454;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;Transformer(ViT)&#23545;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#38477;&#20302;&#65292;&#24182;&#19988;&#26159;&#22312;ViT&#30340;&#23884;&#20837;&#32467;&#26500;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#20855;&#26377;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#30340;&#21152;&#23494;&#22270;&#20687;&#26102;&#38450;&#27490;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#35786;&#26029;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26080;&#26631;&#35760;&#26679;&#26412;&#36828;&#36828;&#22810;&#20110;&#26377;&#26631;&#35760;&#26679;&#26412;&#26102;&#12290;</title><link>http://arxiv.org/abs/2309.02555</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#35786;&#26029;&#20219;&#21153;&#30340;&#24433;&#21709;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images. (arXiv:2309.02555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02555
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#35786;&#26029;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26080;&#26631;&#35760;&#26679;&#26412;&#36828;&#36828;&#22810;&#20110;&#26377;&#26631;&#35760;&#26679;&#26412;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35266;&#23519;&#21040;&#22312;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#20197;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#35760;&#25968;&#25454;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#36817;&#26399;&#20851;&#20110;&#20854;&#22312;X&#20809;&#12289;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12289;&#30913;&#20849;&#25391;&#21644;&#36229;&#22768;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#27604;&#36739;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#35786;&#26029;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#21457;&#29616;&#26159;&#65292;&#30456;&#23545;&#20110;&#23436;&#20840;&#30417;&#30563;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#26631;&#35760;&#26679;&#26412;&#36828;&#36828;&#22810;&#20110;&#26377;&#26631;&#35760;&#26679;&#26412;&#26102;&#12290;&#26681;&#25454;&#32508;&#21512;&#35777;&#25454;&#65292;&#20026;&#32771;&#34385;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;&#37492;&#20110;&#24403;&#21069;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#21644;&#20570;&#27861;&#65292;&#22914;&#23558;&#20020;&#24202;&#30693;&#35782;&#19982;&#29702;&#35770;&#19978;&#26377;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining has been observed to be effective at improving feature representations for transfer learning, leveraging large amounts of unlabelled data. This review summarizes recent research into its usage in X-ray, computed tomography, magnetic resonance, and ultrasound imaging, concentrating on studies that compare self-supervised pretraining to fully supervised learning for diagnostic tasks such as classification and segmentation. The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples. Based on the aggregate evidence, recommendations are provided for practitioners considering using self-supervised learning. Motivated by limitations identified in current research, directions and practices for future study are suggested, such as integrating clinical knowledge with theoretically justified self-supervised learni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#25968;&#25454;&#32858;&#21512;&#31639;&#27861;BETULA&#20351;&#24471;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19978;&#30340;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;HAC&#21464;&#24471;&#21487;&#34892;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02552</link><description>&lt;p&gt;
&#25968;&#25454;&#32858;&#21512;&#29992;&#20110;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Aggregation for Hierarchical Clustering. (arXiv:2309.02552v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#25968;&#25454;&#32858;&#21512;&#31639;&#27861;BETULA&#20351;&#24471;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19978;&#30340;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;HAC&#21464;&#24471;&#21487;&#34892;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20957;&#32858;&#32858;&#31867;&#65288;HAC&#65289;&#21487;&#33021;&#26159;&#26368;&#26089;&#21644;&#26368;&#28789;&#27963;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#19982;&#35768;&#22810;&#36317;&#31163;&#12289;&#30456;&#20284;&#24230;&#21644;&#19981;&#21516;&#30340;&#38142;&#25509;&#31574;&#30053;&#19968;&#36215;&#20351;&#29992;&#12290;&#24403;&#25968;&#25454;&#38598;&#24418;&#25104;&#30340;&#32858;&#31867;&#25968;&#37327;&#26410;&#30693;&#19988;&#25968;&#25454;&#20013;&#23384;&#22312;&#19968;&#23450;&#30340;&#23618;&#27425;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#20351;&#29992;HAC&#12290;&#22823;&#22810;&#25968;HAC&#31639;&#27861;&#22312;&#20840;&#36317;&#31163;&#30697;&#38453;&#19978;&#25805;&#20316;&#65292;&#22240;&#27492;&#38656;&#35201;&#20108;&#27425;&#23384;&#20648;&#12290;&#26631;&#20934;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20063;&#26159;&#31435;&#26041;&#32423;&#21035;&#30340;&#65292;&#29992;&#20110;&#29983;&#25104;&#23436;&#25972;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#23884;&#20837;&#24335;&#25110;&#20854;&#20182;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#20013;&#65292;&#23384;&#20648;&#21644;&#36816;&#34892;&#26102;&#38388;&#23588;&#20854;&#25104;&#38382;&#39064;&#12290;&#22312;&#26412;&#33410;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;BETULA&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#65292;&#23427;&#26159;&#33879;&#21517;&#30340;BIRCH&#25968;&#25454;&#32858;&#21512;&#31639;&#27861;&#30340;&#25968;&#20540;&#31283;&#23450;&#29256;&#26412;&#65292;&#21487;&#20351;HAC&#22312;&#20855;&#26377;&#21463;&#38480;&#36164;&#28304;&#30340;&#31995;&#32479;&#19978;&#21487;&#34892;&#65292;&#21482;&#36896;&#25104;&#36739;&#23567;&#30340;&#32858;&#31867;&#36136;&#37327;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#38408;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.02551</link><description>&lt;p&gt;
&#25345;&#32493;&#25913;&#36827;&#22522;&#20110;&#38408;&#20540;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Continual Improvement of Threshold-Based Novelty Detection. (arXiv:2309.02551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02551
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#38408;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26816;&#27979;&#26410;&#35265;&#31867;&#21035;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#20351;&#24471;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#37096;&#32626;&#25345;&#32493;&#23398;&#20064;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#20195;&#29702;&#31243;&#24207;&#22312;&#36935;&#21040;&#26032;&#31867;&#22411;&#26102;&#24182;&#27809;&#26377;&#26126;&#30830;&#30340;&#36890;&#30693;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#28857;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#38408;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#36825;&#20123;&#38408;&#20540;&#30340;&#20540;&#65288;&#25552;&#21069;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#36866;&#24212;&#25968;&#25454;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#36825;&#20123;&#38408;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#38408;&#20540;&#30340;&#26032;&#26041;&#27861;&#22312;MNIST&#65292;&#26102;&#23578;MNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#24635;&#20307;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#36866;&#24212;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#20381;&#36182;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#20197;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02547</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65292;&#23454;&#29616;&#22810;&#23618;&#27425;&#37325;&#32452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning. (arXiv:2309.02547v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#36866;&#24212;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#20381;&#36182;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#20197;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#29289;&#20307;&#37325;&#32452;&#65292;&#22312;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#22797;&#26434;&#21644;&#20219;&#24847;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#23618;&#37325;&#32452;&#35268;&#21010;&#19978;&#65292;&#21363;&#20351;&#23384;&#22312;&#22810;&#20010;&#23618;&#27425;&#65292;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20960;&#20309;&#19978;&#20063;&#36739;&#31616;&#21333;&#65292;&#22914;&#22612;&#24335;&#22534;&#21472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#30452;&#35266;&#30340;&#32467;&#26500;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#32467;&#26500;&#20381;&#36182;&#23618;&#27425;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#65292;&#36890;&#36807;&#22810;&#20010;&#25805;&#32437;&#22120;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19968;&#31995;&#21015;&#32463;&#20856;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02539</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#26159;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#23376;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#28151;&#38899;&#20013;&#25552;&#21462;&#23545;&#35805;&#38899;&#36712;&#12289;&#38899;&#20048;&#38899;&#36712;&#21644;&#29305;&#25928;&#38899;&#36712;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#39057;&#29575;&#36724;&#30340;&#20219;&#20309;&#23436;&#20840;&#25110;&#36807;&#23436;&#22791;&#30340;&#20998;&#21306;&#36827;&#34892;&#27867;&#21270;&#12290;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#29992;&#20110;&#30830;&#23450;&#24102;&#36890;&#30340;&#23450;&#20041;&#65292;&#29616;&#22312;&#20855;&#22791;&#20887;&#20313;&#24615;&#20197;&#36827;&#34892;&#26356;&#21487;&#38752;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#20449;&#22122;&#27604;&#21644;1-&#33539;&#25968;&#30340;&#31232;&#30095;&#20419;&#36827;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25913;&#21892;&#38590;&#20197;&#27867;&#21270;&#30340;&#22768;&#38899;&#31867;&#21035;&#30340;&#20998;&#31163;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#21487;&#36731;&#26494;&#20998;&#31163;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;Divide and Remaster&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#65292;&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#23545;&#27169;&#24335;&#36827;&#34892;&#21306;&#20998;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.02534</link><description>&lt;p&gt;
&#32463;&#39564;&#19982;&#39044;&#27979;:&#19968;&#31181;&#26032;&#22411;&#30828;&#24230;&#24230;&#37327;&#30340;&#26631;&#20934;&#21270;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experience and Prediction: A Metric of Hardness for a Novel Litmus Test. (arXiv:2309.02534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#65292;&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#23545;&#27169;&#24335;&#36827;&#34892;&#21306;&#20998;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;Winograd Schema Challenge (WSC) &#24050;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#20934;&#21270;&#35797;&#39564;&#12290;&#22240;&#27492;&#65292;WSC&#24341;&#36215;&#20102;&#30740;&#31350;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#34987;&#35270;&#20026;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;Winograd&#27169;&#24335;&#33021;&#22815;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#35774;&#35745;&#26032;&#22411;&#30340;CAPTCHAs&#24418;&#24335;&#12290;&#26089;&#20123;&#26102;&#20505;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#20026;&#20154;&#31867;&#25104;&#24180;&#20154;&#22312;WSC&#19978;&#30340;&#34920;&#29616;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#34920;&#26126;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#27169;&#24335;&#37117;&#26159;&#30456;&#21516;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20154;&#31867;&#30340;&#24863;&#30693;&#38590;&#24230;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#36825;&#31181;"&#30828;&#24230;&#24230;&#37327;"&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#20351;&#29992;&#65292;&#20197;&#21306;&#20998;Winograd&#27169;&#24335;&#12290;&#25105;&#20204;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.  Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.  Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;softmax&#20989;&#25968;&#24212;&#29992;&#20110;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26102;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#65292;&#20174;&#32780;&#22312;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.02530</link><description>&lt;p&gt;
&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on the Probability Simplex. (arXiv:2309.02530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;softmax&#20989;&#25968;&#24212;&#29992;&#20110;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26102;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#65292;&#20174;&#32780;&#22312;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#36870;&#36716;&#25968;&#25454;&#20998;&#24067;&#30340;&#36880;&#28176;&#22122;&#22768;&#21270;&#26469;&#21019;&#24314;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#30340;&#22122;&#22768;&#21270;&#36807;&#31243;&#19982;&#31163;&#25955;&#25968;&#25454;&#20043;&#38388;&#30340;&#26399;&#26395;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#33258;&#28982;&#22320;&#21019;&#24314;&#20102;&#19968;&#31181;&#35299;&#37322;&#65292;&#20854;&#20013;&#28857;&#23545;&#24212;&#20110;&#20998;&#31867;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;softmax&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#21253;&#25324;&#23545;&#21333;&#20301;&#31435;&#26041;&#20307;&#30340;&#25193;&#25955;&#65292;&#36825;&#23545;&#20110;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#24212;&#29992;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#25152;&#24341;&#36215;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.02528</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#19981;&#22686;&#21152;&#36861;&#28335;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Adaptive Adversarial Training Does Not Increase Recourse Costs. (arXiv:2309.02528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#25152;&#24341;&#36215;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#23545;&#25239;&#25915;&#20987;&#21644;&#31639;&#27861;&#22238;&#28335;&#26041;&#27861;&#32852;&#31995;&#22312;&#19968;&#36215;&#65306;&#20004;&#32773;&#37117;&#23547;&#27714;&#23545;&#36755;&#20837;&#23454;&#20363;&#36827;&#34892;&#26368;&#23567;&#26356;&#25913;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#20998;&#31867;&#20915;&#31574;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#20256;&#32479;&#30340;&#23545;&#25239;&#35757;&#32451;&#20250;&#22686;&#21152;&#29983;&#25104;&#22238;&#28335;&#30340;&#25104;&#26412;&#65307;&#23545;&#25239;&#35757;&#32451;&#21322;&#24452;&#36234;&#22823;&#65292;&#22238;&#28335;&#25104;&#26412;&#36234;&#39640;&#12290;&#28982;&#32780;&#65292;&#20174;&#31639;&#27861;&#22238;&#28335;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36866;&#24403;&#30340;&#23545;&#25239;&#35757;&#32451;&#21322;&#24452;&#19968;&#30452;&#26159;&#26410;&#30693;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#36866;&#24212;&#35757;&#32451;&#21322;&#24452;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#20363;&#38388;&#21487;&#21464;&#23545;&#25239;&#33030;&#24369;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26410;&#30693;&#25915;&#20987;&#21322;&#24452;&#30340;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#25152;&#24341;&#36215;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#23545;&#31639;&#27861;&#22238;&#28335;&#25104;&#26412;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.02521</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;CPU&#21644;GPU&#24615;&#33021;&#20998;&#26512;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;GPU&#21644;CPU&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20110;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#22825;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#21644;&#26426;&#22120;&#23398;&#20064;(ML)&#24212;&#29992;&#27491;&#22312;&#24555;&#36895;&#22686;&#21152;&#12290;&#22823;&#37327;&#30340;&#25968;&#25454;&#36890;&#36807;&#20114;&#32852;&#32593;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ML&#21644;DL&#31639;&#27861;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#30828;&#20214;&#36164;&#28304;&#21644;&#24320;&#28304;&#24211;&#20351;&#24471;&#23454;&#29616;&#36825;&#20123;&#31639;&#27861;&#21464;&#24471;&#23481;&#26131;&#12290;Tensorflow&#21644;PyTorch&#26159;&#23454;&#29616;ML&#39033;&#30446;&#30340;&#39046;&#20808;&#26694;&#26550;&#20043;&#19968;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36319;&#36394;&#22312;GPU&#21644;CPU&#19978;&#25191;&#34892;&#30340;&#25805;&#20316;&#65292;&#20197;&#20998;&#26512;&#36164;&#28304;&#20998;&#37197;&#21644;&#28040;&#32791;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;PyTorch&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;CPU&#21644;GPU&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#20998;&#37197;&#24773;&#20917;&#12290;&#35813;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;CPU&#30456;&#27604;&#65292;GPU&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#23545;&#20110;&#19968;&#20010;&#36739;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;GPU&#22312;CPU&#19978;&#24182;&#27809;&#26377;&#22826;&#22810;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#26469;&#25351;&#23548;&#21487;&#34892;&#34892;&#21160;&#29305;&#24449;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#29992;&#25143;&#20248;&#36873;&#30340;&#21487;&#34892;&#24615;&#34917;&#25937;&#12290;</title><link>http://arxiv.org/abs/2309.02517</link><description>&lt;p&gt;
&#26397;&#30528;&#29992;&#25143;&#24341;&#23548;&#30340;&#21487;&#34892;&#24615;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Towards User Guided Actionable Recourse. (arXiv:2309.02517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#26469;&#25351;&#23548;&#21487;&#34892;&#34892;&#21160;&#29305;&#24449;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#29992;&#25143;&#20248;&#36873;&#30340;&#21487;&#34892;&#24615;&#34917;&#25937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#12289;&#38134;&#34892;&#21644;&#21009;&#20107;&#21496;&#27861;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#20419;&#20351;&#20102;&#22312;ML&#27169;&#22411;&#20013;&#30830;&#20445;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#30340;&#24037;&#20855;&#30340;&#21019;&#24314;&#12290;&#20854;&#20013;&#19968;&#20010;&#24037;&#20855;&#26159;&#20026;&#21463;&#21040;&#36127;&#38754;&#24433;&#21709;&#30340;&#29992;&#25143;&#25552;&#20379;&#21487;&#34892;&#24615;&#34917;&#25937;&#65288;AR&#65289;&#12290;AR&#25551;&#36848;&#20102;&#23545;&#29992;&#25143;&#21487;&#34892;&#34892;&#21160;&#29305;&#24449;&#30340;&#32463;&#27982;&#25928;&#30410;&#21464;&#21270;&#30340;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#33719;&#24471;&#26377;&#21033;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#25552;&#20379;&#34917;&#25937;&#30340;&#26041;&#27861;&#20248;&#21270;&#20102;&#25509;&#36817;&#24615;&#12289;&#31232;&#30095;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;&#25104;&#26412;&#31561;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21487;&#25191;&#34892;&#24615;&#32780;&#35328;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#21364;&#33267;&#20851;&#37325;&#35201;&#30340;&#35201;&#27714;&#26159;&#32771;&#34385;&#29992;&#25143;&#20559;&#22909;&#26469;&#25351;&#23548;&#34917;&#25937;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#19977;&#31181;&#31616;&#21333;&#24418;&#24335;&#30340;&#36719;&#32422;&#26463;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65306;i&#65289;&#23545;&#36830;&#32493;&#29305;&#24449;&#36827;&#34892;&#35780;&#20998;&#65292;ii&#65289;&#23545;&#29305;&#24449;&#20540;&#36827;&#34892;&#36793;&#30028;&#35774;&#23450;&#21644;iii&#65289;&#23545;&#20998;&#31867;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#29992;&#25143;&#20248;&#36873;&#30340;&#21487;&#34892;&#24615;&#34917;&#25937;&#65288;UP-AR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experi
&lt;/p&gt;</description></item><item><title>&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#65292;&#35299;&#20915;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#21644;&#22788;&#29702;&#36890;&#36947;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02478</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449; - ICASSP&#29305;&#21035;&#20250;&#35758;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview. (arXiv:2309.02478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02478
&lt;/p&gt;
&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#65292;&#35299;&#20915;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#21644;&#22788;&#29702;&#36890;&#36947;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#22312;&#22609;&#36896;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#35821;&#20041;&#36890;&#20449;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#12289;&#25552;&#21462;&#21644;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#20197;&#21450;&#23545;&#36890;&#36947;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#26041;&#38754;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;&#38500;&#20102;&#24314;&#31435;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#26412;&#25991;&#36824;&#20026;&#19979;&#19968;&#20195;&#29983;&#25104;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#30340;&#26032;&#30740;&#31350;&#36884;&#24452;&#20570;&#20102;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26368;&#20248;&#35299;&#8212;&#8212;COPS&#65288;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20248;&#23376;&#37319;&#26679;&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38598;&#25104;&#26412;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.02476</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23454;&#29616;&#20248;&#21270;&#26679;&#26412;&#36873;&#25321;&#21450;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning. (arXiv:2309.02476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26368;&#20248;&#35299;&#8212;&#8212;COPS&#65288;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20248;&#23376;&#37319;&#26679;&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38598;&#25104;&#26412;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20294;&#22312;&#25163;&#21160;&#26631;&#27880;&#21644;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#24448;&#24448;&#20250;&#24102;&#26469;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#19968;&#20123;&#20449;&#24687;&#24615;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#21253;&#25324;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26680;&#24515;&#38598;&#36873;&#25321;&#28041;&#21450;&#21040;&#21516;&#26102;&#37319;&#26679;&#36755;&#20837;&#65288;$\bx$&#65289;&#21644;&#36755;&#20986;&#65288;$\by$&#65289;&#30340;&#25968;&#25454;&#65292;&#32780;&#20027;&#21160;&#23398;&#20064;&#20165;&#20851;&#27880;&#36755;&#20837;&#25968;&#25454;&#65288;$\bx$&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#32447;&#24615;softmax&#22238;&#24402;&#32972;&#26223;&#19979;&#21516;&#26102;&#35299;&#20915;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#35770;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;COPS&#65288;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20248;&#23376;&#37319;&#26679;&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22522;&#20110;&#23376;&#37319;&#26679;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#26174;&#24335;&#35745;&#31639;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#20013;&#19981;&#23481;&#26131;&#24212;&#29992;&#12290;COPS&#21033;&#29992;&#27169;&#22411;&#30340;&#36923;&#36753;&#22238;&#24402;&#20540;&#26469;&#20272;&#35745;&#37319;&#26679;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).  In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#22312;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02473</link><description>&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#32508;&#36848;&#65306;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges. (arXiv:2309.02473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#22312;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#20110;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#31354;&#20013;&#26426;&#22120;&#20154;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#38656;&#35201;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#27492;&#25163;&#21160;&#32534;&#31243;&#34892;&#20026;&#25110;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#26469;&#23450;&#20041;&#34892;&#20026;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20570;&#27861;&#65289;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36825;&#23601;&#26159;&#27169;&#20223;&#23398;&#20064;&#30340;&#20316;&#29992; - &#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#32780;&#36825;&#20123;&#34892;&#20026;&#26159;&#36890;&#36807;&#28436;&#31034;&#25552;&#20379;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.  This paper a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#27969;&#31243;&#65292;&#21517;&#20026;&#20010;&#24615;&#21270;&#22810;&#31038;&#20132;&#39118;&#38505;&#35780;&#20998;&#65288;iPsRS&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20303;&#38498;&#39118;&#38505;&#30456;&#20851;&#30340;&#26410;&#28385;&#36275;&#31038;&#20250;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.02467</link><description>&lt;p&gt;
&#20026;&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#31038;&#20132;&#39118;&#38505;&#22686;&#21152;&#32780;&#24320;&#21457;&#20844;&#24179;&#30340;&#20010;&#24615;&#21270;&#22810;&#31038;&#20132;&#39118;&#38505;&#35780;&#20998;&#65288;iPsRS&#65289;
&lt;/p&gt;
&lt;p&gt;
Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D). (arXiv:2309.02467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#27969;&#31243;&#65292;&#21517;&#20026;&#20010;&#24615;&#21270;&#22810;&#31038;&#20132;&#39118;&#38505;&#35780;&#20998;&#65288;iPsRS&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20303;&#38498;&#39118;&#38505;&#30456;&#20851;&#30340;&#26410;&#28385;&#36275;&#31038;&#20250;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#31181;&#26063;&#21644;&#23569;&#25968;&#27665;&#26063;&#32676;&#20307;&#20197;&#21450;&#38754;&#20020;&#31038;&#20250;&#19981;&#21033;&#22240;&#32032;&#30340;&#20010;&#20307;&#65292;&#36825;&#20123;&#22240;&#32032;&#24448;&#24448;&#28304;&#20110;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#25215;&#25285;&#30528;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21450;&#20854;&#24182;&#21457;&#30151;&#30340;&#19981;&#25104;&#27604;&#20363;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#22312;&#25252;&#29702;&#28857;&#23454;&#26045;&#26377;&#25928;&#30340;&#31038;&#20250;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#26631;&#65306;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#26512;&#27969;&#31243;&#65292;&#20197;&#35782;&#21035;&#19982;T2D&#24739;&#32773;&#20303;&#38498;&#39118;&#38505;&#30456;&#20851;&#30340;&#26410;&#28385;&#36275;&#31038;&#20250;&#38656;&#27714;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#25972;&#21512;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;EHR&#25968;&#25454;&#65288;2012&#24180;&#33267;2022&#24180;&#65289;&#20013;&#35782;&#21035;&#20986;10,192&#21517;T2D&#24739;&#32773;&#65292;&#21253;&#25324;&#32972;&#26223;SDoH&#65288;&#22914;&#31038;&#21306;&#36139;&#22256;&#21270;&#65289;&#21644;&#20010;&#20307;&#32423;SDoH&#65288;&#22914;&#20303;&#25151;&#31283;&#23450;&#24615;&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#26512;&#27969;&#31243;&#65292;&#21363;&#20010;&#24615;&#21270;&#22810;&#31038;&#20132;&#39118;&#38505;&#35780;&#20998;&#65288;iPsRS&#65289;&#65292;&#20197;&#35782;&#21035;T2D&#24739;&#32773;&#20303;&#38498;&#39118;&#38505;&#39640;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, alon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.02465</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#30340;&#22522;&#30784;AI&#27169;&#22411;&#65306;&#29992;&#20110;G&#20195;&#30721;&#35843;&#35797;&#12289;&#25805;&#20316;&#21644;&#29702;&#35299;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension. (arXiv:2309.02465v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25171;&#21360;&#25110;&#22686;&#26448;&#21046;&#36896;&#26159;&#19968;&#39033;&#38761;&#21629;&#24615;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#23383;&#27169;&#22411;&#20013;&#21019;&#24314;&#29289;&#29702;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#25171;&#21360;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;G&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;G&#20195;&#30721;&#26159;&#19968;&#31181;&#20302;&#32423;&#25968;&#25511;&#32534;&#31243;&#35821;&#35328;&#65292;&#25351;&#23548;&#19977;&#32500;&#25171;&#21360;&#26426;&#22914;&#20309;&#31227;&#21160;&#21644;&#25380;&#20986;&#26448;&#26009;&#12290;&#35843;&#35797;G&#20195;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;G&#20195;&#30721;&#26684;&#24335;&#21644;&#25152;&#25171;&#21360;&#38646;&#20214;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#24191;&#27867;&#35780;&#20272;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35843;&#35797;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#25805;&#20316;G&#20195;&#30721;&#65292;&#24182;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#26816;&#27979;&#21644;&#20462;&#27491;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#25191;&#34892;&#20960;&#20309;&#21464;&#25442;&#26041;&#38754;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strength
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21033;&#29992;&#22810;&#20010;&#29420;&#31435;&#25511;&#21046;&#30340;&#21512;&#25104;&#21943;&#27969;&#36827;&#34892;&#19977;&#32500;&#22278;&#26609;&#30340;&#20027;&#21160;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#38459;&#21147;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.02462</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19977;&#32500;&#22278;&#26609;&#30340;&#20027;&#21160;&#27969;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Active flow control for three-dimensional cylinders through deep reinforcement learning. (arXiv:2309.02462v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21033;&#29992;&#22810;&#20010;&#29420;&#31435;&#25511;&#21046;&#30340;&#21512;&#25104;&#21943;&#27969;&#36827;&#34892;&#19977;&#32500;&#22278;&#26609;&#30340;&#20027;&#21160;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#38459;&#21147;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25104;&#21151;&#23637;&#31034;&#20102;&#36890;&#36807;&#22810;&#20010;&#29420;&#31435;&#25511;&#21046;&#30340;&#38646;&#20928;&#36136;&#37327;&#27969;&#37327;&#21512;&#25104;&#21943;&#27969;&#36827;&#34892;&#20027;&#21160;&#27969;&#25511;&#21046;&#30340;&#32467;&#26524;&#12290;&#21943;&#27969;&#34987;&#25918;&#32622;&#22312;&#19977;&#32500;&#22278;&#26609;&#30340;&#36328;&#24230;&#19978;&#65292;&#26088;&#22312;&#20943;&#23567;&#38459;&#21147;&#31995;&#25968;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#27714;&#35299;&#22120;&#19982;&#37319;&#29992;&#36817;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#30340;&#26234;&#33021;&#20307;&#32806;&#21512;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65306;&#23427;&#21033;&#29992;&#20102;&#23616;&#37096;&#19981;&#21464;&#37327;&#65292;&#20351;&#25511;&#21046;&#36866;&#24212;&#19981;&#21516;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20415;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#26234;&#33021;&#20307;&#30340;&#20132;&#21449;&#24212;&#29992;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#22312;&#35813;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#19979;&#65292;&#38024;&#23545;&#19977;&#31181;&#19981;&#21516;&#37197;&#32622;&#30340;&#38382;&#39064;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#38459;&#21147;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#33410;&#20461;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#26469;&#20195;&#26367;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.02458</link><description>&lt;p&gt;
&#36208;&#21521;&#33410;&#20461;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards frugal unsupervised detection of subtle abnormalities in medical imaging. (arXiv:2309.02458v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#33410;&#20461;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#26469;&#20195;&#26367;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#24322;&#24120;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#24433;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#27491;&#24120;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#23454;&#29616;&#26368;&#20248;&#30340;&#26435;&#34913;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#21644;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36807;&#22810;&#35774;&#35745;&#21644;&#35843;&#25972;&#12290;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#20351;&#23427;&#20204;&#25104;&#20026;&#35299;&#37322;&#22797;&#26434;&#22810;&#20803;&#21442;&#32771;&#27169;&#22411;&#30340;&#33391;&#22909;&#36873;&#25321;&#12290;&#23427;&#20204;&#30340;&#21442;&#25968;&#25968;&#37327;&#26356;&#23567;&#65292;&#26356;&#23481;&#26131;&#35299;&#37322;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#20272;&#35745;&#31639;&#27861;&#65288;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65289;&#22312;&#22823;&#25968;&#25454;&#37327;&#19979;&#19981;&#26131;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data vo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#12298;&#33521;&#38596;&#32852;&#30431;&#12299;&#27604;&#36187;&#32467;&#26524;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#20351;&#29992;&#26410;&#21457;&#24067;&#30340;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#19981;&#21516;&#38454;&#27573;&#65292;&#21508;&#31181;&#27169;&#22411;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22522;&#20110;LightGBM&#27169;&#22411;&#22312;&#20013;&#26399;&#38454;&#27573;&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#36798;81.62\%&#12290;</title><link>http://arxiv.org/abs/2309.02449</link><description>&lt;p&gt;
&#33521;&#38596;&#32852;&#30431;&#65306;&#23454;&#26102;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
League of Legends: Real-Time Result Prediction. (arXiv:2309.02449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#12298;&#33521;&#38596;&#32852;&#30431;&#12299;&#27604;&#36187;&#32467;&#26524;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#20351;&#29992;&#26410;&#21457;&#24067;&#30340;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#19981;&#21516;&#38454;&#27573;&#65292;&#21508;&#31181;&#27169;&#22411;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22522;&#20110;LightGBM&#27169;&#22411;&#22312;&#20013;&#26399;&#38454;&#27573;&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#36798;81.62\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#30005;&#23376;&#28216;&#25103;&#12298;&#33521;&#38596;&#32852;&#30431;&#12299;(LoL)&#30340;&#27604;&#36187;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#22312;&#32771;&#34385;&#21040;&#27604;&#36187;&#30340;&#19981;&#21516;&#21464;&#37327;&#21644;&#38454;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#23454;&#26102;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20351;&#29992;&#26410;&#21457;&#24067;&#30340;&#25968;&#25454;&#20316;&#20026;&#36825;&#19968;&#36807;&#31243;&#30340;&#22522;&#26412;&#37096;&#20998;&#12290;&#38543;&#30528;&#33521;&#38596;&#32852;&#30431;&#30340;&#26085;&#30410;&#27969;&#34892;&#21644;&#36187;&#20107;&#30340;&#20986;&#29616;&#65292;&#19982;&#28216;&#25103;&#30456;&#20851;&#30340;&#36172;&#21338;&#20063;&#20986;&#29616;&#20102;&#65292;&#20351;&#24471;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#26356;&#21152;&#37325;&#35201;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#12290;&#22522;&#20110;LightGBM&#27169;&#22411;&#30340;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;&#27604;&#36187;&#36827;&#34892;60\%&#33267;80\%&#30340;&#38454;&#27573;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;81.62\%&#12290;&#32780;&#36923;&#36753;&#22238;&#24402;&#21644;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#22312;&#27604;&#36187;&#26089;&#26399;&#38454;&#27573;&#34920;&#29616;&#26356;&#20026;&#26377;&#25928;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a study on the prediction of outcomes in matches of the electronic game League of Legends (LoL) using machine learning techniques. With the aim of exploring the ability to predict real-time results, considering different variables and stages of the match, we highlight the use of unpublished data as a fundamental part of this process. With the increasing popularity of LoL and the emergence of tournaments, betting related to the game has also emerged, making the investigation in this area even more relevant. A variety of models were evaluated and the results were encouraging. A model based on LightGBM showed the best performance, achieving an average accuracy of 81.62\% in intermediate stages of the match when the percentage of elapsed time was between 60\% and 80\%. On the other hand, the Logistic Regression and Gradient Boosting models proved to be more effective in early stages of the game, with promising results. This study contributes to the field of machine lear
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#30340;&#26694;&#26550;&#22312;&#21305;&#37197;&#25968;&#25454;&#26684;&#24335;&#21644;&#36991;&#20813;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2309.02442</link><description>&lt;p&gt;
&#35266;&#23519;&#23616;&#37096;&#65292;&#20840;&#23616;&#20998;&#31867;&#65306;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Observe Locally, Classify Globally: Using GNNs to Identify Sparse Matrix Structure. (arXiv:2309.02442v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02442
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#30340;&#26694;&#26550;&#22312;&#21305;&#37197;&#25968;&#25454;&#26684;&#24335;&#21644;&#36991;&#20813;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30697;&#38453;&#35745;&#31639;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#30697;&#38453;&#26684;&#24335;&#19982;&#35201;&#35745;&#31639;&#30340;&#25968;&#25454;&#30340;&#24213;&#23618;&#32467;&#26500;&#30340;&#21305;&#37197;&#12290;&#19981;&#21516;&#30340;&#31232;&#30095;&#30697;&#38453;&#26684;&#24335;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#39318;&#35201;&#25361;&#25112;&#26159;&#22312;&#35745;&#31639;&#20043;&#21069;&#35782;&#21035;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23558;&#20854;&#19982;&#36866;&#24403;&#30340;&#25968;&#25454;&#26684;&#24335;&#30456;&#21305;&#37197;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#36991;&#20813;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#20043;&#21069;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#30697;&#38453;&#32467;&#26500;&#36890;&#36807;&#26679;&#26412;&#21450;&#20854;&#29305;&#24449;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#29305;&#24449;&#21487;&#33021;&#26080;&#27861;&#20174;&#19968;&#20010;&#37319;&#26679;&#38598;&#20013;&#30830;&#23450;&#65292;&#32780;&#24517;&#39035;&#20174;&#23616;&#37096;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#29983;&#25104;&#22120;&#25193;&#23637;&#21040;&#20854;&#20182;&#30697;&#38453;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#31232;&#30095;&#30697;&#38453;&#19978;&#23454;&#29616;&#20102;97%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of sparse matrix computation highly depends on the matching of the matrix format with the underlying structure of the data being computed on. Different sparse matrix formats are suitable for different structures of data. Therefore, the first challenge is identifying the matrix structure before the computation to match it with an appropriate data format. The second challenge is to avoid reading the entire dataset before classifying it. This can be done by identifying the matrix structure through samples and their features. Yet, it is possible that global features cannot be determined from a sampling set and must instead be inferred from local features. To address these challenges, we develop a framework that generates sparse matrix structure classifiers using graph convolutional networks. The framework can also be extended to other matrix structures using user-provided generators. The approach achieves 97% classification accuracy on a set of representative sparse matrix 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02332</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#65306;&#25968;&#25454;&#21644;&#25805;&#20316;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02332
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21754;&#20083;&#21160;&#29289;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30340;&#22797;&#26434;&#32467;&#26500;&#20013;&#65292;&#31070;&#32463;&#20803;&#24418;&#25104;&#32676;&#20307;&#12290;&#36724;&#32034;&#26463;&#36890;&#36807;&#33033;&#20914;&#21015;&#20316;&#20026;&#23186;&#20171;&#22312;&#36825;&#20123;&#32676;&#38598;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#30340;&#31934;&#30830;&#32534;&#30721;&#21644;&#25805;&#20316;&#36824;&#26377;&#24453;&#21457;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#20986;&#21457;&#28857;&#26159;&#19968;&#20010;&#20855;&#26377;&#21487;&#22609;&#24615;&#30340;&#36890;&#29992;&#31070;&#32463;&#20803;&#30340;&#20808;&#36827;&#30340;&#26426;&#26800;&#27169;&#22411;&#12290;&#20174;&#36825;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#28145;&#21051;&#30340;&#25968;&#23398;&#26500;&#36896;&#65306;&#36890;&#36807;&#26377;&#38480;&#20984;&#38181;&#30340;&#20195;&#25968;&#21487;&#20197;&#20934;&#30830;&#22320;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#19981;&#20165;&#20165;&#26159;&#34987;&#21160;&#20256;&#36755;&#32773;&#12290;&#23427;&#20204;&#22312;&#36825;&#20010;&#20195;&#25968;&#32467;&#26500;&#20013;&#25198;&#28436;&#30528;&#36816;&#31639;&#31526;&#30340;&#35282;&#33394;&#65292;&#21453;&#26144;&#20102;&#20302;&#32423;&#32534;&#31243;&#35821;&#35328;&#30340;&#21151;&#33021;&#12290;&#24403;&#36825;&#20123;&#32676;&#20307;&#20114;&#36830;&#26102;&#65292;&#23427;&#20204;&#20855;&#26377;&#31616;&#27905;&#32780;&#24378;&#22823;&#30340;&#20195;&#25968;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#32593;&#32476;&#20351;&#23427;&#20204;&#33021;&#22815;&#23454;&#29616;&#35768;&#22810;&#25805;&#20316;&#65292;&#22914;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#12289;&#32500;&#24230;&#38477;&#20302;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35823;&#24046;&#20943;&#23569;(ER)&#31163;&#32676;&#24471;&#20998;&#26469;&#25913;&#36827;&#26222;&#36890;VAE&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02084</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders. (arXiv:2309.02084v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35823;&#24046;&#20943;&#23569;(ER)&#31163;&#32676;&#24471;&#20998;&#26469;&#25913;&#36827;&#26222;&#36890;VAE&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#26080;&#30417;&#30563;&#30340;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#30340;&#26222;&#36890;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;DGMs&#30456;&#27604;&#65292;&#20351;&#23427;&#20204;&#38750;&#24120;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32676;&#24471;&#20998;&#31216;&#20026;&#35823;&#24046;&#20943;&#23569;(ER)&#65292;&#19987;&#38376;&#20026;&#26222;&#36890;VAE&#35774;&#35745;&#12290;ER&#34701;&#21512;&#20102;&#20174;&#26377;&#25439;&#22270;&#20687;&#36755;&#20837;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#24605;&#24819;&#65292;&#24182;&#32771;&#34385;&#20102;&#22270;&#20687;&#30340;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#12290;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#33719;&#21462;&#65306;https://github.com/ZJLAB-AMMI/VAE4OOD&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
&lt;/p&gt;</description></item><item><title>MvFS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23376;&#32593;&#32476;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23545;&#39057;&#32321;&#20986;&#29616;&#29305;&#24449;&#30340;&#20559;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02064</link><description>&lt;p&gt;
MvFS: &#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MvFS: Multi-view Feature Selection for Recommender System. (arXiv:2309.02064v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02064
&lt;/p&gt;
&lt;p&gt;
MvFS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23376;&#32593;&#32476;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23545;&#39057;&#32321;&#20986;&#29616;&#29305;&#24449;&#30340;&#20559;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#36873;&#25321;&#20851;&#38190;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#65288;AdaFS&#65289;&#36890;&#36807;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#65292;&#32771;&#34385;&#21040;&#32473;&#23450;&#29305;&#24449;&#23383;&#27573;&#30340;&#37325;&#35201;&#24615;&#22312;&#25968;&#25454;&#20013;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#20173;&#28982;&#23384;&#22312;&#23481;&#26131;&#20559;&#21521;&#39057;&#32321;&#20986;&#29616;&#30340;&#20027;&#35201;&#29305;&#24449;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65288;MvFS&#65289;&#65292;&#23427;&#26356;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#23454;&#20363;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;MvFS&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#32593;&#32476;&#65292;&#30001;&#22810;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65292;&#27599;&#20010;&#23376;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;MvFS&#32531;&#35299;&#20102;&#26397;&#21521;&#20027;&#23548;&#27169;&#24335;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#20102;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;MvFS&#37319;&#29992;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;i
&lt;/p&gt;
&lt;p&gt;
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#39640;&#25928;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23545;&#21508;&#31181;&#20027;&#27969;&#26041;&#27861;&#21644;&#26432;&#27602;&#36719;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01866</link><description>&lt;p&gt;
&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#65292;&#23545;&#22522;&#20110;&#26597;&#35810;&#30340;&#39640;&#25928;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting. (arXiv:2309.01866v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#39640;&#25928;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23545;&#21508;&#31181;&#20027;&#27969;&#26041;&#27861;&#21644;&#26432;&#27602;&#36719;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#21331;&#25805;&#20316;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#20351;&#24471;&#24694;&#24847;&#23433;&#21331;&#24212;&#29992;&#25104;&#20026;&#25915;&#20987;&#32773;&#30340;&#21560;&#24341;&#30446;&#26631;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#30446;&#21069;&#23545;ML-based AMD&#26041;&#27861;&#30340;&#25915;&#20987;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25915;&#20987;&#20381;&#36182;&#20110;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#29616;&#23454;&#30340;&#24378;&#20551;&#35774;&#65292;&#20363;&#22914;&#29305;&#24449;&#31354;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdvDroidZero&#65292;&#19968;&#31181;&#23545;ML-based AMD&#26041;&#27861;&#30340;&#39640;&#25928;&#26597;&#35810;&#24335;&#25915;&#20987;&#26694;&#26550;&#65292;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AdvDroidZero&#23545;&#21508;&#31181;&#20027;&#27969;ML-based AMD&#26041;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#23454;&#38469;&#30340;&#26432;&#27602;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#35780;&#20998;&#20989;&#25968;CONFIDERAI&#65292;&#23427;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#30340;&#20960;&#20309;&#20301;&#32622;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.01778</link><description>&lt;p&gt;
CONFIDERAI&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;CONFIRMAL&#21487;&#35299;&#37322;&#35774;&#35745;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
CONFIDERAI: a novel CONFormal Interpretable-by-Design score function forExplainable and Reliable Artificial Intelligence. (arXiv:2309.01778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#35780;&#20998;&#20989;&#25968;CONFIDERAI&#65292;&#23427;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#30340;&#20960;&#20309;&#20301;&#32622;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#21463;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24517;&#39035;&#20026;&#25152;&#26377;&#20154;&#35774;&#35745;&#25104;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#28385;&#36275;&#35299;&#37322;&#24615;&#12289;&#20581;&#22766;&#24615;&#12289;&#36879;&#26126;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#36825;&#20116;&#20010;&#26041;&#38754;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#35748;&#20026;&#23427;&#26159;&#23433;&#20840;&#21644;&#21487;&#20449;&#36182;&#30340;&#12290;&#38500;&#20102;&#36825;&#20116;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#20845;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#19968;&#33268;&#24615;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#32773;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#27010;&#29575;&#24615;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;CONFIDERAI&#65292;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#30340;&#26032;&#35780;&#20998;&#20989;&#25968;&#65292;&#23558;&#19968;&#33268;&#24615;&#39044;&#27979;&#19982;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#35268;&#21017;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#28857;&#22312;&#35268;&#21017;&#36793;&#30028;&#20869;&#30340;&#20960;&#20309;&#20301;&#32622;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21033;&#29992;&#25511;&#21046;&#38750;&#19968;&#33268;&#24615;&#30340;&#25968;&#37327;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23450;&#20041;&#28385;&#36275;&#19968;&#33268;&#24615;&#20445;&#35777;&#30340;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conforma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#65292;&#22312;COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#24212;&#29992;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20197;&#21457;&#29616;&#32954;&#26643;&#22622;&#21644;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2309.01740</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867; COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports. (arXiv:2309.01740v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#65292;&#22312;COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#24212;&#29992;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20197;&#21457;&#29616;&#32954;&#26643;&#22622;&#21644;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#21253;&#25324;&#21307;&#23398;&#26816;&#26597;&#22686;&#21152;&#22312;&#20869;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23613;&#31649;&#19982;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30456;&#27604;&#65292;X&#23556;&#32447;&#22270;&#20687;&#30340;&#31934;&#30830;&#24230;&#36739;&#20302;&#65292;&#20294;&#20197;&#24448;&#20851;&#20110; COVID-19 &#30340;&#33258;&#21160;&#35786;&#26029;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312; X&#23556;&#32447;&#22270;&#20687;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#38498;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#21033;&#29992; CT &#25195;&#25551;&#25552;&#20379;&#30340;&#32454;&#33410;&#36827;&#34892;&#22522;&#20110;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#19982;&#20154;&#31867;&#19987;&#23478;&#21512;&#20316;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#26816;&#27979;&#32954;&#26643;&#22622;&#65292;&#24182;&#35782;&#21035;&#35832;&#22914;&#22320;&#29627;&#29827;&#29366;&#27985;&#27978;&#21644;&#23454;&#21464;&#31561;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#25552;&#20379;&#20102;&#30446;&#21069;&#22312;&#21307;&#23398;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25991;&#29486;&#20013;&#34987;&#24573;&#35270;&#30340;&#35299;&#20915;&#36825;&#20123;&#32454;&#31890;&#24230;&#20219;&#21153;&#30340;&#21487;&#33021;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01507</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#20855;&#26377;4&#20301;&#29366;&#24577;&#30340;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22120;&#29366;&#24577;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#20027;&#35201;&#20869;&#23384;&#28040;&#32791;&#26469;&#28304;&#65292;&#38480;&#21046;&#20102;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#20869;&#21487;&#35757;&#32451;&#30340;&#26368;&#22823;&#27169;&#22411;&#12290;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20174;32&#20301;&#28014;&#28857;&#25968;&#21387;&#32553;&#21040;&#26356;&#20302;&#30340;&#20301;&#23485;&#26377;&#26395;&#20943;&#23567;&#35757;&#32451;&#20869;&#23384;&#21344;&#29992;&#65292;&#32780;&#24403;&#21069;&#26368;&#20302;&#21487;&#36798;&#21040;&#30340;&#20301;&#23485;&#20026;8&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20301;&#23485;&#38477;&#33267;4&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#30697;&#20855;&#26377;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#65292;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#20934;&#30830;&#36817;&#20284;&#12290;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#20013;&#20005;&#26684;&#32422;&#26463;&#22270;&#30340;&#26080;&#29615;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25299;&#25169;&#25490;&#24207;&#30693;&#35782;&#65292;&#33021;&#22815;&#20943;&#23569;&#25512;&#29702;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#30340;&#32467;&#26500;&#26159;&#26080;&#29615;&#30340;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#30456;&#20851;&#30340;&#36125;&#21494;&#26031;&#22522;&#20110;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01392</link><description>&lt;p&gt;
&#19981;&#21516;iable&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#25490;&#24207;&#19982;&#20445;&#35777;&#26080;&#29615;&#24615;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological Ordering in Differentiable Bayesian Structure Learning with Guaranteed Acyclicity Constraint. (arXiv:2309.01392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#20013;&#20005;&#26684;&#32422;&#26463;&#22270;&#30340;&#26080;&#29615;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25299;&#25169;&#25490;&#24207;&#30693;&#35782;&#65292;&#33021;&#22815;&#20943;&#23569;&#25512;&#29702;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#30340;&#32467;&#26500;&#26159;&#26080;&#29615;&#30340;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#30456;&#20851;&#30340;&#36125;&#21494;&#26031;&#22522;&#20110;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#22240;&#20854;&#21487;&#25193;&#23637;&#24615;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#36830;&#32493;&#26494;&#24347;&#26159;&#36825;&#19968;&#36827;&#23637;&#30340;&#20851;&#38190;&#21407;&#22240;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#23450;&#20041;&#30340;&#24471;&#20998;&#26469;&#30830;&#20445;&#20174;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#30340;&#22270;&#26159;&#26080;&#29615;&#30340;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#36824;&#23384;&#22312;&#21478;&#19968;&#31181;&#22522;&#20110;&#32622;&#25442;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#30340;&#26159;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#21464;&#37327;&#30340;&#25299;&#25169;&#25490;&#24207;&#30340;&#25628;&#32034;&#65292;&#20197;&#38480;&#21046;&#22270;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25299;&#25169;&#25490;&#24207;&#30693;&#35782;&#26469;&#20005;&#26684;&#38480;&#21046;&#22270;&#30340;&#26080;&#29615;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#25512;&#29702;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#30340;&#32467;&#26500;&#26159;&#26080;&#29615;&#30340;&#12290;&#25105;&#20204;&#23545;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#30456;&#20851;&#30340;&#36125;&#21494;&#26031;&#22522;&#20110;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based approaches in the structure learning task are thriving because of their scalability. Continuous relaxation has been the key reason for this advancement. Despite achieving promising outcomes, most of these methods are still struggling to ensure that the graphs generated from the latent space are acyclic by minimizing a defined score. There has also been another trend of permutation-based approaches, which concern the search for the topological ordering of the variables in the directed acyclic graph (DAG) in order to limit the search space of the graph. In this study, we propose an alternative approach for strictly constraining the acyclicty of the graphs with an integration of the knowledge from the topological orderings. Our approach can reduce inference complexity while ensuring the structures of the generated graphs to be acyclic. Our empirical experiments with simulated and real-world data show that our approach can outperform related Bayesian score-based approaches.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21319;&#32423;&#20102;Python RTNI&#30340;&#31532;&#20108;&#29256;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#36827;&#34892;&#31526;&#21495;&#24615;&#25972;&#21512;&#65292;&#25903;&#25345;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#12289;&#27491;&#20132;&#30697;&#38453;&#21644;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#36890;&#36807;&#23548;&#20986;TensorNetwork&#26684;&#24335;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#21487;&#20197;&#36827;&#34892;&#20302;&#32500;&#35745;&#31639;&#65292;&#24182;&#35299;&#37322;&#20102;&#25968;&#23398;&#21407;&#29702;&#21644;&#24352;&#37327;&#32593;&#32476;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.01167</link><description>&lt;p&gt;
&#31526;&#21495;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#30340;&#24352;&#37327;&#32593;&#32476;&#35745;&#31639; - Python RTNI&#30340;&#31532;&#20108;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Symbolically integrating tensor networks over various random tensors -- the second version of Python RTNI. (arXiv:2309.01167v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01167
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21319;&#32423;&#20102;Python RTNI&#30340;&#31532;&#20108;&#29256;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#38543;&#26426;&#24352;&#37327;&#36827;&#34892;&#31526;&#21495;&#24615;&#25972;&#21512;&#65292;&#25903;&#25345;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#12289;&#27491;&#20132;&#30697;&#38453;&#21644;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#36890;&#36807;&#23548;&#20986;TensorNetwork&#26684;&#24335;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#21487;&#20197;&#36827;&#34892;&#20302;&#32500;&#35745;&#31639;&#65292;&#24182;&#35299;&#37322;&#20102;&#25968;&#23398;&#21407;&#29702;&#21644;&#24352;&#37327;&#32593;&#32476;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#21319;&#32423;RTNI&#30340;Python&#29256;&#26412;&#65292;&#35813;&#29256;&#26412;&#33021;&#22815;&#31526;&#21495;&#24615;&#22320;&#25972;&#21512;Haar&#20998;&#24067;&#30340;&#37193;&#30697;&#38453;&#19978;&#30340;&#24352;&#37327;&#32593;&#32476;&#12290;&#29616;&#22312;&#65292;PyRTNI2&#36824;&#21487;&#20197;&#22788;&#29702;Haar&#20998;&#24067;&#30340;&#27491;&#20132;&#30697;&#38453;&#20197;&#21450;&#23454;&#25968;&#21644;&#22797;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#23558;&#24352;&#37327;&#32593;&#32476;&#20197;TensorNetwork&#30340;&#26684;&#24335;&#23548;&#20986;&#65292;&#36825;&#26679;&#21487;&#20197;&#20351;&#29992;&#20855;&#20307;&#30340;&#24352;&#37327;&#36827;&#34892;&#36827;&#19968;&#27493;&#35745;&#31639;&#65292;&#21363;&#20351;&#26159;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#65292;&#20854;&#20013;Weingarten&#20989;&#25968;&#19982;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#20989;&#25968;&#19981;&#21516;&#12290;&#25945;&#31243;&#31508;&#35760;&#26412;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/MotohisaFukuda/PyRTNI2&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#31243;&#24207;&#32972;&#21518;&#30340;&#25968;&#23398;&#21407;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#23427;&#36827;&#34892;&#30340;&#21508;&#31181;&#24352;&#37327;&#32593;&#32476;&#35745;&#31639;&#12290;&#20851;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#23558;&#19978;&#36848;&#38543;&#26426;&#30697;&#38453;&#21644;&#24352;&#37327;&#30340;&#36880;&#20803;&#32032;&#30697;&#38453;&#24494;&#31215;&#20998;&#35299;&#37322;&#20026;&#24352;&#37327;&#32593;&#32476;&#22270;&#65292;&#35748;&#20026;&#36825;&#31181;&#35266;&#28857;&#26159;&#33258;&#28982;&#30340;&#65292;&#23558;&#24494;&#31215;&#20998;&#20013;&#30340;delta&#20989;&#25968;&#19982;&#24352;&#37327;&#32593;&#32476;&#22270;&#20013;&#30340;&#36793;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are upgrading the Python-version of RTNI, which symbolically integrates tensor networks over the Haar-distributed unitary matrices. Now, PyRTNI2 can treat the Haar-distributed orthogonal matrices and the real and complex normal Gaussian tensors as well. Moreover, it can export tensor networks in the format of TensorNetwork so that one can make further calculations with concrete tensors, even for low dimensions, where the Weingarten functions differ from the ones for high dimensions. The tutorial notebooks are found at GitHub: https://github.com/MotohisaFukuda/PyRTNI2. In this paper, we explain maths behind the program and show what kind of tensor network calculations can be made with it. For the former, we interpret the element-wise moment calculus of the above random matrices and tensors in terms of tensor network diagrams, and argue that the view is natural, relating delta functions in the calculus to edges in tensor network diagrams.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#25968;&#25454;&#38598;&#21512;&#26469;&#20943;&#23569;&#20027;&#35266;&#24615;&#65292;&#24182;&#19988;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.00855</link><description>&lt;p&gt;
DoRA: &#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00855
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#25968;&#25454;&#38598;&#21512;&#26469;&#20943;&#23569;&#20027;&#35266;&#24615;&#65292;&#24182;&#19988;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#38656;&#27714;&#21644;&#20379;&#24212;&#30340;&#24066;&#22330;&#31995;&#32479;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#24320;&#21457;&#23545;&#25151;&#22320;&#20135;&#36827;&#34892;&#20844;&#27491;&#30340;&#20915;&#31574;&#12290;&#25151;&#22320;&#20135;&#35780;&#20272;&#26159;&#37329;&#34701;&#26426;&#26500;&#20013;&#19968;&#39033;&#39640;&#25104;&#26412;&#30340;&#36164;&#20135;&#20272;&#20540;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#22522;&#20110;&#30456;&#24212;&#30340;&#30693;&#35782;&#21644;&#24066;&#22330;&#30340;&#21028;&#26029;&#26469;&#35780;&#20272;&#20272;&#20215;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#20272;&#20540;&#27169;&#22411;&#20943;&#23569;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#20027;&#35266;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#20132;&#26131;&#29992;&#20110;&#26377;&#25928;&#35780;&#20272;&#65292;&#36825;&#20027;&#35201;&#21463;&#38480;&#20110;&#20132;&#26131;&#30340;&#26631;&#35760;&#24037;&#20316;&#20197;&#21450;&#23545;&#26032;&#24320;&#21457;&#21644;&#20892;&#26449;&#22320;&#21306;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#38598;&#21512;&#26102;&#65292;&#24573;&#35270;&#20102;&#21508;&#31181;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#19988;&#26080;&#27861;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DoRA&#65292;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#23545;&#22238;&#24402;&#31995;&#25968;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#19982;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00564</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#35299;&#37322;&#65306;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#22312;&#30005;&#27744;&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data. (arXiv:2309.00564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#23545;&#22238;&#24402;&#31995;&#25968;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#19982;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20174;&#21270;&#23398;&#25110;&#29983;&#29289;&#31995;&#32479;&#20013;&#32463;&#24120;&#24471;&#21040;&#30340;&#22522;&#30784;&#24179;&#28369;&#28508;&#22312;&#36807;&#31243;&#30340;&#31163;&#25955;&#27979;&#37327;&#25968;&#25454;&#12290;&#22312;&#39640;&#32500;&#24230;&#20013;&#35299;&#37322;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#31354;&#38388;&#38646;&#20540;&#21450;&#20854;&#19982;&#27491;&#21017;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#20250;&#22609;&#36896;&#22238;&#24402;&#31995;&#25968;&#12290;&#25968;&#25454;&#30340;&#31354;&#38388;&#38646;&#20540;&#21253;&#21547;&#25152;&#26377;&#28385;&#36275;$\mathbf{Xw}=\mathbf{0}$&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#38750;&#24120;&#19981;&#21516;&#30340;&#31995;&#25968;&#20135;&#29983;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#21644;&#36890;&#36807;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20197;&#20102;&#35299;&#31995;&#25968;&#24046;&#24322;&#30340;&#21738;&#20123;&#37096;&#20998;&#25509;&#36817;&#20110;&#31354;&#38388;&#38646;&#20540;&#12290;&#36825;&#31181;&#31354;&#38388;&#38646;&#20540;&#26041;&#27861;&#22312;&#19968;&#20010;&#21512;&#25104;&#31034;&#20363;&#21644;&#38146;&#31163;&#23376;&#30005;&#27744;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#26681;&#25454;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36873;&#25321;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21644;z-score&#22788;&#29702;&#65292;&#21487;&#20197;&#24471;&#21040;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. 
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16453</link><description>&lt;p&gt;
&#21548;&#21462;&#23569;&#25968;&#32676;&#20307;&#30340;&#22768;&#38899;&#65306;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;PASS&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26631;&#31614;&#20559;&#35265;&#21644;&#27969;&#37327;&#22343;&#21248;&#24615;&#31561;&#38382;&#39064;&#65292;&#20026;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#21508;&#20010;&#26041;&#38754;&#28145;&#21051;&#22320;&#25913;&#21464;&#20102;&#29616;&#20195;&#29983;&#27963;&#26041;&#24335;&#12290;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22312;&#31649;&#29702;&#31227;&#21160;&#20114;&#32852;&#32593;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#21152;&#23494;&#36890;&#20449;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#20013;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#38480;&#21046;&#65306;1&#65289;&#30001;&#27969;&#37327;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#65292;2&#65289;&#30001;&#32452;&#20214;&#20849;&#20139;&#24341;&#36215;&#30340;&#27969;&#37327;&#22343;&#21248;&#24615;&#65292;&#20197;&#21450;3&#65289;&#20381;&#36182;&#20805;&#36275;&#26631;&#35760;&#27969;&#37327;&#36827;&#34892;&#35757;&#32451;&#12290;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#31216;&#20026;PASS&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#37325;&#26032;&#37319;&#26679;&#21407;&#22987;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#30452;&#25509;&#20351;&#29992;&#20010;&#20307;&#24212;&#29992;&#31243;&#24207;&#26631;&#31614;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#65292;&#21516;&#26102;&#33719;&#24471;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#21306;&#20998;&#37325;&#21472;&#30340;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.15605</link><description>&lt;p&gt;
&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#23545;&#20248;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#27979;&#37327;&#31713;&#25913;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#22810;&#20010;&#27979;&#37327;&#32467;&#26524;&#65292;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#20551;&#35937;&#65292;&#32780;&#19981;&#26159;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#25991;&#26412;&#36755;&#20837;&#21644;&#27979;&#37327;&#32467;&#26524;&#65292;&#26088;&#22312;&#30830;&#23450;&#26576;&#20010;&#32467;&#26524;&#26159;&#21542;&#21457;&#29983;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#37327;&#32467;&#26524;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#37117;&#34920;&#26126;&#32467;&#26524;&#21457;&#29983;&#30340;&#31034;&#20363;&#26159;&#21542;&#30830;&#23454;&#21457;&#29983;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#36825;&#26159;&#30001;&#20110;&#27979;&#37327;&#31713;&#25913;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31616;&#21333;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#22312;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#25105;&#20204;&#24863;&#21040;&#20852;&#22859;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#65292;&#20026;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20855;&#20307;&#26694;&#26550;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#19968;&#31867;&#33258;&#36866;&#24212;&#26725;&#25509;&#21462;&#26679;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12355</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Renormalizing Diffusion Models. (arXiv:2308.12355v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#65292;&#20026;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20855;&#20307;&#26694;&#26550;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#19968;&#31867;&#33258;&#36866;&#24212;&#26725;&#25509;&#21462;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#32479;&#35745;&#23398;&#21644;&#37327;&#23376;&#22330;&#35770;&#30340;&#36870;&#35268;&#33539;&#21270;&#32676;&#27969;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#30340;&#36870;&#36807;&#31243;&#65292;&#29983;&#25104;&#22797;&#26434;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#24067;&#12290;&#38750;&#24494;&#25200;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#21487;&#20197;&#33258;&#28982;&#22320;&#20889;&#25104;&#22330;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#32467;&#21512;&#21040;&#19968;&#20010;&#20855;&#20307;&#30340;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#26500;&#24314;&#29992;&#20110;&#30740;&#31350;&#22330;&#35770;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#21040;&#19968;&#20010;&#26126;&#30830;&#25351;&#23450;&#30340;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#30340;&#36870;&#36807;&#31243;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23450;&#20041;&#26684;&#28857;&#22330;&#35770;&#30340;&#33258;&#36866;&#24212;&#26725;&#25509;&#65288;&#25110;&#24179;&#34892;&#28140;&#28779;&#65289;&#21462;&#26679;&#22120;&#30340;&#19968;&#31867;&#12290;&#30001;&#20110;&#35268;&#33539;&#21270;&#32676;&#26041;&#26696;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26126;&#30830;&#27604;&#36739;&#19981;&#21516;&#26041;&#26696;&#30340;&#25351;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare r
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#21482;&#38656;&#35201;&#36827;&#34892;1&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#36870;&#38382;&#39064;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#26500;&#24314;&#33021;&#22815;&#21033;&#29992;&#20998;&#24067;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#38543;&#26426;&#21464;&#24322;&#21644;&#21464;&#24322;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.09444</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network. (arXiv:2308.09444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09444
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#21482;&#38656;&#35201;&#36827;&#34892;1&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#36870;&#38382;&#39064;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#26500;&#24314;&#33021;&#22815;&#21033;&#29992;&#20998;&#24067;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#38543;&#26426;&#21464;&#24322;&#21644;&#21464;&#24322;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;GMM&#25193;&#23637;&#24605;&#24819;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#26032;&#31639;&#27861;&#27604;&#20256;&#32479;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21333;&#24615;&#12290;&#23427;&#36824;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;1&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31639;&#27861;&#26080;&#35770;&#21442;&#25968;&#21021;&#22987;&#21270;&#22914;&#20309;&#37117;&#33021;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;GMM&#25193;&#23637;&#26041;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32463;&#20856;&#27010;&#29575;&#23618;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#20811;&#26381;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36870;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22522;&#20110;GMM&#30340;&#29983;&#25104;&#22120;&#65292;&#26174;&#31034;&#20986;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#20998;&#24067;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#38543;&#26426;&#21464;&#24322;&#21644;&#21464;&#24322;&#25511;&#21046;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an Gaussian Mixture Model (GMM) learning algorithm, based on our previous work of GMM expansion idea. The new algorithm brings more robustness and simplicity than classic Expectation Maximization (EM) algorithm. It also improves the accuracy and only take 1 iteration for learning. We theoretically proof that this new algorithm is guarantee to converge regardless the parameters initialisation. We compare our GMM expansion method with classic probability layers in neural network leads to demonstrably better capability to overcome data uncertainty and inverse problem. Finally, we test GMM based generator which shows a potential to build further application that able to utilized distribution random sampling for stochastic variation as well as variation control.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15778</link><description>&lt;p&gt;
&#22312;&#22270;&#23884;&#20837;&#20013;&#22522;&#20110;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#30340;&#32534;&#30721;&#24212;&#29992;&#20110;Ising MRF&#27169;&#22411;&#65306;&#32463;&#20856;&#21644;&#37327;&#23376;&#25299;&#25169;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning. (arXiv:2307.15778v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#20449;&#24687;&#20960;&#20309;&#24212;&#29992;&#20110;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#25176;&#37324;&#20811;&#21644;&#29699;&#38754;&#25299;&#25169;&#19978;&#30340;&#24490;&#29615;&#21644;&#20934;&#24490;&#29615;&#30721;&#30340;&#22855;&#20598;&#26816;&#39564;&#30697;&#38453;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21516;&#26500;&#21644;&#20934;&#24490;&#29615;&#30721;&#24490;&#29615;&#30697;&#38453;&#30340;&#23610;&#23544;&#26041;&#38754;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22522;&#20110;&#25429;&#33719;&#38598;&#30340;&#23884;&#20837;&#26041;&#27861;&#30340;&#21457;&#23637;&#20855;&#26377;&#24433;&#21709;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#25968;&#23383;&#20960;&#20309;&#23398;&#26469;&#20248;&#21270;&#32416;&#38169;&#30721;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;&#23884;&#20837;&#21644;&#31232;&#30095;&#22240;&#23376;&#21270;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#28436;&#31034;&#38271;&#36317;&#31163;&#39046;&#22495;&#30340;&#26368;&#26032;DNN&#26550;&#26500;&#65288;ChordMixer&#65292;Mega&#65292;Mega-chunk&#65292;CDIL&#65292;...&#65289;&#19982;&#29305;&#23450;&#31867;&#22411;&#65288;Cage-graph&#65292;Repeat Accumulate&#65289;&#30340;&#21306;&#22359;&#21644;&#21367;&#31215;LDPC&#30721;&#31561;&#20215;&#30340;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;DNN&#26550;&#26500;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. Q
&lt;/p&gt;</description></item><item><title>GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05735</link><description>&lt;p&gt;
GOKU-UI&#65306;&#36890;&#36807;&#20851;&#27880;&#21147;&#21644;&#22810;&#23556;&#20987;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#36866;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05735
&lt;/p&gt;
&lt;p&gt;
GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#23558;&#39046;&#22495;&#24863;&#30693;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#19982;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GOKU-UI&#65292;&#36825;&#26159;SciML&#29983;&#25104;&#27169;&#22411;GOKU-nets&#30340;&#19968;&#31181;&#28436;&#36827;&#12290;GOKU-UI&#25193;&#23637;&#20102;&#21407;&#22987;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#20182;&#31867;&#21035;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#22914;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#65292;&#34701;&#20837;&#20854;&#20013;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#22411;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#30340;&#12289;&#21363;&#26080;&#22788;&#19981;&#22312;&#30340;&#25512;&#29702;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#20854;&#22312;&#37325;&#24314;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#35777;&#25968;&#25454;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#32553;&#23567;&#20102;32&#20493;&#65292;GOKU-UI&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#20984;&#26174;&#20854;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#23454;&#35777;&#30340;&#20154;&#33041;&#25968;&#25454;&#26102;&#65292;&#21516;&#26102;&#34701;&#21512;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GOKU-UI&#20063;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04869</link><description>&lt;p&gt;
Fed-CPrompt: &#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#30340;&#23545;&#27604;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#20174;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#26426;&#23494;&#25968;&#25454;&#38598;&#20013;&#36880;&#27493;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;FCL&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#23384;&#22312;&#22240;&#26080;&#27861;&#35775;&#38382;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#32780;&#23548;&#33268;&#20005;&#37325;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;Fed-CPrompt&#65292;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#26041;&#24335;&#33719;&#24471;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#12290;Fed-CPrompt&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#65292;&#20197;&#20998;&#21035;&#22788;&#29702;FCL&#20013;&#30340;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Fed-CPrompt&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#37325;&#22797;&#23398;&#20064;FCL&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04056</link><description>&lt;p&gt;
&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;(MNNs)&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#12290;&#36825;&#20010;&#31867;&#21035;&#21253;&#25324;&#20102;Wang&#12289;Ruiz&#21644;Ribeiro&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;MNNs&#65292;&#27969;&#24418;&#25955;&#23556;&#21464;&#25442;(&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;)&#65292;&#20197;&#21450;&#20854;&#20182;&#26377;&#36259;&#30340;&#20043;&#21069;&#22312;&#25991;&#29486;&#20013;&#26410;&#32771;&#34385;&#30340;&#31034;&#20363;&#65292;&#22914;Kipf&#21644;Welling&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27969;&#24418;&#31561;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#23545;&#27969;&#24418;&#26377;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#22312;&#26679;&#26412;&#28857;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#21040;&#20854;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;(&#20027;&#35201;&#20851;&#27880;&#29305;&#23450;&#30340;MNN&#32467;&#26500;&#21644;&#22270;&#26500;&#24314;)&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;&#32780;&#19988;&#65292;&#23427;&#34920;&#29616;&#20986;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20849;&#21516;&#30340;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65292;&#21363;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#21644;&#21518;&#35265;&#36866;&#24212;&#65292;&#32479;&#19968;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10587</link><description>&lt;p&gt;
&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimism and Adaptivity in Policy Optimization. (arXiv:2306.10587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20849;&#21516;&#30340;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65292;&#21363;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#21644;&#21518;&#35265;&#36866;&#24212;&#65292;&#32479;&#19968;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#8220;&#20048;&#35266;&#24615;&#8221;&#21644;&#8220;&#36866;&#24212;&#24615;&#8221;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#30340;&#32479;&#19968;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#31574;&#30053;&#36845;&#20195;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#65292;&#25105;&#20204;&#23558;&#19968;&#20123;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65288;i&#65289;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;&#22120;&#20351;&#29992;&#8220;&#26799;&#24230;&#19978;&#21319;&#39044;&#27979;&#8221;&#23558;&#20808;&#21069;&#30340;&#31574;&#30053;$\pi_t$&#26144;&#23556;&#21040;&#19968;&#20010;&#20551;&#35774;$\pi_{t+1}$&#65292;&#28982;&#21518;&#65288;ii&#65289;&#23545;$\pi_{t+1}$&#30340;&#24615;&#33021;&#36827;&#34892;&#37096;&#20998;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#27492;&#36827;&#34892;&#8220;&#21518;&#35265;&#36866;&#24212;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20849;&#20139;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#34920;&#36798;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#36719;&#20214;&#21644;&#20048;&#35266;&#31574;&#30053;&#36845;&#20195;&#12289;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12289;&#22522;&#20110;&#21069;&#21521;&#25628;&#32034;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#25913;&#36827;&#21644;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#36890;&#36807;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#21152;&#36895;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) through \emph{optimism} \&amp; \emph{adaptivity}. Leveraging the deep connection between policy iteration and policy gradient methods, we recast seemingly unrelated policy optimization algorithms as the repeated application of two interleaving steps (i) an \emph{optimistic policy improvement operator} maps a prior policy $\pi_t$ to a hypothesis $\pi_{t+1}$ using a \emph{gradient ascent prediction}, followed by (ii) a \emph{hindsight adaptation} of the optimistic prediction based on a partial evaluation of the performance of $\pi_{t+1}$. We use this shared lens to jointly express other well-known algorithms, including soft and optimistic policy iteration, natural actor-critic methods, model-based policy improvement based on forward search, and meta-learning algorithms. By doing so, we shed light on collective theoretical properties related to acceleration via optimism \&amp; adaptivit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#21487;&#20197;&#20419;&#36827;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08848</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#34920;
&lt;/p&gt;
&lt;p&gt;
Datasheets for Machine Learning Sensors. (arXiv:2306.08848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#21487;&#20197;&#20419;&#36827;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20256;&#24863;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24863;&#30693;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#36827;&#34892;&#26234;&#33021;&#21270;&#65292;&#21516;&#26102;&#36171;&#20104;&#32456;&#31471;&#29992;&#25143;&#26356;&#22810;&#23545;&#20854;&#25968;&#25454;&#30340;&#25511;&#21046;&#26435;&#12290;&#30001;&#20110;&#36825;&#20123;ML&#20256;&#24863;&#22120;&#22312;&#26234;&#33021;&#35774;&#22791;&#30340;&#21457;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28165;&#26224;&#22320;&#35760;&#24405;&#20854;&#35268;&#26684;&#12289;&#21151;&#33021;&#21644;&#38480;&#21046;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;ML&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#31995;&#32479;&#30340;&#30828;&#20214;&#12289;ML&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23646;&#24615;&#12289;&#31471;&#21040;&#31471;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#33258;&#24049;ML&#20256;&#24863;&#22120;&#30340;&#31034;&#20363;&#25968;&#25454;&#34920;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#24378;&#35843;&#36825;&#20123;&#25968;&#25454;&#34920;&#22914;&#20309;&#20419;&#36827;&#23545;ML&#24212;&#29992;&#20013;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26356;&#22909;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#34913;&#37327;&#31995;&#32479;&#24615;&#33021;&#30340;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;ML&#20256;&#24863;&#22120;&#21450;&#20854;&#25968;&#25454;&#34920;&#20849;&#21516;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#23457;&#35745;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) sensors offer a new paradigm for sensing that enables intelligence at the edge while empowering end-users with greater control of their data. As these ML sensors play a crucial role in the development of intelligent devices, clear documentation of their specifications, functionalities, and limitations is pivotal. This paper introduces a standard datasheet template for ML sensors and discusses its essential components including: the system's hardware, ML model and dataset attributes, end-to-end performance metrics, and environmental impact. We provide an example datasheet for our own ML sensor and discuss each section in detail. We highlight how these datasheets can facilitate better understanding and utilization of sensor data in ML applications, and we provide objective measures upon which system performance can be evaluated and compared. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07056</link><description>&lt;p&gt;
&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#65288;RPD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;&#20013;&#65292;RPD&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#35745;&#31639;&#12290;&#20511;&#21161;&#20869;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#25105;&#20204;&#26399;&#26395;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#19978;&#36848;&#22810;&#31181;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;RPD&#65292;&#24182;&#21487;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#29616;&#26377;&#30340;&#26816;&#27979;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20851;&#20110;&#25509;&#25910;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#22522;&#20110;&#31616;&#21333;&#30340;&#23454;&#26102;kNN&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65292;&#24182;&#20855;&#26377;94.5%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.19347</link><description>&lt;p&gt;
&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#65306;&#35299;&#21078;&#19982;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy Seizure Detection: Anatomy and Analysis. (arXiv:2305.19347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#22522;&#20110;&#31616;&#21333;&#30340;&#23454;&#26102;kNN&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65292;&#24182;&#20855;&#26377;94.5%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#36319;&#36394;&#31995;&#32479;&#23545;&#20110;&#30417;&#27979;&#21644;&#35780;&#20272;&#30315;&#30187;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30315;&#30187;&#25252;&#29702;&#20013;&#20351;&#29992;&#30340;&#25252;&#29702;&#35760;&#24405;&#21487;&#33021;&#20250;&#38169;&#36807;&#30315;&#30187;&#21457;&#20316;&#12290;&#21487;&#31359;&#25140;&#30340;&#30417;&#27979;&#35774;&#22791;&#21487;&#33021;&#26356;&#23481;&#26131;&#34987;&#32784;&#21463;&#65292;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#26399;&#36827;&#34892;&#12290;&#35768;&#22810;&#25216;&#26415;&#21644;&#26041;&#27861;&#34987;&#25552;&#20986;&#29992;&#20110;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#65307;&#28982;&#32780;&#65292;&#22312;&#20445;&#25345;&#26816;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#31616;&#21333;&#24615;&#21644;&#36153;&#29992;&#37117;&#26159;&#26085;&#24120;&#20351;&#29992;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#23454;&#26102;k-&#26368;&#36817;&#37051;&#65288;kNN&#65289;&#26426;&#22120;&#23398;&#20064;&#30340;&#36890;&#29992;&#12289;&#32463;&#27982;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#30315;&#30187;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#21487;&#22312;&#19981;&#21040;&#22235;&#31186;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23450;&#21046;&#21644;&#36866;&#24212;&#20010;&#20154;&#29992;&#25143;&#65307;&#35813;&#31995;&#32479;&#32463;&#36807;&#20102;500&#20010;&#34987;&#35797;&#39564;&#32773;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65292;&#25277;&#26679;&#39057;&#29575;&#20026;178 Hz&#65292;&#20854;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;94.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A seizure tracking system is crucial for monitoring and evaluating epilepsy treatments. Caretaker seizure diaries are used in epilepsy care today, but clinical seizure monitoring may miss seizures. Monitoring devices that can be worn may be better tolerated and more suitable for long-term ambulatory use. Many techniques and methods are proposed for seizure detection; However, simplicity and affordability are key concepts for daily use while preserving the accuracy of the detection. In this study, we propose a versal, affordable noninvasive based on a simple real-time k-Nearest-Neighbors (kNN) machine learning that can be customized and adapted to individual users in less than four (4) seconds of training time; the system was verified and validated using 500 subjects, with seizure detection data sampled at 178 Hz, the operated with a mean accuracy of (94.5%).
&lt;/p&gt;</description></item><item><title>AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18798</link><description>&lt;p&gt;
AnoOnly:&#26080;&#38656;&#25439;&#22833;&#27491;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18798
&lt;/p&gt;
&lt;p&gt;
AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(SSAD)&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#20294;&#26377;&#25351;&#23548;&#20316;&#29992;&#30340;&#24322;&#24120;&#23454;&#20363;&#65292;&#22686;&#24378;&#20102;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(UAD)&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21516;&#36136;&#27491;&#24120;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#32479;&#27835;&#20351;&#24471;SSAD&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#24863;&#30693;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;AnoOnly(&#20165;&#24322;&#24120;)&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#19981;&#21516;&#65292;AnoOnly&#26242;&#20572;&#20102;&#20005;&#26684;&#30340;&#25439;&#22833;&#30417;&#30563;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#12290;&#36825;&#31181;&#24369;&#30417;&#30563;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#23454;&#29616;&#65292;&#38544;&#24335;&#22320;&#23545;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23398;&#20064;&#12290;&#24403;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#20013;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;AnoOnly&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;A
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18394</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#23398;&#20064;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#24120;&#29992;&#20110;&#35299;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20808;&#39564;&#20449;&#24687;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#21442;&#25968;&#21152;&#20197;&#26435;&#34913;&#65292;&#32780;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#20363;&#22914;&#24046;&#24322;&#21407;&#21017;&#21644;L-&#26354;&#32447;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#21512;&#36866;&#30340;&#21442;&#25968;&#20540;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#21483;&#20570;&#21452;&#23618;&#23398;&#20064;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#20110;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#31574;&#30053;&#26377;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21452;&#23618;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#26465;&#20214;&#26469;&#34920;&#24449;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#27491;&#20540;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#31561;&#21387;&#31561;&#28201;&#27969;&#24471;&#21040;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#65292;&#24182;&#22312;&#21333;&#21407;&#23376;&#27700;&#30340;&#32467;&#26230;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13233</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21387;&#31561;&#28201;&#27969;&#33719;&#24471;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;
&lt;/p&gt;
&lt;p&gt;
Gibbs free energies via isobaric-isothermal flows. (arXiv:2305.13233v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13233
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#31561;&#21387;&#31561;&#28201;&#27969;&#24471;&#21040;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#65292;&#24182;&#22312;&#21333;&#21407;&#23376;&#27700;&#30340;&#32467;&#26230;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20174;&#31561;&#21387;&#31561;&#28201;&#65288;NPT&#65289;&#38598;&#21512;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36817;&#20284;&#26041;&#27861;&#26469;&#24471;&#21040;&#23436;&#20840;&#28789;&#27963;&#30340;&#19977;&#26012;&#26230;&#31995;&#32479;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#31890;&#23376;&#22352;&#26631;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#20869;&#37096;&#21387;&#21147;&#12290;&#25105;&#20204;&#23545;&#21333;&#21407;&#23376;&#27700;&#22312;&#31435;&#26041;&#21644;&#20845;&#35282;&#20912;&#30456;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#19982;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#21644;&#20854;&#20182;&#21487;&#35266;&#27979;&#37327;&#30340;&#32467;&#26524;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a machine-learning model based on normalizing flows that is trained to sample from the isobaric-isothermal (NPT) ensemble. In our approach, we approximate the joint distribution of a fully-flexible triclinic simulation box and particle coordinates to achieve a desired internal pressure. We test our model on monatomic water in the cubic and hexagonal ice phases and find excellent agreement of Gibbs free energies and other observables compared with established baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#36890;&#36807;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#26631;&#31614;&#26469;&#22686;&#24378;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;&#26032;&#30340;&#20998;&#31867;&#22120;&#26159;&#36755;&#20837;&#23545;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20351;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10293</link><description>&lt;p&gt;
&#26080;&#38480;&#31867;&#21035;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#36890;&#36807;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#26631;&#31614;&#26469;&#22686;&#24378;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;&#26032;&#30340;&#20998;&#31867;&#22120;&#26159;&#36755;&#20837;&#23545;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20351;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#25554;&#20540;&#36755;&#20837;&#21644;&#26631;&#31614;&#30340;&#35757;&#32451;&#23545;&#26469;&#22686;&#21152;&#39069;&#22806;&#30340;&#26679;&#26412;&#12290; Mixup &#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12289;&#32593;&#32476;&#26657;&#20934;&#21644;&#36229;&#20986;&#20998;&#24067;&#27010;&#25324;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;Mixup&#30340;&#19968;&#20010;&#22522;&#30707;&#26159;&#32593;&#32476;&#22312;&#31867;&#21035;&#20043;&#38388;&#23398;&#20064;&#32447;&#24615;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23427;&#21482;&#26159;&#38388;&#25509;&#22320;&#36890;&#36807;&#27010;&#29575;&#32423;&#21035;&#36827;&#34892;&#36755;&#20986;&#25554;&#20540;&#32780;&#24378;&#21046;&#25191;&#34892;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#28151;&#21512;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#28151;&#21512;&#27599;&#20010;&#28151;&#21512;&#23545;&#30340;&#26631;&#31614;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#27599;&#20010;&#22686;&#24378;&#30340;&#26679;&#26412;&#30340;&#30446;&#26631;&#23450;&#20041;&#20026;&#19968;&#20010;&#21807;&#19968;&#30340;&#26032;&#20998;&#31867;&#22120;&#65292;&#20854;&#21442;&#25968;&#26159;&#36755;&#20837;&#23545;&#30340;&#20998;&#31867;&#22120;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#25152;&#26377;&#21487;&#33021;&#20998;&#31867;&#22120;&#30340;&#31354;&#38388;&#26159;&#36830;&#32493;&#30340;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#22120;&#23545;&#20043;&#38388;&#30340;&#25152;&#26377;&#25554;&#20540;&#12290;&#20026;&#20102;&#20351;&#20248;&#21270;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23545;&#27604;&#26080;&#38480;&#31867;&#28151;&#21512;&#25439;&#22833;&#65292;&#20854;&#20013;&#25105;&#20204;&#23545;&#27599;&#20010;&#26679;&#26412;&#23545;&#21521;&#25152;&#26377;&#20854;&#20182;&#26679;&#26412;&#23545;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#26631;&#31614;&#30340;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#19981;&#20439;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08977</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27969;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#22686;&#37327;&#23398;&#20064;&#21644;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation. (arXiv:2305.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#26631;&#31614;&#30340;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#19981;&#20439;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#23431;&#23449;&#20013;&#65292;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20197;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#27969;&#24335;&#29983;&#25104;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#31561;&#32597;&#35265;&#20107;&#20214;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#32780;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23588;&#20854;&#22256;&#38590;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23545;strAEm++DD&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our digital universe nowadays, enormous amount of data are produced in a streaming manner in a variety of application areas. These data are often unlabelled. In this case, identifying infrequent events, such as anomalies, poses a great challenge. This problem becomes even more difficult in non-stationary environments, which can cause deterioration of the predictive performance of a model. To address the above challenges, the paper proposes an autoencoder-based incremental learning method with drift detection (strAEm++DD). Our proposed method strAEm++DD leverages on the advantages of both incremental learning and drift detection. We conduct an experimental study using real-world and synthetic datasets with severe or extreme class imbalance, and provide an empirical analysis of strAEm++DD. We further conduct a comparative study, showing that the proposed method significantly outperforms existing baseline and advanced methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.03859</link><description>&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#65306;&#20197;&#33521;&#22269;COVID-19&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#22270;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#31639;&#27861;&#25552;&#20379;&#30340;&#22240;&#26524;&#34920;&#31034;&#20351;&#24471;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20851;&#32852;&#24615;&#26426;&#22120;&#23398;&#20064;&#30456;&#27604;&#65292;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#21508;&#31181;&#20844;&#20849;&#26469;&#28304;&#25972;&#21512;&#25968;&#25454;&#65292;&#24182;&#30740;&#31350;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21450;&#31639;&#27861;&#32452;&#20135;&#29983;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22270;&#24418;&#32467;&#26500;&#12289;&#27169;&#22411;&#32500;&#24230;&#12289;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#28151;&#28102;&#21464;&#37327;&#12289;&#39044;&#27979;&#21644;&#24178;&#39044;&#25512;&#26029;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#31361;&#20986;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#36817;&#24180;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38656;&#35201;&#26356;&#21152;&#20005;&#26684;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;&#26469;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#26356;&#23433;&#20840;&#22320;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.00048</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification against in-situ observations for Data-Driven Weather Prediction. (arXiv:2305.00048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00048
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#36817;&#24180;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38656;&#35201;&#26356;&#21152;&#20005;&#26684;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;&#26469;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#26356;&#23433;&#20840;&#22320;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#25509;&#36817;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#20302;&#25104;&#26412;&#30340;DDWP&#39044;&#25253;&#20351;&#20854;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#30340;&#20351;&#29992;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#65292;&#20294;&#26159;&#65292;&#22312;&#30495;&#27491;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#23545;DDWPs&#36827;&#34892;&#20005;&#26684;&#30340;&#35780;&#20272;&#20173;&#28982;&#38656;&#35201;&#21162;&#21147;&#12290;DDWP&#36890;&#24120;&#20351;&#29992;ERA5&#37325;&#26032;&#20998;&#26512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20294;&#26159;DDWP&#20165;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#36807;&#27979;&#35797;&#65292;&#21363;&#20351;&#27169;&#25311;&#36136;&#37327;&#24456;&#39640;&#65292;&#20063;&#26080;&#27861;&#23436;&#20840;&#20934;&#30830;&#22320;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#12290;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#23433;&#20840;&#22320;&#20351;&#29992;DDWP&#38656;&#35201;&#26356;&#21152;&#24443;&#24213;&#30340;&#8220;&#30495;&#23454;&#19990;&#30028;&#8221;&#39564;&#35777;&#65292;&#20197;&#21450;&#23545;&#24403;&#21069;DDWP&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#24335;&#36827;&#34892;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#20540;&#24471;&#38382;&#19968;&#19979;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#35757;&#32451;&#30340;&#37325;&#26032;&#20998;&#26512;&#25968;&#25454;&#38598;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#25311;&#25928;&#26524;&#22914;&#20309;&#65311;&#36825;&#23545;&#20110;&#27668;&#20505;&#20844;&#27491;&#21644;&#27668;&#35937;&#25968;&#25454;&#30340;&#19981;&#22343;&#21248;&#21487;&#29992;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather prediction models (DDWPs) have made rapid strides in recent years, demonstrating an ability to approximate Numerical Weather Prediction (NWP) models to a high degree of accuracy. The fast, accurate, and low-cost DDWP forecasts make their use in operational forecasting an attractive proposition, however, there remains work to be done in rigorously evaluating DDWPs in a true operational setting. Typically trained and evaluated using ERA5 reanalysis data, DDWPs have been tested only in a simulation, which cannot represent the real world with complete accuracy even if it is of a very high quality. The safe use of DDWPs in operational forecasting requires more thorough "real-world" verification, as well as a careful examination of how DDWPs are currently trained and evaluated. It is worth asking, for instance, how well do the reanalysis datasets, used for training, simulate the real world? With an eye towards climate justice and the uneven availability of weather data: i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#26041;&#27861;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#30446;&#26631;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#20026;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#25552;&#20379;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13145</link><description>&lt;p&gt;
T&#32454;&#32990;&#21463;&#20307;&#34507;&#30333;&#24207;&#21015;&#21644;&#31232;&#30095;&#32534;&#30721;&#65306;&#30284;&#30151;&#20998;&#31867;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification. (arXiv:2304.13145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#26041;&#27861;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#30446;&#26631;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#20026;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#25552;&#20379;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19968;&#31181;&#20197;&#19981;&#21463;&#25511;&#21046;&#30340;&#32454;&#32990;&#29983;&#38271;&#21644;&#22686;&#27542;&#20026;&#29305;&#24449;&#30340;&#22797;&#26434;&#30142;&#30149;&#12290;T&#32454;&#32990;&#21463;&#20307;&#65288;TCR&#65289;&#26159;&#36866;&#24212;&#24615;&#20813;&#30123;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#65292;&#23427;&#20204;&#23545;&#25239;&#21407;&#30340;&#29305;&#24322;&#24615;&#35782;&#21035;&#22312;&#20813;&#30123;&#21453;&#24212;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#23545;&#25239;&#30284;&#30151;&#12290;TCR&#30340;&#22810;&#26679;&#24615;&#21644;&#29305;&#24322;&#24615;&#20351;&#23427;&#20204;&#25104;&#20026;&#30596;&#20934;&#30284;&#32454;&#32990;&#30340;&#29702;&#24819;&#36873;&#25321;&#65292;&#36817;&#26399;&#27979;&#24207;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;TCR&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#36825;&#23548;&#33268;&#21457;&#29616;&#20102;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#25239;&#30284;&#27963;&#24615;&#30340;TCR&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;TCR&#30340;&#20813;&#30123;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23545;&#20855;&#26377;&#30284;&#30151;&#20998;&#31867;&#20316;&#20026;&#30446;&#26631;&#26631;&#31614;&#30340;TCR&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#24212;&#29992;&#12290;&#31232;&#30095;&#32534;&#30721;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#19968;&#32452;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#21487;&#20197;&#25429;&#25417;&#27688;&#22522;&#37240;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#24494;&#23567;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is a complex disease characterized by uncontrolled cell growth and proliferation. T cell receptors (TCRs) are essential proteins for the adaptive immune system, and their specific recognition of antigens plays a crucial role in the immune response against diseases, including cancer. The diversity and specificity of TCRs make them ideal for targeting cancer cells, and recent advancements in sequencing technologies have enabled the comprehensive profiling of TCR repertoires. This has led to the discovery of TCRs with potent anti-cancer activity and the development of TCR-based immunotherapies. In this study, we investigate the use of sparse coding for the multi-class classification of TCR protein sequences with cancer categories as target labels. Sparse coding is a popular technique in machine learning that enables the representation of data with a set of informative features and can capture complex relationships between amino acids and identify subtle patterns in the sequence tha
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#39640;&#24230;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.09835</link><description>&lt;p&gt;
&#36879;&#26126;&#19988;&#31283;&#20581;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards transparent and robust data-driven wind turbine power curve models. (arXiv:2304.09835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#39640;&#24230;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#23558;&#29615;&#22659;&#26465;&#20214;&#36716;&#21270;&#20026;&#28065;&#36718;&#26426;&#30340;&#21151;&#29575;&#36755;&#20986;&#12290;&#23427;&#20204;&#23545;&#20110;&#33021;&#37327;&#20135;&#20986;&#39044;&#27979;&#21644;&#28065;&#36718;&#26426;&#24615;&#33021;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#20248;&#20110;&#22522;&#20110;&#21442;&#25968;&#21644;&#29289;&#29702;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#34987;&#25209;&#35780;&#20026;&#26159;&#19981;&#36879;&#26126;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23427;&#20204;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#39118;&#21147;&#28065;&#36718;&#26426;&#25152;&#38754;&#20020;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26694;&#26550;&#65292;&#26681;&#25454;&#36816;&#34892;&#20013;&#30340;SCADA&#25968;&#25454;&#26469;&#30740;&#31350;&#21644;&#39564;&#35777;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#23427;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32771;&#34385;&#19982;Shapley&#20540;&#21644;&#26368;&#26032;&#30340;XAI&#22238;&#24402;&#30740;&#31350;&#32467;&#26524;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#26159;&#39564;&#35777;&#25110;&#27979;&#35797;&#38598;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#39640;&#24230;&#22797;&#26434;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#31574;&#30053;&#65292;&#36825;&#19968;&#28857;&#24212;&#24341;&#36215;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind turbine power curve models translate ambient conditions into turbine power output. They are essential for energy yield prediction and turbine performance monitoring. In recent years, data-driven machine learning methods have outperformed parametric, physics-informed approaches. However, they are often criticised for being opaque "black boxes" which raises concerns regarding their robustness in non-stationary environments, such as faced by wind turbines. We, therefore, introduce an explainable artificial intelligence (XAI) framework to investigate and validate strategies learned by data-driven power curve models from operational SCADA data. It combines domain-specific considerations with Shapley Values and the latest findings from XAI for regression. Our results suggest, that learned strategies can be better indicators for model robustness than validation or test set errors. Moreover, we observe that highly complex, state-of-the-art ML models are prone to learn physically implausib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06237</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation. (arXiv:2304.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21010;&#20998;ECG&#20013;&#30340;&#20851;&#38190;&#27874;&#24418;&#26159;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#20197;&#25903;&#25345;&#35786;&#26029;&#21644;&#27835;&#30103;&#24515;&#33039;&#30142;&#30149;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#34429;&#28982;&#21033;&#29992;&#20998;&#21106;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23450;&#20301;P&#12289;QRS&#21644;T&#27874;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22788;&#29702;&#21576;&#29616;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20855;&#26377;&#24191;&#27867;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#65292;&#23558;&#20998;&#21106;&#19982;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#21253;&#21547;&#21508;&#31181;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#38598;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20102;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#21516;&#26102;&#32467;&#21512;&#20998;&#31867;&#25351;&#23548;&#30340;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate delineation of key waveforms in an ECG is a critical initial step in extracting relevant features to support the diagnosis and treatment of heart conditions. Although deep learning based methods using a segmentation model to locate P, QRS and T waves have shown promising results, their ability to handle signals exhibiting arrhythmia remains unclear. In this study, we propose a novel approach that leverages a deep learning model to accurately delineate signals with a wide range of arrhythmia. Our approach involves training a segmentation model using a hybrid loss function that combines segmentation with the task of arrhythmia classification. In addition, we use a diverse training set containing various arrhythmia types, enabling our model to handle a wide range of challenging cases. Experimental results show that our model accurately delineates signals with a broad range of abnormal rhythm types, and the combined training with classification guidance can effectively reduce fals
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06044</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38750;&#32447;&#24615;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#65306;COMM-PINN&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#36335;&#24452;&#30456;&#20851;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19981;&#20165;&#28385;&#36275;&#25152;&#26377;&#28909;&#21147;&#23398;&#32422;&#26463;&#65292;&#32780;&#19988;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#65292;&#31435;&#21363;&#25552;&#20379;&#20851;&#20110;&#24403;&#21069;&#26448;&#26009;&#29366;&#24577;&#65288;&#21363;&#33258;&#30001;&#33021;&#65292;&#24212;&#21147;&#21644;&#20869;&#37096;&#21464;&#37327;&#30340;&#28436;&#21464;&#65289;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#21021;&#22987;&#25968;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#23427;&#35268;&#36991;&#20102;&#27714;&#35299;&#22797;&#26448;&#26009;&#27169;&#22411;&#20013;&#38750;&#32447;&#24615;&#26041;&#31243;&#25152;&#38656;&#30340;&#37325;&#22797;&#29275;&#39039;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20943;&#23569;&#33719;&#21462;&#20999;&#21521;&#31639;&#23376;&#25152;&#38656;&#30340;&#23548;&#25968;&#27425;&#24207;&#30340;&#31574;&#30053;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#20219;&#20309;&#26377;&#38480;&#20803;&#31243;&#24207;&#65288;&#25110;&#20854;&#20182;&#25968;&#20540;&#26041;&#27861;&#65289;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23450;&#20041;&#37197;&#28857;&#21644;&#25972;&#21512;&#21516;&#26102;&#28608;&#27963;&#25110;&#38750;&#28608;&#27963;&#30340;&#22810;&#20010;&#38750;&#30456;&#31561;&#32422;&#26463;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
&lt;/p&gt;</description></item><item><title>META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04497</link><description>&lt;p&gt;
&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#30340;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#31038;&#21306;&#26816;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology. (arXiv:2304.04497v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04497
&lt;/p&gt;
&lt;p&gt;
META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#21457;&#29616;&#31038;&#21306;&#32467;&#26500;&#20316;&#20026;&#21508;&#31181;&#32593;&#32476;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#25110;&#35775;&#38382;&#38480;&#21046;&#65292;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#32593;&#32476;&#25299;&#25169;&#33719;&#21462;&#30340;&#24773;&#20917;&#19979;&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; META-CODE&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;META-CODE &#38500;&#20102;&#21021;&#22987;&#30340;&#32593;&#32476;&#25512;&#29702;&#27493;&#39588;&#22806;&#65292;&#36824;&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#65306;1) &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33410;&#28857;&#32423;&#31038;&#21306;&#24402;&#23646;&#23884;&#20837;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;2) &#22522;&#20110;&#31038;&#21306;&#24402;&#23646;&#30340;&#33410;&#28857;&#26597;&#35810;&#36827;&#34892;&#32593;&#32476;&#25506;&#32034;&#65292;3) &#20351;&#29992;&#25506;&#32034;&#32593;&#32476;&#20013;&#30340;&#22522;&#20110;&#36793;&#36830;&#25509;&#30340;&#36830;&#20307;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102; META-CODE &#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04027</link><description>&lt;p&gt;
NeBLa: &#20351;&#29992;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#37325;&#24314;&#21475;&#33108;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;X&#32447;&#29255;&#65288;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#65292;PX&#65289;&#26159;&#24120;&#29992;&#20110;&#29273;&#31185;&#26816;&#26597;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;3D&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#30456;&#27604;&#65292;PX&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;PX&#21482;&#25552;&#20379;&#21475;&#33108;&#32467;&#26500;&#30340;&#20108;&#32500;&#25153;&#24179;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#30340;PX&#22270;&#20687;&#20272;&#35745;3D&#21475;&#33108;&#32467;&#26500;&#12290;&#30001;&#20110;PX&#21644;CBCT&#25968;&#25454;&#30340;&#21305;&#37197;&#19981;&#22810;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#20102;&#20174;CBCT&#27169;&#25311;&#30340;PX&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#20102;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#32447;&#37319;&#26679;&#26041;&#27861;&#65292;&#21463;&#21040;&#20840;&#26223;&#25918;&#23556;&#32447;&#25104;&#20687;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#21860;&#37202;-&#20848;&#20271;&#29305;&#23450;&#24459;&#23548;&#20986;&#28210;&#26579;&#20989;&#25968;&#29983;&#25104;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#36716;&#25442;&#27169;&#22359;&#65292;&#29983;&#25104;&#27169;&#22359;&#21644;&#31934;&#28860;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23558;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;&#35757;&#32451;&#22270;&#20687;&#39118;&#26684;&#12290;&#29983;&#25104;&#27169;&#22359;&#21033;&#29992;&#23556;&#32447;&#37319;&#26679;&#26041;&#27861;&#24471;&#21040;&#30340;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#32422;&#26463;&#19979;&#30340;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#32467;&#26500;&#12290;&#31934;&#28860;&#27169;&#22359;&#25913;&#21892;&#20102;3D&#32467;&#26500;&#30340;&#24179;&#28369;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#29273;&#31185;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
&lt;/p&gt;</description></item><item><title>TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.01951</link><description>&lt;p&gt;
TransPimLib&#65306;&#29992;&#20110;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01951
&lt;/p&gt;
&lt;p&gt;
TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#65288;PIM&#65289;&#25215;&#35834;&#20943;&#36731;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30495;&#23454;PIM&#31995;&#32479;&#26377;&#19968;&#20010;&#20869;&#22312;&#30340;&#21155;&#21183;&#65292;&#21363;&#23427;&#20204;&#30340;&#30828;&#20214;&#27604;&#20256;&#32479;&#30340;&#22788;&#29702;&#22120;&#65288;CPU&#12289;GPU&#65289;&#26356;&#21152;&#21463;&#38480;&#65292;&#22240;&#20026;&#22312;&#20869;&#23384;&#38468;&#36817;&#25110;&#20869;&#37096;&#26500;&#24314;&#22788;&#29702;&#20803;&#20214;&#30340;&#38590;&#24230;&#21644;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#29992;&#30340;PIM&#26550;&#26500;&#25903;&#25345;&#30456;&#24403;&#26377;&#38480;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#36229;&#36234;&#20989;&#25968;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#30340;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#26041;&#26681;&#65289;&#12290;&#36825;&#20123;&#25805;&#20316;&#23545;&#20110;&#19968;&#20123;&#29616;&#20195;&#24037;&#20316;&#36127;&#36733;&#23588;&#20854;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#29992;&#30340;PIM&#31995;&#32479;&#20013;&#25552;&#20379;&#23545;&#36229;&#36234;&#65288;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#65289;&#20989;&#25968;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TransPimLib&#65292;&#36825;&#26159;&#19968;&#20010;&#24211;&#65292;&#25552;&#20379;&#22522;&#20110;CORDIC&#21644;LUT&#30340;&#19977;&#35282;&#20989;&#25968;&#12289;&#21452;&#26354;&#20989;&#25968;&#12289;&#25351;&#25968;&#12289;&#23545;&#25968;&#12289;&#24179;&#26041;&#26681;&#31561;&#38590;&#20197;&#35745;&#31639;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34987;&#25552;&#20986;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#21644;&#20808;&#36827;&#30340;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#32467;&#26524;&#65292;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.15233</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-Image Diffusion Models are Zero-Shot Classifiers. (arXiv:2303.15233v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15233
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34987;&#25552;&#20986;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#21644;&#20808;&#36827;&#30340;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#32467;&#26524;&#65292;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#20248;&#31168;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#23398;&#20064;&#20102;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25152;&#25429;&#25417;&#30340;&#30693;&#35782;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23578;&#26410;&#36827;&#34892;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26681;&#25454;&#26631;&#31614;&#30340;&#25991;&#26412;&#25551;&#36848;&#21435;&#38500;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#21147;&#20316;&#20026;&#35813;&#26631;&#31614;&#27010;&#29575;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#31283;&#23450;&#25193;&#25955;&#21644;Imagen&#65292;&#24182;&#19982;CLIP&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#36827;&#34892;&#23545;&#27604;&#65292;&#25506;&#32034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#12290;&#22312;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20182;&#20204;&#19982;CLIP&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#22312;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#65292;&#32780;CLIP&#19981;&#33021;&#12290;&#23613;&#31649;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24456;&#24120;&#35265;&#65292;v
&lt;/p&gt;
&lt;p&gt;
The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, v
&lt;/p&gt;</description></item><item><title>EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08028</link><description>&lt;p&gt;
EdgeServe:&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#25191;&#34892;&#23618;
&lt;/p&gt;
&lt;p&gt;
EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08028
&lt;/p&gt;
&lt;p&gt;
EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#21487;&#33021;&#26469;&#33258;&#20110;&#32593;&#32476;&#20013;&#19981;&#21516;&#33410;&#28857;&#25910;&#38598;&#30340;&#25968;&#25454;&#28304;&#12290;&#36825;&#31181;&#38382;&#39064;&#34987;&#31216;&#20043;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#65292;&#24182;&#22312;&#25968;&#25454;&#36335;&#30001;&#12289;&#35745;&#31639;&#24067;&#23616;&#21644;&#26102;&#38388;&#21516;&#27493;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#26377;&#36259;&#30340;&#31995;&#32479;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EdgeServe&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#25552;&#20379;&#26381;&#21153;&#12290; EdgeServe &#20381;&#36182;&#20110;&#19968;&#20010;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#32593;&#32476;&#36335;&#30001;&#25968;&#25454;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;EdgeServe &#20381;&#36182;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;EdgeServe&#65306;&#65288;1&#65289;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#65288;2&#65289;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#65288;3&#65289;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.07647</link><description>&lt;p&gt;
&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#22312;&#34920;&#24449;&#21644;&#29702;&#35299;&#22825;&#28982;&#21644;&#26032;&#26448;&#26009;&#30340;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#21253;&#25324;&#23454;&#39564;&#35774;&#35745;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21453;&#38382;&#39064;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#22240;&#27492;&#21450;&#26102;&#36827;&#34892;&#20840;&#38754;&#21644;&#26356;&#26032;&#30340;&#32508;&#36848;&#65292;&#23545;&#20110;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19982;&#35813;&#32508;&#36848;&#30456;&#20851;&#30340;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26415;&#35821;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#28085;&#30422;&#20102;&#23454;&#39564;&#21147;&#23398;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#21253;&#25324;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#26410;&#26469;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03428</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#65292;&#20854;&#29942;&#39048;&#21253;&#25324;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12289;&#21151;&#32791;&#21644;&#26102;&#38388;&#65292;&#26082;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#20063;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#21487;&#33021;&#20250;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#32553;&#25918;&#20026; $\mathcal{O}(T^2 \times \text{polylog}(n))$&#65292;&#20854;&#20013; $n$ &#26159;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;$T$ &#26159;&#35757;&#32451;&#20013;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#12290;&#22522;&#20110;&#26089;&#26399;&#29992;&#20110;&#32791;&#25955;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#31639;&#27861;&#21487;&#29992;&#20110;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23545;&#25317;&#26377;&#20174;700&#19975;&#21040;1.03&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#26174;&#28982;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#37327;&#23376;&#24577;&#65292;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#37327;&#23376;&#36164;&#28304;&#20351;&#29992;&#19979;&#23454;&#29616;&#26356;&#24555;&#30340;&#20934;&#22791;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.02131</link><description>&lt;p&gt;
&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#37327;&#23376;&#24577;&#20934;&#22791;&#26041;&#27861;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Spacetime-Efficient Low-Depth Quantum State Preparation with Applications. (arXiv:2303.02131v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#37327;&#23376;&#24577;&#65292;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#37327;&#23376;&#36164;&#28304;&#20351;&#29992;&#19979;&#23454;&#29616;&#26356;&#24555;&#30340;&#20934;&#22791;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#30340;&#37327;&#23376;&#24577;&#12290;&#24403;&#23558;&#25105;&#20204;&#30340;&#21327;&#35758;&#32534;&#35793;&#20026;CNOT&#38376;&#21644;&#20219;&#24847;&#30340;&#21333;&#27604;&#29305;&#38376;&#26102;&#65292;&#23427;&#21487;&#20197;&#22312;&#28145;&#24230;$O(\log(N))$&#21644;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;$O(N)$&#30340;&#24773;&#20917;&#19979;&#20934;&#22791;&#19968;&#20010;$N$&#32500;&#30340;&#37327;&#23376;&#24577;&#65292;&#36825;&#20004;&#20010;&#21442;&#25968;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#24403;&#32534;&#35793;&#20026;$\{\mathrm{H,S,T,CNOT}\}$&#38376;&#38598;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#25152;&#38656;&#30340;&#37327;&#23376;&#36164;&#28304;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#28145;&#24230;$O(\log(N/\epsilon))$&#21644;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;$O(N\log(\log(N)/\epsilon))$&#19979;&#65292;&#21487;&#20197;&#20934;&#22791;&#19968;&#20010;&#35823;&#24046;&#20026;$\epsilon$&#30340;&#20219;&#24847;&#30340;&#37327;&#23376;&#24577;&#65292;&#36825;&#27604;&#20043;&#21069;&#26041;&#27861;&#30340;$O(\log(N)\log(N/\epsilon))$&#21644;$O(N\log(N/\epsilon))$&#26377;&#25152;&#25913;&#36827;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#25105;&#20204;&#21327;&#35758;&#30340;&#20943;&#23567;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;&#21487;&#20197;&#26377;&#25928;&#22320;&#24555;&#36895;&#20934;&#22791;&#22810;&#20010;&#19981;&#30456;&#20132;&#29366;&#24577;&#65292;&#21482;&#38656;&#35201;&#24120;&#25968;&#22240;&#23376;&#30340;&#36741;&#21161;&#37327;&#23376;&#27604;&#29305;&#24320;&#38144;--&#36890;&#36807;&#39640;&#25928;&#22320;&#37325;&#29992;$O(N)$&#20010;&#36741;&#21161;&#27604;&#29305;&#26469;&#20934;&#22791;&#19968;&#20010;&#20056;&#31215;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel deterministic method for preparing arbitrary quantum states. When our protocol is compiled into CNOT and arbitrary single-qubit gates, it prepares an $N$-dimensional state in depth $O(\log(N))$ and spacetime allocation (a metric that accounts for the fact that oftentimes some ancilla qubits need not be active for the entire circuit) $O(N)$, which are both optimal. When compiled into the $\{\mathrm{H,S,T,CNOT}\}$ gate set, we show that it requires asymptotically fewer quantum resources than previous methods. Specifically, it prepares an arbitrary state up to error $\epsilon$ in depth $O(\log(N/\epsilon))$ and spacetime allocation $O(N\log(\log(N)/\epsilon))$, improving over $O(\log(N)\log(N/\epsilon))$ and $O(N\log(N/\epsilon))$, respectively. We illustrate how the reduced spacetime allocation of our protocol enables rapid preparation of many disjoint states with only constant-factor ancilla overhead -- $O(N)$ ancilla qubits are reused efficiently to prepare a product
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#28023;&#33609;&#20998;&#31867;&#37325;&#26032;&#23450;&#20041;&#20026;&#24369;&#30417;&#30563;&#30340;&#31895;&#31961;&#20998;&#21106;&#38382;&#39064;&#65292;&#20351;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#25512;&#29702;&#26102;&#33719;&#24471;&#34917;&#19969;&#32423;&#36755;&#20986;&#12290;&#36890;&#36807;SeaFeats&#21644;SeaCLIP&#27169;&#22411;&#30340;&#24341;&#20837;&#21644;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28023;&#33609;&#35782;&#21035;&#21644;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00973</link><description>&lt;p&gt;
&#22270;&#20687;&#26631;&#31614;&#26159;&#31895;&#31961;&#28023;&#33609;&#20998;&#21106;&#30340;&#20840;&#37096;&#25152;&#38656;
&lt;/p&gt;
&lt;p&gt;
Image Labels Are All You Need for Coarse Seagrass Segmentation. (arXiv:2303.00973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#28023;&#33609;&#20998;&#31867;&#37325;&#26032;&#23450;&#20041;&#20026;&#24369;&#30417;&#30563;&#30340;&#31895;&#31961;&#20998;&#21106;&#38382;&#39064;&#65292;&#20351;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#25512;&#29702;&#26102;&#33719;&#24471;&#34917;&#19969;&#32423;&#36755;&#20986;&#12290;&#36890;&#36807;SeaFeats&#21644;SeaCLIP&#27169;&#22411;&#30340;&#24341;&#20837;&#21644;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28023;&#33609;&#35782;&#21035;&#21644;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#33609;&#33609;&#21407;&#26159;&#37325;&#35201;&#30340;&#30899;&#27719;&#65292;&#20294;&#20272;&#35745;&#23384;&#20648;&#30340;&#30899;&#37327;&#38656;&#35201;&#30693;&#36947;&#23384;&#22312;&#30340;&#28023;&#33609;&#29289;&#31181;&#12290;&#37197;&#22791;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27700;&#19979;&#21644;&#27700;&#38754;&#36733;&#20855;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#20272;&#35745;&#28023;&#33609;&#33609;&#21407;&#30340;&#32452;&#25104;&#21644;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#28023;&#33609;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#38656;&#35201;&#20174;&#34917;&#19969;&#32423;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#28023;&#33609;&#20998;&#31867;&#37325;&#26032;&#23450;&#20041;&#20026;&#24369;&#30417;&#30563;&#30340;&#31895;&#31961;&#20998;&#21106;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#65288;&#27604;&#34917;&#19969;&#32423;&#26631;&#27880;&#23569;25&#20493;&#65289;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#33719;&#24471;&#34917;&#19969;&#32423;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SeaFeats&#65292;&#19968;&#31181;&#20351;&#29992;&#26080;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#21450;SeaCLIP&#65292;&#19968;&#20010;&#23637;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#30417;&#30563;&#20449;&#21495;&#25928;&#26524;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;SeaFeats&#38598;&#21512;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#25429;&#25417;&#22810;&#23610;&#24230;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#31934;&#30830;&#19988;&#23454;&#26102;&#30340;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.12598</link><description>&lt;p&gt;
&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Convolutional Network with Attention Fusion for Traffic Flow Prediction. (arXiv:2302.12598v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#25429;&#25417;&#22810;&#23610;&#24230;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#31934;&#30830;&#19988;&#23454;&#26102;&#30340;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#23545;&#20110;&#22478;&#24066;&#20132;&#36890;&#25511;&#21046;&#21644;&#32593;&#32476;&#22320;&#22270;&#26381;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#22312;&#22823;&#25968;&#25454;&#30340;&#25903;&#25345;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25429;&#25417;&#20132;&#36890;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#27169;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#21644;&#31616;&#21333;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32452;&#20214;&#65292;&#38590;&#20197;&#24314;&#27169;&#22810;&#23610;&#24230;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#27880;&#24847;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22686;&#24378;&#20102;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#28982;&#21518;&#23558;&#21160;&#24577;&#22270;&#23398;&#20064;&#22120;&#19982;GRU&#30456;&#32467;&#21512;&#65292;&#20849;&#21516;&#24314;&#27169;&#21516;&#27493;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#34701;&#20837;&#20102;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and real-time traffic state prediction is of great practical importance for urban traffic control and web mapping services. With the support of massive data, deep learning methods have shown their powerful capability in capturing the complex spatialtemporal patterns of traffic networks. However, existing approaches use pre-defined graphs and a simple set of spatial-temporal components, making it difficult to model multi-scale spatial-temporal dependencies. In this paper, we propose a novel dynamic graph convolution network with attention fusion to tackle this gap. The method first enhances the interaction of temporal feature dimensions, and then it combines a dynamic graph learner with GRU to jointly model synchronous spatial-temporal correlations. We also incorporate spatial-temporal attention modules to effectively capture longrange, multifaceted domain spatial-temporal patterns. We conduct extensive experiments in four real-world traffic datasets to demonstrate that our met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11962</link><description>&lt;p&gt;
&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#32479;&#19968;&#25910;&#25947;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods. (arXiv:2302.11962v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21487;&#33021;&#38750;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#36741;&#21161;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#24102;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#36741;&#21161;&#26694;&#26550;&#20026;&#31639;&#27861;&#35774;&#35745;&#32773;&#25552;&#20379;&#20102;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#24182;&#19988;&#20351;&#29992;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#23558;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#26032;&#30340;&#24816;&#24615;&#38543;&#26426;&#20108;&#38454;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#25913;&#36827;&#20102;&#22823;&#32500;&#38382;&#39064;&#30340;&#31639;&#26415;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic Cubic Newton methods for solving general possibly non-convex minimization problems. We propose a new framework, which we call the helper framework, that provides a unified view of the stochastic and variance-reduced second-order algorithms equipped with global complexity guarantees. It can also be applied to learning with auxiliary information. Our helper framework offers the algorithm designer high flexibility for constructing and analyzing the stochastic Cubic Newton methods, allowing arbitrary size batches, and the use of noisy and possibly biased estimates of the gradients and Hessians, incorporating both the variance reduction and the lazy Hessian updates. We recover the best-known complexities for the stochastic and variance-reduced Cubic Newton, under weak assumptions on the noise. A direct consequence of our theory is the new lazy stochastic second-order method, which significantly improves the arithmetic complexity for large dimension problems. We also esta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.03347</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#20154;&#26426;&#21322;&#30417;&#30563;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Informative Path Planning Framework for Active Learning in UAV-based Semantic Mapping. (arXiv:2302.03347v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#33322;&#31354;&#27979;&#32472;&#21644;&#30417;&#27979;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#20041;&#20998;&#21106;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#22823;&#35268;&#27169;&#22797;&#26434;&#29615;&#22659;&#12290;&#24120;&#29992;&#30340;&#29992;&#20110;&#20998;&#21106;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#22823;&#37327;&#20687;&#32032;&#32423;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#26631;&#27880;&#36153;&#29992;&#39640;&#26114;&#19988;&#36153;&#26102;&#12290;&#32780;&#33322;&#31354;&#29615;&#22659;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#22806;&#35266;&#36890;&#24120;&#38459;&#30861;&#20102;&#20351;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#26080;&#20154;&#26426;&#21487;&#20197;&#33258;&#20027;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#24433;&#20687;&#20197;&#20379;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#23558;&#22810;&#20010;&#37319;&#38598;&#20989;&#25968;&#32467;&#21512;&#24182;&#34701;&#21512;&#21040;&#27010;&#29575;&#22320;&#24418;&#22270;&#20013;&#12290;&#28982;&#21518;&#23558;&#22320;&#22270;&#20013;&#30340;&#21508;&#31181;&#20449;&#24687;&#34701;&#21512;&#21040;&#26080;&#20154;&#26426;&#30340;&#35268;&#21010;&#30446;&#26631;&#20013;&#12290;&#26080;&#20154;&#26426;&#20250;&#33258;&#36866;&#24212;&#22320;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#33322;&#31354;&#24433;&#20687;&#65292;&#20197;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#29992;&#20110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAVs) are frequently used for aerial mapping and general monitoring tasks. Recent progress in deep learning enabled automated semantic segmentation of imagery to facilitate the interpretation of large-scale complex environments. Commonly used supervised deep learning for segmentation relies on large amounts of pixel-wise labelled data, which is tedious and costly to annotate. The domain-specific visual appearance of aerial environments often prevents the usage of models pre-trained on publicly available datasets. To address this, we propose a novel general planning framework for UAVs to autonomously acquire informative training images for model re-training. We leverage multiple acquisition functions and fuse them into probabilistic terrain maps. Our framework combines the mapped acquisition function information into the UAV's planning objectives. In this way, the UAV adaptively acquires informative aerial images to be manually labelled for model re-training. E
&lt;/p&gt;</description></item><item><title>LUT-NN&#36890;&#36807;&#36136;&#24515;&#23398;&#20064;&#21644;&#34920;&#26684;&#26597;&#25214;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#26524;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LUT-NN&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36136;&#24515;&#23398;&#20064;&#26469;&#26368;&#23567;&#21270;&#36136;&#24515;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#34920;&#26684;&#26597;&#25214;&#30452;&#25509;&#35835;&#21462;&#36817;&#20284;&#36755;&#20986;&#32467;&#26524;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2302.03213</link><description>&lt;p&gt;
LUT-NN&#65306;&#36890;&#36807;&#36136;&#24515;&#23398;&#20064;&#21644;&#34920;&#26684;&#26597;&#25214;&#25552;&#39640;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
LUT-NN: Empower Efficient Neural Network Inference with Centroid Learning and Table Lookup. (arXiv:2302.03213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03213
&lt;/p&gt;
&lt;p&gt;
LUT-NN&#36890;&#36807;&#36136;&#24515;&#23398;&#20064;&#21644;&#34920;&#26684;&#26597;&#25214;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#26524;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LUT-NN&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36136;&#24515;&#23398;&#20064;&#26469;&#26368;&#23567;&#21270;&#36136;&#24515;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#34920;&#26684;&#26597;&#25214;&#30452;&#25509;&#35835;&#21462;&#36817;&#20284;&#36755;&#20986;&#32467;&#26524;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#28040;&#32791;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#24320;&#21457;&#24037;&#20316;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LUT-NN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#34920;&#26684;&#26597;&#25214;&#25552;&#39640;&#25512;&#29702;&#25928;&#26524;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#31995;&#32479;&#12290;LUT-NN&#23398;&#20064;&#20102;&#27599;&#20010;&#36816;&#31639;&#31526;&#30340;&#20856;&#22411;&#29305;&#24449;&#65292;&#34987;&#31216;&#20026;&#36136;&#24515;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#20102;&#36825;&#20123;&#36136;&#24515;&#30340;&#32467;&#26524;&#65292;&#20445;&#23384;&#22312;&#26597;&#25214;&#34920;&#20013;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#34920;&#26684;&#20013;&#35835;&#21462;&#19982;&#36755;&#20837;&#26368;&#25509;&#36817;&#30340;&#36136;&#24515;&#30340;&#32467;&#26524;&#20316;&#20026;&#36817;&#20284;&#36755;&#20986;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#12290;LUT-NN&#25972;&#21512;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#25216;&#26415;&#65306;&#65288;1&#65289;&#21487;&#24494;&#20998;&#30340;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#36136;&#24515;&#23398;&#20064;&#65292;&#36890;&#36807;&#36136;&#24515;&#36866;&#24212;&#19977;&#20010;&#32423;&#21035;&#30340;&#36817;&#20284;&#26469;&#26368;&#23567;&#21270;&#36136;&#24515;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65307;&#65288;2&#65289;&#34920;&#26684;&#26597;&#25214;&#25512;&#29702;&#25191;&#34892;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#24182;&#34892;&#24615;&#12289;&#20869;&#23384;&#35775;&#38382;&#30340;&#38477;&#20302;&#21644;&#19987;&#29992;&#30828;&#20214;&#21333;&#20803;&#65292;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;LUT-NN&#22312;&#22810;&#20010;&#23454;&#38469;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device Deep Neural Network (DNN) inference consumes significant computing resources and development efforts. To alleviate that, we propose LUT-NN, the first system to empower inference by table lookup, to reduce inference cost. LUT-NN learns the typical features for each operator, named centroid, and precompute the results for these centroids to save in lookup tables. During inference, the results of the closest centroids with the inputs can be read directly from the table, as the approximated outputs without computations. LUT-NN integrates two major novel techniques: (1) differentiable centroid learning through backpropagation, which adapts three levels of approximation to minimize the accuracy impact by centroids; (2) table lookup inference execution, which comprehensively considers different levels of parallelism, memory access reduction, and dedicated hardware units for optimal performance. LUT-NN is evaluated on multiple real tasks, covering image and speech recognition, and na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.02662</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;
&lt;/p&gt;
&lt;p&gt;
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25429;&#25417;&#19990;&#30028;&#29289;&#29702;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#32780;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;GLAM&#65289;&#65306;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20351;&#29992;LLM&#20316;&#20026;&#31574;&#30053;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#38543;&#30528;&#20195;&#29702;&#31243;&#24207;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#32780;&#36880;&#27493;&#26356;&#26032;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#35299;&#20915;&#30446;&#26631;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25991;&#26412;&#29615;&#22659;&#35774;&#35745;&#26469;&#30740;&#31350;&#26356;&#39640;&#32423;&#24418;&#24335;&#30340;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#20197;&#21450;&#19968;&#32452;&#31354;&#38388;&#21644;&#23548;&#33322;&#20219;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;LLMs&#33021;&#21542;&#25552;&#39640;&#21508;&#31181;RL&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65311;2&#65289;&#23427;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#24418;&#24335;&#30340;&#27867;&#21270;&#65311;3&#65289;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#21151;&#33021;&#26041;&#24335;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2301.12811</link><description>&lt;p&gt;
SAN: &#21033;&#29992;&#21028;&#21035;&#24335;&#24402;&#19968;&#21270;&#32447;&#24615;&#23618;&#35825;&#23548;GAN&#30340;&#21487;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer. (arXiv:2301.12811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#20248;&#21270;&#26159;&#21542;&#30495;&#27491;&#25552;&#20379;&#20102;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GAN&#30340;&#24418;&#24335;&#19982;&#20999;&#29255;&#26368;&#20248;&#36755;&#36816;&#30340;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#25512;&#23548;&#20102;&#21487;&#27979;&#24615;&#26465;&#20214;&#65292;&#21363;&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#31216;&#20026;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#23558;&#24191;&#27867;&#31867;&#21035;&#30340;&#29616;&#26377;GAN&#36716;&#21270;&#20026;SAN&#12290;&#22312;&#21512;&#25104;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#21644;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;SAN&#24212;&#29992;&#20110;StyleGAN-XL&#65292;&#22312;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;GAN&#20013;&#26368;&#20808;&#36827;&#30340;FID&#65288;Frechet Inception Distance&#65289;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#21516;&#26102;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.09164</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30340;&#32479;&#19968;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#21516;&#26102;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#31574;&#30053;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#33719;&#21462;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12289;&#21160;&#24577;&#35745;&#31639;&#65288;DC&#65289;&#21644;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#36825;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#24494;&#35843;&#65288;&#25110;&#33976;&#39311;&#27493;&#39588;&#65289;&#65292;&#20351;&#24471;&#35745;&#31639;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#33539;&#24335;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#65292;&#21516;&#26102;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#26399;&#38388;&#23494;&#38598;&#21644;&#38376;&#25511;&#32534;&#30721;&#22120;&#30340;&#20849;&#21516;&#28436;&#21270;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20026;&#24212;&#29992;&#29305;&#23450;&#30340;&#24037;&#19994;&#29615;&#22659;&#25552;&#20379;&#20102;&#36890;&#29992;&#19988;&#22810;&#21151;&#33021;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CIFAR-10/100&#31561;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#31361;&#35302;&#36755;&#20986;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#34920;&#31034;&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2212.05037</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#31361;&#35302;&#35299;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Topological Deep Learning Framework for Neural Spike Decoding. (arXiv:2212.05037v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#31361;&#35302;&#36755;&#20986;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#34920;&#31034;&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#31354;&#38388;&#23450;&#20301;&#31995;&#32479;&#20351;&#29992;&#19981;&#21516;&#30340;&#31070;&#32463;&#20803;&#38598;&#21512;&#26469;&#36741;&#21161;&#22522;&#20110;&#29615;&#22659;&#30340;&#23548;&#33322;&#12290;&#22823;&#33041;&#36890;&#36807;&#22836;&#26041;&#21521;&#32454;&#32990;&#21644;&#32593;&#26684;&#32454;&#32990;&#20004;&#31181;&#26041;&#24335;&#32534;&#30721;&#31354;&#38388;&#20449;&#24687;&#12290;&#22836;&#26041;&#21521;&#32454;&#32990;&#29992;&#20110;&#30830;&#23450;&#26041;&#21521;&#65292;&#32780;&#32593;&#26684;&#32454;&#32990;&#30001;&#21472;&#21152;&#30340;&#31070;&#32463;&#20803;&#23618;&#32452;&#25104;&#65292;&#25552;&#20379;&#22522;&#20110;&#29615;&#22659;&#30340;&#23548;&#33322;&#12290;&#36825;&#20123;&#31070;&#32463;&#20803;&#20197;&#38598;&#21512;&#30340;&#24418;&#24335;&#21457;&#25918;&#20449;&#21495;&#65292;&#22810;&#20010;&#31070;&#32463;&#20803;&#21516;&#26102;&#21457;&#25918;&#20449;&#21495;&#20197;&#28608;&#27963;&#21333;&#20010;&#22836;&#26041;&#21521;&#25110;&#32593;&#26684;&#12290;&#25105;&#20204;&#24076;&#26395;&#25429;&#25417;&#36825;&#31181;&#21457;&#25918;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#30721;&#22836;&#26041;&#21521;&#21644;&#32593;&#26684;&#32454;&#32990;&#25968;&#25454;&#12290;&#29702;&#35299;&#12289;&#34920;&#31034;&#21644;&#35299;&#30721;&#36825;&#20123;&#31070;&#32463;&#32467;&#26500;&#38656;&#35201;&#21253;&#21547;&#39640;&#38454;&#36830;&#25509;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20256;&#32479;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#25552;&#20379;&#30340;&#19968;&#32500;&#36830;&#25509;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#30721;&#31070;&#32463;&#31361;&#35302;&#36755;&#20986;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#30340;&#31616;&#21333;&#22797;&#21512;&#20307;&#21457;&#29616;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain's spatial orientation system uses different neuron ensembles to aid in environment-based navigation. Two of the ways brains encode spatial information is through head direction cells and grid cells. Brains use head direction cells to determine orientation whereas grid cells consist of layers of decked neurons that overlay to provide environment-based navigation. These neurons fire in ensembles where several neurons fire at once to activate a single head direction or grid. We want to capture this firing structure and use it to decode head direction grid cell data. Understanding, representing, and decoding these neural structures requires models that encompass higher order connectivity, more than the 1-dimensional connectivity that traditional graph-based models provide. To that end, in this work, we develop a topological deep learning framework for neural spike train decoding. Our framework combines unsupervised simplicial complex discovery with the power of deep learning via 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DPP&#30340;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#26356;&#26032;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02055</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#20351;&#29992;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#26469;&#36827;&#34892;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes. (arXiv:2212.02055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DPP&#30340;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#26356;&#26032;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#36890;&#36807;&#20174;&#33410;&#28857;&#21644;&#23427;&#20204;&#30340;&#25299;&#25169;&#20013;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#20998;&#35299;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#38500;&#36793;&#32536;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#33410;&#28857;&#37117;&#34987;&#32771;&#34385;&#22312;&#20869;&#65292;&#36825;&#20123;&#33410;&#28857;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26377;&#21033;&#20110;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) have achieved great success in graph representation learning by extracting high-level features from nodes and their topology. Since GCNs generally follow a message-passing mechanism, each node aggregates information from its first-order neighbour to update its representation. As a result, the representations of nodes with edges between them should be positively correlated and thus can be considered positive samples. However, there are more non-neighbour nodes in the whole graph, which provide diverse and useful information for the representation update. Two non-adjacent nodes usually have different representations, which can be seen as negative samples. Besides the node representations, the structural information of the graph is also crucial for learning. In this paper, we used quality-diversity decomposition in determinant point processes (DPP) to obtain diverse negative samples. When defining a distribution on diverse subsets of all non-neighbourin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#31080;&#32858;&#21512;&#21644;&#27010;&#29575;&#24314;&#27169;&#26469;&#32771;&#34385;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#25552;&#39640;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13398</link><description>&lt;p&gt;
CPPF++&#65306;&#22522;&#20110;&#25237;&#31080;&#32858;&#21512;&#30340;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation. (arXiv:2211.13398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#31080;&#32858;&#21512;&#21644;&#27010;&#29575;&#24314;&#27169;&#26469;&#32771;&#34385;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#25552;&#39640;&#23039;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26159;&#19977;&#32500;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#21482;&#21033;&#29992;&#19977;&#32500;CAD&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CPPF++&#26041;&#27861;&#65292;&#29992;&#20110;Sim2Real&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27010;&#29575;&#24615;&#35270;&#35282;&#37325;&#26032;&#26500;&#24605;&#20102;CPPF&#30340;&#22522;&#30784;&#28857;&#23545;&#25237;&#31080;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#25237;&#31080;&#30896;&#25758;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#35268;&#33539;&#31354;&#38388;&#20013;&#27599;&#20010;&#28857;&#23545;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#24314;&#27169;&#25237;&#31080;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#36807;&#28388;&#26469;&#28040;&#38500;&#19982;&#32972;&#26223;&#25110;&#26434;&#27874;&#26377;&#20851;&#30340;&#25237;&#31080;&#65292;&#24182;&#22686;&#24378;&#20102;&#27599;&#20010;&#25237;&#31080;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object pose estimation constitutes a critical area within the domain of 3D vision. While contemporary state-of-the-art methods that leverage real-world pose annotations have demonstrated commendable performance, the procurement of such real-world training data incurs substantial costs. This paper focuses on a specific setting wherein only 3D CAD models are utilized as a priori knowledge, devoid of any background or clutter information. We introduce a novel method, CPPF++, designed for sim-to-real pose estimation. This method builds upon the foundational point-pair voting scheme of CPPF, reconceptualizing it through a probabilistic lens. To address the challenge of voting collision, we model voting uncertainty by estimating the probabilistic distribution of each point pair within the canonical space. This approach is further augmented by iterative noise filtering, employed to eradicate votes associated with backgrounds or clutters. Additionally, we enhance the context provided by each v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.06919</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38754;&#21521;&#38544;&#31169;&#24863;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Causal Structure Learning in Federated Setting. (arXiv:2211.06919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#23558;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#20013;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#32622;&#20013;&#65292;&#19981;&#21487;&#33021;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#24182;&#25918;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#22312;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#21512;&#35774;&#32622;&#20013;&#30340;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#37319;&#29992;&#20004;&#31181;&#26032;&#31574;&#30053;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#32780;&#19981;&#38598;&#20013;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36880;&#23618;&#32858;&#21512;&#31574;&#30053;&#65292;&#23558;PC&#31639;&#27861;&#24179;&#31283;&#22320;&#36866;&#24212;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#20197;&#23454;&#29616;&#32852;&#21512;&#39592;&#26550;&#23398;&#20064;&#65292;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attracted much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel Federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04218</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#37197;&#23545;&#34701;&#21512;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#21363;&#20855;&#26377;&#38750;i.i.d.&#25968;&#25454;&#30340;FL&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#22312;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#35774;&#22791;&#34987;&#20998;&#25104;&#32858;&#31867;&#65292;&#24182;&#19988;&#27599;&#20010;&#32858;&#31867;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#26469;&#26368;&#20248;&#22320;&#21305;&#37197;&#20854;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#38750;&#20984;&#24809;&#32602;&#20197;&#37197;&#23545;&#21442;&#25968;&#24046;&#24322;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32858;&#31867;&#32467;&#26500;&#65292;&#26080;&#38656;&#39044;&#20808;&#30693;&#36947;&#32858;&#31867;&#30340;&#25968;&#37327;&#21644;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#35774;&#22791;&#38598;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#34701;&#21512;&#24809;&#32602;&#32852;&#37030;&#32858;&#31867;&#65288;FPFC&#65289;&#30340;&#26032;&#22411;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#22522;&#20110;&#26631;&#20934;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;ADMM&#65289;&#65292;FPFC&#22312;&#24182;&#34892;&#20013;&#23454;&#26045;&#65292;&#20165;&#22312;&#27599;&#36718;&#36890;&#20449;&#20013;&#26356;&#26032;&#35774;&#22791;&#30340;&#23376;&#38598;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#35774;&#22791;&#30340;&#21487;&#21464;&#24037;&#20316;&#36127;&#36733;&#12290;&#36825;&#20123;&#31574;&#30053;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#38544;&#31169;&#24615;&#65292;&#20351;&#20854;&#22312;FL&#20013;&#23454;&#38469;&#21487;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#28909;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.12860</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee. (arXiv:2210.12860v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#19981;&#31934;&#30830;&#27491;&#21017;&#21270;&#29275;&#39039;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#38797;&#28857;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#20110;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#33719;&#24471;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#19981;&#31934;&#30830;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#36845;&#20195;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#24182;&#19988;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010; $\epsilon$-&#38797;&#28857;&#65292;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#20026; $O(\epsilon^{-2/3})$&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#21463;&#38480;&#38388;&#38553;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#35813;&#39046;&#22495;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20108;&#38454;&#26041;&#27861;&#25910;&#25947;&#20998;&#26512;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#30028;&#24615;&#35201;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
We propose and analyze exact and inexact regularized Newton-type methods for finding a global saddle point of \emph{convex-concave} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information is much more involved. In this paper, we examine how second-order information can be used to speed up extra-gradient methods, even under inexactness. Specifically, we show that the proposed algorithms generate iterates that remain within a bounded set and the averaged iterates converge to an $\epsilon$-saddle point within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gap function. Our algorithms match the theoretically established lower bound in this context and our analysis provides a simple and intuitive convergence analysis for second-order methods without any boundedness requirements. Finally, we present a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#26435;&#37325;&#24314;&#27169;&#20026;&#20302;&#31209;&#21644;&#31232;&#30095;&#20998;&#37327;&#30340;&#24635;&#21644;&#65292;&#26082;&#25429;&#25417;&#20102;&#22810;&#20010;&#29992;&#25143;&#38388;&#30340;&#20849;&#21516;&#20449;&#24687;&#65292;&#21448;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2210.03505</link><description>&lt;p&gt;
&#39640;&#25928;&#20010;&#24615;&#21270;&#65306;&#23558;&#29992;&#25143;&#21442;&#25968;&#24314;&#27169;&#20026;&#20302;&#31209;&#21152;&#31232;&#30095;&#20998;&#37327;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components. (arXiv:2210.03505v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#26435;&#37325;&#24314;&#27169;&#20026;&#20302;&#31209;&#21644;&#31232;&#30095;&#20998;&#37327;&#30340;&#24635;&#21644;&#65292;&#26082;&#25429;&#25417;&#20102;&#22810;&#20010;&#29992;&#25143;&#38388;&#30340;&#20849;&#21516;&#20449;&#24687;&#65292;&#21448;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#20010;&#20307;&#29992;&#25143;/&#22495;/&#20225;&#19994;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#26631;&#20934;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#19968;&#20010;&#29992;&#25143;/&#22495;&#29305;&#23450;&#30340;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#20837;&#19968;&#20010;&#22266;&#23450;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#38480;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#27599;&#20010;&#29992;&#25143;/&#22495;&#33258;&#36523;&#20010;&#24615;&#21270;/&#24494;&#35843;&#27169;&#22411;&#26412;&#36523;&#65292;&#21363;&#20803;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#23384;&#20648;/&#22522;&#30784;&#26550;&#26500;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23545;&#21487;&#25193;&#23637;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#20005;&#26684;&#29702;&#35770;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#23558;&#32593;&#32476;&#26435;&#37325;&#24314;&#27169;&#20026;&#20302;&#31209;&#21644;&#31232;&#30095;&#20998;&#37327;&#30340;&#24635;&#21644;&#12290;&#36825;&#22312;&#20302;&#31209;&#37096;&#20998;&#25429;&#25417;&#20102;&#22810;&#20010;&#20010;&#20307;/&#29992;&#25143;&#30340;&#20849;&#21516;&#20449;&#24687;&#65292;&#32780;&#31232;&#30095;&#37096;&#20998;&#21017;&#25429;&#25417;&#20102;&#29992;&#25143;&#29305;&#23450;&#30340;&#29305;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#32447;&#24615;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#35813;&#26694;&#26550;&#65292;&#20854;&#20013;&#38382;&#39064;&#31616;&#21270;&#20026;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#20272;&#35745;&#31209;&#20026;$r$&#21644;$k$&#21015;&#30340;&#31232;&#30095;&#30697;&#38453;&#30340;&#24635;&#21644;
&lt;/p&gt;
&lt;p&gt;
Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific embedding that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain -a.k.a meta-learning -- has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a sma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#19982;&#26377;&#38480;&#26426;&#20250;&#32422;&#26463;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#30456;&#20851;&#32852;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#30340;&#26368;&#23567;&#21270;&#32773;&#28385;&#36275;&#26426;&#20250;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#22810;&#20010;&#20195;&#29702;&#26377;&#21161;&#20110;&#24615;&#33021;&#25552;&#21319;&#65292;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#35299;&#20915;DML&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.09060</link><description>&lt;p&gt;
&#24102;&#26377;&#26426;&#20250;&#32422;&#26463;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning with Chance Constraints. (arXiv:2209.09060v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#19982;&#26377;&#38480;&#26426;&#20250;&#32422;&#26463;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#30456;&#20851;&#32852;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#30340;&#26368;&#23567;&#21270;&#32773;&#28385;&#36275;&#26426;&#20250;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#22810;&#20010;&#20195;&#29702;&#26377;&#21161;&#20110;&#24615;&#33021;&#25552;&#21319;&#65292;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#35299;&#20915;DML&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26088;&#22312;&#26368;&#23567;&#21270;&#23884;&#20837;&#31354;&#38388;&#20013;&#25104;&#23545;&#20869;/&#38388;&#31867;&#36817;&#20284;&#36829;&#35268;&#30340;&#32463;&#39564;&#39044;&#26399;&#25439;&#22833;&#12290;&#25105;&#20204;&#23558;DML&#19982;&#26377;&#38480;&#26426;&#20250;&#32422;&#26463;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#30340;&#26368;&#23567;&#21270;&#32773;&#28385;&#36275;&#26576;&#20123;&#26426;&#20250;&#32422;&#26463;&#65292;&#24182;&#19988;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#27867;&#21270;&#24615;&#33021;&#21487;&#20197;&#29992;&#35206;&#30422;&#23545;&#24212;&#31867;&#26679;&#26412;&#25972;&#20010;&#22495;&#30340;&#26368;&#23567;&#29699;&#30340;&#21322;&#24452;&#26469;&#25551;&#36848;&#65292;&#36825;&#34920;&#26126;&#27599;&#20010;&#31867;&#21035;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#26377;&#21161;&#20110;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#20102;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#24182;&#21033;&#29992;&#26356;&#22810;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;DML&#23454;&#20363;&#26368;&#23567;&#21270;&#32773;&#25152;&#34164;&#21547;&#30340;&#26426;&#20250;&#32422;&#26463;&#65292;&#24182;&#23558;DML&#37325;&#26032;&#21046;&#23450;&#20026;&#22312;&#36825;&#20123;&#32422;&#26463;&#20132;&#38598;&#20013;&#25214;&#21040;&#21487;&#34892;&#28857;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26469;&#36817;&#20284;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#22797;&#35757;&#32451;&#27491;&#21017;&#21270;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#25439;&#22833;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#23884;&#20837;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding space. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#20855;&#26377;&#19981;&#21516;&#36845;&#20195;&#27425;&#25968;&#25110;&#28145;&#24230;&#30340;&#27169;&#22411;&#26102;&#20943;&#23569;&#36890;&#20449;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09478</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Neural ODE Models with Different Iteration Counts. (arXiv:2208.09478v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#20855;&#26377;&#19981;&#21516;&#36845;&#20195;&#27425;&#25968;&#25110;&#28145;&#24230;&#30340;&#27169;&#22411;&#26102;&#20943;&#23569;&#36890;&#20449;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20351;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#20197;&#20415;&#22312;&#23427;&#20204;&#20043;&#38388;&#20849;&#20139;&#35757;&#32451;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#23558;&#21407;&#22987;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#36890;&#20449;&#37327;&#30340;&#20943;&#23569;&#21644;&#23458;&#25143;&#31471;&#30340;&#24322;&#36136;&#24615;&#12290;&#21069;&#32773;&#21487;&#20197;&#20943;&#36731;&#36890;&#20449;&#24320;&#38144;&#65292;&#21518;&#32773;&#21487;&#20197;&#35753;&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#28789;&#27963;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#32858;&#21512;&#20855;&#26377;&#19981;&#21516;&#36845;&#20195;&#27425;&#25968;&#25110;&#28145;&#24230;&#30340;&#27169;&#22411;&#26102;&#20943;&#23569;&#36890;&#20449;&#37327;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#32858;&#21512;&#20855;&#26377;&#19981;&#21516;&#36845;&#20195;&#27425;&#25968;&#25110;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;&#23427;&#19982;&#21478;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#36890;&#20449;&#24320;&#38144;&#12289;&#27169;&#22411;&#36136;&#37327;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning approach in which clients train models locally with their own data and upload them to a server so that their trained results are shared between them without uploading raw data to the server. There are some challenges in federated learning, such as communication size reduction and client heterogeneity. The former can mitigate the communication overheads, and the latter can allow the clients to choose proper models depending on their available compute resources. To address these challenges, in this paper, we utilize Neural ODE based models for federated learning. The proposed flexible federated learning approach can reduce the communication size while aggregating models with different iteration counts or depths. Our contribution is that we experimentally demonstrate that the proposed federated learning can aggregate models with different iteration counts or depths. It is compared with a different federated learning approach in terms of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#25805;&#20316;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#30340;&#25915;&#20987;&#25361;&#25112;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2208.06651</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#23545;&#22270;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification. (arXiv:2208.06651v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#25805;&#20316;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#30340;&#25915;&#20987;&#25361;&#25112;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#21450;&#20854;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#22312;&#23398;&#20064;&#22270;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24403;&#21069;&#30340;GNN&#27169;&#22411;&#26174;&#31034;&#20986;&#23545;&#21487;&#33021;&#23384;&#22312;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#32467;&#26500;&#25915;&#20987;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#36843;&#20999;&#38656;&#35201;&#35774;&#35745;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#22270;&#20998;&#31867;&#25915;&#20987;&#26694;&#26550;&#65292;&#38754;&#20020;&#30528;&#20351;&#29992;&#20840;&#23616;&#22270;&#32423;&#20449;&#24687;&#29983;&#25104;&#23616;&#37096;&#33410;&#28857;&#32423;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#8220;&#20174;&#20840;&#23616;&#21040;&#23616;&#37096;&#8221;&#30340;&#25915;&#20987;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25805;&#20316;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26032;&#22411;&#36890;&#29992;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;&#21450;&#20854;&#21464;&#31181;&#26469;&#29983;&#25104;&#19982;&#22270;&#20998;&#31867;&#20219;&#21153;&#23545;&#24212;&#30340;&#33410;&#28857;&#32423;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this "global-to-local" attack challenge, we present a novel and general framework to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.05318</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generative Action Description Prompts for Skeleton-based Action Recognition. (arXiv:2208.05318v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#21333;&#28909;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21160;&#20316;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#8220;&#20570;&#32988;&#21033;&#25163;&#21183;&#8221;&#21644;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#26159;&#25163;&#21183;&#30340;&#20004;&#31181;&#21160;&#20316;&#65292;&#20854;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#25163;&#37096;&#30340;&#36816;&#21160;&#12290;&#36825;&#20123;&#20449;&#24687;&#19982;&#21160;&#20316;&#31867;&#21035;&#30340;&#21333;&#28909;&#32534;&#30721;&#26080;&#20851;&#65292;&#20294;&#21487;&#20197;&#20174;&#21160;&#20316;&#25551;&#36848;&#20013;&#25581;&#31034;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20013;&#21033;&#29992;&#21160;&#20316;&#25551;&#36848;&#21487;&#33021;&#26377;&#21161;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24341;&#25806;&#65292;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled from the action description. Therefore, utilizing action description in training could potentially benefit representation learning. In this work, we propose a Generative Action-description Prompts (GAP) approach for skeleton-based action recognition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automatically generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23618;&#20248;&#21270;&#23398;&#20064;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#21435;&#22122;&#20449;&#21495;&#21644;&#22270;&#20687;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#30340;&#21442;&#25968;&#23398;&#20064;&#26368;&#23567;&#21270;&#37325;&#24314;&#32467;&#26524;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#21495;&#37325;&#24314;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08939</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#23618;&#20248;&#21270;&#23398;&#20064;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Sparsity-Promoting Regularizers using Bilevel Optimization. (arXiv:2207.08939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23618;&#20248;&#21270;&#23398;&#20064;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#21435;&#22122;&#20449;&#21495;&#21644;&#22270;&#20687;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#30340;&#21442;&#25968;&#23398;&#20064;&#26368;&#23567;&#21270;&#37325;&#24314;&#32467;&#26524;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#21495;&#37325;&#24314;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#26469;&#21435;&#22122;&#20449;&#21495;&#21644;&#22270;&#20687;&#12290;&#31232;&#30095;&#20419;&#36827;&#27491;&#21017;&#21270;&#22120;&#26159;&#35299;&#20915;&#29616;&#20195;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#30340;&#25805;&#20316;&#31526;&#36890;&#24120;&#35201;&#20040;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#65292;&#35201;&#20040;&#26159;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#30417;&#30563;&#23398;&#20064;&#65288;&#20027;&#35201;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#26159;&#35774;&#35745;&#27491;&#21017;&#21270;&#22120;&#30340;&#19968;&#31181;&#23500;&#26377;&#25104;&#25928;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21464;&#20998;&#34920;&#36798;&#24335;&#21644;&#21442;&#25968;&#21270;&#30340;&#12289;&#31232;&#30095;&#20419;&#36827;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#21435;&#22122;&#20449;&#21495;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#22120;&#30340;&#21442;&#25968;&#36890;&#36807;&#26368;&#23567;&#21270;&#35757;&#32451;&#38598;&#19978;&#37325;&#24314;&#32467;&#26524;&#19982;&#30495;&#23454;&#22270;&#20687;&#21450;&#27979;&#37327;&#23545;&#30340;&#22343;&#26041;&#35823;&#24046;&#26469;&#23398;&#20064;&#12290;&#35757;&#32451;&#28041;&#21450;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65307;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35757;&#32451;&#25439;&#22833;&#30340;&#26799;&#24230;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
We present a method for supervised learning of sparsity-promoting regularizers for denoising signals and images. Sparsity-promoting regularization is a key ingredient in solving modern signal reconstruction problems; however, the operators underlying these regularizers are usually either designed by hand or learned from data in an unsupervised way. The recent success of supervised learning (mainly convolutional neural networks) in solving image reconstruction problems suggests that it could be a fruitful approach to designing regularizers. Towards this end, we propose to denoise signals using a variational formulation with a parametric, sparsity-promoting regularizer, where the parameters of the regularizer are learned to minimize the mean squared error of reconstructions on a training set of ground truth image and measurement pairs. Training involves solving a challenging bilievel optimization problem; we derive an expression for the gradient of the training loss using the closed-form
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#33021;&#37327;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#24102;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#65292;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2206.10991</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#19978;&#30340;&#33021;&#37327;&#29702;&#35299;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#33021;&#37327;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#24102;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#65292;&#20351;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#24120;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#65292;&#20854;&#20013;&#33410;&#28857;&#30340;&#29366;&#24577;&#26159;&#22522;&#20110;&#20854;&#37051;&#23621;&#25910;&#21040;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#22823;&#22810;&#25968;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#37117;&#26159;&#20316;&#20026;&#22270;&#21367;&#31215;&#36827;&#34892;&#25805;&#20316;&#30340;&#65292;&#20854;&#20013;&#29305;&#24449;&#22312;&#34987;&#20256;&#25773;&#21040;&#36793;&#32536;&#20043;&#21069;&#36890;&#36807;&#20849;&#20139;&#30340;&#32447;&#24615;&#21464;&#25442;&#28151;&#21512;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22270;&#21367;&#31215;&#24050;&#32463;&#34920;&#29616;&#20986;&#20004;&#20010;&#38480;&#21046;&#65306;&#22312;heterophilic&#22270;&#19978;&#34920;&#29616;&#27424;&#20339;&#65292;&#24182;&#19988;&#36807;&#24230;&#24179;&#28369;&#12290;&#24120;&#35265;&#30340;&#30475;&#27861;&#26159;&#65292;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#21457;&#29983;&#26159;&#22240;&#20026;&#36825;&#31181;&#27169;&#22411;&#34920;&#29616;&#20026;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#24847;&#21619;&#30528;&#22312;&#22270;&#23618;&#38388;&#29305;&#24449;&#30340;Dirichlet&#33021;&#37327;&#20250;&#20943;&#23569;&#65292;&#23548;&#33268;&#24179;&#28369;&#25928;&#24212;&#65292;&#26368;&#32456;&#29305;&#24449;&#19981;&#20877;&#21487;&#21306;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20005;&#35880;&#22320;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#23454;&#38469;&#19978;&#21487;&#20197;&#22686;&#24378;&#39640;&#39057;&#29575;&#29978;&#33267;&#24341;&#23548;&#19968;&#31181;&#25105;&#20204;&#25152;&#31216;&#30340;&#36807;&#24230;&#38160;&#21270;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#19982;&#36807;&#24230;&#24179;&#28369;&#30456;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#26126;&#23545;&#31216;&#28388;&#27874;&#22120;&#30340;&#32447;&#24615;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#22270;&#24418;&#19978;&#30340;&#33021;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#33021;&#37327;&#20989;&#25968;&#24809;&#32602;&#39640;&#33021;&#20449;&#21495;&#65292;&#26377;&#25928;&#22320;&#25233;&#21046;&#20302;&#39057;&#65292;&#21516;&#26102;&#20419;&#36827;&#30456;&#20851;&#30340;&#39640;&#39057;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#21487;&#20197;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.06817</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A novel physics-informed machine learning strategy to accelerate unsteady heat and mass transfer simulations. (arXiv:2206.06817v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#30340;&#24615;&#33021;&#36805;&#36895;&#25552;&#21319;&#65292;&#20294;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#22312;&#22823;&#22411;&#39046;&#22495;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#21152;&#36895;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#27425;&#35757;&#32451;&#26041;&#27861;&#20013;&#65292;&#38543;&#30528;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#22686;&#21152;&#65292;&#23436;&#20840;&#28040;&#38500;&#38169;&#35823;&#22686;&#38271;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#32422;&#26463;&#36801;&#31227;&#23398;&#20064;&#65288;RePIT&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;ML-CFD&#20132;&#21449;&#35745;&#31639;&#21152;&#36895;&#38750;&#31283;&#24577;&#20256;&#28909;&#21644;&#20256;&#36136;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22914;&#26524;&#23450;&#26399;&#36827;&#34892;&#36830;&#32493;&#30340;ML-CFD&#20132;&#21449;&#35745;&#31639;&#65292;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#27531;&#24046;&#30340;&#22686;&#21152;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#26368;&#26032;&#30340;CFD&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65288;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#38271;&#26399;CFD&#27169;&#25311;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancements in the performance of central processing units (CPUs), the simulation of unsteady heat and mass transfer is computationally very costly, particularly in large domains. While a big wave of machine learning (ML) has propagated in accelerating computational fluid dynamics (CFD) studies, recent research has revealed that it is unrealistic to completely suppress the error increase as the gap between the training and prediction times increases in single training approach. In this study, we propose a residual-based physics-informed transfer learning (RePIT) strategy to accelerate unsteady heat and mass transfer simulations using ML-CFD cross computation. Our hypothesis is that long-term CFD simulations become feasible if continuous ML-CFD cross computation is periodically carried out to not only reduce increased residuals but also update network parameters with the latest CFD time series data (transfer learning approach). The cross point of ML-CFD is determined 
&lt;/p&gt;</description></item><item><title>&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#26512;&#39640;&#32500;MRI&#29305;&#24449;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2203.09096</link><description>&lt;p&gt;
DeepAD:&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#20020;&#24202;&#24212;&#29992;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09096
&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#26512;&#39640;&#32500;MRI&#29305;&#24449;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31561;&#22797;&#26434;&#30142;&#30149;&#30340;&#33647;&#29289;&#30740;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#20219;&#21153;&#35201;&#20040;&#26159;&#21333;&#27169;&#22411;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#21253;&#21547;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#39640;&#32500;&#24433;&#20687;&#30340;&#25105;&#20204;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#35757;&#32451;&#20110;&#21333;&#20010;&#25968;&#25454;&#38598;&#65288;&#21363;&#38431;&#21015;&#65289;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#20854;&#20182;&#38431;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#32437;&#21521;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;MRI&#29305;&#24449;&#19982;&#20020;&#24202;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#31561;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#25972;&#21512;&#65292;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;&#25239;&#25439;&#22833;&#26469;&#32531;&#35299;&#38431;&#21015;&#29305;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38169;&#35823;&#32553;&#25918;&#23450;&#24459;&#65292;&#38024;&#23545;&#28385;&#36275;&#28304;&#26465;&#20214;&#21644;&#23481;&#37327;&#26465;&#20214;&#30340;&#25968;&#25454;&#38598;&#31867;&#21035;&#65292;&#22312;&#39640;&#26031;&#35774;&#35745;&#19979;&#23548;&#20986;&#20102;&#35823;&#24046;&#34928;&#20943;&#29575;&#19982;&#28304;&#21644;&#23481;&#37327;&#31995;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#27604;&#20102;&#26368;&#22823;&#21270;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#23725;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.12655</link><description>&lt;p&gt;
&#26680;&#20998;&#31867;&#38382;&#39064;&#19979;&#30340;&#38169;&#35823;&#32553;&#25918;&#23450;&#24459;&#65306;&#28304;&#26465;&#20214;&#21644;&#23481;&#37327;&#26465;&#20214;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Error Scaling Laws for Kernel Classification under Source and Capacity Conditions. (arXiv:2201.12655v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38169;&#35823;&#32553;&#25918;&#23450;&#24459;&#65292;&#38024;&#23545;&#28385;&#36275;&#28304;&#26465;&#20214;&#21644;&#23481;&#37327;&#26465;&#20214;&#30340;&#25968;&#25454;&#38598;&#31867;&#21035;&#65292;&#22312;&#39640;&#26031;&#35774;&#35745;&#19979;&#23548;&#20986;&#20102;&#35823;&#24046;&#34928;&#20943;&#29575;&#19982;&#28304;&#21644;&#23481;&#37327;&#31995;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#27604;&#20102;&#26368;&#22823;&#21270;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#23725;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26680;&#20998;&#31867;&#38382;&#39064;&#12290;&#23613;&#31649;&#26576;&#20123;&#20998;&#31867;&#22120;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#26679;&#26412;&#25968;&#37327;&#19982;&#39044;&#27979;&#38169;&#35823;&#30340;&#34928;&#20943;&#29575;&#30340;&#36793;&#30028;&#24050;&#30693;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#19981;&#33021;&#20934;&#30830;&#25551;&#36848;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#28385;&#36275;&#26631;&#20934;&#28304;&#26465;&#20214;&#21644;&#23481;&#37327;&#26465;&#20214;&#30340;&#37325;&#35201;&#25968;&#25454;&#38598;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20123;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#39640;&#26031;&#35774;&#35745;&#19979;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#38169;&#35823;&#20998;&#31867;&#65288;&#39044;&#27979;&#65289;&#35823;&#24046;&#30340;&#34928;&#20943;&#29575;&#20316;&#20026;&#28304;&#21644;&#23481;&#37327;&#31995;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#26631;&#20934;&#30340;&#26680;&#20998;&#31867;&#35774;&#32622;&#65288;&#21363;&#26368;&#22823;&#21270;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#23725;&#20998;&#31867;&#65289;&#36827;&#34892;&#20102;&#36825;&#26679;&#30340;&#25512;&#23548;&#65292;&#24182;&#23545;&#27604;&#20102;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#34928;&#20943;&#29575;&#32039;&#23494;&#22320;&#25551;&#36848;&#20102;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#24182;&#19988;&#20063;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#26680;&#20998;&#31867;&#30340;&#32553;&#25918;&#23450;&#24459;&#25351;&#25968;&#30340;&#26174;&#24335;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of kernel classification. While worst-case bounds on the decay rate of the prediction error with the number of samples are known for some classifiers, they often fail to accurately describe the learning curves of real data sets. In this work, we consider the important class of data sets satisfying the standard source and capacity conditions, comprising a number of real data sets as we show numerically. Under the Gaussian design, we derive the decay rates for the misclassification (prediction) error as a function of the source and capacity coefficients. We do so for two standard kernel classification settings, namely margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two methods. We find that our rates tightly describe the learning curves for this class of data sets, and are also observed on real data. Our results can also be seen as an explicit prediction of the exponents of a scaling law for kernel classification that is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24179;&#21488;&#20174;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#25968;&#25454;&#20197;&#20272;&#35745;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20248;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#24341;&#21457;&#29992;&#25143;&#30340;&#30495;&#23454;&#25253;&#21578;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26368;&#20248;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#37327;&#21270;&#24322;&#36136;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#19979;&#30028;&#24182;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#20272;&#35745;&#22120;&#21644;&#25903;&#20184;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2201.03968</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#33719;&#21462;: &#20013;&#22830;&#21644;&#26412;&#22320;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Optimal and Differentially Private Data Acquisition: Central and Local Mechanisms. (arXiv:2201.03968v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24179;&#21488;&#20174;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#25968;&#25454;&#20197;&#20272;&#35745;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20248;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#24341;&#21457;&#29992;&#25143;&#30340;&#30495;&#23454;&#25253;&#21578;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26368;&#20248;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#37327;&#21270;&#24322;&#36136;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#19979;&#30028;&#24182;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#20272;&#35745;&#22120;&#21644;&#25903;&#20184;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24179;&#21488;&#20174;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#25968;&#25454;&#20197;&#20272;&#35745;&#24863;&#20852;&#36259;&#30340;&#22522;&#26412;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36125;&#21494;&#26031;&#26368;&#20248;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20010;&#20307;&#21487;&#20197;&#20998;&#20139;&#22905;&#30340;&#65288;&#21487;&#39564;&#35777;&#30340;&#65289;&#25968;&#25454;&#20197;&#25442;&#21462;&#36135;&#24065;&#22870;&#21169;&#25110;&#26381;&#21153;&#65292;&#20294;&#21516;&#26102;&#20855;&#26377;&#65288;&#31169;&#23494;&#30340;&#65289;&#24322;&#36136;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#37327;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#24120;&#35265;&#30340;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65306;&#20013;&#22830;&#21644;&#26412;&#22320;&#12290;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26368;&#23567;&#20108;&#20056;&#19979;&#30028;&#65292;&#24182;&#26681;&#25454;&#32473;&#23450;&#30340;&#19981;&#21516;&#38544;&#31169;&#25439;&#22833;&#27700;&#24179;&#20026;&#29992;&#25143;&#25512;&#23548;&#20986;&#65288;&#36817;&#65289;&#26368;&#20248;&#30340;&#20272;&#35745;&#22120;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20316;&#20026;&#36873;&#25321;&#26368;&#20248;&#20272;&#35745;&#22120;&#21644;&#25903;&#20184;&#30340;&#38382;&#39064;&#65292;&#22312;&#29992;&#25143;&#25253;&#21578;&#38544;&#31169;&#25935;&#24863;&#24230;&#26102;&#24341;&#21457;&#30495;&#23454;&#25253;&#21578;&#12290;&#22312;&#38544;&#31169;&#25935;&#24863;&#24230;&#20998;&#24067;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
We consider a platform's problem of collecting data from privacy sensitive users to estimate an underlying parameter of interest. We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her (verifiable) data in exchange for a monetary reward or services, but at the same time has a (private) heterogeneous privacy cost which we quantify using differential privacy. We consider two popular differential privacy settings for providing privacy guarantees for the users: central and local. In both settings, we establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Building on this characterization, we pose the mechanism design problem as the optimal selection of an estimator and payments that will elicit truthful reporting of users' privacy sensitivities. Under a regularity condition on the distribution of privacy sensitivities we develop efficient alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26102;&#24207;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22240;&#26524;&#32467;&#26500;&#21644;&#24341;&#20837;&#22240;&#26524;&#26465;&#20214;&#36716;&#31227;&#30340;&#20551;&#35774;&#26469;&#22788;&#29702;&#36328;&#39046;&#22495;&#26102;&#24207;&#25968;&#25454;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2111.03422</link><description>&lt;p&gt;
&#21487;&#36801;&#31227;&#30340;&#22240;&#26524;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#26102;&#24207;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transferable Time-Series Forecasting under Causal Conditional Shift. (arXiv:2111.03422v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#26102;&#24207;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22240;&#26524;&#32467;&#26500;&#21644;&#24341;&#20837;&#22240;&#26524;&#26465;&#20214;&#36716;&#31227;&#30340;&#20551;&#35774;&#26469;&#22788;&#29702;&#36328;&#39046;&#22495;&#26102;&#24207;&#25968;&#25454;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#26102;&#24207;&#39044;&#27979;&#38382;&#39064;&#65292;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#36935;&#21040;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#20173;&#28982;&#19981;&#22815;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#26102;&#24207;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#20027;&#35201;&#36981;&#24490;&#20026;&#38745;&#24577;&#25968;&#25454;&#35774;&#35745;&#30340;&#33539;&#20363;&#65292;&#26080;&#27861;&#22788;&#29702;&#30001;&#25968;&#25454;&#20559;&#31227;&#12289;&#26102;&#38388;&#28382;&#21518;&#21644;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#24341;&#21457;&#30340;&#39046;&#22495;&#29305;&#23450;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#36890;&#24120;&#22312;&#39046;&#22495;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#36716;&#31227;&#30340;&#20551;&#35774;&#12290;&#22312;&#36825;&#19968;&#20551;&#35774;&#30340;&#21551;&#31034;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26102;&#24207;&#25968;&#25454;&#30340;&#22240;&#26524;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#26102;&#24207;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#21457;&#29616;&#36328;&#39046;&#22495;&#25968;&#25454;&#20043;&#38388;&#30340;Granger-&#22240;&#26524;&#32467;&#26500;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#36328;&#39046;&#22495;&#26102;&#24207;&#25968;&#25454;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the problem of semi-supervised domain adaptation for time-series forecasting, which is underexplored in literatures, despite being often encountered in practice. Existing methods on time-series domain adaptation mainly follow the paradigm designed for the static data, which cannot handle domain-specific complex conditional dependencies raised by data offset, time lags, and variant data distributions. In order to address these challenges, we analyze variational conditional dependencies in time-series data and find that the causal structures are usually stable among domains, and further raise the causal conditional shift assumption. Enlightened by this assumption, we consider the causal generation process for time-series data and propose an end-to-end model for the semi-supervised domain adaptation problem on time-series forecasting. Our method can not only discover the Granger-Causal structures among cross-domain data but also address the cross-domain time-series f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2006.02482</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#23398;&#20064;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#26041;&#27861;&#22312;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;&#22270;&#20687;&#20687;&#32032;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#8220;&#35299;&#37322;&#21333;&#20803;&#8221;&#26159;&#30456;&#20851;&#39044;&#27979;&#27169;&#22411;&#30340;&#24494;&#35266;&#32423;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65292;&#32780;&#19981;&#26159;&#26356;&#26377;&#29992;&#20110;&#29702;&#35299;&#22914;&#20309;&#21487;&#33021;&#25913;&#21464;&#31639;&#27861;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#30340;&#23439;&#35266;&#32423;&#29305;&#24449;&#65307;&#65288;ii&#65289;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#19982;&#30446;&#26631;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#19981;&#23384;&#22312;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#65292;&#36825;&#22312;&#35299;&#37322;&#21333;&#20803;&#26159;&#23439;&#35266;&#32423;&#21464;&#37327;&#26102;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#20998;&#26512;&#20154;&#21592;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39044;&#27979;&#31639;&#27861;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#32780;&#21482;&#33021;&#26681;&#25454;&#29305;&#23450;&#36755;&#20837;&#26597;&#35810;&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#29702;&#35299;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the "explanatory units" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allo
&lt;/p&gt;</description></item></channel></rss>