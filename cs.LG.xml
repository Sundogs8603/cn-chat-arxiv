<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;"Photo-zSNthesis"&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#26080;&#38656;&#20809;&#35889;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#35266;&#27979;&#20013;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#25552;&#21319;&#65292;&#33021;&#26497;&#22823;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;</title><link>http://arxiv.org/abs/2305.11869</link><description>&lt;p&gt;
Photo-zSNthesis: &#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;Ia&#22411;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#36716;&#21270;&#20026;&#32418;&#31227;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning. (arXiv:2305.11869v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;"Photo-zSNthesis"&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#26080;&#38656;&#20809;&#35889;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#35266;&#27979;&#20013;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#25552;&#21319;&#65292;&#33021;&#26497;&#22823;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#23558;&#21040;&#26469;&#30340;&#20809;&#24230;&#24033;&#22825;&#23558;&#20250;&#21457;&#29616;&#25968;&#20197;&#19975;&#35745;&#30340;Ia&#22411;&#36229;&#26032;&#26143;(SNe Ia)&#65292;&#36828;&#36828;&#36229;&#36807;&#20102;&#25105;&#20204;&#30340;&#20809;&#35889;&#36164;&#28304;&#23481;&#37327;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#20809;&#35889;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#35266;&#27979;&#25968;&#25454;&#30340;&#31185;&#23398;&#22238;&#25253;&#65292;&#25105;&#20204;&#24517;&#39035;&#21482;&#21033;&#29992;&#20809;&#24230;&#20449;&#24687;&#20934;&#30830;&#22320;&#25552;&#21462;&#20851;&#38190;&#21442;&#25968;&#65292;&#20363;&#22914;SN&#32418;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;Photo-zSNthesis&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;SDSS&#21644;LSST&#25968;&#25454;&#20197;&#21450;&#35266;&#27979;&#21040;&#30340;SDSS SNe&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#30340;&#39044;&#27979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26080;&#35770;&#26159;&#22312;&#27169;&#25311;&#36824;&#26159;&#23454;&#38469;&#35266;&#27979;&#20013;&#65292;&#19988;&#23384;&#22312;&#26497;&#23569;&#30340;&#32418;&#31227;&#30456;&#20851;&#20559;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#36873;&#25321;&#25928;&#24212;(&#20363;&#22914;Malmquist&#20559;&#24046;)&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#33021;&#22815;&#24471;&#21040;&#24456;&#22909;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#26368;&#22823;&#31243;&#24230;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upcoming photometric surveys will discover tens of thousands of Type Ia supernovae (SNe Ia), vastly outpacing the capacity of our spectroscopic resources. In order to maximize the science return of these observations in the absence of spectroscopic information, we must accurately extract key parameters, such as SN redshifts, with photometric information alone. We present Photo-zSNthesis, a convolutional neural network-based method for predicting full redshift probability distributions from multi-band supernova lightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and Vera C. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSS SNe. We show major improvements over predictions from existing methods on both simulations and real observations as well as minimal redshift-dependent bias, which is a challenge due to selection effects, e.g. Malmquist bias. The PDFs produced by this method are well-constrained and will maximize the cosmological constraining 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>CoDi&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#36755;&#20986;&#27169;&#24577;&#12290; CoDi &#38750;&#24120;&#28789;&#27963;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#27169;&#24577;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.11846</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32452;&#21512;&#25193;&#25955;&#23454;&#29616;&#20219;&#24847;&#36755;&#20837;&#19982;&#36755;&#20986;&#20043;&#38388;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11846
&lt;/p&gt;
&lt;p&gt;
CoDi&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#36755;&#20986;&#27169;&#24577;&#12290; CoDi &#38750;&#24120;&#28789;&#27963;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#27169;&#24577;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20043;&#20026;&#21487;&#32452;&#21512;&#25193;&#25955;&#65288;CoDi&#65289;&#12290;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#20174;&#20219;&#24847;&#36755;&#20837;&#27169;&#24577;&#21644;&#20219;&#24847;&#32452;&#21512;&#20013;&#29983;&#25104;&#21508;&#31181;&#36755;&#20986;&#27169;&#24577;&#65292;&#22914;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#38899;&#39057;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104; AI &#31995;&#32479;&#19981;&#21516;&#65292;CoDi &#21487;&#20197;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#24182;&#19988;&#20854;&#36755;&#20837;&#19981;&#23616;&#38480;&#20110;&#25991;&#26412;&#25110;&#22270;&#20687;&#30340;&#23376;&#38598;&#12290;&#23613;&#31649;&#24456;&#22810;&#27169;&#24577;&#30340;&#32452;&#21512;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351; CoDi &#21487;&#20197;&#33258;&#30001;&#22320;&#23545;&#20219;&#20309;&#36755;&#20837;&#32452;&#21512;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#29983;&#25104;&#20219;&#20309;&#27169;&#24577;&#32452;&#21512;&#65292;&#21363;&#20351;&#36825;&#20123;&#32452;&#21512;&#19981;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;CoDi&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26500;&#24314;&#20849;&#20139;&#30340;&#22810;&#27169;&#24577;&#31354;&#38388;&#26469;&#23454;&#29616;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20132;&#32455;&#27169;&#24577;&#30340;&#21516;&#27493;&#29983;&#25104;&#65292;&#20363;&#22914;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;&#39640;&#24230;&#21487;&#23450;&#21046;&#21644;&#28789;&#27963;&#65292;CoDi &#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#32852;&#21512;&#27169;&#24577;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20110;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#22806;&#26469;&#35270;&#35282;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#65292;&#23545;T2I&#30340;&#38480;&#21046;&#36827;&#34892;&#30740;&#31350;&#65292;&#28145;&#20837;&#20102;&#35299;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24314;&#35758;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11844</link><description>&lt;p&gt;
AI &#30340;&#34920;&#29616;&#24418;&#24335;&#65306;&#21335;&#20122;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#31038;&#21306;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia. (arXiv:2305.11844v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20110;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#22806;&#26469;&#35270;&#35282;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#65292;&#23545;T2I&#30340;&#38480;&#21046;&#36827;&#34892;&#30740;&#31350;&#65292;&#28145;&#20837;&#20102;&#35299;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24314;&#35758;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#21306;&#20013;&#24515;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#26469;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#25903;&#37197;&#24615;&#30340;&#23186;&#20307;&#34920;&#29616;&#24418;&#24335;&#30340;&#23398;&#26415;&#29702;&#35770;&#26469;&#38416;&#36848;&#36825;&#20123;&#22833;&#36133;&#65292;&#24182;&#23558;&#23427;&#20204;&#23450;&#20301;&#20110;&#21442;&#19982;&#32773;&#23545;&#20182;&#20204;&#29616;&#26377;&#31038;&#20250;&#36793;&#32536;&#21270;&#30340;&#25253;&#21578;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;AI&#20250;&#20877;&#29616;&#19968;&#31181;&#22806;&#37096;&#20154;&#30524;&#20013;&#30475;&#24453;&#21335;&#20122;&#25991;&#21270;&#30340;&#35270;&#35282;&#65292;&#36825;&#31181;&#35270;&#35282;&#26159;&#30001;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#24182;&#24449;&#27714;&#20182;&#20204;&#23545;T2I&#38480;&#21046;&#30340;&#30475;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;&#35780;&#20272;&#26694;&#26550;&#22686;&#28155;&#20102;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#30340;&#25945;&#35757;&#65292;&#25512;&#33616;&#20855;&#20307;&#30340;&#21069;&#36827;&#36335;&#24452;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants' reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#19982;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30456;&#20851;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#26102;&#31639;&#27861;&#21487;&#35299;&#24615;&#23384;&#22312;&#30097;&#38382;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11833</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21450;ETR: &#26377;&#25928;&#36830;&#32493;&#20989;&#25968;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions. (arXiv:2305.11833v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#19982;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30456;&#20851;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#26102;&#31639;&#27861;&#21487;&#35299;&#24615;&#23384;&#22312;&#30097;&#38382;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;&#32447;&#24615;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#32780;&#35328;&#65292;&#24050;&#30693;&#35757;&#32451;&#38382;&#39064;&#26159;&#23384;&#22312;R-&#23436;&#22791;&#30340;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#21644;&#20854;&#20182;&#26377;&#25928;&#36830;&#32493;&#20989;&#25968;&#30340;&#38382;&#39064;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#35757;&#32451;&#38382;&#39064;&#21487;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340; many-one &#21487;&#36824;&#21407;&#21040;&#20851;&#20110;&#30456;&#24212;&#28608;&#27963;&#20989;&#25968;&#25299;&#23637;&#30340;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#23548;&#33268;&#20102;&#24102;&#26377;&#25351;&#25968;&#20989;&#25968;&#30340;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31639;&#27861;&#21487;&#35299;&#24615;&#20197;&#21450;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30340;&#21487;&#21028;&#23450;&#24615;&#31561;&#38382;&#39064;&#37117;&#26159;&#24320;&#25918;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#32852;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#36830;&#36143;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#65292;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#21487;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.11832</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#27969;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#25913;&#36827;&#22810;&#27169;&#24577;&#32852;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis. (arXiv:2305.11832v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#32852;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#36830;&#36143;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#65292;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#21487;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#20174;&#32852;&#21512;&#20998;&#24067;&#29983;&#25104;&#24182;&#38024;&#23545;&#20219;&#24847;&#25968;&#37327;&#30340;&#22797;&#26434;&#27169;&#24577;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#21333;&#27169;&#21518;&#39564;&#20998;&#24067;&#26159;&#22522;&#20110;&#20445;&#30041;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#30340;&#28145;&#24230;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#23884;&#20837;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#36830;&#36143;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#26469;&#20016;&#23500;&#21333;&#27169;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19987;&#23478;&#20056;&#31215;&#26469;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25512;&#26029;&#19968;&#20010;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#21487;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20284;&#28982;&#24230;&#20272;&#35745;&#12289;&#20195;&#34920;&#24615;&#30340;&#29983;&#25104;&#21644;&#22312;&#26465;&#20214;&#29983;&#25104;&#20013;&#29305;&#21035;&#26159;&#36830;&#36143;&#24615;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new multimodal variational autoencoder that enables to generate from the joint distribution and conditionally to any number of complex modalities. The unimodal posteriors are conditioned on the Deep Canonical Correlation Analysis embeddings which preserve the shared information across modalities leading to more coherent cross-modal generations. Furthermore, we use Normalizing Flows to enrich the unimodal posteriors and achieve more diverse data generation. Finally, we propose to use a Product of Experts for inferring one modality from several others which makes the model scalable to any number of modalities. We demonstrate that our method improves likelihood estimates, diversity of the generations and in particular coherence metrics in the conditional generations on several datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11831</link><description>&lt;p&gt;
&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment. (arXiv:2305.11831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#31574;&#30053;&#35780;&#20272;&#12289;&#31574;&#30053;&#25913;&#36827;&#21644;&#28201;&#24230;&#35843;&#25972;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#21644;&#20462;&#25913;&#65292;&#20197;&#26356;&#21152;&#26126;&#30830;&#22320;&#38416;&#36848;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#27169;&#22411;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#24314;&#35758;&#22312;PATE&#30340;&#24212;&#29992;&#20013;&#21152;&#20837;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#20197;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11807</link><description>&lt;p&gt;
&#31169;&#26377;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fairness Impacts of Private Ensembles Models. (arXiv:2305.11807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#27169;&#22411;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#24314;&#35758;&#22312;PATE&#30340;&#24212;&#29992;&#20013;&#21152;&#20837;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#20197;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#21644;&#19968;&#20010;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#31169;&#26377;&#27169;&#22411;&#12290;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#22522;&#20110;&#25945;&#24072;&#30340;&#25237;&#31080;&#30340;&#36755;&#20986;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#12290;&#24050;&#32463;&#35777;&#26126;PATE&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#25110;&#20445;&#25252;&#25968;&#25454;&#26631;&#31614;&#26159;&#20248;&#20808;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#31169;&#26377;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;PATE&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#31639;&#27861;&#21644;&#25968;&#25454;&#23646;&#24615;&#23545;&#36825;&#20123;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#20123;&#26041;&#38754;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#19981;&#21516;&#30340;&#32676;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#36825;&#20123;&#24433;&#21709;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Private Aggregation of Teacher Ensembles (PATE) is a machine learning framework that enables the creation of private models through the combination of multiple "teacher" models and a "student" model. The student model learns to predict an output based on the voting of the teachers, and the resulting model satisfies differential privacy. PATE has been shown to be effective in creating private models in semi-supervised settings or when protecting data labels is a priority. This paper explores whether the use of PATE can result in unfairness, and demonstrates that it can lead to accuracy disparities among groups of individuals. The paper also analyzes the algorithmic and data properties that contribute to these disproportionate impacts, why these aspects are affecting different groups disproportionately, and offers recommendations for mitigating these effects
&lt;/p&gt;</description></item><item><title>PANNA 2.0&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#23376;&#38388;&#21183;&#29983;&#25104;&#20195;&#30721;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;GPU&#25903;&#25345;&#21644;&#33258;&#23450;&#20041;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#30005;&#33655;&#22343;&#34913;&#26041;&#26696;&#35299;&#20915;&#20102;&#38271;&#31243;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11805</link><description>&lt;p&gt;
PANNA 2.0: &#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#21450;&#20854;&#26032;&#26550;&#26500;&#65288;arXiv:2305.11805v1 [physics.comp-ph]&#65289;
&lt;/p&gt;
&lt;p&gt;
PANNA 2.0: Efficient neural network interatomic potentials and new architectures. (arXiv:2305.11805v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11805
&lt;/p&gt;
&lt;p&gt;
PANNA 2.0&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#23376;&#38388;&#21183;&#29983;&#25104;&#20195;&#30721;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;GPU&#25903;&#25345;&#21644;&#33258;&#23450;&#20041;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#30005;&#33655;&#22343;&#34913;&#26041;&#26696;&#35299;&#20915;&#20102;&#38271;&#31243;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;PANNA 2.0&#65288;&#22522;&#20110;&#23616;&#37096;&#21407;&#23376;&#25551;&#36848;&#31526;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#29983;&#25104;&#20195;&#30721;&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#12290;&#35813;&#29256;&#26412;&#22522;&#20110;&#26032;&#30340;&#21518;&#31471;&#26500;&#24314;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#23450;&#20041;&#21644;&#30417;&#27979;&#32593;&#32476;&#35757;&#32451;&#65292;&#25903;&#25345;&#26356;&#22909;&#30340;GPU&#65292;&#21253;&#25324;&#24555;&#36895;&#25551;&#36848;&#31526;&#35745;&#31639;&#22120;&#65292;&#38024;&#23545;&#22806;&#37096;&#20195;&#30721;&#30340;&#26032;&#25554;&#20214;&#20197;&#21450;&#36890;&#36807;&#21464;&#20998;&#30005;&#33655;&#22343;&#34913;&#26041;&#26696;&#21253;&#25324;&#38271;&#31243;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#26550;&#26500;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#26032;&#20195;&#30721;&#30340;&#20027;&#35201;&#29305;&#28857;&#65292;&#24182;&#23545; PANNA &#27169;&#22411;&#30340;&#31934;&#24230;&#36827;&#34892;&#20102;&#33509;&#24178;&#22522;&#20934;&#27979;&#35797;&#65292;&#27604;&#36739;&#20854;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the latest release of PANNA 2.0 (Properties from Artificial Neural Network Architectures), a code for the generation of neural network interatomic potentials based on local atomic descriptors and multilayer perceptrons. Built on a new back end, this new release of PANNA features improved tools for customizing and monitoring network training, better GPU support including a fast descriptor calculator, new plugins for external codes and a new architecture for the inclusion of long-range electrostatic interactions through a variational charge equilibration scheme. We present an overview of the main features of the new code, and several benchmarks comparing the accuracy of PANNA models to the state of the art, on commonly used benchmarks as well as richer datasets.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20379;&#27010;&#29575;&#27969;ODE&#23454;&#29616;&#24471;&#21040;&#22810;&#39033;&#24335;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#30340;&#35777;&#26126;&#65292;&#20351;&#29992;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#30340;&#29305;&#27530;&#36873;&#25321;&#30340;&#26657;&#27491;&#27493;&#39588;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#20984;&#26174;&#20102;ODE&#26694;&#26550;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.11798</link><description>&lt;p&gt;
&#27010;&#29575;&#27969;ODE&#21487;&#35777;&#26126;&#36895;&#24230;&#24555;
&lt;/p&gt;
&lt;p&gt;
The probability flow ODE is provably fast. (arXiv:2305.11798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11798
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20379;&#27010;&#29575;&#27969;ODE&#23454;&#29616;&#24471;&#21040;&#22810;&#39033;&#24335;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#30340;&#35777;&#26126;&#65292;&#20351;&#29992;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#30340;&#29305;&#27530;&#36873;&#25321;&#30340;&#26657;&#27491;&#27493;&#39588;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#20984;&#26174;&#20102;ODE&#26694;&#26550;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#27010;&#29575;&#27969;ODE&#23454;&#29616;&#65288;&#36830;&#21516;&#26657;&#27491;&#27493;&#39588;&#65289;&#24471;&#21040;&#22810;&#39033;&#24335;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#30340;&#35777;&#26126;&#65292;&#29992;&#20110;&#22522;&#20110;&#20998;&#25968;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;&#22312;&#26368;&#36817;&#30340;&#32467;&#26524;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#65292;&#35813;&#32467;&#26524;&#33719;&#24471;&#20102;&#22522;&#20110;SDE&#30340;&#23454;&#29616;&#65288;&#21363;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#24314;&#27169;&#25110;DDPM&#65289;&#30340;&#36825;&#26679;&#30340;&#20445;&#35777;&#65292;&#20294;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#25216;&#26415;&#26469;&#30740;&#31350;&#26080;&#25910;&#32553;&#30340;&#30830;&#23450;&#24615;&#21160;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#30340;&#29305;&#27530;&#36873;&#25321;&#30340;&#26657;&#27491;&#27493;&#39588;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27604;DDPM&#20043;&#21069;&#20316;&#21697;&#26356;&#22909;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65288;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#24179;&#28369;&#65292;&#20026;$ O&#65288;\sqrt {d}&#65289;$&#32780;&#19981;&#26159;$ O&#65288;d&#65289;$&#65289;&#65292;&#20984;&#26174;&#20102;ODE&#26694;&#26550;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first polynomial-time convergence guarantees for the probability flow ODE implementation (together with a corrector step) of score-based generative modeling. Our analysis is carried out in the wake of recent results obtaining such guarantees for the SDE-based implementation (i.e., denoising diffusion probabilistic modeling or DDPM), but requires the development of novel techniques for studying deterministic dynamics without contractivity. Through the use of a specially chosen corrector step based on the underdamped Langevin diffusion, we obtain better dimension dependence than prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data distribution), highlighting potential advantages of the ODE framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#24120;&#25968;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#25351;&#25968;&#25439;&#22833;&#19979;&#30340;&#21457;&#25955;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#36793;&#32536;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11788</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#36793;&#32536;&#22788;&#30340;&#36923;&#36753;&#22238;&#24402;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability. (arXiv:2305.11788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#24120;&#25968;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#25351;&#25968;&#25439;&#22833;&#19979;&#30340;&#21457;&#25955;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#31283;&#23450;&#24615;&#36793;&#32536;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20013;&#65292;&#26799;&#24230;&#19979;&#38477; (GD) &#32463;&#24120;&#22312;&#31283;&#23450;&#24615;&#36793;&#32536; (EoS) [Cohen &#31561;&#65292;2021] &#36816;&#34892;&#65292;&#20854;&#20013;&#27493;&#38271;&#34987;&#35774;&#32622;&#20026;&#22823;&#65292;&#23548;&#33268;&#30001; GD &#36845;&#20195;&#24341;&#36215;&#30340;&#38750;&#21333;&#35843;&#25439;&#22833;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312; EoS &#21306;&#22495;&#20869;&#20351;&#29992;&#24120;&#25968;&#27493;&#38271; GD &#36827;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#20559;&#24046;&#65292;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#23616;&#37096;&#25391;&#33633;&#65292;&#25105;&#20204;&#35777;&#26126;&#36923;&#36753;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#30340; GD &#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20219;&#20309;&#24120;&#25968;&#27493;&#38271;&#19979;&#65292;&#24403;&#25237;&#24433;&#21040;&#26368;&#22823;&#36793;&#38469;&#26041;&#21521; (&#30828;&#36793; SVM &#26041;&#21521;) &#26102;&#65292;GD &#36845;&#20195;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#24182;&#22312;&#25237;&#24433;&#21040;&#26368;&#22823;&#36793;&#32536;&#30340;&#27491;&#20132;&#34917;&#31354;&#38388;&#26102;&#65292;&#25910;&#25947;&#20110;&#26368;&#23567;&#21270;&#24378;&#20984;&#21183;&#33021;&#30340;&#22266;&#23450;&#21521;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#65292;&#22312; EoS &#21306;&#22495;&#65292;GD &#36845;&#20195;&#21487;&#33021;&#22312;&#25351;&#25968;&#25439;&#22833;&#19979;&#21457;&#29983;&#28798;&#38590;&#24615;&#21457;&#25955;&#65292;&#31361;&#26174;&#20102; EoS &#21306;&#22495;&#20013; GD &#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the edge of stability (EoS) [Cohen, et al., 2021], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#33021;&#25552;&#39640;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11778</link><description>&lt;p&gt;
&#12298;&#36328;&#35821;&#31181;&#30417;&#30563;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36136;&#37327;&#12299;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#33021;&#25552;&#39640;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#65288;&#22914;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#25110;&#36328;&#24230;&#25439;&#22351;&#65289;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20027;&#35201;&#20351;&#29992;&#38656;&#35201;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#30340;&#36328;&#35821;&#31181;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#21644;&#21463;&#30417;&#30563;&#30340;&#26426;&#22120;&#32763;&#35793;&#30446;&#26631;&#28151;&#21512;&#65292;&#22240;&#27492;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#21547;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#65292;&#21487;&#20135;&#29983;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#26159;&#38750;&#24120;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#32780;&#22312;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#30340;&#32593;&#26684;&#25628;&#32034;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#22312;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#36825;&#20123;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11774</link><description>&lt;p&gt;
&#20351;&#29992;R2&#25928;&#29992;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#25551;&#36848;&#22810;&#30446;&#26631;&#20043;&#38388;&#26368;&#20339;&#26435;&#34913;&#30340;&#28857;&#38598;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#30690;&#37327;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#19994;&#32773;&#24120;&#24120;&#20351;&#29992;&#26631;&#37327;&#21270;&#20989;&#25968;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#36825;&#32452;&#26631;&#37327;&#21270;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32422;&#23450;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31574;&#30053;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#21407;&#22987;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#36716;&#21270;&#20026;&#23450;&#20041;&#22312;&#38598;&#21512;&#19978;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#26032;&#38382;&#39064;&#30340;&#36866;&#24403;&#31867;&#21035;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;R2&#25928;&#29992;&#20989;&#25968;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#26631;&#37327;&#21270;&#20248;&#21270;&#38382;&#39064;&#30340;&#21152;&#26435;&#31215;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#21644;&#27425;&#27169;&#30340;&#38598;&#21512;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multi-objective optimization is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is the R2 utility function, which is defined as a weighted integral over the scalarized optimization problems. We show that this utility function is a monotone and submodular set function, which can be op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#36716;&#31227;&#31639;&#23376;&#21450;&#20854;&#35889;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11766</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36716;&#31227;&#31639;&#23376;&#65306;&#35889;&#32858;&#31867;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Transfer operators on graphs: Spectral clustering and beyond. (arXiv:2305.11766v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#36716;&#31227;&#31639;&#23376;&#21450;&#20854;&#35889;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21644;&#32593;&#32476;&#22312;&#24314;&#27169;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#30456;&#20851;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#20132;&#36890;&#32593;&#32476;&#65292;&#38598;&#25104;&#30005;&#36335;&#65292;&#30005;&#21147;&#32593;&#26684;&#65292;&#24341;&#25991;&#22270;&#20197;&#21450;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#22312;&#22270;&#19978;&#23450;&#20041;&#20102;&#36716;&#31227;&#31639;&#23376;&#65292;&#22914;Koopman&#31639;&#23376;&#21644;Perron-Frobenius&#31639;&#23376;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#35889;&#29305;&#24615;&#65292;&#24341;&#20837;&#20102;&#36825;&#20123;&#31639;&#23376;&#30340;Galerkin&#25237;&#24433;&#65292;&#24182;&#35828;&#26126;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#38477;&#20302;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#21521;&#22270;&#35889;&#32858;&#31867;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;Koopman&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#36716;&#31227;&#31639;&#23376;&#30340;&#26377;&#21521;&#22270;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#25152;&#24471;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19981;&#21516;&#32858;&#31867;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs and networks play an important role in modeling and analyzing complex interconnected systems such as transportation networks, integrated circuits, power grids, citation graphs, and biological and artificial neural networks. Graph clustering algorithms can be used to detect groups of strongly connected vertices and to derive coarse-grained models. We define transfer operators such as the Koopman operator and the Perron-Frobenius operator on graphs, study their spectral properties, introduce Galerkin projections of these operators, and illustrate how reduced representations can be estimated from data. In particular, we show that spectral clustering of undirected graphs can be interpreted in terms of eigenfunctions of the Koopman operator and propose novel clustering algorithms for directed graphs based on generalized transfer operators. We demonstrate the efficacy of the resulting algorithms on several benchmark problems and provide different interpretations of clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25104;&#21151;&#30340;&#36890;&#29992;&#21322;&#31354;&#38388;&#27979;&#35797;&#23398;&#20064;&#22120;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#32467;&#26500;&#21270;&#30340;&#20998;&#24067;&#19978;&#24037;&#20316;&#65292;&#23454;&#29616;&#35823;&#24046;$ O&#65288;\mathrm {opt}&#65289;+\ \epsilon $&#12290;</title><link>http://arxiv.org/abs/2305.11765</link><description>&lt;p&gt;
&#21322;&#31354;&#38388;&#30340;&#27979;&#35797;&#23398;&#20064;&#22120;&#65306;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tester-Learners for Halfspaces: Universal Algorithms. (arXiv:2305.11765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25104;&#21151;&#30340;&#36890;&#29992;&#21322;&#31354;&#38388;&#27979;&#35797;&#23398;&#20064;&#22120;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#32467;&#26500;&#21270;&#30340;&#20998;&#24067;&#19978;&#24037;&#20316;&#65292;&#23454;&#29616;&#35823;&#24046;$ O&#65288;\mathrm {opt}&#65289;+\ \epsilon $&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#24191;&#27867;&#32467;&#26500;&#21270;&#20998;&#24067;&#19978;&#25104;&#21151;&#30340;&#21322;&#31354;&#38388;&#27979;&#35797;&#23398;&#20064;&#22120;&#65292;&#35813;&#36890;&#29992;&#27979;&#35797;&#23398;&#20064;&#22120;&#22312;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#20855;&#26377;&#20197;&#19979;&#20445;&#35777;&#65306;&#23398;&#20064;&#22120;&#22312;&#27979;&#35797;&#22120;&#25509;&#21463;&#30340;&#20219;&#20309;&#26631;&#35760;&#20998;&#24067;&#19978;&#23454;&#29616;&#38169;&#35823;$ O&#65288;\mathrm {opt}&#65289;+\  \epsilon $&#65292;&#27492;&#22806;&#65292;&#27979;&#35797;&#22120;&#22312;&#36793;&#32536;&#20998;&#24067;&#26159;&#28385;&#36275;Poincar\'e&#19981;&#31561;&#24335;&#30340;&#20219;&#20309;&#20998;&#24067;&#26102;&#37117;&#21487;&#20197;&#25509;&#21463;&#12290;&#19982;&#20043;&#21069;&#22312;&#21487;&#27979;&#35797;&#23398;&#20064;&#26041;&#38754;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27979;&#35797;&#22120;&#27809;&#26377;&#38024;&#23545;&#20219;&#20309;&#21333;&#19968;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#26159;&#23545;&#19968;&#25972;&#20010;&#30446;&#26631;&#20998;&#24067;&#31867;&#25104;&#21151;&#12290;Poincar\'e&#20998;&#24067;&#31867;&#21253;&#25324;&#25152;&#26377;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#19988;&#65292;&#22914;&#26524;&#20551;&#35774;Kannan-L\'{o}vasz-Simonovits&#65288;KLS&#65289;&#29468;&#24819;&#65292;&#21017;&#21253;&#25324;&#25152;&#26377;&#23545;&#25968;&#20985;&#20998;&#24067;&#12290;&#22312;&#26631;&#31614;&#22122;&#22768;&#24050;&#30693;&#20026;Massart&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27979;&#35797;&#23398;&#20064;&#22120;&#22312;&#19981;&#21463;&#26465;&#20214;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#25152;&#26377;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#23454;&#29616;&#35823;&#24046;$ \mathrm {opt} +\  \epsilon $&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincar\'e inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincar\'e distributions includes all strongly log-concave distributions, and, assuming the Kannan--L\'{o}vasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\mathrm{opt} + \epsilon$ while accepting all log-concave distributions unconditionally (withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#36138;&#24515;&#36793;&#32536;&#26463;&#25628;&#32034;&#31639;&#27861;&#21644;&#23616;&#37096;&#28966;&#28857;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24212;&#29992;&#20110;&#23618;&#27425;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#32500;&#29305;&#27604;&#31639;&#27861;&#21644;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#36817;&#20284;&#26368;&#21487;&#33021;&#30340;&#22806;&#37096;&#29366;&#24577;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.11752</link><description>&lt;p&gt;
&#20998;&#31867;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36793;&#38469;&#26463;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Marginalized Beam Search Algorithms for Hierarchical HMMs. (arXiv:2305.11752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#36138;&#24515;&#36793;&#32536;&#26463;&#25628;&#32034;&#31639;&#27861;&#21644;&#23616;&#37096;&#28966;&#28857;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24212;&#29992;&#20110;&#23618;&#27425;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#32500;&#29305;&#27604;&#31639;&#27861;&#21644;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#36817;&#20284;&#26368;&#21487;&#33021;&#30340;&#22806;&#37096;&#29366;&#24577;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19968;&#31995;&#21015;&#27979;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#29366;&#24577;&#24207;&#21015;&#26159;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#32500;&#29305;&#27604;&#31639;&#27861;&#21644;&#26463;&#25628;&#32034;&#31639;&#27861;&#26159;&#27969;&#34892;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#26159;&#24403;&#24212;&#29992;&#20110;&#23618;&#27425;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HHMM&#65289;&#26102;&#65292;&#23427;&#20204;&#20855;&#26377;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#24863;&#20852;&#36259;&#30340;&#26159;&#22806;&#37096;&#29366;&#24577;&#24207;&#21015;&#12290;&#32500;&#29305;&#27604;&#31639;&#27861;&#26080;&#27861;&#22312;&#27809;&#26377;&#20869;&#37096;&#29366;&#24577;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#22806;&#37096;&#29366;&#24577;&#65292;&#32780;&#26463;&#25628;&#32034;&#31639;&#27861;&#38656;&#35201;&#23545;&#36807;&#22810;&#30340;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#36793;&#38469;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#36138;&#24515;&#36793;&#32536;&#26463;&#25628;&#32034;&#31639;&#27861;&#21644;&#23616;&#37096;&#28966;&#28857;&#26463;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#27604;&#32500;&#29305;&#27604;&#31639;&#27861;&#26356;&#20248;&#31168;&#22320;&#36817;&#20284;&#20102;&#26368;&#21487;&#33021;&#30340;&#22806;&#37096;&#29366;&#24577;&#24207;&#21015;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#32435;&#31859;&#23380;&#22522;&#35843;&#29992;&#25968;&#25454;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring a state sequence from a sequence of measurements is a fundamental problem in bioinformatics and natural language processing. The Viterbi and the Beam Search (BS) algorithms are popular inference methods, but they have limitations when applied to Hierarchical Hidden Markov Models (HHMMs), where the interest lies in the outer state sequence. The Viterbi algorithm can not infer outer states without inner states, while the BS algorithm requires marginalization over prohibitively large state spaces. We propose two new algorithms to overcome these limitations: the greedy marginalized BS algorithm and the local focus BS algorithm. We show that they approximate the most likely outer state sequence with higher performance than the Viterbi algorithm, and we evaluate the performance of these algorithms on an explicit duration HMM with simulation and nanopore base calling data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11742</link><description>&lt;p&gt;
MedLens: &#36890;&#36807;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#26469;&#25552;&#39640;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#24182;&#25552;&#21069;&#39044;&#27979;&#27515;&#20129;&#29575;&#23545;&#21450;&#26102;&#25552;&#20379;&#24739;&#32773;&#25252;&#29702;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#22823;&#37327;&#21307;&#23398;&#20307;&#24449;&#34987;&#29992;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#20020;&#24202;&#20307;&#24449;&#30340;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#36739;&#23569;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#21307;&#23398;&#20307;&#24449;&#21644;&#22823;&#37327;&#24739;&#32773;&#20303;&#38498;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#29575;&#21644;&#30456;&#20851;&#20998;&#25968;&#36827;&#34892;&#28145;&#20837;&#27979;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#32570;&#22833;&#29575;&#38750;&#24120;&#39640;&#65292;&#22823;&#37327;&#26080;&#29992;&#30340;&#20307;&#24449;&#21487;&#33021;&#20250;&#25439;&#23475;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#21482;&#26377;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#25165;&#33021;&#25552;&#39640;&#19981;&#21516;&#39044;&#27979;&#31639;&#27861;&#30340;&#22522;&#32447;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;MedLens&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#21160;&#36873;&#25321;&#37325;&#35201;&#21307;&#23398;&#20307;&#24449;&#65292;&#24182;&#20351;&#29992;&#28789;&#27963;&#30340;&#25554;&#20540;&#26041;&#27861;&#22788;&#29702;&#39640;&#32570;&#22833;&#29575;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#31283;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#26469;&#34913;&#37327;&#24615;&#33021;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#21160;&#24577;&#36951;&#25022;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.11726</link><description>&lt;p&gt;
&#21160;&#24577;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#20445;&#35777;&#30340;&#38750;&#24179;&#31283;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Projection-free Online Learning with Dynamic and Adaptive Regret Guarantees. (arXiv:2305.11726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#31283;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#26469;&#34913;&#37327;&#24615;&#33021;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#21160;&#24577;&#36951;&#25022;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#25928;&#29575;&#65292;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#26368;&#23567;&#21270;&#38745;&#24577;&#36951;&#25022;&#19978;&#65292;&#36825;&#24456;&#19981;&#24184;&#22320;&#26080;&#27861;&#25429;&#25417;&#21040;&#21464;&#21270;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#24179;&#31283;&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#36873;&#25321;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#26469;&#34913;&#37327;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#29616;&#26377;&#30340;&#21517;&#20026;$ \text {BOGD} _ \text {IP} $&#30340;&#26080;&#25237;&#24433;&#22312;&#32447;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#36951;&#25022;&#20998;&#26512;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$ \mathcal {O}&#65288;T ^ {3/4}&#65288;1 + P_T&#65289;&#65289;$&#30340;&#21160;&#24577;&#36951;&#25022;&#19978;&#38480;&#65292;&#20854;&#20013;$ P_T $&#34920;&#31034;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24182;&#34892;&#36816;&#34892;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27493;&#38271;&#30340;$ \text {BOGD} _ \text {IP} $&#31639;&#27861;&#65292;&#24182;&#23454;&#26102;&#36319;&#36394;&#26368;&#20339;&#31639;&#27861;&#65292;&#23558;&#19978;&#38480;&#25913;&#36827;&#20026;$ \mathcal {O}&#65288;T ^ {3/4}&#65288;1 + P_T&#65289;^ {1/4}&#65289;$&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25237;&#24433;-free&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#30340;&#31532;&#19968;&#20010;&#19968;&#33324;&#24773;&#20917;&#30340;&#21160;&#24577;&#36951;&#25022;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#33258;&#36866;&#24212;&#36951;&#25022;&#65292;&#23427;&#23481;&#24525;&#27604;&#36739;&#22120;&#24207;&#21015;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;$ \mathcal {O}&#65288;T ^ {2/3}&#65288;1 + P_T&#65289;^ {1/ 3}&#65289;$&#30340;&#33258;&#36866;&#24212;&#36951;&#25022;&#30028;&#38480;&#65292;&#20960;&#20046;&#19982;&#19979;&#38480;&#30456;&#21305;&#37197;&#65292;&#26368;&#22810;&#21482;&#30456;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection-free online learning has drawn increasing interest due to its efficiency in solving high-dimensional problems with complicated constraints. However, most existing projection-free online methods focus on minimizing the static regret, which unfortunately fails to capture the challenge of changing environments. In this paper, we investigate non-stationary projection-free online learning, and choose dynamic regret and adaptive regret to measure the performance. Specifically, we first provide a novel dynamic regret analysis for an existing projection-free method named $\text{BOGD}_\text{IP}$, and establish an $\mathcal{O}(T^{3/4}(1+P_T))$ dynamic regret bound, where $P_T$ denotes the path-length of the comparator sequence. Then, we improve the upper bound to $\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ by running multiple $\text{BOGD}_\text{IP}$ algorithms with different step sizes in parallel, and tracking the best one on the fly. Our results are the first general-case dynamic regret bou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.11701</link><description>&lt;p&gt;
S-JEA: &#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#22522;&#26412;&#33539;&#24335;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39640;&#24230;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26410;&#33021;&#23398;&#20064;&#21040;&#25429;&#33719;&#20998;&#23618;&#35821;&#20041;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#27010;&#24565;&#26159;&#21487;&#20998;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#65288;JEA&#65289;&#26469;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#20854;&#20013;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20351;&#29992;&#36739;&#20302;&#32423;&#21035;JEA&#30340;&#34920;&#31034;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#34920;&#29616;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65288;&#22914;&#36710;&#36742;&#30340;&#22411;&#21495;&#21644;&#39068;&#33394;&#65289;&#22312;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20013;&#12290;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22534;&#21472;JEA&#30340;&#34920;&#31034;&#19982;&#20256;&#32479;JEA&#30456;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#34920;&#31034;&#31354;&#38388;&#20197;&#39564;&#35777;&#35821;&#20041;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11699</link><description>&lt;p&gt;
RGCVAE&#65306;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#34920;&#29616;&#20986;&#26576;&#20123;&#39044;&#23450;&#29305;&#24615;&#30340;&#20998;&#23376;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#12290;&#28145;&#24230;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#26368;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;RGCVAE&#65306;&#65288;i&#65289;&#21033;&#29992;&#20840;&#26032;&#30340;&#24378;&#22823;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#32534;&#30721;&#32593;&#32476;&#65307;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35299;&#30721;&#32452;&#20214;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#25968;&#31181;&#26368;&#20808;&#36827;&#30340;VAE&#26041;&#27861;&#30456;&#27604;&#65292;RGCVAE&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11692</link><description>&lt;p&gt;
&#24102;&#26377;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30340;Transformer&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#30528;&#35745;&#31639;&#26426;&#36741;&#21161;&#27169;&#25311;&#22120;&#21644;&#25163;&#26415;&#36807;&#31243;&#30340;&#24405;&#21046;&#35270;&#39057;&#65292;&#20294;&#21021;&#32423;&#20303;&#38498;&#21307;&#24072;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#19987;&#23478;&#26469;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#22806;&#31185;&#21307;&#29983;&#36890;&#24120;&#25215;&#25285;&#30528;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#38480;&#21046;&#20102;&#20182;&#20204;&#22238;&#31572;&#38382;&#39064;&#30340;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25163;&#26415;&#38382;&#31572;&#31995;&#32479;&#65292;&#20197;&#20415;&#20174;&#24405;&#21046;&#30340;&#35270;&#39057;&#20013;&#20419;&#36827;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#22330;&#26223;&#21644;&#27963;&#21160;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#19982;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#32570;&#22833;&#23450;&#20301;&#31572;&#26696;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11685</link><description>&lt;p&gt;
&#22238;&#25910;&#21644;&#31934;&#39311;&#65306;&#24102;&#26377;&#27880;&#24847;&#21147;&#26144;&#23556;&#37325;&#29992;&#21644;&#33976;&#39311;&#23631;&#34109;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899; SSL &#27169;&#22411;&#20013;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#38656;&#35201;&#21387;&#32553;&#25104;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#23398;&#26415;&#30028;&#25110;&#23567;&#20844;&#21496;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#37325;&#29992;Transformer&#23618;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#22240;&#27492;&#21487;&#20197;&#21024;&#38500;&#38190;&#21644;&#26597;&#35810;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#23618;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#33976;&#39311;&#25439;&#22833;&#65292;&#21033;&#29992;&#36974;&#32617;&#21644;&#26410;&#36974;&#32617;&#30340;&#35821;&#38899;&#24103;&#65292;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#20135;&#29983;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;7.72%&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#65288;PER&#65289;&#21644;9.96%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#24378;&#21270;&#27880;&#24847;&#26426;&#21046;&#65288;SRA&#65289;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#23398;&#20064;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#36880;&#20803;&#32032;&#21521;&#37327;&#20056;&#27861;&#20197;&#21152;&#24378;&#25110;&#20943;&#24369;&#21407;&#22987;&#36755;&#20837;&#30340;&#26576;&#20123;&#32452;&#20214;&#26469;&#23398;&#20064;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11684</link><description>&lt;p&gt;
&#33258;&#25105;&#24378;&#21270;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#34920;&#26684;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Reinforcement Attention Mechanism For Tabular Learning. (arXiv:2305.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#24378;&#21270;&#27880;&#24847;&#26426;&#21046;&#65288;SRA&#65289;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#23398;&#20064;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#36880;&#20803;&#32032;&#21521;&#37327;&#20056;&#27861;&#20197;&#21152;&#24378;&#25110;&#20943;&#24369;&#21407;&#22987;&#36755;&#20837;&#30340;&#26576;&#20123;&#32452;&#20214;&#26469;&#23398;&#20064;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#31934;&#24230;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#22312;&#22788;&#29702;&#29616;&#23454;&#20013;&#30340;&#38382;&#39064;&#65288;&#22914;&#27450;&#35784;&#26816;&#27979;&#12289;&#20449;&#29992;&#35780;&#20998;&#65289;&#26102;&#65292;&#26356;&#20851;&#27880;&#20110;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#24403;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#24179;&#34913;&#29305;&#24449;&#26102;&#12290;&#35299;&#37322;&#33021;&#21147;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#65292;&#24517;&#39035;&#20276;&#38543;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#36890;&#24120;&#20250;&#20248;&#20808;&#36873;&#25321;&#26412;&#36136;&#19978;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#37027;&#20123;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#26159;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#22797;&#26434;&#27169;&#22411;&#12290;&#21363;&#20351;&#38656;&#35201;&#29306;&#29298;&#24615;&#33021;&#65292;&#19968;&#20123;&#39640;&#39118;&#38505;&#39046;&#22495;&#20063;&#20351;&#29992;&#32447;&#24615;&#27169;&#22411;&#26469;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#25105;&#24378;&#21270;&#27880;&#24847;&#26426;&#21046;&#65288;Self-Reinforcement Attention, SRA&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30456;&#20851;&#24615;&#36716;&#21270;&#20026;&#29305;&#24449;&#26435;&#37325;&#21521;&#37327;&#30340;&#26032;&#39062;&#27880;&#24847;&#26426;&#21046;&#65292;&#35813;&#26435;&#37325;&#21521;&#37327;&#29992;&#20110;&#23398;&#20064;&#21487;&#29702;&#35299;&#30340;&#34920;&#36798;&#12290;&#28982;&#21518;&#65292;&#35813;&#26435;&#37325;&#29992;&#20110;&#36890;&#36807;&#36880;&#20803;&#32032;&#21521;&#37327;&#20056;&#27861;&#26469;&#22686;&#24378;&#25110;&#20943;&#23569;&#21407;&#22987;&#36755;&#20837;&#30340;&#26576;&#20123;&#32452;&#20214;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Apart from the high accuracy of machine learning models, what interests many researchers in real-life problems (e.g., fraud detection, credit scoring) is to find hidden patterns in data; particularly when dealing with their challenging imbalanced characteristics. Interpretability is also a key requirement that needs to accompany the used machine learning model. In this concern, often, intrinsically interpretable models are preferred to complex ones, which are in most cases black-box models. Also, linear models are used in some high-risk fields to handle tabular data, even if performance must be sacrificed. In this paper, we introduce Self-Reinforcement Attention (SRA), a novel attention mechanism that provides a relevance of features as a weight vector which is used to learn an intelligible representation. This weight is then used to reinforce or reduce some components of the raw input through element-wise vector multiplication. Our results on synthetic and real-world imbalanced data s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#21560;&#27668;&#20107;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;VRB&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#20026;&#24320;&#21457;VRB&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11683</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#28789;&#24863;&#20107;&#20214;&#24863;&#30693;&#65306;&#28145;&#24230;&#23398;&#20064;&#19982;&#35821;&#35328;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Sensing of inspiration events from speech: comparison of deep learning and linguistic methods. (arXiv:2305.11683v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#21560;&#27668;&#20107;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;VRB&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#20026;&#24320;&#21457;VRB&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#33016;&#24102;&#20256;&#24863;&#22120;&#21487;&#20197;&#29992;&#20110;&#27979;&#37327;&#21628;&#21560;&#29575;&#21644;&#20854;&#20182;&#21628;&#21560;&#20581;&#24247;&#21442;&#25968;&#12290;&#34394;&#25311;&#21628;&#21560;&#33016;&#24102;&#65288;VRB&#65289;&#31639;&#27861;&#21487;&#20197;&#20174;&#35821;&#38899;&#38899;&#39057;&#20272;&#31639;&#20986;&#33016;&#24102;&#20256;&#24863;&#22120;&#27874;&#24418;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;VRB&#31639;&#27861;&#21644;&#22522;&#20110;&#26102;&#38388;&#23545;&#40784;&#35821;&#35328;&#20869;&#23481;&#30340;&#26816;&#27979;&#26469;&#33258;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21560;&#27668;&#20107;&#20214;&#65288;IE&#65289;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;VRB&#26041;&#27861;&#20248;&#20110;&#21333;&#35789;&#26242;&#20572;&#26816;&#27979;&#25110;&#35821;&#27861;&#20869;&#23481;&#20998;&#21106;&#12290;&#26041;&#27861;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#21363;&#21628;&#21560;&#20107;&#20214;&#19981;&#19982;&#35821;&#35328;&#20013;&#30340;&#35821;&#27861;&#27491;&#30830;&#30340;&#20301;&#32622;&#23545;&#40784;&#12290;&#35813;&#30740;&#31350;&#20026;VRB&#26041;&#27861;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#22686;&#21152;&#20102;&#23545;&#35821;&#38899;&#21628;&#21560;&#34892;&#20026;&#30340;&#26222;&#36941;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#28436;&#31034;&#20102;&#19968;&#31181;&#36830;&#32493;&#21628;&#21560;&#27874;&#24418;&#30340;&#37325;&#24314;&#26032;VRB&#26041;&#27861;VRBOLA&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory chest belt sensor can be used to measure the respiratory rate and other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms estimate the belt sensor waveform from speech audio. In this paper we compare the detection of inspiration events (IE) from respiratory belt sensor data using a novel neural VRB algorithm and the detections based on time-aligned linguistic content. The results show the superiority of the VRB method over word pause detection or grammatical content segmentation. The comparison of the methods show that both read and spontaneous speech content has a significant amount of ungrammatical breathing, that is, breathing events that are not aligned with grammatically appropriate places in language. This study gives new insights into the development of VRB methods and adds to the general understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA, for the reconstruction of the continuous breathing waveform is demonstrated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20146;&#20195;&#36873;&#25321;&#31639;&#27861;&#8212;&#8212;&#27010;&#29575;&#35789;&#20856;&#36873;&#25321;&#65288;plexicase selection&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#20256;&#32479;&#35789;&#20856;&#36873;&#25321;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#35821;&#24847;&#24863;&#30693;&#30340;&#36873;&#25321;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11681</link><description>&lt;p&gt;
&#27010;&#29575;&#35789;&#20856;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Lexicase Selection. (arXiv:2305.11681v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20146;&#20195;&#36873;&#25321;&#31639;&#27861;&#8212;&#8212;&#27010;&#29575;&#35789;&#20856;&#36873;&#25321;&#65288;plexicase selection&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#20256;&#32479;&#35789;&#20856;&#36873;&#25321;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#35821;&#24847;&#24863;&#30693;&#30340;&#36873;&#25321;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#36873;&#25321;&#26159;&#36951;&#20256;&#32534;&#31243;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20146;&#20195;&#36873;&#25321;&#31639;&#27861;&#65292;&#22312;&#31243;&#24207;&#21512;&#25104;&#12289;&#31526;&#21495;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#22810;&#20010;&#20219;&#21153;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#12290;&#30001;&#20110;&#20854;&#38750;&#21442;&#25968;&#21644;&#36882;&#24402;&#26412;&#36136;&#65292;&#35745;&#31639;&#27599;&#20010;&#20010;&#20307;&#34987;&#35789;&#20856;&#36873;&#25321;&#36873;&#25321;&#30340;&#27010;&#29575;&#34987;&#35777;&#26126;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#31639;&#27861;&#26356;&#28145;&#20837;&#30340;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#29575;&#35789;&#20856;&#36873;&#25321;&#65288;plexicase selection&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20146;&#20195;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#35789;&#20856;&#36873;&#25321;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20316;&#20026;&#19968;&#20010;&#35821;&#24847;&#24863;&#30693;&#30340;&#36873;&#25321;&#26041;&#27861;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#32780;&#19988;&#30001;&#20110;&#20855;&#26377;&#36873;&#25321;&#36807;&#31243;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#22686;&#24378;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#36951;&#20256;&#32534;&#31243;&#30340;&#20004;&#20010;&#26222;&#36941;&#39046;&#22495;&#65288;&#31243;&#24207;&#21512;&#25104;&#21644;&#31526;&#21495;&#22238;&#24402;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicase selection is a widely used parent selection algorithm in genetic programming, known for its success in various task domains such as program synthesis, symbolic regression, and machine learning. Due to its non-parametric and recursive nature, calculating the probability of each individual being selected by lexicase selection has been proven to be an NP-hard problem, which discourages deeper theoretical understanding and practical improvements to the algorithm. In this work, we introduce probabilistic lexicase selection (plexicase selection), a novel parent selection algorithm that efficiently approximates the probability distribution of lexicase selection. Our method not only demonstrates superior problem-solving capabilities as a semantic-aware selection method, but also benefits from having a probabilistic representation of the selection process for enhanced efficiency and flexibility. Experiments are conducted in two prevalent domains in genetic programming: program synthesi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#21508;&#31867;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#29992;&#24615;&#33021;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#24615;&#33021;&#27169;&#22411;&#29305;&#23450;&#20110;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20869;&#22312;&#19982;&#22806;&#22312;&#22240;&#32032;&#32771;&#34385;&#22312;&#20869;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#65292;&#25214;&#21040;&#26368;&#20339;&#25311;&#21512;&#24120;&#25968;&#20540;&#30340;&#36890;&#29992;&#34920;&#36798;&#24335;&#65292;&#20197;&#25552;&#39640;&#21644;&#37327;&#21270;&#27169;&#22411;&#26694;&#26550;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11665</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#30340;&#36890;&#29992;&#24615;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Generic Performance Model for Deep Learning in a Distributed Environment. (arXiv:2305.11665v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#21508;&#31867;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#29992;&#24615;&#33021;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#29616;&#26377;&#24615;&#33021;&#27169;&#22411;&#29305;&#23450;&#20110;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20869;&#22312;&#19982;&#22806;&#22312;&#22240;&#32032;&#32771;&#34385;&#22312;&#20869;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#65292;&#25214;&#21040;&#26368;&#20339;&#25311;&#21512;&#24120;&#25968;&#20540;&#30340;&#36890;&#29992;&#34920;&#36798;&#24335;&#65292;&#20197;&#25552;&#39640;&#21644;&#37327;&#21270;&#27169;&#22411;&#26694;&#26550;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#24314;&#27169;&#23545;&#20110;&#25913;&#36827;&#21644;&#37327;&#21270;&#27169;&#22411;&#26694;&#26550;&#30340;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#27169;&#22411;&#22823;&#22810;&#37117;&#26159;&#29305;&#23450;&#20110;&#26696;&#20363;&#30340;&#65292;&#23545;&#20110;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;/&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#29992;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#30340;&#36890;&#29992;&#34920;&#36798;&#24335;&#65292;&#32771;&#34385;&#20102;&#20869;&#22312;&#22240;&#32032;/&#25805;&#20316;&#65288;&#20363;&#22914;&#31639;&#27861;&#21442;&#25968;/&#20869;&#37096;&#25805;&#20316;&#65289;&#21644;&#22806;&#22312;&#25193;&#23637;&#22240;&#32032;&#65288;&#20363;&#22914;&#22788;&#29702;&#22120;&#25968;&#37327;&#12289;&#25968;&#25454;&#22359;&#21644;&#25209;&#27425;&#22823;&#23567;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#20026;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#27491;&#21017;&#21270;&#25104;&#26412;&#20989;&#25968;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#25311;&#21512;&#24120;&#25968;&#20540;&#30340;&#36890;&#29992;&#34920;&#36798;&#24335;&#65292;&#20197;&#21305;&#37197;&#23454;&#39564;&#30830;&#23450;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;TensorFlow&#12289;MXnet&#12289;PyTorch&#65289;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance modelling of a deep learning application is essential to improve and quantify the efficiency of the model framework. However, existing performance models are mostly case-specific, with limited capability for the new deep learning frameworks/applications. In this paper, we propose a generic performance model of an application in a distributed environment with a generic expression of the application execution time that considers the influence of both intrinsic factors/operations (e.g. algorithmic parameters/internal operations) and extrinsic scaling factors (e.g. the number of processors, data chunks and batch size). We formulate it as a global optimization problem and solve it using regularization on a cost function and differential evolution algorithm to find the best-fit values of the constants in the generic expression to match the experimentally determined computation time. We have evaluated the proposed model on three deep learning frameworks (i.e., TensorFlow, MXnet, a
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;V2X&#28040;&#24687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#23458;&#25143;&#31471;&#36873;&#25321;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;&#36710;&#36742;&#21160;&#24577;&#29366;&#24577;&#21644;&#32593;&#32476;&#36830;&#25509;&#36136;&#37327;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;</title><link>http://arxiv.org/abs/2305.11654</link><description>&lt;p&gt;
&#22522;&#20110;V2X&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#21327;&#21516;&#26234;&#33021;&#36816;&#36755;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#19982;&#19978;&#19979;&#25991;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
V2X-Boosted Federated Learning for Cooperative Intelligent Transportation Systems with Contextual Client Selection. (arXiv:2305.11654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11654
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;V2X&#28040;&#24687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#23458;&#25143;&#31471;&#36873;&#25321;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;&#36710;&#36742;&#21160;&#24577;&#29366;&#24577;&#21644;&#32593;&#32476;&#36830;&#25509;&#36136;&#37327;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#36816;&#36755;&#31995;&#32479;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#21644;&#26234;&#33021;&#20132;&#36890;&#26381;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#38544;&#31169;&#32422;&#26463;&#12290; &#28982;&#32780;&#65292;&#36830;&#25509;&#36710;&#36742;&#30340;&#21160;&#24577;&#29366;&#24577;&#20250;&#24433;&#21709;&#32593;&#32476;&#36830;&#25509;&#36136;&#37327;&#24182;&#24433;&#21709;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23458;&#25143;&#31471;&#36873;&#25321;&#27969;&#31243;&#65292;&#21033;&#29992;&#36710;&#36742;&#38388;&#36890;&#20449;&#65288;V2X&#65289;&#28040;&#24687;&#26681;&#25454;&#39044;&#27979;&#30340;&#36890;&#20449;&#24310;&#36831;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has revolutionized transportation systems, enabling autonomous driving and smart traffic services. Federated learning (FL) overcomes privacy constraints by training ML models in distributed systems, exchanging model parameters instead of raw data. However, the dynamic states of connected vehicles affect the network connection quality and influence the FL performance. To tackle this challenge, we propose a contextual client selection pipeline that uses Vehicle-to-Everything (V2X) messages to select clients based on the predicted communication latency. The pipeline includes: (i) fusing V2X messages, (ii) predicting future traffic topology, (iii) pre-clustering clients based on local data distribution similarity, and (iv) selecting clients with minimal latency for future model aggregation. Experiments show that our pipeline outperforms baselines on various datasets, particularly in non-iid settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.11650</link><description>&lt;p&gt;
&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#20026;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;EBMs &#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29992;&#20110;&#21487;&#25193;&#23637; EBM &#35757;&#32451;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#65288;DSM&#65289;&#26041;&#27861;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#21040;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65306;&#65288;&#20266;&#65289;Gibbs&#37319;&#26679;&#19982;&#21160;&#37327;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#32463;&#36807;DSM&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#22024;&#26434;&#8221;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22522;&#30784;&#8220;&#24178;&#20928;&#8221;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30456;&#20851;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11640</link><description>&lt;p&gt;
&#20219;&#24847;&#32570;&#22833;&#27169;&#24335;&#19979;&#30340;&#26080;&#20998;&#24067;&#30697;&#38453;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern. (arXiv:2305.11640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34892;/&#21015;&#21487;&#20132;&#25442;&#30697;&#38453;&#20013;&#39044;&#27979;&#32570;&#22833;&#26465;&#30446;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#30697;&#38453;&#35774;&#32622;&#25552;&#20986;&#20102;&#26032;&#39062;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#26159;&#22312;&#36825;&#20010;&#26377;&#36259;&#30340;&#20027;&#39064;&#19978;&#23384;&#22312;&#24456;&#23569;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#31934;&#32454;&#22320;&#23450;&#20041;&#20102;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#20005;&#26684;&#21010;&#20998;&#20102;&#21487;&#36798;&#25104;&#21644;&#19981;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#36793;&#30028;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#39044;&#27979;&#30340;&#24555;&#36895;&#20223;&#30495;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#21033;&#29992;&#31639;&#27861;&#31283;&#23450;&#24615;&#25216;&#26415;&#21152;&#36895;&#35745;&#31639;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20002;&#22833;&#27169;&#24335;&#19979;&#26377;&#25928;&#22320;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#32570;&#22833;&#23545;&#39044;&#27979;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#26412;&#30340;&#26497;&#38480;&#32467;&#26524;&#12290;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35777;&#25454;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the open problem of conformalized entry prediction in a row/column-exchangeable matrix. The matrix setting presents novel and unique challenges, but there exists little work on this interesting topic. We meticulously define the problem, differentiate it from closely related problems, and rigorously delineate the boundary between achievable and impossible goals. We then propose two practical algorithms. The first method provides a fast emulation of the full conformal prediction, while the second method leverages the technique of algorithmic stability for acceleration. Both methods are computationally efficient and can effectively safeguard coverage validity in presence of arbitrary missing pattern. Further, we quantify the impact of missingness on prediction accuracy and establish fundamental limit results. Empirical evidence from synthetic and real-world data sets corroborates the superior performance of our proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27969;&#22788;&#29702;&#31995;&#32479;&#19982;&#29289;&#32852;&#32593;&#20132;&#21449;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#20840;&#38754;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.11638</link><description>&lt;p&gt;
&#27969;&#22788;&#29702;&#31995;&#32479;&#20013;&#23454;&#29616;&#20840;&#38754;&#38544;&#31169;&#20445;&#25252;&#30340;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A Path to Holistic Privacy in Stream Processing Systems. (arXiv:2305.11638v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27969;&#22788;&#29702;&#31995;&#32479;&#19982;&#29289;&#32852;&#32593;&#20132;&#21449;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#20840;&#38754;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#25968;&#25454;&#30340;&#28023;&#37327;&#27969;&#38656;&#35201;&#21450;&#26102;&#20998;&#26512;&#20197;&#20445;&#30041;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#27969;&#22788;&#29702;&#31995;&#32479;&#65288;SPSs&#65289;&#21487;&#20197;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#22312;&#23454;&#26102;&#20013;&#20174;IoT&#25968;&#25454;&#20013;&#25512;&#23548;&#30693;&#35782;&#12290;&#36825;&#26679;&#30340;&#23454;&#26102;&#20998;&#26512;&#26377;&#21033;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#20294;&#20063;&#21487;&#33021;&#29992;&#20110;&#20405;&#29359;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#22240;&#20026;&#20174;&#29992;&#25143;&#25110;&#20854;&#38468;&#36817;&#25910;&#38598;&#30340;IoT&#25968;&#25454;&#26412;&#36523;&#23601;&#26159;&#25935;&#24863;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;SPSs&#21644;IoT&#30340;&#20132;&#21449;&#25152;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#23454;&#29616;SPSs&#20840;&#38754;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The massive streams of Internet of Things (IoT) data require a timely analysis to retain data usefulness. Stream processing systems (SPSs) enable this task, deriving knowledge from the IoT data in real-time. Such real-time analytics benefits many applications but can also be used to violate user privacy, as the IoT data collected from users or their vicinity is inherently sensitive. In this paper, we present our systematic look into privacy issues arising from the intersection of SPSs and IoT, identifying key research challenges towards achieving holistic privacy protection in SPSs and proposing the solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#65292;&#36890;&#36807;PS&#21453;&#39304;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#36890;&#35759;&#39640;&#25928;&#30340;&#35774;&#22791;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11633</link><description>&lt;p&gt;
&#22312;&#39118;&#38505;&#35268;&#36991;&#21442;&#19982;&#21453;&#39304;&#19979;&#30340;&#30446;&#26631;&#23548;&#21521;&#32852;&#37030;&#23398;&#20064;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Goal-Oriented Communications in Federated Learning via Feedback on Risk-Averse Participation. (arXiv:2305.11633v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#39118;&#38505;&#35268;&#36991;&#26426;&#21046;&#65292;&#36890;&#36807;PS&#21453;&#39304;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#36890;&#35759;&#39640;&#25928;&#30340;&#35774;&#22791;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#65292;&#21033;&#29992;&#23398;&#20064;&#30446;&#26631;&#21644;&#26412;&#22320;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#21046;&#23450;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#39118;&#38505;&#35268;&#36991;&#26412;&#36136;&#65292;&#36890;&#36807;&#26469;&#33258;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#21453;&#39304;&#33719;&#24471;&#36890;&#20449;&#39640;&#25928;&#30340;&#35774;&#22791;&#24615;&#33021;&#12290;&#23458;&#25143;&#31471;&#24517;&#39035;&#22522;&#20110;&#23427;&#30340;&#20869;&#22312;&#28608;&#21169;&#36827;&#34892;&#20256;&#36755;&#35745;&#21010;&#65292;&#35813;&#28608;&#21169;&#26159;&#27492;&#23458;&#25143;&#31471;&#21442;&#19982;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;&#26412;&#22320;&#26356;&#26032;&#30340;&#30456;&#20851;&#24615;&#34987;&#35270;&#20026;&#24320;&#21457;&#26412;&#22320;&#20256;&#36755;&#31574;&#30053;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;&#20915;&#23450;&#8220;&#19981;&#20256;&#36755;&#8221;&#30340;&#26102;&#38388;&#12290;&#35774;&#22791;&#20351;&#29992;&#26377;&#20851;PS&#29366;&#24577;&#30340;&#21453;&#39304;&#65292;&#26469;&#21457;&#23637;&#20854;&#31574;&#30053;&#65292;&#20197;&#22312;&#19981;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We treat the problem of client selection in a Federated Learning (FL) setup, where the learning objective and the local incentives of the participants are used to formulate a goal-oriented communication problem. Specifically, we incorporate the risk-averse nature of participants and obtain a communication-efficient on-device performance, while relying on feedback from the Parameter Server (\texttt{PS}). A client has to decide its transmission plan on when not to participate in FL. This is based on its intrinsic incentive, which is the value of the trained global model upon participation by this client. Poor updates not only plunge the performance of the global model with added communication cost but also propagate the loss in performance on other participating devices. We cast the relevance of local updates as \emph{semantic information} for developing local transmission strategies, i.e., making a decision on when to ``not transmit". The devices use feedback about the state of the PS a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>SFP&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#23376;&#32467;&#26500;&#30340;&#20462;&#21098;&#26694;&#26550;&#65292;SFP&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#23545;&#23436;&#20840;&#26292;&#38706;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#20197;&#21450;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#21516;&#26679;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#24102;&#26469;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#65292;&#21033;&#29992;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#26469;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.11615</link><description>&lt;p&gt;
SFP: &#38024;&#23545;&#20266;&#29305;&#24449;&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26080;&#20998;&#24067;&#27010;&#25324;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11615
&lt;/p&gt;
&lt;p&gt;
SFP&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#23376;&#32467;&#26500;&#30340;&#20462;&#21098;&#26694;&#26550;&#65292;SFP&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#23545;&#23436;&#20840;&#26292;&#38706;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#20197;&#21450;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#21516;&#26679;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#24102;&#26469;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#65292;&#21033;&#29992;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#26469;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#23376;&#32467;&#26500;&#23398;&#20064;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#19981;&#21464;&#30340;&#32593;&#32476;&#23376;&#32467;&#26500;&#65292;&#21487;&#20197;&#27604;&#21407;&#22987;&#30340;&#23436;&#25972;&#32467;&#26500;&#26356;&#22909;&#22320;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#27010;&#25324;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#26292;&#38706;&#30340;&#22495;&#22806;&#25968;&#25454;&#26469;&#25628;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#20174;&#32780;&#21487;&#33021;&#24102;&#26469;&#20004;&#20010;&#32570;&#28857;&#65306;1&#65289;&#19981;&#20844;&#24179;&#65292;&#22240;&#20026;&#23436;&#20840;&#26292;&#38706;&#20986;&#22495;&#22806;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65307;&#21644;2&#65289;&#27425;&#20248;&#30340;OOD&#27010;&#25324;&#65292;&#30001;&#20110;&#23545;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#21516;&#26679;&#30340;&#29305;&#24449;&#26410;&#21629;&#20013;&#20462;&#21098;&#12290;&#22522;&#20110;ID&#25968;&#25454;&#20013;&#30340;&#20266;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#26356;&#20302;&#30340;&#20307;&#39564;&#39118;&#38505;&#30340;&#24819;&#27861;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#29305;&#24449;&#23450;&#21521;&#30340;&#27169;&#22411;&#20462;&#21098;&#26694;&#26550;&#65292;&#31216;&#20026;SFP&#65292;&#20197;&#33258;&#21160;&#25506;&#32034;&#19981;&#21464;&#30340;&#23376;&#32467;&#26500;&#65292;&#32780;&#19981;&#32771;&#34385;&#19978;&#36848;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SFP&#22312;&#22521;&#35757;&#36807;&#31243;&#20013;&#20351;&#29992;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#39564;&#35777;&#30340;&#20219;&#21153;&#20002;&#22833;&#35782;&#21035;ID&#23454;&#20363;&#20013;&#30340;&#20266;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#65292;SFP&#20943;&#24369;&#20102;&#30456;&#24212;&#30340;&#29305;&#24449;&#20316;&#29992;&#65292;&#20197;&#25552;&#39640;OOD&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding 
&lt;/p&gt;</description></item><item><title>MIDI-Draw&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30011;&#26354;&#32447;&#26469;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#26356;&#24555;&#36895;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.11605</link><description>&lt;p&gt;
MIDI-Draw: &#29992;&#32472;&#30011;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MIDI-Draw: Sketching to Control Melody Generation. (arXiv:2305.11605v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11605
&lt;/p&gt;
&lt;p&gt;
MIDI-Draw&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30011;&#26354;&#32447;&#26469;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#26356;&#24555;&#36895;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#23454;&#29616;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#26059;&#24459;&#36718;&#24275;&#23558;&#38899;&#31526;&#32423;&#21035;&#30340;&#36755;&#20837;&#34920;&#31034;&#25277;&#35937;&#21270;&#12290;&#20854;&#30446;&#30340;&#26159;&#20801;&#35768;&#29992;&#25143;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#65292;&#32780;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#38899;&#31526;&#22914;&#20309;&#21644;&#35856;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#30446;&#21069;&#65292;&#21487;&#25511;&#26059;&#24459;&#29983;&#25104;&#30340;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#29992;&#25143;&#36873;&#25321;&#25972;&#20010;&#24207;&#21015;&#20013;&#38745;&#24577;&#30340;&#21442;&#25968;&#65292;&#36890;&#36807;&#25353;&#38062;&#25110;&#28369;&#22359;&#36827;&#34892;&#36873;&#25321;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32472;&#21046;&#36718;&#24275;&#24555;&#36895;&#25351;&#23450;&#21442;&#25968;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#25311;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#19978;&#29983;&#25104;&#33258;&#28982;&#37492;&#21035;&#23454;&#20363;&#65292;&#29992;&#20110;&#40657;&#30418;&#20844;&#24179;&#24615;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.11602</link><description>&lt;p&gt;
&#28508;&#22312;&#27169;&#25311;&#22120;: &#29983;&#25104;&#40657;&#30418;&#20844;&#24179;&#24615;&#27979;&#35797;&#30340;&#33258;&#28982;&#37492;&#21035;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing. (arXiv:2305.11602v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#25311;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#19978;&#29983;&#25104;&#33258;&#28982;&#37492;&#21035;&#23454;&#20363;&#65292;&#29992;&#20110;&#40657;&#30418;&#20844;&#24179;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25935;&#24863;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#23637;&#29616;&#20986;&#19981;&#20844;&#24179;&#30340;&#34892;&#20026;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#27979;&#35797;&#20844;&#24179;&#24615;&#65292;&#24037;&#31243;&#24072;&#32463;&#24120;&#29983;&#25104;&#37492;&#21035;&#23454;&#20363;&#26469;&#26292;&#38706;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#30340;&#19981;&#20844;&#24179;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#32447;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#33258;&#28982;&#24615;&#65292;&#24182;&#20135;&#29983;&#20559;&#31163;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#20363;&#65292;&#21487;&#33021;&#26080;&#27861;&#25581;&#31034;&#23454;&#38469;&#30340;&#27169;&#22411;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#19981;&#33258;&#28982;&#30340;&#37492;&#21035;&#23454;&#20363;&#19981;&#21487;&#33021;&#20986;&#29616;&#22312;&#23454;&#36341;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#28508;&#22312;&#27169;&#25311;&#22120;(LIMI)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26469;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#20010;&#20307;&#37492;&#21035;&#23454;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;GAN&#30340;&#35821;&#20041;&#28508;&#22312;&#31354;&#38388;&#20013;&#27169;&#25311;&#30446;&#26631;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#36827;&#19968;&#27493;&#23545;&#20854;&#36827;&#34892;&#26679;&#26412;&#28508;&#22312;&#23454;&#20363;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) systems have achieved remarkable performance across a wide area of applications. However, they frequently exhibit unfair behaviors in sensitive application domains, raising severe fairness concerns. To evaluate and test fairness, engineers often generate individual discriminatory instances to expose unfair behaviors before model deployment. However, existing baselines ignore the naturalness of generation and produce instances that deviate from the real data distribution, which may fail to reveal the actual model fairness since these unnatural discriminatory instances are unlikely to appear in practice. To address the problem, this paper proposes a framework named Latent Imitator (LIMI) to generate more natural individual discriminatory instances with the help of a generative adversarial network (GAN), where we imitate the decision boundary of the target model in the semantic latent space of GAN and further samples latent instances on it. Specifically, we first der
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#36801;&#31227;&#33021;&#21147;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27492;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11589</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21450;Sim2Real&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer. (arXiv:2305.11589v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#22312;&#30495;&#23454;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#36801;&#31227;&#33021;&#21147;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27492;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#65292;&#36710;&#36742;&#24517;&#39035;&#33021;&#22815;&#25345;&#32493;&#25191;&#34892;&#21508;&#31181;&#39550;&#39542;&#20219;&#21153;&#65292;&#21253;&#25324;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#26159;&#22522;&#26412;&#30340;&#24182;&#19988;&#30740;&#31350;&#24471;&#24456;&#20805;&#20998;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#20219;&#21153;&#19978;&#65292;&#32780;&#36319;&#36710;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#23436;&#25972;&#30340;&#39046;&#23548;-&#36319;&#38543;&#32773;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#25805;&#20316;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#32447;&#25511;&#21046;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#26126;&#35757;&#32451;&#30340;DRL&#20195;&#29702;&#30340;Sim2Real&#36716;&#31227;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#20445;&#25345;&#36710;&#36947;&#21644;&#36319;&#36710;&#20195;&#29702;&#21450;&#20854;Sim2Real&#36716;&#31227;&#33021;&#21147;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve fully autonomous driving, vehicles must be capable of continuously performing various driving tasks, including lane keeping and car following, both of which are fundamental and well-studied driving ones. However, previous studies have mainly focused on individual tasks, and car following tasks have typically relied on complete leader-follower information to attain optimal performance. To address this limitation, we propose a vision-based deep reinforcement learning (DRL) agent that can simultaneously perform lane keeping and car following maneuvers. To evaluate the performance of our DRL agent, we compare it with a baseline controller and use various performance metrics for quantitative analysis. Furthermore, we conduct a real-world evaluation to demonstrate the Sim2Real transfer capability of the trained DRL agent. To the best of our knowledge, our vision-based car following and lane keeping agent with Sim2Real transfer capability is the first of its kind.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11586</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#34701;&#20837;&#19981;&#30830;&#23450;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20165;&#20551;&#35774;&#27169;&#22411;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20986;&#20855;&#26377;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24314;&#27169;&#20551;&#35774;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#22240;&#32032;&#65292;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20837;&#20301;&#32622;&#21487;&#33021;&#20063;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#24615;&#34701;&#20837;&#21040;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#12290;&#32771;&#34385;&#20004;&#31181;&#21487;&#35266;&#27979;&#37327;&#8212;&#8212;&#20855;&#26377;&#22266;&#23450;&#36755;&#20837;&#30340;&#22122;&#22768;&#27745;&#26579;&#36755;&#20986;&#21644;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#20197;&#25512;&#26029;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#26041;&#27861;&#23558;&#36825;&#20123;&#36755;&#20837;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#20013;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#22238;&#24402;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#35266;&#23519;&#21040;&#19981;&#21516;&#27700;&#24179;&#36755;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#26222;&#36866;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11584</link><description>&lt;p&gt;
&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#22330;&#26223;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#19968;&#32452;&#26412;&#22320;&#23458;&#25143;&#31471;&#22312;&#20840;&#23616;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#21327;&#20316;&#35757;&#32451;&#24102;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#38548;&#31163;&#30340;&#38750; iid &#25968;&#25454;&#38598;&#65292;&#23458;&#25143;&#31471;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#21040;&#33258;&#24049;&#30340;&#33258;&#36523;&#26368;&#20248;&#35299;&#65292;&#36825;&#26497;&#22823;&#22320;&#20559;&#31163;&#20102;&#20840;&#23616;&#30446;&#26631;&#24182;&#20005;&#37325;&#21066;&#24369;&#20102;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31639;&#27861; FedSMOO&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640; FL &#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#19968;&#20010;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#65292;&#24182;&#22312;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11582</link><description>&lt;p&gt;
&#20320;&#21548;&#21040;&#30340;&#27491;&#26159;&#20320;&#30475;&#21040;&#30340;&#65306;&#20174;&#22270;&#20687;&#36136;&#37327;&#35780;&#20215;&#20013;&#33719;&#24471;&#30340;&#38899;&#39057;&#36136;&#37327;&#35780;&#20215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics. (arXiv:2305.11582v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#19968;&#20010;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#65292;&#24182;&#22312;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;&#20449;&#21495;&#34920;&#31034;&#25104;&#39057;&#35889;&#22270;&#20197;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#21548;&#35273;&#21644;&#35270;&#35273;&#36890;&#36335;&#20013;&#31070;&#32463;&#26426;&#21046;&#30340;&#30456;&#20284;&#24615;&#65292;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;&#19968;&#20010;&#20855;&#26377;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#20960;&#20010;&#22522;&#20934;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21363;&#24230;&#37327;&#26041;&#27861;&#19982;&#20154;&#31867;&#35780;&#20272;&#32773;&#25152;&#35780;&#20272;&#30340;&#38899;&#39057;&#36136;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the feasibility of utilizing state-of-the-art image perceptual metrics for evaluating audio signals by representing them as spectrograms. The encouraging outcome of the proposed approach is based on the similarity between the neural mechanisms in the auditory and visual pathways. Furthermore, we customise one of the metrics which has a psychoacoustically plausible architecture to account for the peculiarities of sound signals. We evaluate the effectiveness of our proposed metric and several baseline metrics using a music dataset, with promising results in terms of the correlation between the metrics and the perceived quality of audio as rated by human evaluators.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#28789;&#27963;&#30340;&#29983;&#23384;&#27169;&#22411;&#38598;&#25104;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#39044;&#27979;&#23384;&#22312;&#27835;&#24840;&#20998;&#25968;&#26102;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#20801;&#35768;&#21327;&#21464;&#37327;&#21644;&#29983;&#23384;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#39640;&#32500;&#20132;&#20114;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#30495;&#23454;&#30340;&#21327;&#21464;&#37327;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.11575</link><description>&lt;p&gt;
&#28145;&#24230;&#25512;&#36827;&#26102;&#38388;&#27835;&#24840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Deep Promotion Time Cure Model. (arXiv:2305.11575v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#28789;&#27963;&#30340;&#29983;&#23384;&#27169;&#22411;&#38598;&#25104;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#39044;&#27979;&#23384;&#22312;&#27835;&#24840;&#20998;&#25968;&#26102;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#20801;&#35768;&#21327;&#21464;&#37327;&#21644;&#29983;&#23384;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#39640;&#32500;&#20132;&#20114;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#30495;&#23454;&#30340;&#21327;&#21464;&#37327;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28789;&#27963;&#30340;&#29983;&#23384;&#27169;&#22411;&#38598;&#25104;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#29992;&#20110;&#39044;&#27979;&#23384;&#22312;&#27835;&#24840;&#20998;&#25968;&#26102;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#21327;&#21464;&#37327;&#21644;&#29983;&#23384;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#39640;&#32500;&#20132;&#20114;&#65292;&#24182;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20801;&#35768;&#35813;&#26041;&#27861;&#21512;&#24182;&#19968;&#20010;&#30001;&#21487;&#35299;&#37322;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#25928;&#24212;&#30340;&#38468;&#21152;&#20998;&#35299;&#24418;&#25104;&#30340;&#24050;&#35782;&#21035;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#28155;&#21152;&#27491;&#20132;&#21270;&#23618;&#20197;&#25429;&#33719;&#28508;&#22312;&#30340;&#26356;&#39640;&#32500;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#37327;&#30340;&#32654;&#22269;&#25269;&#25276;&#36151;&#27454;&#32452;&#21512;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#21457;&#29616;&#20102;&#25105;&#20204;&#26694;&#26550;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#21457;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#21327;&#21464;&#37327;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for predicting time-to-event in the presence of cure fractions based on flexible survivals models integrated into a deep neural network framework. Our approach allows for non-linear relationships and high-dimensional interactions between covariates and survival and is suitable for large-scale applications. Furthermore, we allow the method to incorporate an identified predictor formed of an additive decomposition of interpretable linear and non-linear effects and add an orthogonalization layer to capture potential higher dimensional interactions. We demonstrate the usefulness and computational efficiency of our method via simulations and apply it to a large portfolio of US mortgage loans. Here, we find not only a better predictive performance of our framework but also a more realistic picture of covariate effects.
&lt;/p&gt;</description></item><item><title>TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11567</link><description>&lt;p&gt;
TSGM&#65306;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11567
&lt;/p&gt;
&lt;p&gt;
TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#20063;&#24456;&#26377;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#25110;&#39640;&#24230;&#25935;&#24863;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#22312;&#30740;&#31350;&#32773;&#21644;&#24037;&#19994;&#32452;&#32455;&#20043;&#38388;&#30340;&#20849;&#20139;&#20197;&#21450;&#29616;&#26377;&#21644;&#26032;&#30340;&#25968;&#25454;&#23494;&#38598;&#22411; ML &#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#30340;&#21487;&#33021;&#26041;&#27861;&#26159;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65288;TSGM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;TSGM&#21253;&#25324;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#29983;&#25104;&#27169;&#22411;&#12289;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#65306;&#30456;&#20284;&#24615;&#12289;&#19979;&#28216;&#25928;&#26524;&#12289;&#39044;&#27979;&#19968;&#33268;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#38544;&#31169;&#12290;&#35813;&#26694;&#26550;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;TSGM&#23558;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporally indexed data are essential in a wide range of fields and of interest to machine learning researchers. Time series data, however, are often scarce or highly sensitive, which precludes the sharing of data between researchers and industrial organizations and the application of existing and new data-intensive ML methods. A possible solution to this bottleneck is to generate synthetic data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling of synthetic time series. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, and simulator-based approaches. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, and privacy. The framework is extensible, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. TSGM w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11531</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#26032;&#22411;&#30005;&#30913;&#37327;&#35745;&#20960;&#20309;&#27169;&#25311;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11531
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#29983;&#25104;&#23545;&#25758;&#20135;&#29289;&#30340;&#27169;&#25311;&#25506;&#27979;&#22120;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20854;&#20013;&#19968;&#20010;&#23376;&#25506;&#27979;&#22120;&#65292;&#30005;&#30913;&#37327;&#35745;&#30001;&#20110;&#20854;&#21333;&#20803;&#26684;&#30340;&#39640;&#31890;&#24230;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#32780;&#21344;&#25454;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#26679;&#26412;&#29983;&#25104;&#65292;&#20294;&#30446;&#21069;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#26469;&#20248;&#21270;&#29305;&#23450;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#32593;&#32476;&#26469;&#25551;&#36848;&#19981;&#21516;&#30340;&#21333;&#20803;&#26684;&#22823;&#23567;&#21644;&#25490;&#21015;&#26041;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#27169;&#25311;&#21709;&#24212;&#32780;&#26080;&#38656;&#20854;&#20182;&#35757;&#32451;&#12290;&#35813;&#20960;&#20309;&#24863;&#30693;&#27169;&#22411;&#22312;&#28041;&#21450;&#20851;&#38190;&#21709;&#24212;&#30340;&#29983;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#25351;&#26631;&#19978;&#27604;&#22522;&#32447;&#27169;&#22411;&#20248;&#36234;50&#65285;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#25193;&#23637;&#21040;&#38750;&#24179;&#38754;&#20960;&#20309;&#24418;&#29366;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#36824;&#25506;&#35752;&#20102;&#19968;&#20123;&#23545;&#27169;&#22411;&#30340;&#20462;&#25913;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11529</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sequence-to-Sequence Approach for Arabic Pronoun Resolution. (arXiv:2305.11529v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#36824;&#25506;&#35752;&#20102;&#19968;&#20123;&#23545;&#27169;&#22411;&#30340;&#20462;&#25913;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;Bi-LSTM&#21644;BERT&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#20960;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;KNN&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;SVM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27169;&#22411;&#30340;&#21508;&#31181;&#20462;&#25913;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23558;&#25351;&#20195;&#35789;&#25991;&#26412;&#19982;&#27573;&#33853;&#25991;&#26412;&#36830;&#25509;&#20316;&#20026;&#36755;&#20837;&#12289;&#28155;&#21152;&#25513;&#30721;&#20197;&#20851;&#27880;&#20505;&#36873;&#20998;&#25968;&#20197;&#21450;&#22522;&#20110;&#25351;&#20195;&#35789;&#30340;&#24615;&#21035;&#21644;&#25968;&#37327;&#21327;&#35758;&#26469;&#36807;&#28388;&#20505;&#36873;&#39033;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sequence-to-sequence learning approach for Arabic pronoun resolution, which explores the effectiveness of using advanced natural language processing (NLP) techniques, specifically Bi-LSTM and the BERT pre-trained Language Model, in solving the pronoun resolution problem in Arabic. The proposed approach is evaluated on the AnATAr dataset, and its performance is compared to several baseline models, including traditional machine learning models and handcrafted feature-based models. Our results demonstrate that the proposed model outperforms the baseline models, which include KNN, logistic regression, and SVM, across all metrics. In addition, we explore the effectiveness of various modifications to the model, including concatenating the anaphor text beside the paragraph text as input, adding a mask to focus on candidate scores, and filtering candidates based on gender and number agreement with the anaphor. Our results show that these modifications significantly improv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;GFST-WSF&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms. (arXiv:2305.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;GFST-WSF&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#30005;&#22823;&#35268;&#27169;&#38598;&#25104;&#30005;&#32593;&#20013;&#65292;&#39118;&#21147;&#30340;&#39640;&#21487;&#21464;&#24615;&#21644;&#38543;&#26426;&#24615;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#36816;&#34892;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#39118;&#21147;&#39044;&#27979;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#39118;&#36895;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#21644;&#39057;&#29575;&#22686;&#24378;&#26426;&#21046;&#30340;&#22270;&#27880;&#24847;&#21147;&#39057;&#29575;&#22686;&#24378;&#26102;&#31354;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#65288;GFST-WSF&#65289;&#65292;&#20197;&#25552;&#39640;&#30701;&#26399;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;GFST-WSF&#21253;&#25324;&#29992;&#20110;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#30340;Transformer&#26550;&#26500;&#21644;&#29992;&#20110;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#12290;GAT&#34987;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#39118;&#36895;&#31449;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32858;&#21512;&#22270;&#20013;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#25454;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#20026;&#20102;&#27169;&#25311;&#37051;&#36817;&#39118;&#22330;&#20043;&#38388;&#30340;&#39118;&#36895;&#30456;&#20851;&#30340;&#26102;&#38388;&#28382;&#21518;
&lt;/p&gt;
&lt;p&gt;
The safe and stable operation of power systems is greatly challenged by the high variability and randomness of wind power in large-scale wind-power-integrated grids. Wind power forecasting is an effective solution to tackle this issue, with wind speed forecasting being an essential aspect. In this paper, a Graph-attentive Frequency-enhanced Spatial-Temporal Wind Speed Forecasting model based on graph attention and frequency-enhanced mechanisms, i.e., GFST-WSF, is proposed to improve the accuracy of short-term wind speed forecasting. The GFST-WSF comprises a Transformer architecture for temporal feature extraction and a Graph Attention Network (GAT) for spatial feature extraction. The GAT is specifically designed to capture the complex spatial dependencies among wind speed stations to effectively aggregate information from neighboring nodes in the graph, thus enhancing the spatial representation of the data. To model the time lag in wind speed correlation between adjacent wind farms cau
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11512</link><description>&lt;p&gt;
&#20016;&#23500;&#35299;&#32544;&#32467;&#65306;&#20174;&#23450;&#20041;&#21040;&#24230;&#37327;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enriching Disentanglement: Definitions to Metrics. (arXiv:2305.11512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#20998;&#31163;&#22810;&#20010;&#21464;&#21270;&#22240;&#32032;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#23398;&#20064;&#21644;&#35780;&#20272;&#35299;&#32544;&#32467;&#34920;&#31034;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#30495;&#27491;&#37327;&#21270;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27604;&#36739;&#23427;&#20204;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19968;&#38454;&#26041;&#31243;&#35859;&#35789;&#23450;&#20041;&#30340;&#35299;&#32544;&#32467;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#20860;&#23481;&#30340;&#23450;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#24230;&#37327;&#25110;&#31163;&#25955;&#24230;&#26367;&#25442;(i) &#31561;&#24335;&#65292;&#29992;&#25490;&#24207;&#25805;&#20316;&#26367;&#25442; (ii) &#36923;&#36753;&#32852;&#32467;&#35789;&#65292;&#29992;&#32858;&#21512;&#26367;&#25442; (iii) &#36890;&#29992;&#37327;&#35789;&#65292;&#29992;&#26368;&#20339;&#36924;&#36817;&#26367;&#25442; (iv) &#23384;&#22312;&#37327;&#35789;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#34920;&#31034;&#25552;&#21462;&#22120;&#25152;&#38656;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#35299;&#32544;&#32467;&#34920;&#31034;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#65292;&#24182;&#34913;&#37327;&#19968;&#31181;&#26041;&#27861;&#20013;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled representation learning is a challenging task that involves separating multiple factors of variation in complex data. Although various metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what these metrics truly quantify and how to compare them. In this work, we study the definitions of disentanglement given by first-order equational predicates and introduce a systematic approach for transforming an equational definition into a compatible quantitative metric based on enriched category theory. Specifically, we show how to replace (i) equality with metric or divergence, (ii) logical connectives with order operations, (iii) universal quantifier with aggregation, and (iv) existential quantifier with the best approximation. Using this approach, we derive metrics for measuring the desired properties of a disentangled representation extractor and demonstrate their effectiveness on synthetic data. Our proposed approach provides p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.11509</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25628;&#32034;&#21040;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#24456;&#23569;&#26377;&#38750;&#21551;&#21457;&#24335;&#30340;&#29702;&#35770;&#29992;&#20110;&#25551;&#36848;&#20854;&#24037;&#20316;&#26426;&#21046;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20851;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#24182;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29615;&#22659;&#27809;&#26377;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#20854;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $&#65292;&#20854;&#20013;$ d_s \ge 0 $&#26159;&#24213;&#23618;&#20989;&#25968;&#30340;&#25955;&#23556;&#32500;&#24230;&#12290;&#24403;&#35266;&#23519;&#21040;&#30340;&#20989;&#25968;&#20540;&#21463;&#21040;&#26377;&#30028;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;JOINDEDTrans&#26694;&#26550;&#29992;&#20110;&#32852;&#21512;OD/OC &#20998;&#21106;&#21450;&#40644;&#26001;&#26816;&#27979;, &#37319;&#29992;&#20808;&#39564;&#20449;&#24687;&#24341;&#23548;&#22810;&#20219;&#21153;Transformer&#65292; &#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#21106;&#21644;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11504</link><description>&lt;p&gt;
JOINEDTrans: &#20248;&#20808;&#24341;&#23548;&#30340;&#22810;&#20219;&#21153;Transformer&#29992;&#20110;&#32852;&#21512;&#35270;&#30424;/&#26479;&#20998;&#21106;&#21644;&#40644;&#26001;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic Disc/Cup Segmentation and Fovea Detection. (arXiv:2305.11504v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;JOINDEDTrans&#26694;&#26550;&#29992;&#20110;&#32852;&#21512;OD/OC &#20998;&#21106;&#21450;&#40644;&#26001;&#26816;&#27979;, &#37319;&#29992;&#20808;&#39564;&#20449;&#24687;&#24341;&#23548;&#22810;&#20219;&#21153;Transformer&#65292; &#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#21106;&#21644;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#22312;&#20998;&#26512;&#35270;&#32593;&#33180;&#26631;&#24535;&#29289;&#65288;&#22914;&#35270;&#30424;&#65288;OD&#65289;&#65292;&#35270;&#26479;&#65288;OC&#65289;&#21644;&#40644;&#26001;&#65289;&#26041;&#38754;&#30340;&#25928;&#29575;&#24050;&#22823;&#22823;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30524;&#31185;&#30142;&#30149;&#30456;&#20851;&#30149;&#21464;&#21644;&#20302;&#22270;&#20687;&#36136;&#37327;&#38382;&#39064;&#31561;&#22240;&#32032;&#21487;&#33021;&#20005;&#37325;&#22797;&#26434;&#21270;&#33258;&#21160;OD / OC&#20998;&#21106;&#21644;&#40644;&#26001;&#26816;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23558;&#27599;&#20010;&#26631;&#24535;&#29289;&#30340;&#35782;&#21035;&#35270;&#20026;&#21333;&#20010;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#39564;&#24341;&#23548;&#22810;&#20219;&#21153;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;OD / OC&#20998;&#21106;&#21644;&#40644;&#26001;&#26816;&#27979;&#65292;&#21629;&#21517;&#20026;JOINEDTrans&#12290;JOINEDTrans&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#21508;&#31181;&#31354;&#38388;&#29305;&#24449;&#65292;&#32531;&#35299;&#20102;&#30149;&#21464;&#21644;&#20854;&#20182;&#25104;&#20687;&#38382;&#39064;&#24341;&#36215;&#30340;&#32467;&#26500;&#22833;&#30495;&#12290;&#23427;&#21253;&#21547;&#19968;&#20010;&#20998;&#21106;&#20998;&#25903;&#21644;&#19968;&#20010;&#26816;&#27979;&#20998;&#25903;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22312;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#34880;&#31649;&#65292;OD / OC&#21644;&#40644;&#26001;&#20043;&#38388;&#30340;&#20301;&#32622;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based image segmentation and detection models have largely improved the efficiency of analyzing retinal landmarks such as optic disc (OD), optic cup (OC), and fovea. However, factors including ophthalmic disease-related lesions and low image quality issues may severely complicate automatic OD/OC segmentation and fovea detection. Most existing works treat the identification of each landmark as a single task, and take into account no prior information. To address these issues, we propose a prior guided multi-task transformer framework for joint OD/OC segmentation and fovea detection, named JOINEDTrans. JOINEDTrans effectively combines various spatial features of the fundus images, relieving the structural distortions induced by lesions and other imaging issues. It contains a segmentation branch and a detection branch. To be noted, we employ an encoder pretrained in a vessel segmentation task to effectively exploit the positional relationship among vessel, OD/OC, and fovea, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39640;&#25928;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#26041;&#27861;&#21644;&#21452;&#38750;&#20984;&#27169;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#24555;&#36895;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#39640;&#38454;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11495</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20302;&#31209;&#36924;&#36817;&#30340;&#38750;&#20984;&#40065;&#26834;&#39640;&#38454;&#24352;&#37327;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank Approximation. (arXiv:2305.11495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39640;&#25928;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#26041;&#27861;&#21644;&#21452;&#38750;&#20984;&#27169;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#24555;&#36895;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#39640;&#38454;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;T-SVD&#65289;&#26694;&#26550;&#19979;&#65292;&#29616;&#26377;&#30340;&#31283;&#20581;&#20302;&#31209;&#24352;&#37327;&#23436;&#25104;&#26041;&#27861;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;T-SVD&#30340;&#20302;&#31209;&#36924;&#36817;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#25968;&#25454;&#26102;&#38754;&#20020;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#20165;&#36866;&#29992;&#20110;&#19977;&#38454;&#24352;&#37327;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;d&#38454;&#65288;d&gt;=3&#65289;T-SVD&#26694;&#26550;&#19979;&#35774;&#35745;&#20102;&#20004;&#31181;&#34701;&#21512;&#38543;&#26426;&#25216;&#26415;&#30340;&#39640;&#25928;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#40065;&#26834;&#39640;&#38454;&#24352;&#37327;&#23436;&#25104;&#65288;RHTC&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#21452;&#38750;&#20984;&#27169;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#24555;&#36895;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#38543;&#26426;&#20302;&#31209;&#36924;&#36817;&#32435;&#20837;RHTC&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#23545;&#22823;&#35268;&#27169;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#39640;&#38454;&#24352;&#37327;&#23436;&#25104;&#20219;&#21153;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the tensor singular value decomposition (T-SVD) framework, existing robust low-rank tensor completion approaches have made great achievements in various areas of science and engineering. Nevertheless, these methods involve the T-SVD based low-rank approximation, which suffers from high computational costs when dealing with large-scale tensor data. Moreover, most of them are only applicable to third-order tensors. Against these issues, in this article, two efficient low-rank tensor approximation approaches fusing randomized techniques are first devised under the order-d (d &gt;= 3) T-SVD framework. On this basis, we then further investigate the robust high-order tensor completion (RHTC) problem, in which a double nonconvex model along with its corresponding fast optimization algorithms with convergence guarantees are developed. To the best of our knowledge, this is the first study to incorporate the randomized low-rank approximation into the RHTC problem. Empirical studies on large-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11489</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#34917;&#20840;&#23454;&#29616;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Incomplete Multi-view Clustering via Diffusion Completion. (arXiv:2305.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#20026;&#20102;&#23545;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#38750;&#24120;&#35268;&#30340;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#37117;&#38656;&#35201;&#35299;&#20915;&#22914;&#20309;&#20943;&#23569;&#32570;&#22833;&#35270;&#35282;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#12290;&#22522;&#20110;&#21487;&#35266;&#23519;&#35270;&#35282;&#20449;&#24687;&#65292;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24674;&#22797;&#32570;&#22833;&#30340;&#35270;&#35282;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#21487;&#33021;&#26159;&#23558;&#25193;&#25955;&#27169;&#22411;&#32435;&#20837;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#30340;&#39318;&#20010;&#24037;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#26041;&#27861;&#22312;&#24674;&#22797;&#32570;&#22833;&#30340;&#35270;&#35282;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering is a challenging and non-trivial task to provide effective data analysis for large amounts of unlabeled data in the real world. All incomplete multi-view clustering methods need to address the problem of how to reduce the impact of missing views. To address this issue, we propose diffusion completion to recover the missing views integrated into an incomplete multi-view clustering framework. Based on the observable views information, the diffusion model is used to recover the missing views, and then the consistency information of the multi-view data is learned by contrastive learning to improve the performance of multi-view clustering. To the best of our knowledge, this may be the first work to incorporate diffusion models into an incomplete multi-view clustering framework. Experimental results show that the proposed method performs well in recovering the missing views while achieving superior clustering performance compared to state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11476</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#22810;&#26679;&#39118;&#38505;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11476
&lt;/p&gt;
&lt;p&gt;
RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#20013;&#65292;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#35299;&#20915;&#31454;&#20105;&#24615;&#28216;&#25103;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#24403;&#21069;&#30340;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#20248;&#21270;&#20195;&#29702;&#31243;&#24207;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#32988;&#29575;&#26102;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#24182;&#20135;&#29983;&#21333;&#19968;&#21516;&#36136;&#21270;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25171;&#30772;&#20725;&#23616;&#24182;&#22686;&#24378;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#26041;&#27861;&#21487;&#33021;&#22312;&#20110;&#22686;&#21152;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#22686;&#21152;&#22810;&#26679;&#24615;&#24182;&#19981;&#26159;&#26131;&#22914;&#21453;&#25484;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#21487;&#20197;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#36825;&#19968;&#35270;&#35282;&#20986;&#21457;&#22686;&#21152;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#39118;&#38505;&#25935;&#24863;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(RPPO)&#65292;&#23427;&#22312;&#26368;&#22351;&#21644;&#26368;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#20043;&#38388;&#24179;&#28369;&#22320;&#25554;&#20540;&#65292;&#20801;&#35768;&#20855;&#26377;&#25152;&#38656;&#39118;&#38505;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the great successes of Reinforcement Learning (RL), self-play algorithms play an essential role in solving competitive games. Current self-play algorithms optimize the agent to maximize expected win-rates against its current or historical copies, making it often stuck in the local optimum and its strategy style simple and homogeneous. A possible solution is to improve the diversity of policies, which helps the agent break the stalemate and enhances its robustness when facing different opponents. However, enhancing diversity in the self-play algorithms is not trivial. In this paper, we aim to introduce diversity from the perspective that agents could have diverse risk preferences in the face of uncertainty. Specifically, we design a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning and allows for policy learning with desired risk preferences. Seamlessly inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11475</link><description>&lt;p&gt;
&#26354;&#32447;&#19978;&#25196;&#65306;&#22312;&#21487;&#24494;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20849;&#26354;&#25233;&#21046;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#24212;&#23545;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26131;&#21463;&#20849;&#38169;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24809;&#32602;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;GAM&#65289;&#21487;&#34920;&#36798;&#30446;&#26631;&#21464;&#37327;&#20026;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#35299;&#37322;&#24615;&#65292;&#20854;&#20877;&#27425;&#21463;&#21040;&#27426;&#36814;&#12290;&#23613;&#31649;GAM&#30446;&#21069;&#22791;&#21463;&#28909;&#25447;&#65292;&#20294;&#20854;&#26131;&#21463;&#20849;&#38169;&#24615;&#65292;&#21363;&#29305;&#24449;&#20043;&#38388;&#30340;&#65288;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65289;&#20381;&#36182;&#24615;&#36804;&#20170;&#20026;&#27490;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20849;&#38169;&#24615;&#22914;&#20309;&#20005;&#37325;&#30772;&#22351;GAM&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#27861;&#65306;&#19968;&#20010;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#30340;&#29305;&#24449;&#21464;&#37327;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#19978;&#36827;&#34892;&#24809;&#32602;&#30340;&#27010;&#24565;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#35813;&#36807;&#31243;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#30340;&#21152;&#24615;&#27169;&#22411;&#65292;&#22914;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#25110;&#31070;&#32463;&#39044;&#35328;&#12290;&#24182;&#19988;&#36890;&#36807;&#28040;&#38500;&#33258;&#25105;&#25269;&#28040;&#30340;&#29305;&#24449;&#36129;&#29486;&#30340;&#27495;&#20041;&#65292;&#22686;&#24378;&#20102;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possibly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11463</link><description>&lt;p&gt;
&#21033;&#29992;Riesz&#26680;&#30340;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#20013;&#65292;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#24230;(MMD)&#27969;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Riesz&#26680;$K(x,y)=-\|x-y\|^r$&#65292;$r \in (0,2)$&#30340;MMD&#27969;&#20855;&#26377;&#26480;&#20986;&#30340;&#24615;&#36136;&#65292;&#21487;&#20801;&#35768;&#20854;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;Riesz&#26680;&#30340;MMD&#19982;&#20854;&#20998;&#21106;&#29256;&#26412;&#30340;MMD&#37325;&#21512;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#19968;&#32500;&#35774;&#32622;&#20013;&#36827;&#34892;MMD&#26799;&#24230;&#30340;&#35745;&#31639;&#12290;&#22312;&#27492;&#22788;&#65292;&#23545;&#20110;$r=1$&#65292;&#21487;&#20197;&#24212;&#29992;&#31616;&#21333;&#30340;&#25490;&#24207;&#31639;&#27861;&#23558;&#20004;&#20010;&#32463;&#39564;&#24230;&#37327;&#30340;&#22797;&#26434;&#24230;&#20174;$O(MN+N^2)$&#38477;&#20302;&#21040;$O((M+N)\log(M+N))$&#65292;&#20854;&#20013;$M$&#21644;$N$&#26159;&#25903;&#25345;&#28857;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;$P$&#20010;&#20999;&#29255;&#26469;&#36817;&#20284;&#20998;&#21106;MMD&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#35823;&#24046;&#20855;&#26377;$O(\sqrt{d/P})$&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#25968;&#25454;&#32500;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;MMD&#26799;&#24230;&#27969;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#29978;&#33267;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.11455</link><description>&lt;p&gt;
&#31361;&#30772;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20248;&#21270;&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65288;RLHF&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#26174;&#24335;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#32780;&#19981;&#26159;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20010;&#22870;&#21169;&#27169;&#22411;&#19982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#32806;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19982;&#26399;&#26395;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#26159;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#12290;&#36825;&#20010;&#35266;&#28857;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#65292;&#21487;&#20197;&#21516;&#26102;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#19979;&#28216;&#31574;&#30053;&#20248;&#21270;&#12290;&#34429;&#28982;&#36825;&#20010;&#35266;&#28857;&#30830;&#23454;&#25171;&#30772;&#20102;&#20256;&#32479;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#35748;&#20026;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#20256;&#32479;&#31639;&#27861;&#27010;&#24565;&#36816;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#20013;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN&#65292;&#36890;&#36807;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#25968;&#25454;&#29615;&#22659;&#19979;&#25429;&#25417;&#26412;&#22320;&#25968;&#25454;&#24635;&#20307;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26694;&#26550;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#36890;&#20449;&#24320;&#38144;&#21644;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11437</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#30340;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN &#29992;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy. (arXiv:2305.11437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN&#65292;&#36890;&#36807;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#25968;&#25454;&#29615;&#22659;&#19979;&#25429;&#25417;&#26412;&#22320;&#25968;&#25454;&#24635;&#20307;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26694;&#26550;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#36890;&#20449;&#24320;&#38144;&#21644;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#30784;&#25968;&#25454;&#32479;&#35745;&#20449;&#24687;&#32780;&#25104;&#20026;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#26377;&#25928;&#23398;&#20064;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#23454;&#38469;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;FL&#26694;&#26550;&#22312;&#25429;&#25417;&#23637;&#29616;&#19981;&#21516;&#20998;&#24067;&#30340;&#26412;&#22320;&#29992;&#25143;&#25968;&#25454;&#30340;&#24635;&#20307;&#29305;&#24449;&#24615;&#33021;&#19978;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#26694;&#26550;PS-FedGAN&#65292;&#21482;&#38656;&#35201;&#37096;&#20998;&#30340;GAN&#27169;&#22411;&#20849;&#20139;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#23616;&#37492;&#21035;&#32593;&#32476;&#21644;&#37096;&#20998;&#20849;&#20139;&#30340;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#20197;&#20197;&#37096;&#20998;&#21512;&#20316;&#30340;&#26041;&#24335;&#21327;&#20316;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#32479;&#35745;&#24182;&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;GAN&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#20877;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as an effective learning paradigm for distributed computation owing to its strong potential in capturing underlying data statistics while preserving data privacy. However, in cases of practical data heterogeneity among FL clients, existing FL frameworks still exhibit deficiency in capturing the overall feature properties of local client data that exhibit disparate distributions. In response, generative adversarial networks (GANs) have recently been exploited in FL to address data heterogeneity since GANs can be integrated for data regeneration without exposing original raw data. Despite some successes, existing GAN-related FL frameworks often incur heavy communication cost and also elicit other privacy concerns, which limit their applications in real scenarios. To this end, this work proposes a novel FL framework that requires only partial GAN model sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and training mechanism to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#25299;&#25169;&#8212;&#8212;&#22522;&#30784;$(k+1)$&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#20998;&#25955;&#24335;SGD&#12290;</title><link>http://arxiv.org/abs/2305.11420</link><description>&lt;p&gt;
&#36229;&#36234;&#25351;&#25968;&#22270;&#65306;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#30340;&#36890;&#20449;&#25928;&#29575;&#25299;&#25169;&#29992;&#20110;&#20998;&#25955;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence. (arXiv:2305.11420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#25299;&#25169;&#8212;&#8212;&#22522;&#30784;$(k+1)$&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#20998;&#25955;&#24335;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#24182;&#34892;&#35745;&#31639;&#21644;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#20855;&#26377;&#26356;&#24555;&#20849;&#35782;&#29575;&#65288;&#21363;&#35889;&#38388;&#38553;&#65289;&#30340;&#24213;&#23618;&#32593;&#32476;&#25299;&#25169;&#21487;&#23548;&#33268;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#26356;&#22909;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#30340;&#25299;&#25169;&#65292;&#22914;&#25351;&#25968;&#22270;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#22823;&#30340;&#26368;&#22823;&#24230;&#25968;&#65292;&#36825;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#23547;&#27714;&#26082;&#20855;&#26377;&#24555;&#36895;&#20849;&#35782;&#29575;&#21448;&#20855;&#26377;&#23567;&#30340;&#26368;&#22823;&#24230;&#25968;&#30340;&#25299;&#25169;&#26159;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24555;&#36895;&#20849;&#35782;&#29575;&#21644;&#23567;&#26368;&#22823;&#24230;&#30340;&#26032;&#22411;&#25299;&#25169;&#65292;&#31216;&#20026;&#22522;&#30784;$(k+1)$ &#22270;&#12290;&#19982;&#29616;&#26377;&#30340;&#25299;&#25169;&#19981;&#21516;&#65292;&#22522;&#30784;$(k+1)$ &#22270;&#20351;&#25152;&#26377;&#33410;&#28857;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#37117;&#33021;&#36798;&#21040;&#30830;&#20999;&#30340;&#20849;&#35782;&#65292;&#23545;&#20110;&#20219;&#20309;&#33410;&#28857;&#25968;&#21644;&#26368;&#22823;&#24230;k&#37117;&#36866;&#29992;&#12290;&#24471;&#30410;&#20110;&#36825;&#20010;&#26377;&#21033;&#30340;&#23646;&#24615;&#65292;&#22522;&#30784;$(k+1)$ &#22270;&#36171;&#20104;&#20102;&#20998;&#25955;&#24335;SGD
&lt;/p&gt;
&lt;p&gt;
Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k + 1)$ Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree k. Thanks to this favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD
&lt;/p&gt;</description></item><item><title>JetSeg&#26159;&#19968;&#20010;&#19987;&#20026;GPU-&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#22411;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;JetBlock&#21644;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#30340;&#31574;&#30053;JetConv&#20197;&#21450;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11419</link><description>&lt;p&gt;
JetSeg: &#20302;&#21151;&#32791;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems. (arXiv:2305.11419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11419
&lt;/p&gt;
&lt;p&gt;
JetSeg&#26159;&#19968;&#20010;&#19987;&#20026;GPU-&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#22411;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;JetBlock&#21644;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#30340;&#31574;&#30053;JetConv&#20197;&#21450;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#31934;&#20934;&#21644;&#20855;&#26377;&#20302;&#25512;&#29702;&#26102;&#38388;&#30340;&#27169;&#22411;&#12290;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#30828;&#20214;&#33021;&#21147;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#20102;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JetSeg&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#39640;&#25928;&#27169;&#22411;&#65292;&#30001;&#19968;&#20010;&#31216;&#20026;JetNet&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25913;&#36827;&#30340;RegSeg&#35299;&#30721;&#22120;&#32452;&#25104;&#12290;JetNet&#19987;&#20026;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#65292;&#24182;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#31216;&#20026;JetBlock&#30340;&#26032;&#22411;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#26469;&#26368;&#23567;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#65307;&#19968;&#31181;&#31216;&#20026;JetConv &#30340;&#26032;&#31574;&#30053;&#65292;&#23427;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#23427;&#38598;&#25104;&#20102;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JetSeg&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a challenging task that requires high-accuracy models with low-inference times. Implementing these models on embedded systems is limited by hardware capability and memory usage, which produces bottlenecks. We propose an efficient model for real-time semantic segmentation called JetSeg, consisting of an encoder called JetNet, and an improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and includes two main components: a new light-weight efficient block called JetBlock, that reduces the number of parameters minimizing memory usage and inference time without sacrificing accuracy; a new strategy that involves the combination of asymmetric and non-asymmetric convolutions with depthwise-dilated convolutions called JetConv, a channel shuffle operation, light-weight activation functions, and a convenient number of group convolutions for embedded systems, and an innovative loss function named JetLoss, which integrates the Precision, Recall,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.11417</link><description>&lt;p&gt;
&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#30475;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#23519;&#21151;&#33021;&#31561;&#20215;&#30340;&#27010;&#24565;&#26469;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#27010;&#24565;&#34920;&#26126;&#19981;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#21270;&#21487;&#20197;&#23548;&#33268;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#20026;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#31867;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35206;&#30422;&#25968;&#19978;&#30028;&#65292;&#21457;&#29616;&#21033;&#29992;&#35813;&#24615;&#36136;&#21487;&#20197;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#36866;&#24403;&#30340;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#21487;&#20197;&#22686;&#21152;&#20248;&#21270;&#25910;&#25947;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#24448;&#24448;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#21363;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#20250;&#23548;&#33268;&#26377;&#25928;&#21442;&#25968;&#31354;&#38388;&#30340;&#20307;&#31215;&#36235;&#36817;&#20110;&#38646;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11414</link><description>&lt;p&gt;
&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65306;&#29992;&#20110;&#22823;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;BERT&#12289;GPT&#12289;ViT&#21644;CLIP&#65292;&#20294;&#20854;&#20248;&#21270;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
&lt;/p&gt;</description></item><item><title>AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.11408</link><description>&lt;p&gt;
AlignAtt&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#20316;&#20026;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11408
&lt;/p&gt;
&lt;p&gt;
AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#24182;&#24050;&#20174;&#35768;&#22810;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#27880;&#24847;&#21147;&#22312;&#36755;&#20837;&#25991;&#26412;&#34987;&#26367;&#25442;&#20026;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#26159;&#33719;&#21462;&#26377;&#20851;&#21333;&#35789;&#23545;&#40784;&#30340;&#26377;&#29992;&#20449;&#24687;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20363;&#22914;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignAtt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;ST&#65288;SimulST&#65289;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#20449;&#24687;&#26469;&#29983;&#25104;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25351;&#23548;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MuST-C v1.0&#30340;8&#31181;&#35821;&#35328;&#23545;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#24212;&#29992;&#20808;&#21069;&#30340;&#26368;&#26032;SimulST&#31574;&#30053;&#65292;AlignAtt&#22312;BLEU&#26041;&#38754;&#33719;&#24471;&#20102;2&#20010;&#20998;&#25968;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;8&#31181;&#35821;&#35328;&#30340;&#24310;&#36831;&#32553;&#20943;&#22312;0.5&#31186;&#21040;0.8&#31186;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11400</link><description>&lt;p&gt;
&#38754;&#21521;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26681;&#25454;cGAN&#30340;&#21028;&#21035;&#22120;&#25968;&#25454;&#35782;&#21035;&#20986;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#24517;&#39035;&#23398;&#20064;&#30446;&#26631;&#27169;&#24335;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#27169;&#24335;&#20146;&#21644;&#21147;&#37327;&#24230;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#23436;&#20840;&#22522;&#20110;cGAN&#30340;&#21028;&#21035;&#22120;&#65292;&#21487;&#20197;&#35782;&#21035;&#26368;&#25509;&#36817;&#30446;&#26631;&#30340;&#29616;&#26377;&#27169;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#22522;&#20110;&#26368;&#25509;&#36817;&#27169;&#24335;&#30340;&#21152;&#26435;&#26631;&#31614;&#26469;&#25193;&#23637;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#39044;&#38450;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;cGAN&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#22238;&#25918;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#30446;&#26631;&#27169;&#24335;&#30340;cGAN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#21508;&#31181;&#26631;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.11391</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20026;&#32456;&#31471;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#21644;&#26377;&#26465;&#29702;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;AI&#30340;&#19968;&#27874;&#26032;&#28909;&#28526;&#12290;&#20026;&#20102;&#24212;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#26412;&#27425;&#35843;&#26597;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;LLM&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23558;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20256;&#32479;&#36719;&#20214;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65288;V&#65286;V&#65289;&#25216;&#26415;&#65292;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;LLM&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#65292;&#20197;&#25552;&#20379;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#30830;&#20445;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#65306;&#34394;&#20551;&#24615;&#21644;&#35780;&#20272;&#12289;&#39564;&#35777;&#12289;&#36816;&#34892;&#26102;&#30417;&#35270;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#32771;&#34385;&#21040;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&amp;V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20154;&#21147;&#21644;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24314;&#27169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.11390</link><description>&lt;p&gt;
ALT: &#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ALT: An Automatic System for Long Tail Scenario Modeling. (arXiv:2305.11390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20154;&#21147;&#21644;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24314;&#27169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#20154;&#21147;&#36164;&#28304;&#19981;&#36275;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#25216;&#26415;&#65292;&#37319;&#29992;&#20803;&#23398;&#20064;&#21746;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#39044;&#31639;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#31561;&#65292;&#20197;&#25913;&#36827;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#31995;&#32479;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#22810;&#39033;&#20248;&#21270;&#65292;&#24182;&#21152;&#20837;&#20102;&#24517;&#35201;&#30340;&#27169;&#22359;&#65292;&#20351;&#31995;&#32479;&#26356;&#20855;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#20013;&#20851;&#38190;&#27169;&#22359;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of long tail scenario modeling with budget limitation, i.e., insufficient human resources for model training stage and limited time and computing resources for model inference stage. This problem is widely encountered in various applications, yet has received deficient attention so far. We present an automatic system named ALT to deal with this problem. Several efforts are taken to improve the algorithms used in our system, such as employing various automatic machine learning related techniques, adopting the meta learning philosophy, and proposing an essential budget-limited neural architecture search method, etc. Moreover, to build the system, many optimizations are performed from a systematic perspective, and essential modules are armed, making the system more feasible and efficient. We perform abundant experiments to validate the effectiveness of our system and demonstrate the usefulness of the critical modules in our system. Moreover, online r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11389</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization Deep Graph Transformation. (arXiv:2305.11389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#24418;&#20174;&#19968;&#31181;&#27169;&#24335;&#36716;&#21464;&#20026;&#21478;&#19968;&#31181;&#27169;&#24335;&#30340;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#37325;&#35201;&#21644;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#24320;&#21457;&#20808;&#36827;&#30340;&#22270;&#24418;&#36716;&#25442;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21487;&#29992;&#30340;&#22270;&#24418;&#30340;&#39046;&#22495;&#27867;&#21270;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#24403;&#35757;&#32451;&#25152;&#26377;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#32452;&#21512;&#26102;&#30340;&#26497;&#31471;&#31354;&#38388;&#22797;&#26434;&#24230;&#12289;&#65288;2&#65289;&#36755;&#20837;&#21644;&#36755;&#20986;&#27169;&#24335;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#24046;&#24322;&#65292;&#20197;&#21450;(3)&#22914;&#20309;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#30446;&#26631;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#36755;&#20837;-&#22810;&#36755;&#20986;&#12289;&#36229;&#32593;&#32476;&#30340;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65288;MultiHyperGNN&#65289;&#65292;&#23427;&#21033;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#32534;&#30721;&#36755;&#20837;&#21644;&#36755;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#36755;&#20986;&#22270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#25429;&#25417;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#26469;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#20989;&#25968;&#65292;&#35777;&#26126;&#28145;&#24230;&#23398;&#20064;&#20013;ReLU&#28608;&#27963;&#19979;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#65292;&#25361;&#25112;&#20102;&#23545;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#29702;&#35770;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11387</link><description>&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#20844;&#27491;&#20043;&#22768;
&lt;/p&gt;
&lt;p&gt;
Justices for Information Bottleneck Theory. (arXiv:2305.11387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#20989;&#25968;&#65292;&#35777;&#26126;&#28145;&#24230;&#23398;&#20064;&#20013;ReLU&#28608;&#27963;&#19979;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#65292;&#25361;&#25112;&#20102;&#23545;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#29702;&#35770;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#29702;&#35770;&#19981;&#26029;&#22686;&#21152;&#30340;&#36136;&#30097;&#20570;&#20986;&#21450;&#26102;&#22238;&#24212;&#65292;&#27880;&#20837;&#26032;&#30340;&#35266;&#28857;&#20197;&#32416;&#27491;&#35823;&#35299;&#24182;&#37325;&#30003;&#20854;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36741;&#21161;&#20989;&#25968;&#65292;&#23558;&#26368;&#22823;&#32534;&#30721;&#29575;&#38477;&#20302;&#27861;&#35299;&#37322;&#20026;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#29305;&#27530;&#20294;&#23616;&#37096;&#26368;&#20248;&#24773;&#20917;&#12290;&#36890;&#36807;&#36825;&#31181;&#36741;&#21161;&#20989;&#25968;&#65292;&#25105;&#20204;&#28548;&#28165;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#20013;&#24212;&#29992;ReLU&#28608;&#27963;&#26102;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#36741;&#21161;&#20989;&#25968;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;IB&#29702;&#35770;&#35299;&#37322;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#22312;&#38544;&#34255;&#23618;&#20013;&#27809;&#26377;&#21387;&#32553;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#20102;&#23545;IB&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35266;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;IB&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#23454;&#39564;&#35777;&#25454;&#30456;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26159;&#23545;IB&#29702;&#35770;&#30340;&#20844;&#27491;&#20043;&#22768;&#65292;&#20063;&#20026;&#26410;&#26469;&#35813;&#29702;&#35770;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21069;&#30651;&#24615;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, poten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#21307;&#30103;AI&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#21435;&#20559;&#35265;&#21644;&#20844;&#24179;&#32858;&#21512;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#20351;&#21307;&#30103;&#26426;&#26500;&#21487;&#20197;&#26377;&#25928;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.11386</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#39640;AI&#27169;&#22411;&#20844;&#24179;&#24615;&#65306;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods. (arXiv:2305.11386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#21307;&#30103;AI&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#21435;&#20559;&#35265;&#21644;&#20844;&#24179;&#32858;&#21512;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#20351;&#21307;&#30103;&#26426;&#26500;&#21487;&#20197;&#26377;&#25928;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;AI&#24037;&#20855;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#36890;&#24120;&#20248;&#20808;&#20110;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21307;&#30103;&#26426;&#26500;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65288;FL&#65289;&#21512;&#20316;&#26469;&#32531;&#35299;&#20559;&#35265;&#38382;&#39064;&#65288;&#36825;&#26159;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#36873;&#25321;&#65289;&#12290;&#34429;&#28982;&#20808;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;FL&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#22522;&#26412;&#27169;&#22411;&#21644;&#26412;&#22320;&#23454;&#26045;&#25216;&#26415;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#65292;&#20173;&#28982;&#24191;&#27867;&#26410;&#32463;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;FL&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25239;&#21435;&#20559;&#35265;&#21644;&#20844;&#24179;&#32858;&#21512;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#30830;&#22320;&#32531;&#35299;&#20102;&#20559;&#35265;&#20316;&#20026;&#20248;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#36824;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models' overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#20248;&#21270;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#24179;&#21488;&#25910;&#30410;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22522;&#20110;&#22238;&#25253;&#30340;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20004;&#31181;&#21512;&#21516;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11381</link><description>&lt;p&gt;
&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning in a Creator Economy. (arXiv:2305.11381v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#20248;&#21270;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#20013;&#30340;&#24179;&#21488;&#25910;&#30410;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22522;&#20110;&#22238;&#25253;&#30340;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20004;&#31181;&#21512;&#21516;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#38761;&#26032;&#20102;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#33719;&#21462;&#21033;&#28070;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21019;&#20316;&#32773;&#32463;&#27982;&#23398;&#24314;&#27169;&#20026;&#29992;&#25143;&#12289;&#24179;&#21488;&#21644;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#19977;&#26041;&#21338;&#24328;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#24212;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#21516;&#21644;&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#24179;&#21488;&#25910;&#30410;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#21512;&#21516;&#65306;&#22522;&#20110;&#22238;&#25253;&#30340;&#21512;&#21516;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creator economy has revolutionized the way individuals can profit through online platforms. In this paper, we initiate the study of online learning in the creator economy by modeling the creator economy as a three-party game between the users, platform, and content creators, with the platform interacting with the content creator under a principal-agent model through contracts to encourage better content. Additionally, the platform interacts with the users to recommend new content, receive an evaluation, and ultimately profit from the content, which can be modeled as a recommender system.  Our study aims to explore how the platform can jointly optimize the contract and recommender system to maximize the utility in an online learning fashion. We primarily analyze and compare two families of contracts: return-based contracts and feature-based contracts. Return-based contracts pay the content creator a fraction of the reward the platform gains. In contrast, feature-based contracts pay 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31934;&#24230;&#30697;&#38453;&#65288;GPM&#65289;&#29992;&#20110;&#25551;&#36848;&#25152;&#26377;&#25968;&#25454;&#31867;&#22411;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#21464;&#37327;&#20043;&#38388;&#30340;&#19968;&#33324;&#21151;&#33021;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#22270;&#26102;&#65292;&#20351;&#29992;&#20102;&#32479;&#19968;&#30340;&#27491;&#21017;&#21270;&#24471;&#20998;&#21305;&#37197;&#26694;&#26550;&#20197;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11379</link><description>&lt;p&gt;
&#24191;&#20041;&#31934;&#24230;&#30697;&#38453;&#29992;&#20110;&#21487;&#20280;&#32553;&#20272;&#35745;&#38750;&#21442;&#25968;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks. (arXiv:2305.11379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31934;&#24230;&#30697;&#38453;&#65288;GPM&#65289;&#29992;&#20110;&#25551;&#36848;&#25152;&#26377;&#25968;&#25454;&#31867;&#22411;&#30340;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#21464;&#37327;&#20043;&#38388;&#30340;&#19968;&#33324;&#21151;&#33021;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#22270;&#26102;&#65292;&#20351;&#29992;&#20102;&#32479;&#19968;&#30340;&#27491;&#21017;&#21270;&#24471;&#20998;&#21305;&#37197;&#26694;&#26550;&#20197;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#32473;&#20986;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#32467;&#26500;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#29305;&#23450;&#30340;&#20998;&#24067;&#26063;&#65288;&#20363;&#22914;&#25351;&#25968;&#26063;&#65289;&#21644;/&#25110;&#29305;&#23450;&#30340;&#22270;&#32467;&#26500;&#65292;&#32780;&#19988;&#22823;&#22810;&#25968;&#21482;&#33021;&#22788;&#29702;&#21516;&#19968;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#21464;&#37327;&#65288;&#36830;&#32493;&#25110;&#31163;&#25955;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#31934;&#24230;&#30697;&#38453;&#65288;GPM&#65289;&#22312;&#25152;&#26377;&#25968;&#25454;&#31867;&#22411;&#65288;&#21363;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31867;&#22411;&#65289;&#30340;&#19968;&#33324;&#20998;&#24067;&#20013;&#25551;&#36848;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;&#21464;&#37327;&#20043;&#38388;&#30340;&#19968;&#33324;&#21151;&#33021;&#20851;&#31995;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#26368;&#19968;&#33324;&#30340;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#38382;&#39064;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22270;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#24773;&#20917;&#32479;&#19968;&#21040;&#19968;&#20010;&#27491;&#21017;&#21270;&#24471;&#20998;&#21305;&#37197;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#28436;&#31034;&#20102;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Markov network characterizes the conditional independence structure, or Markov property, among a set of random variables. Existing work focuses on specific families of distributions (e.g., exponential families) and/or certain structures of graphs, and most of them can only handle variables of a single data type (continuous or discrete). In this work, we characterize the conditional independence structure in general distributions for all data types (i.e., continuous, discrete, and mixed-type) with a Generalized Precision Matrix (GPM). Besides, we also allow general functional relations among variables, thus giving rise to a Markov network structure learning algorithm in one of the most general settings. To deal with the computational challenge of the problem, especially for large graphs, we unify all cases under the same umbrella of a regularized score matching framework. We validate the theoretical results and demonstrate the scalability empirically in various settings.
&lt;/p&gt;</description></item><item><title>GraphFC&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11377</link><description>&lt;p&gt;
GraphFC: &#24102;&#26377;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphFC: Customs Fraud Detection with Label Scarcity. (arXiv:2305.11377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11377
&lt;/p&gt;
&lt;p&gt;
GraphFC&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30340;&#28023;&#20851;&#23448;&#21592;&#27599;&#24180;&#38754;&#23545;&#30528;&#28023;&#37327;&#30340;&#20132;&#26131;&#12290;&#38543;&#30528;&#36830;&#36890;&#24615;&#21644;&#20840;&#29699;&#21270;&#30340;&#22686;&#21152;&#65292;&#28023;&#20851;&#20132;&#26131;&#25345;&#32493;&#22686;&#38271;&#12290;&#19982;&#28023;&#20851;&#20132;&#26131;&#30456;&#20851;&#30340;&#26159;&#28023;&#20851;&#27450;&#35784;&#8212;&#8212;&#21363;&#26377;&#24847;&#20462;&#25913;&#36135;&#29289;&#30003;&#25253;&#20197;&#36991;&#20813;&#31246;&#27454;&#21644;&#20851;&#31246;&#12290;&#30001;&#20110;&#20154;&#25163;&#19981;&#36275;&#65292;&#28023;&#20851;&#21150;&#20844;&#23460;&#21482;&#33021;&#23545;&#26377;&#38480;&#25968;&#37327;&#30340;&#30003;&#25253;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26032;&#36827;&#30003;&#25253;&#26816;&#26597;&#30340;&#26631;&#31614;&#25968;&#25454;&#21463;&#38480;&#65292;ML&#26041;&#27861;&#24212;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;$\textbf{GraphFC}$&#65288;&#28023;&#20851;&#27450;&#35784;$\textbf{GNN}$&#65288;$\textbf{G}$raph $\textbf{N}$eural $\textbf{N}$etworks&#65289;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;$\textbf{GraphFC}$&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#28023;&#20851;&#20132;&#26131;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#28023;&#20851;&#27450;&#35784;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Custom officials across the world encounter huge volumes of transactions. With increased connectivity and globalization, the customs transactions continue to grow every year. Associated with customs transactions is the customs fraud - the intentional manipulation of goods declarations to avoid the taxes and duties. With limited manpower, the custom offices can only undertake manual inspection of a limited number of declarations. This necessitates the need for automating the customs fraud detection by machine learning (ML) techniques. Due the limited manual inspection for labeling the new-incoming declarations, the ML approach should have robust performance subject to the scarcity of labeled data. However, current approaches for customs fraud detection are not well suited and designed for this real-world setting. In this work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for $\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific, semi-supervised graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11367</link><description>&lt;p&gt;
&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#29992;&#20110;&#20154;&#31867;&#30561;&#30496;&#23039;&#21183;&#21644;&#21160;&#24577;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition. (arXiv:2305.11367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#35843;&#21307;&#30103;&#20445;&#20581;&#12289;&#26089;&#26399;&#25945;&#32946;&#21644;&#20581;&#36523;&#26041;&#38754;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38750;&#20405;&#20837;&#24335;&#27979;&#37327;&#21644;&#35782;&#21035;&#26041;&#27861;&#21463;&#21040;&#20851;&#27880;&#12290;&#21387;&#21147;&#24863;&#24212;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#12289;&#26131;&#20110;&#35775;&#38382;&#12289;&#21487;&#35270;&#21270;&#24212;&#29992;&#21644;&#26080;&#23475;&#24615;&#32780;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#25935;&#26448;&#26009;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;(SP e-Mat)&#31995;&#32479;&#65292;&#29992;&#20110;&#20154;&#20307;&#30417;&#27979;&#24212;&#29992;&#65292;&#21253;&#25324;&#30561;&#30496;&#23039;&#21183;&#12289;&#36816;&#21160;&#21644;&#29788;&#20285;&#35782;&#21035;&#12290;&#22312;&#23376;&#31995;&#32479;&#25195;&#25551;&#30005;&#23376;&#22443;&#35835;&#25968;&#24182;&#22788;&#29702;&#20449;&#21495;&#21518;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21387;&#21147;&#22270;&#20687;&#27969;&#12290;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26469;&#25311;&#21512;&#21644;&#35757;&#32451;&#21387;&#21147;&#22270;&#20687;&#27969;&#65292;&#24182;&#35782;&#21035;&#30456;&#24212;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#22235;&#31181;&#30561;&#30496;&#23039;&#21183;&#21644;&#21463;Nintendo Switch Ring Fit Adventure(RFA)&#21551;&#21457;&#30340;&#20116;&#31181;&#21160;&#24577;&#27963;&#21160;&#34987;&#29992;&#20316;&#25311;&#35758;&#30340;SPeM&#31995;&#32479;&#30340;&#21021;&#27493;&#39564;&#35777;&#12290;SPeM&#31995;&#32479;&#22312;&#20004;&#31181;&#24212;&#29992;&#20013;&#22343;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#35777;&#26126;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emphasis on healthcare, early childhood education, and fitness, non-invasive measurement and recognition methods have received more attention. Pressure sensing has been extensively studied due to its advantages of simple structure, easy access, visualization application, and harmlessness. This paper introduces a smart pressure e-mat (SPeM) system based on a piezoresistive material Velostat for human monitoring applications, including sleeping postures, sports, and yoga recognition. After a subsystem scans e-mat readings and processes the signal, it generates a pressure image stream. Deep neural networks (DNNs) are used to fit and train the pressure image stream and recognize the corresponding human behavior. Four sleeping postures and five dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are used as a preliminary validation of the proposed SPeM system. The SPeM system achieves high accuracies on both applications, which demonstrates the high accuracy and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11362</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#29289;&#21697;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20445;&#25252;&#20080;&#26041;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#23427;&#19982;&#20080;&#26041;&#30340;&#36755;&#20837;&#23545;&#65288;&#21830;&#21697;&#36873;&#25321;&#21644;&#20986;&#20215;&#65289;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;(regret)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25351;&#25968;&#26435;&#37325;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#25200;&#21160;&#32531;&#35299;&#20102;&#25910;&#30410;&#20989;&#25968;&#19981;&#36830;&#32493;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#19982;&#25351;&#25968;&#26426;&#21046;&#30340;&#32467;&#26500;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22266;&#26377;&#22320;&#20445;&#35777;&#20102;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#36880;&#36718;&#31574;&#30053;&#24615;&#20986;&#20215;&#30340;&#24773;&#20917;&#12290;&#20869;&#22312;&#30340;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26368;&#23567;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#30830;&#20445;&#20854;&#20111;&#25439;&#23376;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#26696;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#24341;&#20837;&#36328;&#35821;&#35328;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#36866;&#37197;&#20013;&#65292;&#20351;&#29992;&#25945;&#24072;-&#23398;&#29983;&#38598;&#25104;&#21644;&#27531;&#24046;&#36866;&#37197;&#22120;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;DP&#20445;&#35777;&#65292;&#20351;&#24471;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;97.5%&#19988;&#24615;&#33021;&#22522;&#26412;&#19981;&#21463;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11360</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#36866;&#37197;&#22120;&#19982;&#21442;&#25968;&#39640;&#25928;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Adapters for Parameter Efficient Acoustic Modeling. (arXiv:2305.11360v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#26696;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#24341;&#20837;&#36328;&#35821;&#35328;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#36866;&#37197;&#20013;&#65292;&#20351;&#29992;&#25945;&#24072;-&#23398;&#29983;&#38598;&#25104;&#21644;&#27531;&#24046;&#36866;&#37197;&#22120;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;DP&#20445;&#35777;&#65292;&#20351;&#24471;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;97.5%&#19988;&#24615;&#33021;&#22522;&#26412;&#19981;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#26696;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#24341;&#20837;&#36328;&#35821;&#35328;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#36866;&#37197;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#20923;&#32467;&#39044;&#35757;&#32451;&#36866;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25345;DP&#20445;&#25252;&#35821;&#38899;&#24314;&#27169;&#65292;&#26080;&#38656;&#20840;&#27169;&#22411;&#24494;&#35843;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26377;&#22122;&#22768;&#30340;&#25945;&#24072;-&#23398;&#29983;&#38598;&#25104;&#24341;&#20837;&#20256;&#32479;&#30340;&#36866;&#37197;&#26041;&#26696;&#20013;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#27604;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;DP&#65288;DPSGD&#65289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;&#23618;&#20043;&#38388;&#25554;&#20837;&#27531;&#24046;&#36866;&#37197;&#22120;&#65288;RA&#65289;&#12290;RA&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#19979;&#38477;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#22312;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#21475;&#35821;&#35789;&#35821;&#65288;MLSW&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#20351;&#29992;RA&#38477;&#20302;&#20102;97.5%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#32780;&#19982;&#24494;&#35843;&#36328;&#35821;&#35328;&#35821;&#38899;&#20998;&#31867;&#22120;&#30456;&#27604;&#20165;&#26377;4%&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#21516;&#26102;&#20445;&#25345;DP&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pre-trained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#21033;&#29702;&#24615;&#20195;&#29702;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#32676;&#20307;&#22312;&#22788;&#29702;&#21487;&#33021;&#20986;&#29616;&#30340;&#31038;&#20250;&#22256;&#22659;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#23637;&#31034;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#26469;&#35299;&#20915;&#31038;&#20250;&#22256;&#22659;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.11358</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29702;&#35299;&#19990;&#30028;&#20197;&#35299;&#20915;&#31038;&#20250;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning. (arXiv:2305.11358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#21033;&#29702;&#24615;&#20195;&#29702;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#32676;&#20307;&#22312;&#22788;&#29702;&#21487;&#33021;&#20986;&#29616;&#30340;&#31038;&#20250;&#22256;&#22659;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#23637;&#31034;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#26469;&#35299;&#20915;&#31038;&#20250;&#22256;&#22659;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#22256;&#22659;&#26159;&#19968;&#31181;&#24773;&#20917;&#65292;&#20010;&#20307;&#22242;&#20307;&#20043;&#38388;&#33021;&#22815;&#20174;&#30456;&#20114;&#21512;&#20316;&#20013;&#33719;&#30410;&#65292;&#20294;&#20914;&#31361;&#30340;&#21033;&#30410;&#38459;&#30861;&#20102;&#20182;&#20204;&#30340;&#21512;&#20316;&#12290;&#36825;&#31181;&#24773;&#20917;&#31867;&#20284;&#20110;&#20154;&#31867;&#35768;&#22810;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20419;&#36827;&#21512;&#20316;&#34892;&#20026;&#20986;&#29616;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#33258;&#21033;&#29702;&#24615;&#20195;&#29702;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#20849;&#23384;&#20110;&#21487;&#33021;&#20986;&#29616;&#31038;&#20250;&#22256;&#22659;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#32676;&#20307;&#22312;&#22788;&#29702;&#21487;&#33021;&#20986;&#29616;&#31038;&#20250;&#22256;&#22659;&#30340;&#24773;&#20917;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#32467;&#26500;&#23450;&#24615;&#35780;&#20272;&#25152;&#23398;&#21160;&#24577;&#65292;&#24182;&#30830;&#35748;&#27599;&#20010;&#20195;&#29702;&#30340;&#19990;&#30028;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#21464;&#21270;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#34892;&#21160;&#20449;&#24687;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#23637;&#31034;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#26469;&#35299;&#20915;&#31038;&#20250;&#22256;&#22659;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social dilemmas are situations where groups of individuals can benefit from mutual cooperation but conflicting interests impede them from doing so. This type of situations resembles many of humanity's most critical challenges, and discovering mechanisms that facilitate the emergence of cooperative behaviors is still an open problem. In this paper, we study the behavior of self-interested rational agents that learn world models in a multi-agent reinforcement learning (RL) setting and that coexist in environments where social dilemmas can arise. Our simulation results show that groups of agents endowed with world models outperform all the other tested ones when dealing with scenarios where social dilemmas can arise. We exploit the world model architecture to qualitatively assess the learnt dynamics and confirm that each agent's world model is capable to encode information of the behavior of the changing environment and the other agent's actions. This is the first work that shows that wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;CATE&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#24182;&#20351;&#29992;&#38381;&#24335;&#27714;&#35299;&#22120;&#33719;&#24471;&#21442;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#20248;&#21270;CATE&#20272;&#35745;&#34920;&#29616;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.11353</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#24335;&#27714;&#35299;&#22120;&#30340;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-learning for heterogeneous treatment effect estimation with closed-form solvers. (arXiv:2305.11353v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;CATE&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#24182;&#20351;&#29992;&#38381;&#24335;&#27714;&#35299;&#22120;&#33719;&#24471;&#21442;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#20248;&#21270;CATE&#20272;&#35745;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#20174;&#22810;&#20010;&#20219;&#21153;&#20013;&#20272;&#35745;CATE&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#36827;&#34892;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;CATE&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20219;&#21153;&#20849;&#20139;&#21644;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#20272;&#35745;&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20844;&#24335;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#21487;&#24494;&#20998;&#30340;&#38381;&#24335;&#30340;&#26368;&#20248;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#36825;&#20123;&#21442;&#25968;&#33021;&#22815;&#30456;&#23545;&#20110;&#20219;&#21153;&#20849;&#20139;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#20803;&#23398;&#20064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20219;&#21153;&#20849;&#20139;&#21442;&#25968;&#65292;&#20197;&#20351;&#23569;&#31034;&#28857;&#35774;&#32622;&#19979;&#30340;CATE&#20272;&#35745;&#34920;&#29616;&#36890;&#36807;&#23558;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#20272;&#35745;&#30340;CATE&#19982;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#20272;&#35745;&#30340;CATE&#20043;&#38388;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#24471;&#21040;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CATE&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11351</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#20256;&#32479;&#30340;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#37325;&#26032;&#35757;&#32451;&#12289;&#36807;&#28388;&#25110;&#32534;&#36753;&#65307;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#20250;&#34987;&#31532;&#19977;&#26041;&#22238;&#36991;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#22914;&#20309;&#21518;&#26399;&#32534;&#36753;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#20854;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36825;&#26159;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#33021;&#29992;&#20110;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#25968;&#25454;&#32534;&#36753;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#30417;&#30563;&#12289;&#36328;&#39046;&#22495;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#25216;&#26415;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11349</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#24369;&#20449;&#21495;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#30417;&#30563;&#12289;&#36328;&#39046;&#22495;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#25216;&#26415;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20316;&#20026;&#20154;&#20204;&#33719;&#21462;&#26032;&#38395;&#30340;&#20027;&#35201;&#24179;&#21488;&#20043;&#19968;&#30340;&#20986;&#29616;&#65292;&#20419;&#36827;&#20102;&#34394;&#20551;&#26032;&#38395;&#30340;&#24191;&#27867;&#20256;&#25773;&#12290;&#36825;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#33258;&#21160;&#21270;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#19968;&#20123;&#26410;&#32463;&#30417;&#30563;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#23581;&#35797;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26410;&#21033;&#29992;&#19982;&#26032;&#38395;&#35760;&#24405;&#30456;&#20851;&#30340;&#21508;&#31181;&#27169;&#24577;&#30693;&#35782;&#21644;&#29616;&#26377;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#28508;&#22312;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#39318;&#20808;&#23558;&#26032;&#38395;&#35760;&#24405;&#20013;&#22235;&#31181;&#27169;&#24577;&#30340;&#30693;&#35782;&#23884;&#20837;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22122;&#22768;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#35782;&#21035;&#26032;&#38395;&#35760;&#24405;&#30340;&#30495;&#23454;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26500;&#24314;&#26032;&#38395;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#29616;&#26377;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25353;&#29031;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#21046;&#20316;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of social media as one of the main platforms for people to access news has enabled the wide dissemination of fake news. This has motivated numerous studies on automating fake news detection. Although there have been limited attempts at unsupervised fake news detection, their performance suffers due to not exploiting the knowledge from various modalities related to news records and due to the presence of various latent biases in the existing news datasets. To address these limitations, this work proposes an effective framework for unsupervised fake news detection, which first embeds the knowledge available in four modalities in news records and then proposes a novel noise-robust self-supervised learning technique to identify the veracity of news records from the multi-modal embeddings. Also, we propose a novel technique to construct news datasets minimizing the latent biases in existing news datasets. Following the proposed approach for dataset construction, we produce a L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11348</link><description>&lt;p&gt;
&#20197;&#20844;&#24179;&#21517;&#20041;&#65306;&#35780;&#20272;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#23545;&#20110;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21512;&#27861;&#20849;&#20139;&#20020;&#24202;&#25968;&#25454;&#38656;&#35201;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#21024;&#38500;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36825;&#20010;&#36807;&#31243;&#65292;&#31216;&#20026;&#21435;&#35782;&#21035;&#65292;&#36890;&#24120;&#36890;&#36807;&#35768;&#22810;&#21830;&#19994;&#21644;&#24320;&#28304;&#31995;&#32479;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30340;&#26816;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#21517;&#31216;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;16&#20010;&#21517;&#31216;&#38598;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#32500;&#24230;&#65306;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#21517;&#31216;&#27969;&#34892;&#24230;&#21644;&#27969;&#34892;&#30340;&#21313;&#24180;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21517;&#31216;&#25554;&#20837;&#21040;100&#20010;&#25163;&#21160;&#31579;&#36873;&#30340;&#20020;&#24202;&#27169;&#26495;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#20061;&#31181;&#20844;&#20849;&#21644;&#31169;&#20154;&#21435;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#21517;&#31216;&#26041;&#38754;&#23384;&#22312;&#32479;&#35745;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22810;&#20809;&#35889;&#25968;&#25454;&#19981;&#33021;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.11347</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#20809;&#35889;&#20998;&#21106;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#21644;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning. (arXiv:2305.11347v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22810;&#20809;&#35889;&#25968;&#25454;&#19981;&#33021;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33322;&#31354;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;RGB&#36890;&#36947;&#20043;&#22806;&#65292;&#21253;&#25324;&#26356;&#22810;&#20809;&#35889;&#27874;&#27573;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39069;&#22806;&#25968;&#25454;&#32435;&#20837;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#33258;&#28982;&#25200;&#21160;&#30340;&#25269;&#25239;&#21147;&#22914;&#20309;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#26088;&#22312;&#34920;&#24449;&#22810;&#20809;&#35889;&#65288;RGB&#21644;&#36817;&#32418;&#22806;&#65289;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#33258;&#28982;&#25200;&#21160;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#22810;&#20809;&#35889;&#25968;&#25454;&#33021;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#33021;&#25552;&#39640;&#20854;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In overhead image segmentation tasks, including additional spectral bands beyond the traditional RGB channels can improve model performance. However, it is still unclear how incorporating this additional data impacts model robustness to adversarial attacks and natural perturbations. For adversarial robustness, the additional information could improve the model's ability to distinguish malicious inputs, or simply provide new attack avenues and vulnerabilities. For natural perturbations, the additional information could better inform model decisions and weaken perturbation effects or have no significant influence at all. In this work, we seek to characterize the performance and robustness of a multispectral (RGB and near infrared) image segmentation model subjected to adversarial attacks and natural perturbations. While existing adversarial and natural robustness research has focused primarily on digital perturbations, we prioritize on creating realistic perturbations designed with physi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;BR-RCRL&#65292;&#23427;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#37325;&#21442;&#25968;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22870;&#21169;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26679;&#26412;&#22806;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11340</link><description>&lt;p&gt;
&#33021;&#37327;&#27169;&#22411;&#19979;&#30340;&#36125;&#21494;&#26031;&#37325;&#21442;&#25968;&#21270;&#22870;&#21169;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models. (arXiv:2305.11340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11340
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;BR-RCRL&#65292;&#23427;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#37325;&#21442;&#25968;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22870;&#21169;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26679;&#26412;&#22806;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#28789;&#27963;&#21644;&#31163;&#32447;&#31574;&#30053;&#29305;&#24615;&#65292;&#22870;&#21169;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;RCRL&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#24403;&#21069;&#30340;RCRL&#26041;&#27861;&#23384;&#22312;&#26681;&#26412;&#24615;&#23616;&#38480;&#24615;&#65292;&#24182;&#26410;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#30340;RCRL&#25361;&#25112; - &#22914;&#20309;&#25913;&#21892;&#39640;&#22870;&#21169;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#22914;&#20309;&#36991;&#20813;&#27979;&#35797;&#26399;&#38388;&#30340;&#26679;&#26412;&#22806;&#22870;&#21169;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#37325;&#21442;&#25968;&#21270;RCRL&#65288;BR-RCRL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;RCRL&#24402;&#32435;&#20559;&#32622;&#35774;&#35745;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#36125;&#21494;&#26031;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reward-conditioned reinforcement learning (RCRL) has gained popularity due to its simplicity, flexibility, and off-policy nature. However, we will show that current RCRL approaches are fundamentally limited and fail to address two critical challenges of RCRL -- improving generalization on high reward-to-go (RTG) inputs, and avoiding out-of-distribution (OOD) RTG queries during testing time. To address these challenges when training vanilla RCRL architectures, we propose Bayesian Reparameterized RCRL (BR-RCRL), a novel set of inductive biases for RCRL inspired by Bayes' theorem. BR-RCRL removes a core obstacle preventing vanilla RCRL from generalizing on high RTG inputs -- a tendency that the model treats different RTG inputs as independent values, which we term ``RTG Independence". BR-RCRL also allows us to design an accompanying adaptive inference method, which maximizes total returns while avoiding OOD queries that yield unpredictable behaviors in vanilla RCRL methods. We s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.11311</link><description>&lt;p&gt;
BELLA: &#36890;&#36807;&#26412;&#22320;&#32447;&#24615;&#36924;&#36817;&#36827;&#34892;&#40657;&#30418;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
BELLA: Black box model Explanations by Local Linear Approximations. (arXiv:2305.11311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29702;&#35299;&#40657;&#30418;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#20165;&#25104;&#20026;&#27861;&#24459;&#35201;&#27714;&#65292;&#20063;&#25104;&#20026;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#21478;&#19968;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#24182;&#21487;&#33021;&#25439;&#23475;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204; tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;
&lt;p&gt;
In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;</description></item><item><title>AMII&#26159;&#19968;&#31181;&#38754;&#37096;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#24577;&#35760;&#24518;&#32534;&#30721;&#27169;&#24335;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#25105;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#36866;&#24212;&#24615;&#22320;&#26174;&#31034;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.11310</link><description>&lt;p&gt;
AMII&#65306;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#20154;&#38469;&#21644;&#33258;&#25105;&#27169;&#22411;&#29992;&#20110;&#34892;&#20026;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis. (arXiv:2305.11310v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11310
&lt;/p&gt;
&lt;p&gt;
AMII&#26159;&#19968;&#31181;&#38754;&#37096;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#24577;&#35760;&#24518;&#32534;&#30721;&#27169;&#24335;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#25105;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#36866;&#24212;&#24615;&#22320;&#26174;&#31034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#20132;&#20114;&#20195;&#29702;&#65288;SIAs&#65289;&#26159;&#26174;&#31034;&#19982;&#20154;&#31867;&#22810;&#27169;&#24577;&#34892;&#20026;&#31867;&#20284;&#30340;&#23454;&#20307;&#25110;&#34394;&#25311;&#21270;&#36523;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#24314;&#27169;SIAs&#30340;&#38750;&#35821;&#35328;&#34892;&#20026;&#65288;&#22914;&#35821;&#38899;&#21644;&#38754;&#37096;&#25163;&#21183;&#65289;&#65292;&#22987;&#32456;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;SIA&#21487;&#20197;&#25198;&#28436;&#21457;&#35328;&#20154;&#25110;&#21548;&#20247;&#30340;&#35282;&#33394;&#12290;SIA&#22312;&#20004;&#31181;&#35282;&#33394;&#19979;&#37117;&#24517;&#39035;&#21457;&#20986;&#36866;&#24403;&#30340;&#34892;&#20026;&#65292;&#20197;&#36866;&#24212;&#20854;&#33258;&#24049;&#30340;&#35821;&#38899;&#12289;&#20854;&#20197;&#21069;&#30340;&#34892;&#20026;&#65288;&#33258;&#25105;&#65289;&#20197;&#21450;&#29992;&#25143;&#30340;&#34892;&#20026;&#65288;&#20154;&#38469;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMII&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;SIAs&#19982;&#29992;&#25143;&#20132;&#20114;&#24182;&#20132;&#26367;&#25198;&#28436;&#21457;&#35328;&#20154;&#25110;&#21548;&#20247;&#26102;&#21512;&#25104;&#33258;&#36866;&#24212;&#38754;&#37096;&#25163;&#21183;&#12290;AMII&#30340;&#29305;&#28857;&#26159;&#27169;&#24577;&#35760;&#24518;&#32534;&#30721;&#27169;&#24335;&#65292;&#20854;&#20013;&#27169;&#24577;&#23545;&#24212;&#20110;&#35821;&#38899;&#25110;&#38754;&#37096;&#25163;&#21183;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#33258;&#25105;&#21644;&#20154;&#38469;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23458;&#35266;&#35780;&#20272;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Socially Interactive Agents (SIAs) are physical or virtual embodied agents that display similar behavior as human multimodal behavior. Modeling SIAs' non-verbal behavior, such as speech and facial gestures, has always been a challenging task, given that a SIA can take the role of a speaker or a listener. A SIA must emit appropriate behavior adapted to its own speech, its previous behaviors (intra-personal), and the User's behaviors (inter-personal) for both roles. We propose AMII, a novel approach to synthesize adaptive facial gestures for SIAs while interacting with Users and acting interchangeably as a speaker or as a listener. AMII is characterized by modality memory encoding schema - where modality corresponds to either speech or facial gestures - and makes use of attention mechanisms to capture the intra-personal and inter-personal relationships. We validate our approach by conducting objective evaluations and comparing it with the state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.11304</link><description>&lt;p&gt;
pTSE:&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#24182;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#36873;&#25321;&#39640;&#24230;&#20381;&#36182;&#20110;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#22522;&#20110;&#30340;&#22266;&#23450;&#20998;&#24067;&#12290;&#30001;&#20110;&#27010;&#29575;&#20998;&#24067;&#19981;&#33021;&#30452;&#25509;&#24179;&#22343;&#19981;&#21516;&#27169;&#22411;&#65292;&#30446;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;pTSE&#65292;&#19968;&#31181;&#22522;&#20110;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#22810;&#27169;&#22411;&#20998;&#24067;&#38598;&#25104;&#26041;&#27861;&#12290;pTSE&#21482;&#38656;&#20174;&#25104;&#21592;&#27169;&#22411;&#33719;&#21462;&#29616;&#25104;&#36755;&#20986;&#65292;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#20102;&#35299;&#27599;&#20010;&#27169;&#22411;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;pTSE&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26102;&#38388;&#24207;&#21015;&#32463;HMM&#22788;&#29702;&#21518;&#30340;&#32463;&#39564;&#20998;&#24067;&#20960;&#20046;&#19968;&#23450;&#25910;&#25947;&#20110;&#31283;&#24577;&#20998;&#24067;&#12290;&#22522;&#20934;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#21333;&#20010;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#26032;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;pTSE&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various probabilistic time series forecasting models have sprung up and shown remarkably good performance. However, the choice of model highly relies on the characteristics of the input time series and the fixed distribution that the model is based on. Due to the fact that the probability distributions cannot be averaged over different models straightforwardly, the current time series model ensemble methods cannot be directly applied to improve the robustness and accuracy of forecasting. To address this issue, we propose pTSE, a multi-model distribution ensemble method for probabilistic forecasting based on Hidden Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models without requiring further information about each model. Besides, we provide a complete theoretical analysis of pTSE to prove that the empirical distribution of time series subject to an HMM will converge to the stationary distribution almost surely. Experiments on benchmarks show the superiority of p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31934;&#24230;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#33021;&#37327;&#65292;&#35813;&#26041;&#27861;&#23558;&#39640;&#31934;&#24230;&#30340;&#25968;&#25454;&#19982;&#25104;&#26412;&#26356;&#20302;&#12289;&#31934;&#24230;&#26356;&#20302;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11292</link><description>&lt;p&gt;
&#22810;&#31934;&#24230;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#28608;&#21457;&#24577;&#33021;&#37327;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Machine Learning for Excited State Energies of Molecules. (arXiv:2305.11292v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31934;&#24230;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#33021;&#37327;&#65292;&#35813;&#26041;&#27861;&#23558;&#39640;&#31934;&#24230;&#30340;&#25968;&#25454;&#19982;&#25104;&#26412;&#26356;&#20302;&#12289;&#31934;&#24230;&#26356;&#20302;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#28608;&#21457;&#24577;&#33021;&#37327;&#30340;&#20934;&#30830;&#20294;&#24555;&#36895;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35838;&#39064;&#12290;&#20026;&#20102;&#35768;&#22810;&#24212;&#29992;&#65292;&#23545;&#20110;&#36739;&#22823;&#20998;&#23376;&#32858;&#21512;&#29289;&#20013;&#30340;&#33021;&#37327;&#28431;&#26007;&#30340;&#35814;&#32454;&#20102;&#35299;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#38656;&#35201;&#39640;&#31934;&#24230;&#30340;&#28608;&#21457;&#24577;&#33021;&#37327;&#12290;&#20026;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#26159;&#19968;&#20010;&#26497;&#20854;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#23613;&#31649;&#29983;&#25104;&#39640;&#31934;&#24230;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#31934;&#24230;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22823;&#37327;&#26469;&#33258;&#39640;&#31934;&#24230;&#30340;&#35757;&#32451;&#25968;&#25454;&#19982;&#25104;&#26412;&#26356;&#20302;&#12289;&#31934;&#24230;&#26356;&#20302;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#29992;&#26469;&#39044;&#27979;&#19977;&#31181;&#20998;&#23376;&#30340;&#31532;&#19968;&#28608;&#21457;&#24577;&#33021;&#37327;&#65292;&#20998;&#21035;&#20026;&#33519;&#12289;&#33816;&#21644;&#33981;&#65292;&#24182;&#38024;&#23545;&#32463;&#20856;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#23454;&#26102;&#23494;&#24230;&#27867;&#20989;&#32039;&#26463;&#32538;&#25130;&#26029;&#20013;&#30340;&#26500;&#35937;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate but fast calculation of molecular excited states is still a very challenging topic. For many applications, detailed knowledge of the energy funnel in larger molecular aggregates is of key importance requiring highly accurate excited state energies. To this end, machine learning techniques can be an extremely useful tool though the cost of generating highly accurate training datasets still remains a severe challenge. To overcome this hurdle, this work proposes the use of multi-fidelity machine learning where very little training data from high accuracies is combined with cheaper and less accurate data to achieve the accuracy of the costlier level. In the present study, the approach is employed to predict the first excited state energies for three molecules of increasing size, namely, benzene, naphthalene, and anthracene. The energies are trained and tested for conformations stemming from classical molecular dynamics simulations and from real-time density functional tight-bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.11290</link><description>&lt;p&gt;
&#35895;&#27468;&#22320;&#22270;&#20013;&#30340;&#22823;&#35268;&#27169;&#21487;&#25193;&#23637;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#28508;&#22312;&#20559;&#22909;&#26159;&#36335;&#32447;&#25512;&#33616;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#20840;&#29699;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#30340;&#30740;&#31350;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21019;&#24314;&#20102;&#36234;&#26469;&#36234;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23578;&#26410;&#25104;&#21151;&#25193;&#23637;&#21040;&#19990;&#30028;&#35268;&#27169;&#30340;MDP&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#28041;&#21450;&#25968;&#20159;&#20010;&#29366;&#24577;&#12289;&#36712;&#36857;&#21644;&#21442;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25913;&#36827;&#65292;&#32858;&#28966;&#20110;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#65292;&#31361;&#30772;&#20197;&#24448;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36870;&#21521;&#35268;&#21010;&#36882;&#36827;&#22320;&#24179;&#38754;(RHIP)&#65292;&#23427;&#21487;&#20197;&#27010;&#25324;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#20854;&#35268;&#21010;&#27700;&#24179;&#25511;&#21046;&#20851;&#38190;&#24615;&#33021;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;16-24%&#30340;&#25913;&#36827;&#65292;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#23454;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#26368;&#22823;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#22909;&#30340;&#23548;&#33322;&#34892;&#20026;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#20998;&#31867;&#22120;&#29992;&#20110;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20869;&#22312;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#26041;&#24335;&#65292;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11288</link><description>&lt;p&gt;
Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#29992;&#20110;SPD&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Riemannian Multiclass Logistics Regression for SPD Neural Networks. (arXiv:2305.11288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#20998;&#31867;&#22120;&#29992;&#20110;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20869;&#22312;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#26041;&#24335;&#65292;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23398;&#20064;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;SPD&#32593;&#32476;&#20351;&#29992;&#20256;&#32479;&#30340;&#27431;&#20960;&#37324;&#24471;&#20998;&#31867;&#22120;&#22312;&#36817;&#20284;&#31354;&#38388;&#19978;&#32780;&#19981;&#26159;&#22312;&#20934;&#30830;&#25429;&#25417;SPD&#27969;&#24418;&#20960;&#20309;&#30340;&#20869;&#22312;&#20998;&#31867;&#22120;&#19978;&#12290;&#21463;&#36229;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#65288;HNN&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Riemannian&#22810;&#31867;Logistic&#22238;&#24402;&#65288;RMLR&#65289;&#29992;&#20110;SPD&#32593;&#32476;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;SPD&#27969;&#24418;&#19978;&#19968;&#26063;Riemannian&#24230;&#37327;&#30340;&#36890;&#29992;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#23450;&#30340; $\orth{n}$-&#19981;&#21464;&#30340;Log-Euclidean Metrics&#36866;&#29992;&#20110;SPD&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;SPD&#32593;&#32476;&#20013;&#26368;&#27969;&#34892;&#30340;&#20998;&#31867;&#22120;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#27969;&#34892;&#30340;SPD&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#20998;&#31867;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks for learning symmetric positive definite (SPD) matrices are gaining increasing attention in machine learning. Despite the significant progress, most existing SPD networks use traditional Euclidean classifiers on approximated spaces rather than intrinsic classifiers that accurately capture the geometry of SPD manifolds. Inspired by the success of hyperbolic neural networks (HNNs), we propose Riemannian multiclass logistics regression (RMLR) for SPD networks. We introduce a general unified framework for a family of Riemannian metrics on SPD manifolds and showcase the specific $\orth{n}$-invariant Log-Euclidean Metrics for SPD networks. Moreover, we encompass the most popular classifier in existing SPD networks as a special case of our framework. Extensive experiments on popular SPD learning benchmarks demonstrate the superiority of our classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11284</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#23433;&#20840;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35828;&#35805;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#24085;&#37329;&#26862;&#30149;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20005;&#26684;&#30340;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#38459;&#30861;&#20102;&#26426;&#26500;&#38388;&#20849;&#20139;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19981;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#31561;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25552;&#39640;&#29289;&#20307;&#20013;&#24515;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11281</link><description>&lt;p&gt;
SlotDiffusion: &#22522;&#20110;Diffusion&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. (arXiv:2305.11281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25552;&#39640;&#29289;&#20307;&#20013;&#24515;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#29992;&#23545;&#35937;&#23454;&#20307;&#65288;&#20063;&#31216;&#20026;&#27133;&#65289;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#25552;&#20379;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#21270;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#26088;&#22312;&#25552;&#39640;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#26159;&#19968;&#31181;&#26082;&#21487;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#21448;&#21487;&#29992;&#20110;&#35270;&#39057;&#25968;&#25454;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#30001;&#20110;LDM&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#65292;SlotDiffusion&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#20998;&#21106;&#21644;&#35270;&#35273;&#29983;&#25104;&#26041;&#38754;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#25512;&#26029;&#31070;&#32463;&#36712;&#36857;&#21450;&#20854;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#20219;&#24847;&#20284;&#28982;&#65292;&#21516;&#26102;&#26377;&#25928;&#36319;&#36394;&#31070;&#32463;&#20803;&#20013;&#38041;&#25104;&#20687;&#25968;&#25454;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.11278</link><description>&lt;p&gt;
&#23454;&#26102;&#21464;&#20998;&#26041;&#27861;&#23398;&#20064;&#31070;&#32463;&#36712;&#36857;&#21450;&#20854;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Real-Time Variational Method for Learning Neural Trajectory and its Dynamics. (arXiv:2305.11278v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#25512;&#26029;&#31070;&#32463;&#36712;&#36857;&#21450;&#20854;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#20219;&#24847;&#20284;&#28982;&#65292;&#21516;&#26102;&#26377;&#25928;&#36319;&#36394;&#31070;&#32463;&#20803;&#20013;&#38041;&#25104;&#20687;&#25968;&#25454;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#20013;&#24050;&#25104;&#20026;&#25512;&#29702;&#31070;&#32463;&#35745;&#31639;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#36825;&#20419;&#36827;&#20102;&#20174;&#31070;&#32463;&#35760;&#24405;&#20013;&#25552;&#21462;&#28508;&#22312;&#31070;&#32463;&#36712;&#36857;&#30340;&#24378;&#22823;&#31163;&#32447;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23454;&#26102;&#26367;&#20195;&#26041;&#26696;&#33021;&#22815;&#20026;&#23454;&#39564;&#32773;&#31435;&#21363;&#25552;&#20379;&#21453;&#39304;&#24182;&#22686;&#24378;&#23454;&#39564;&#35774;&#35745;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24471;&#21040;&#30340;&#20851;&#27880;&#35201;&#23569;&#24471;&#22810;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25351;&#25968;&#26063;&#21464;&#20998;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;eVKF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#36882;&#24402;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#26029;&#28508;&#22312;&#36712;&#36857;&#21516;&#26102;&#23398;&#20064;&#20135;&#29983;&#23427;&#20204;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;eVKF&#36866;&#29992;&#20110;&#20219;&#24847;&#20284;&#28982;&#65292;&#24182;&#21033;&#29992;&#24120;&#25968;&#22522;&#26412;&#27979;&#24230;&#25351;&#25968;&#26063;&#26469;&#27169;&#25311;&#28508;&#22312;&#29366;&#24577;&#30340;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39044;&#27979;&#27493;&#39588;&#30340;&#38381;&#24335;&#21464;&#20998;&#31867;&#27604;&#65292;&#23427;&#27604;&#21478;&#19968;&#31181;&#22312;&#32447;&#21464;&#20998;&#26041;&#27861;&#20135;&#29983;&#20102;&#21487;&#35777;&#26126;&#26356;&#32039;&#30340;ELBO&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#36319;&#36394;&#31070;&#32463;&#20803;&#20013;&#38041;&#25104;&#20687;&#25968;&#25454;&#30340;&#21160;&#24577;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent variable models have become instrumental in computational neuroscience for reasoning about neural computation. This has fostered the development of powerful offline algorithms for extracting latent neural trajectories from neural recordings. However, despite the potential of real time alternatives to give immediate feedback to experimentalists, and enhance experimental design, they have received markedly less attention. In this work, we introduce the exponential family variational Kalman filter (eVKF), an online recursive Bayesian method aimed at inferring latent trajectories while simultaneously learning the dynamical system generating them. eVKF works for arbitrary likelihoods and utilizes the constant base measure exponential family to model the latent state stochasticity. We derive a closed-form variational analogue to the predict step of the Kalman filter which leads to a provably tighter bound on the ELBO compared to another online variational method. We validate our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.11271</link><description>&lt;p&gt;
&#38754;&#21521;&#24773;&#22659;&#23545;&#35805;&#20013;&#30340;&#24515;&#26234;&#24314;&#27169;&#65292;&#23454;&#29616;&#21327;&#21516;&#35745;&#21010;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20219;&#21153;&#36890;&#24120;&#22987;&#20110;&#21452;&#26041;&#25317;&#26377;&#19981;&#23436;&#20840;&#30340;&#20219;&#21153;&#30693;&#35782;&#21644;&#19981;&#23436;&#25972;&#30340;&#21021;&#22987;&#35745;&#21010;&#12290;&#20026;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#19982;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#23454;&#22320;&#20132;&#27969;&#65292;&#24182;&#21327;&#35843;&#20182;&#20204;&#30340;&#37096;&#20998;&#35745;&#21010;&#20197;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30446;&#26631;&#12290;&#34429;&#28982;&#36825;&#31181;&#21327;&#20316;&#22312;&#20154;&#19982;&#20154;&#30340;&#22242;&#38431;&#20013;&#20284;&#20046;&#36731;&#32780;&#26131;&#20030;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#20316;&#26469;&#35828;&#21364;&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#20195;&#29702;&#20154;&#21162;&#21147;&#23398;&#20064;&#24182;&#30456;&#20114;&#20132;&#27969;&#65292;&#20197;&#33719;&#21462;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#12290;&#20855;&#20307;&#22320;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#35753;&#20195;&#29702;&#20154;&#22522;&#20110;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19977;&#32500;&#26041;&#22359;&#19990;&#30028;&#30340;&#23545;&#31216;&#21327;&#20316;&#20219;&#21153;&#20013;&#25193;&#23637;&#20102;&#19968;&#20010;&#24773;&#22659;&#23545;&#35805;&#22522;&#20934;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#21010;&#33719;&#21462;&#30340;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#20219;&#21153;&#30693;&#35782;&#26159;&#35745;&#21010;&#33719;&#21462;&#36807;&#31243;&#20013;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29615;&#22659;&#35270;&#20026;&#20915;&#31574;&#21464;&#37327;&#65292;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#29615;&#22659;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#20248;&#20808;&#32423;&#22312;&#29615;&#22659;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.11260</link><description>&lt;p&gt;
&#20026;&#20248;&#20808;&#25490;&#24207;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#23454;&#29616;&#32422;&#26463;&#29615;&#22659;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Environment Optimization for Prioritized Multi-Agent Navigation. (arXiv:2305.11260v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29615;&#22659;&#35270;&#20026;&#20915;&#31574;&#21464;&#37327;&#65292;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#29615;&#22659;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#20248;&#20808;&#32423;&#22312;&#29615;&#22659;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#35748;&#20026;&#29615;&#22659;&#26159;&#22266;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20294;&#26159;&#31354;&#38388;&#32422;&#26463;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#26377;&#24433;&#21709;&#12290;&#25163;&#21160;&#35774;&#35745;&#36866;&#21512;&#29615;&#22659;&#30340;&#24067;&#23616;&#25928;&#29575;&#20302;&#19979;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23558;&#29615;&#22659;&#35270;&#20026;&#31995;&#32479;&#32423;&#20248;&#21270;&#38382;&#39064;&#30340;&#20915;&#31574;&#21464;&#37327;&#65292;&#32771;&#34385;&#26234;&#33021;&#20307;&#24615;&#33021;&#21644;&#29615;&#22659;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#20248;&#20808;&#32423;&#29615;&#22659;&#20248;&#21270;&#21644;&#20248;&#20808;&#32423;&#29615;&#22659;&#20248;&#21270;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#21069;&#32773;&#22312;&#20844;&#27491;&#22320;&#32771;&#34385;&#26234;&#33021;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#32773;&#32771;&#34385;&#26234;&#33021;&#20307;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#35777;&#26126;&#23637;&#31034;&#20102;&#22312;&#20445;&#35777;&#23436;&#25972;&#24615;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#33021;&#21040;&#36798;&#30446;&#26631;&#65289;&#65292;&#29615;&#22659;&#21487;&#20197;&#22914;&#20309;&#25913;&#21464;&#65292;&#24182;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#20248;&#20808;&#32423;&#22312;&#29615;&#22659;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#29615;&#22659;&#20248;&#21270;&#26045;&#21152;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#25968;&#23398;&#21270;&#20026;&#24102;&#32422;&#26463;&#30340;&#29615;&#22659;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches to the design of multi-agent navigation algorithms consider the environment as a fixed constraint, despite the influence of spatial constraints on agents' performance. Yet hand-designing conducive environment layouts is inefficient and potentially expensive. The goal of this paper is to consider the environment as a decision variable in a system-level optimization problem, where both agent performance and environment cost are incorporated. Towards this end, we propose novel problems of unprioritized and prioritized environment optimization, where the former considers agents unbiasedly and the latter accounts for agent priorities. We show, through formal proofs, under which conditions the environment can change while guaranteeing completeness (i.e., all agents reach goals), and analyze the role of agent priorities in the environment optimization. We proceed to impose real-world constraints on the environment optimization and formulate it mathematically as a constr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#25214;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.11252</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;: &#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired learning in artificial neural networks: a review. (arXiv:2305.11252v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#25214;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24037;&#20855;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#12289;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#20316;&#26426;&#21046;&#19982;&#29983;&#29289;&#22823;&#33041;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#23588;&#20854;&#26159;&#23398;&#20064;&#36807;&#31243;&#26041;&#38754;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25506;&#35752;&#20102;&#25972;&#21512;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#26426;&#21046;&#65288;&#22914;&#31361;&#35302;&#21487;&#22609;&#24615;&#65289;&#20197;&#25552;&#39640;&#36825;&#20123;&#32593;&#32476;&#33021;&#21147;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#25214;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs' operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to enhance these networks' capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. Ultimately, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35777;&#25454;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#19982;&#23884;&#22871;&#25277;&#26679;&#26080;&#27861;&#32988;&#20219;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#12289;&#26356;&#26377;&#25928;&#22320;&#20272;&#31639;&#36125;&#21494;&#26031;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.11241</link><description>&lt;p&gt;
&#35777;&#25454;&#32593;&#32476;&#65306;&#29992;&#31616;&#21333;&#30340;&#25439;&#22833;&#20989;&#25968;&#24555;&#36895;&#12289;&#20998;&#25674;&#24335;&#22320;&#36827;&#34892;&#31070;&#32463;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison. (arXiv:2305.11241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35777;&#25454;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#19982;&#23884;&#22871;&#25277;&#26679;&#26080;&#27861;&#32988;&#20219;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#12289;&#26356;&#26377;&#25928;&#22320;&#20272;&#31639;&#36125;&#21494;&#26031;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#32593;&#32476;&#21487;&#22312;&#24403;&#29616;&#26377;&#30340;&#26041;&#27861;&#65288;&#22914;&#23884;&#22871;&#25277;&#26679;&#65289;&#22833;&#36133;&#12289;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#38590;&#20197;&#22788;&#29702;&#25110;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#21487;&#30475;&#20316;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#29992;&#36125;&#21494;&#26031;&#27861;&#36827;&#34892;&#26368;&#20248;&#20998;&#31867;&#30340;&#35299;&#37322;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25913;&#21464;&#20102;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20135;&#29983;&#24555;&#36895;&#12289;&#20998;&#25674;&#24335;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#30452;&#25509;&#20272;&#31639;&#26041;&#20415;&#30340;&#36125;&#21494;&#26031;&#22240;&#23376;&#30340;&#20989;&#25968;&#12290;&#36825;&#20943;&#23569;&#20102;&#20272;&#31639;&#21333;&#20010;&#27169;&#22411;&#27010;&#29575;&#26102;&#30340;&#25968;&#23383;&#19981;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28183;&#28431;&#22855; parity-odd power&#65288;l-POP&#65289;&#21464;&#25442;&#65292;&#24341;&#23548;&#20102;&#26032;&#30340;&#8220;l-Pop-Exponential&#8221;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#23545;&#25968;&#25454;&#27010;&#29575;&#36827;&#34892;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#35777;&#25454;&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#37117;&#35201;&#20302;&#12290;&#22810;&#31181;&#23454;&#38469;&#21644;&#20154;&#36896;&#20363;&#23376;&#35777;&#26126;&#20102;&#35777;&#25454;&#32593;&#32476;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence Networks can enable Bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. Bayesian model comparison, i.e. the computation of Bayes factors or evidence ratios, can be cast as an optimization problem. Though the Bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the Bayes factor. This mitigates numerical inaccuracies associated with estimating individual model probabilities. We introduce the leaky parity-odd power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss function. We explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than Evidence Networks. Multiple real-world and synthetic examples illustrate that Evidence Networks are e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#26377;&#25928;&#30340;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#31446;&#30452;&#25968;&#25454;&#38598;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#22823;&#37327;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11236</link><description>&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#30340;&#39640;&#25928;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Vertical Federated Learning with Secure Aggregation. (arXiv:2305.11236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#26377;&#25928;&#30340;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#31446;&#30452;&#25968;&#25454;&#38598;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#22823;&#37327;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#27700;&#24179;&#20998;&#21306;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20013;&#23458;&#25143;&#20849;&#20139;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#24182;&#21487;&#20197;&#29420;&#31435;&#22320;&#35757;&#32451;&#23436;&#25972;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#38382;&#39064;&#20013;&#65292;&#20363;&#22914;&#37329;&#34701;&#27450;&#35784;&#21644;&#30142;&#30149;&#26816;&#27979;&#65292;&#20010;&#21035;&#25968;&#25454;&#28857;&#25955;&#33853;&#22312;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#21516;&#23458;&#25143;/&#32452;&#32455;&#20013;&#12290;&#38024;&#23545;&#36825;&#31181;FL&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#24456;&#23569;&#32771;&#34385;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#65292;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#36827;&#34892;&#31446;&#21521;FL&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21516;&#24577;&#21152;&#23494;(HE)&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#20102;9.1e2~3.8e4&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, such as financial fraud detection and disease detection, individual data points are scattered across different clients/organizations in vertical federated learning. Solutions for this type of FL require the exchange of gradients between participants and rarely consider privacy and security concerns, posing a potential risk of privacy leakage. In this work, we present a novel design for training vertical FL securely and efficiently using state-of-the-art security modules for secure aggregation. We demonstrate empirically that our method does not impact training performance whilst obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#25490;&#24207;&#29942;&#39048;&#65288;IOB&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#25968;&#25454;&#33258;&#36866;&#24212;&#22320;&#21387;&#32553;&#20026;&#25353;&#39034;&#24207;&#25490;&#21015;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20855;&#26377;&#39640;&#25928;&#21387;&#32553;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#25490;&#24207;&#20449;&#21495;&#24182;&#23545;&#20840;&#23616;&#22266;&#26377;&#32500;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11213</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#30340;&#20449;&#24687;&#25490;&#24207;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Information-Ordered Bottlenecks for Adaptive Semantic Compression. (arXiv:2305.11213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#25490;&#24207;&#29942;&#39048;&#65288;IOB&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#25968;&#25454;&#33258;&#36866;&#24212;&#22320;&#21387;&#32553;&#20026;&#25353;&#39034;&#24207;&#25490;&#21015;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20855;&#26377;&#39640;&#25928;&#21387;&#32553;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#25490;&#24207;&#20449;&#21495;&#24182;&#23545;&#20840;&#23616;&#22266;&#26377;&#32500;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#25490;&#24207;&#29942;&#39048;&#65288;IOB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#23618;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#21487;&#33021;&#24615;&#23545;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#21387;&#32553;&#65292;&#23558;&#20854;&#21387;&#32553;&#25104;&#25353;&#39034;&#24207;&#25490;&#21015;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;IOB&#33410;&#28857;&#21487;&#20197;&#22312;&#20219;&#20309;&#29942;&#39048;&#23485;&#24230;&#22788;&#25130;&#26029;&#65292;&#25429;&#25417;&#21069;&#20960;&#20010;&#28508;&#22312;&#21464;&#37327;&#20013;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#32479;&#19968;&#20960;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;IOB&#23454;&#29616;&#20102;&#38024;&#23545;&#32473;&#23450;&#32534;&#30721;&#20307;&#31995;&#32467;&#26500;&#30340;&#36817;&#20046;&#26368;&#20248;&#21387;&#32553;&#65292;&#24182;&#21487;&#25353;&#21547;&#20041;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#23545;&#28508;&#22312;&#20449;&#21495;&#36827;&#34892;&#25490;&#24207;&#12290;IOB&#23637;&#31034;&#20102;&#21387;&#32553;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#23884;&#20837;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#65288;&#22914;CNN&#12289;transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;IOB&#20272;&#35745;&#20840;&#23616;&#22266;&#26377;&#32500;&#24230;&#30340;&#26032;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#24674;&#22797;&#22797;&#26434;&#21512;&#25104;&#25968;&#25454;&#30340;SOTA&#32500;&#24230;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#24322;&#36136;&#25968;&#25454;&#30340;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the information-ordered bottleneck (IOB), a neural layer designed to adaptively compress data into latent variables ordered by likelihood maximization. Without retraining, IOB nodes can be truncated at any bottleneck width, capturing the most crucial information in the first latent variables. Unifying several previous approaches, we show that IOBs achieve near-optimal compression for a given encoding architecture and can assign ordering to latent signals in a manner that is semantically meaningful. IOBs demonstrate a remarkable ability to compress embeddings of image and text data, leveraging the performance of SOTA architectures such as CNNs, transformers, and diffusion models. Moreover, we introduce a novel theory for estimating global intrinsic dimensionality with IOBs and show that they recover SOTA dimensionality estimates for complex synthetic data. Furthermore, we showcase the utility of these models for exploratory analysis through applications on heterogeneous datas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11206</link><description>&lt;p&gt;
LIMA: &#23545;&#40784;&#30340;&#26356;&#23569;&#21363;&#20026;&#26356;&#20248;&#65288;Less Is More for Alignment&#65289;
&lt;/p&gt;
&lt;p&gt;
LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;(1)&#26080;&#30417;&#30563;&#30340;&#21407;&#22987;&#25991;&#26412;&#39044;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#65307;(2)&#22823;&#35268;&#27169;&#30340;&#25351;&#20196;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#26368;&#32456;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;LIMA&#65292;&#19968;&#20010;&#20351;&#29992;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#20540;&#36827;&#34892;&#30340;65B&#21442;&#25968;LLaMa&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;1000&#20010;&#32463;&#36807;&#31579;&#36873;&#30340;&#25552;&#31034;&#21644;&#22238;&#22797;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#65292;&#34913;&#37327;&#20102;&#36825;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290; LIMA&#34920;&#29616;&#20986;&#20102;&#26497;&#24378;&#30340;&#24615;&#33021;&#65292;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#22914;&#20309;&#36981;&#24490;&#29305;&#23450;&#30340;&#21709;&#24212;&#26684;&#24335;&#65292;&#21253;&#25324;&#20174;&#35268;&#21010;&#26053;&#34892;&#34892;&#31243;&#21040;&#25512;&#27979;&#26367;&#20195;&#21382;&#21490;&#30340;&#22797;&#26434;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20542;&#21521;&#20110;&#33391;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#19968;&#39033;&#25511;&#21046;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#19982;GPT-4&#30456;&#27604;&#65292;LIMA&#30340;&#21709;&#24212;&#22312;43%&#30340;&#24773;&#20917;&#19979;&#31561;&#25928;&#25110;&#20005;&#26684;&#20248;&#20808;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29486;&#32508;&#36848;&#26088;&#22312;&#38416;&#26126;&#26032;&#20852;&#36235;&#21183;&#21644;&#36827;&#23637;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#27169;&#22411;&#22312;&#31995;&#22806;&#34892;&#26143;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#35282;&#33394;&#65292;&#25581;&#31034;&#22914;&#20309;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#23452;&#23621;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11204</link><description>&lt;p&gt;
&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35780;&#20272;&#31995;&#22806;&#34892;&#26143;&#30340;&#23452;&#23621;&#24615;&#65306;&#19968;&#31687;&#32508;&#21512;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Assessing Exoplanet Habitability through Data-driven Approaches: A Comprehensive Literature Review. (arXiv:2305.11204v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29486;&#32508;&#36848;&#26088;&#22312;&#38416;&#26126;&#26032;&#20852;&#36235;&#21183;&#21644;&#36827;&#23637;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#27169;&#22411;&#22312;&#31995;&#22806;&#34892;&#26143;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#35282;&#33394;&#65292;&#25581;&#31034;&#22914;&#20309;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#23452;&#23621;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#30740;&#31350;&#31995;&#22806;&#34892;&#26143;&#20173;&#28982;&#22788;&#20110;&#22825;&#25991;&#23398;&#30740;&#31350;&#30340;&#21069;&#27839;&#65292;&#25361;&#25112;&#30528;&#31185;&#23398;&#23478;&#20204;&#19981;&#26029;&#21019;&#26032;&#21644;&#25913;&#36827;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#22825;&#20307;&#20135;&#29983;&#30340;&#28009;&#28698;&#32780;&#22797;&#26434;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#29486;&#32508;&#36848;&#26088;&#22312;&#38416;&#26126;&#36825;&#19968;&#39046;&#22495;&#20869;&#26032;&#20852;&#36235;&#21183;&#21644;&#36827;&#23637;&#65292;&#29305;&#21035;&#20851;&#27880;&#31995;&#22806;&#34892;&#26143;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#27169;&#22411;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#20043;&#26053;&#22987;&#20110;&#23545;&#35813;&#39046;&#22495;&#20013;15&#31687;&#31934;&#36873;&#30340;&#37325;&#35201;&#35770;&#25991;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#36825;&#20123;&#35770;&#25991;&#21508;&#33258;&#20195;&#34920;&#30528;&#31995;&#22806;&#34892;&#26143;&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20849;&#21516;&#21576;&#29616;&#20102;&#24403;&#21069;&#39046;&#22495;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#21363;&#22914;&#20309;&#21019;&#26032;&#22320;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20811;&#26381;&#20998;&#26512;&#21644;&#35299;&#37322;&#22825;&#25991;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#39044;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#23452;&#23621;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration and study of exoplanets remain at the frontier of astronomical research, challenging scientists to continuously innovate and refine methodologies to navigate the vast, complex data these celestial bodies produce. This literature the review aims to illuminate the emerging trends and advancements within this sphere, specifically focusing on the interplay between exoplanet detection, classification, and visualization, and the the increasingly pivotal role of machine learning and computational models. Our journey through this realm of exploration commences with a comprehensive analysis of fifteen meticulously selected, seminal papers in the field. These papers, each representing a distinct facet of exoplanet research, collectively offer a multi-dimensional perspective on the current state of the field. They provide valuable insights into the innovative application of machine learning techniques to overcome the challenges posed by the analysis and interpretation of astronomi
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#20837;&#38498;&#26102;&#30340;PE&#20107;&#20214;&#21644;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#22312;&#25509;&#21463;&#34880;&#26643;&#39044;&#38450;&#27835;&#30103;&#30340;&#24739;&#32773;&#23376;&#32676;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;PE&#39118;&#38505;&#35780;&#20998;&#31995;&#32479;&#65292;&#20026;&#26089;&#26399;&#35782;&#21035;&#21644;&#31649;&#29702;&#39640;&#21361;&#24739;&#32773;&#25552;&#20379;&#20102;&#26032;&#30340;&#20934;&#30830;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.11199</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;COVID-19&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#21644;&#32954;&#26643;&#22622;&#36827;&#34892;&#20837;&#38498;&#39044;&#27979;&#65306;&#19968;&#39033;&#22269;&#38469;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
At-Admission Prediction of Mortality and Pulmonary Embolism in COVID-19 Patients Using Statistical and Machine Learning Methods: An International Cohort Study. (arXiv:2305.11199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#20837;&#38498;&#26102;&#30340;PE&#20107;&#20214;&#21644;&#27515;&#20129;&#39118;&#38505;&#12290;&#35813;&#27169;&#22411;&#22312;&#25509;&#21463;&#34880;&#26643;&#39044;&#38450;&#27835;&#30103;&#30340;&#24739;&#32773;&#23376;&#32676;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;PE&#39118;&#38505;&#35780;&#20998;&#31995;&#32479;&#65292;&#20026;&#26089;&#26399;&#35782;&#21035;&#21644;&#31649;&#29702;&#39640;&#21361;&#24739;&#32773;&#25552;&#20379;&#20102;&#26032;&#30340;&#20934;&#30830;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25130;&#33267;2022&#24180;9&#26376;&#65292;&#20840;&#29699;&#24050;&#25253;&#21578;&#36229;&#36807;6&#20159;&#22810;&#20363;SARS-CoV-2&#24863;&#26579;&#65292;&#36896;&#25104;&#36229;&#36807;650&#19975;&#20154;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;COVID-19&#27515;&#20129;&#39118;&#38505;&#20272;&#35745;&#22120;&#36890;&#24120;&#26159;&#22522;&#20110;&#23567;&#22411;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#20197;&#21450;&#26041;&#27861;&#23398;&#38480;&#21046;&#32780;&#24320;&#21457;&#30340;&#12290;&#24320;&#21457;COVID-19&#24739;&#32773;&#32954;&#26643;&#22622;&#30340;&#39044;&#27979;&#24037;&#20855;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22269;&#38469;&#38431;&#21015;&#36229;&#36807;80&#19975;COVID-19&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#38498;&#26102;&#30340;PE&#20107;&#20214;&#21644;&#27515;&#20129;&#39118;&#38505;&#12290;&#37319;&#29992;&#36923;&#36753;&#22238;&#24402;&#12289;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21644;Shapley&#20540;&#31561;&#26041;&#27861;&#65292;&#35782;&#21035;PE&#21644;&#27515;&#20129;&#30340;&#20851;&#38190;&#39044;&#27979;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#27979;&#35797;AUROC&#20998;&#21035;&#20026;75.9&#65285;&#21644;74.2&#65285;&#65292;&#24182;&#19988;&#22312;PE&#21644;&#25152;&#26377;&#21407;&#22240;&#30340;&#27515;&#20129;&#26041;&#38754;&#30340;&#25935;&#24863;&#24615;&#20998;&#21035;&#20026;67.5&#65285;&#21644;72.7&#65285;&#12290;PE&#39044;&#27979;&#27169;&#22411;&#36824;&#22312;&#25509;&#21463;&#34880;&#26643;&#39044;&#38450;&#27835;&#30103;&#30340;&#24739;&#32773;&#23376;&#32676;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;PE&#39118;&#38505;&#35780;&#20998;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#20934;&#30830;&#30340;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#22312;&#20837;&#38498;&#26102;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#21644;PE, &#26377;&#21161;&#20110;&#26089;&#26399;&#35782;&#21035;&#21644;&#31649;&#29702;&#39640;&#21361;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
By September, 2022, more than 600 million cases of SARS-CoV-2 infection have been reported globally, resulting in over 6.5 million deaths. COVID-19 mortality risk estimators are often, however, developed with small unrepresentative samples and with methodological limitations. It is highly important to develop predictive tools for pulmonary embolism (PE) in COVID-19 patients as one of the most severe preventable complications of COVID-19. Using a dataset of more than 800,000 COVID-19 patients from an international cohort, we propose a cost-sensitive gradient-boosted machine learning model that predicts occurrence of PE and death at admission. Logistic regression, Cox proportional hazards models, and Shapley values were used to identify key predictors for PE and death. Our prediction model had a test AUROC of 75.9% and 74.2%, and sensitivities of 67.5% and 72.7% for PE and all-cause mortality respectively on a highly diverse and held-out test set. The PE prediction model was also evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#32570;&#22833;&#27169;&#24335;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#30721;&#30340;&#19981;&#21464;&#26368;&#20248;&#39044;&#27979;&#22120;&#23454;&#29616;&#27867;&#21270;&#65292;&#36890;&#36807;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#36991;&#20813;&#25351;&#25968;&#29190;&#28856;&#65292;&#21516;&#26102;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#20445;&#35777;&#39044;&#27979;&#22120;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11197</link><description>&lt;p&gt;
&#22312;&#19981;&#30693;&#36947;&#36974;&#30422;&#20998;&#24067;&#31227;&#20301;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Prediction with Incomplete Data under Agnostic Mask Distribution Shift. (arXiv:2305.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#32570;&#22833;&#27169;&#24335;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#30721;&#30340;&#19981;&#21464;&#26368;&#20248;&#39044;&#27979;&#22120;&#23454;&#29616;&#27867;&#21270;&#65292;&#36890;&#36807;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#36991;&#20813;&#25351;&#25968;&#29190;&#28856;&#65292;&#21516;&#26102;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#20445;&#35777;&#39044;&#27979;&#22120;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#20204;&#23545;&#20165;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#21644;&#25351;&#31034;&#32570;&#22833;&#27169;&#24335;&#30340;&#25513;&#30721;&#30340;&#19981;&#23436;&#25972;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30456;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#34987;&#36829;&#21453;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#30784;&#23436;&#25972;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#19981;&#21464;&#65292;&#20294;&#32570;&#22833;&#27169;&#24335;&#21363;&#25513;&#30721;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#19981;&#30693;&#36947;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#27867;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#23545;&#20110;&#27599;&#20010;&#25513;&#30721;&#65292;&#37117;&#26377;&#19968;&#20010;&#19981;&#21464;&#30340;&#26368;&#20248;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#21333;&#29420;&#23398;&#20064;&#23427;&#20204;&#26102;&#20986;&#29616;&#25351;&#25968;&#29190;&#28856;&#65292;&#22312;&#20351;&#29992;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#12290;&#36825;&#20855;&#26377;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#65292;&#21363;&#20801;&#35768;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#22120;&#23545;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#36827;&#23545;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data with missing values is ubiquitous in many applications. Recent years have witnessed increasing attention on prediction with only incomplete data consisting of observed features and a mask that indicates the missing pattern. Existing methods assume that the training and testing distributions are the same, which may be violated in real-world scenarios. In this paper, we consider prediction with incomplete data in the presence of distribution shift. We focus on the case where the underlying joint distribution of complete features and label is invariant, but the missing pattern, i.e., mask distribution may shift agnostically between training and testing. To achieve generalization, we leverage the observation that for each mask, there is an invariant optimal predictor. To avoid the exponential explosion when learning them separately, we approximate the optimal predictors jointly using a double parameterization technique. This has the undesirable side effect of allowing the learned pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;DClEVerNet&#65292;&#21487;&#20197;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#30340;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.11195</link><description>&lt;p&gt;
DClEVerNet: &#28145;&#24230;&#32452;&#21512;&#23398;&#20064;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#20805;&#30005;&#35774;&#26045;&#30340;&#39640;&#25928;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities. (arXiv:2305.11195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;DClEVerNet&#65292;&#21487;&#20197;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#30340;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20132;&#36890;&#30005;&#27668;&#21270;&#65292;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#30340;&#26222;&#21450;&#21487;&#33021;&#20250;&#26174;&#30528;&#22686;&#21152;&#37197;&#30005;&#32593;&#32476;&#30340;&#21387;&#21147;&#65292;&#23548;&#33268;&#20854;&#24615;&#33021;&#19979;&#38477;&#21644;&#31283;&#23450;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#20197;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#24335;&#23481;&#32435;&#36825;&#20123;&#26032;&#36127;&#36733;&#65292;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#38656;&#35201;&#21327;&#35843;&#25110;&#8220;&#26234;&#33021;&#8221;&#20805;&#30005;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;EV&#20805;&#30005;&#35843;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22823;&#35268;&#27169;&#12289;&#32593;&#32476;&#21270;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26102;&#32806;&#21512;&#30340;&#20108;&#36827;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#39640;&#35299;&#20915;&#36136;&#37327;&#30340;&#21516;&#26102;&#22823;&#35268;&#27169;&#35299;&#20915;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36755;&#20837;&#36755;&#20986;&#22788;&#29702;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the electrification of transportation, the rising uptake of electric vehicles (EVs) might stress distribution networks significantly, leaving their performance degraded and stability jeopardized. To accommodate these new loads cost-effectively, modern power grids require coordinated or ``smart'' charging strategies capable of optimizing EV charging scheduling in a scalable and efficient fashion. With this in view, the present work focuses on reservation management programs for large-scale, networked EV charging stations. We formulate a time-coupled binary optimization problem that maximizes EV users' total welfare gain while accounting for the network's available power capacity and stations' occupancy limits. To tackle the problem at scale while retaining high solution quality, a data-driven optimization framework combining techniques from the fields of Deep Learning and Approximation Algorithms is introduced. The framework's key ingredient is a novel input-output processing schem
&lt;/p&gt;</description></item><item><title>Vaxformer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#26465;&#20214;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25239;&#21407;&#24615;&#21463;&#25511;&#30340;SARS-CoV-2&#21050;&#31361;&#34507;&#30333;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#30123;&#33495;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2305.11194</link><description>&lt;p&gt;
Vaxformer&#65306;&#38024;&#23545;SARS-CoV-2&#30123;&#33495;&#35774;&#35745;&#30340;&#25239;&#21407;&#24615;&#25511;&#21046;Transformer
&lt;/p&gt;
&lt;p&gt;
Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2. (arXiv:2305.11194v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11194
&lt;/p&gt;
&lt;p&gt;
Vaxformer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#26465;&#20214;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25239;&#21407;&#24615;&#21463;&#25511;&#30340;SARS-CoV-2&#21050;&#31361;&#34507;&#30333;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#30123;&#33495;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SARS-CoV-2&#22823;&#27969;&#34892;&#24378;&#35843;&#20102;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#30123;&#33495;&#20197;&#20445;&#25252;&#20813;&#21463;&#24403;&#21069;&#21644;&#26410;&#26469;&#21464;&#31181;&#30149;&#27602;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26465;&#20214;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#65292;&#31216;&#20026;Vaxformer&#65292;&#23427;&#26088;&#22312;&#20135;&#29983;&#22806;&#35266;&#33258;&#28982;&#30340;&#25239;&#21407;&#24615;&#25511;&#21046;&#30340;SARS-CoV-2&#21050;&#31361;&#34507;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;DDGun&#34507;&#30333;&#36136;&#31283;&#23450;&#24615;&#27979;&#37327;&#12289;netMHCpan&#25239;&#21407;&#24615;&#35780;&#20998;&#21644;&#24102;&#26377;AlphaFold&#30340;&#32467;&#26500;&#20445;&#30495;&#24230;&#35780;&#20998;&#35780;&#20272;&#20102;Vaxformer&#27169;&#22411;&#30340;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#20197;&#34913;&#37327;&#20854;&#29992;&#20110;&#30123;&#33495;&#24320;&#21457;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Vaxformer&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#20197;&#29983;&#25104;&#25239;&#21407;&#24615;&#21463;&#25511;&#30340;SARS-CoV-2&#21050;&#31361;&#34507;&#30333;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#26465;&#20214;Transformer&#27169;&#22411;&#20026;&#25193;&#23637;&#25105;&#20204;&#23545;&#30123;&#33495;&#35774;&#35745;&#30340;&#29702;&#35299;&#20197;&#21450;&#23427;&#20204;&#22312;&#32531;&#35299;&#20840;&#29699;&#20581;&#24247;&#25361;&#25112;&#20013;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#12290;&#27492;&#30740;&#31350;&#20351;&#29992;&#30340;&#20195;&#30721;&#21487;&#22312;https://git&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SARS-CoV-2 pandemic has emphasised the importance of developing a universal vaccine that can protect against current and future variants of the virus. The present study proposes a novel conditional protein Language Model architecture, called Vaxformer, which is designed to produce natural-looking antigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generated protein sequences of the Vaxformer model using DDGun protein stability measure, netMHCpan antigenicity score, and a structure fidelity score with AlphaFold to gauge its viability for vaccine development. Our results show that Vaxformer outperforms the existing state-of-the-art Conditional Variational Autoencoder model to generate antigenicity-controlled SARS-CoV-2 spike proteins. These findings suggest promising opportunities for conditional Transformer models to expand our understanding of vaccine design and their role in mitigating global health challenges. The code used in this study is available at https://git
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992; AISecOps &#23545;&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102; AISecOps &#26159;&#22914;&#20309;&#23558; IT &#36816;&#33829;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#39046;&#22495;&#25972;&#21512;&#36215;&#26469;&#21327;&#21516;&#36816;&#20316;&#20197;&#30830;&#20445;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11189</link><description>&lt;p&gt;
&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340; AISecOps &#23041;&#32961;&#24314;&#27169;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots. (arXiv:2305.11189v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992; AISecOps &#23545;&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102; AISecOps &#26159;&#22914;&#20309;&#23558; IT &#36816;&#33829;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#39046;&#22495;&#25972;&#21512;&#36215;&#26469;&#21327;&#21516;&#36816;&#20316;&#20197;&#30830;&#20445;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25216;&#26415;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#21253;&#25324;&#32593;&#32476;&#23433;&#20840;&#12290;&#21307;&#30103;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#38656;&#35201;&#21450;&#26102;&#33719;&#24471;&#21307;&#30103;&#25588;&#21161;&#30340;&#24739;&#32773;&#25552;&#20379;&#26041;&#20415;&#12290;&#30001;&#20110;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#28041;&#21450;&#22823;&#37327;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#20113;&#20027;&#26426;&#36164;&#20135;&#30340;&#26426;&#23494;&#24615;&#65292;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992; AISecOps&#65288;&#20026;&#23433;&#20840; IT &#36816;&#33829;&#32780;&#35774;&#35745;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#23545;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#12290; AISecOps &#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#23558; IT &#36816;&#33829;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#19981;&#21516;&#20294;&#23494;&#20999;&#30456;&#20851;&#30340;&#39046;&#22495;&#25972;&#21512;&#20026;&#19968;&#20010;&#22495;&#65292;&#22312;&#27492;&#22495;&#20013;&#65292;&#26469;&#33258;&#36825;&#19977;&#20010;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#34987;&#21327;&#21516;&#20351;&#29992;&#20197;&#20445;&#25252;&#32593;&#32476;&#23433;&#20840;&#36164;&#20135;&#12290;&#23427;&#32771;&#34385;&#21040;&#20113;&#25805;&#20316;&#21644;&#23433;&#20840;&#24615;&#22240;&#32032;&#65292;&#37319;&#29992;&#32508;&#21512;&#26694;&#26550;&#25910;&#38598;&#35780;&#20272;&#23433;&#20840;&#23041;&#32961;&#25152;&#38656;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35757;&#32451; AI &#27169;&#22411;&#20197;&#31435;&#21363;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is playing a vital role in all aspects of technology including cyber security. Application of Conversational AI like the chatbots are also becoming very popular in the medical field to provide timely and immediate medical assistance to patients in need. As medical chatbots deal with a lot of sensitive information, the security of these chatbots is crucial. To secure the confidentiality, integrity, and availability of cloud-hosted assets like these, medical chatbots can be monitored using AISecOps (Artificial Intelligence for Secure IT Operations). AISecOPs is an emerging field that integrates three different but interrelated domains like the IT operation, AI, and security as one domain, where the expertise from all these three domains are used cohesively to secure the cyber assets. It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11186</link><description>&lt;p&gt;
&#21387;&#32553;&#65292;&#28982;&#21518;&#25552;&#31034;&#65306;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#25913;&#21892;LLM&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#24102;&#26469;&#26174;&#30528;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#35265;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#65288;&#20363;&#22914;&#21333;&#20010;GPU&#65289;&#26102;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21387;&#32553;&#26469;&#26368;&#23567;&#21270;LLM&#25512;&#29702;&#30340;&#24310;&#36831;&#65292;&#21363;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#27492;&#36807;&#31243;&#24517;&#28982;&#24341;&#21457;&#25928;&#29575;&#21644;&#31934;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#22240;&#20026;&#21387;&#32553;&#30340;LLMs&#36890;&#24120;&#20250;&#32463;&#21382;&#39044;&#27979;&#31934;&#24230;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35270;&#35282;&#65306;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24179;&#34913;&#65292;&#21387;&#32553;&#30340;LLMs&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#20110;&#21407;&#22987;&#27169;&#22411;&#30340;&#29420;&#29305;&#36755;&#20837;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#31934;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36716;&#31227;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26377;&#25928;&#25552;&#31034;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#36895;&#24230;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPS&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22320;&#38663;&#20313;&#38663;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#20854;&#39044;&#27979;&#33021;&#21147;&#20381;&#36182;&#20110;GPS&#31449;&#30340;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11183</link><description>&lt;p&gt;
&#21033;&#29992;GPS&#25968;&#25454;&#35780;&#20272;&#22320;&#38663;&#20313;&#38663;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the predicting power of GPS data for aftershocks forecasting. (arXiv:2305.11183v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPS&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22320;&#38663;&#20313;&#38663;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#20854;&#39044;&#27979;&#33021;&#21147;&#20381;&#36182;&#20110;GPS&#31449;&#30340;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;2015&#24180;&#33267;2019&#24180;&#26085;&#26412;&#22320;&#38663;&#30446;&#24405;&#20013;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479;&#65288;GPS&#65289;&#31449;&#22312;&#20027;&#38663;&#21457;&#29983;&#24403;&#22825;&#27979;&#37327;&#21040;&#30340;&#22320;&#38754;&#24418;&#21464;&#20316;&#20026;&#21807;&#19968;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#25429;&#25417;&#36755;&#20837;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#23545;&#22320;&#38663;&#20313;&#38663;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#25968;&#25454;&#37327;&#36866;&#20013;&#65292;&#20294;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#24615;&#33021;&#38750;&#24120;&#26377;&#21069;&#36884;&#12290;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;GPS&#31449;&#30340;&#23494;&#24230;&#65306;&#24403;&#20027;&#38663;&#21457;&#29983;&#22312;&#31163;&#27979;&#37327;&#31449;&#36828;&#30340;&#31163;&#23736;&#22320;&#21306;&#26102;&#65292;&#39044;&#27979;&#33021;&#21147;&#23601;&#20250;&#20007;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a machine learning approach for the aftershock forecasting of Japanese earthquake catalogue from 2015 to 2019. Our method takes as sole input the ground surface deformation as measured by Global Positioning System (GPS) stations at the day of the mainshock, and processes it with a Convolutional Neural Network (CNN), thus capturing the input's spatial correlations. Despite the moderate amount of data the performance of this new approach is very promising. The accuracy of the prediction heavily relies on the density of GPS stations: the predictive power is lost when the mainshocks occur far from measurement stations, as in offshore regions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#24314;&#27169;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11181</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#22686;&#26448;&#21046;&#36896;&#27169;&#22411;&#27604;&#36739;&#65306;&#20197;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Comparison of Transfer Learning based Additive Manufacturing Models via A Case Study. (arXiv:2305.11181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#24314;&#27169;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#22686;&#26448;&#21046;&#36896;&#24314;&#27169;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#20854;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20135;&#21697;&#30340;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#24314;&#27169;&#26032;&#20135;&#21697;&#26102;&#25968;&#25454;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#65292;&#20294;&#26159;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#24314;&#27169;&#26041;&#38754;&#30340;&#22266;&#26377;&#25361;&#25112;&#21364;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#20363;&#22914;&#20351;&#29992;&#21738;&#20010;&#28304;&#22495;&#65292;&#38656;&#35201;&#22810;&#23569;&#30446;&#26631;&#25968;&#25454;&#65292;&#20197;&#21450;&#26159;&#21542;&#24212;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#31561;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#38598;&#21046;&#23450;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#35813;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23558;&#20116;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#19982;&#20915;&#31574;&#26641;&#22238;&#24402;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20845;&#20010;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#22522;&#20934;&#20915;&#31574;&#26641;&#22238;&#24402;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#20010;&#25552;&#20986;&#30340;&#39564;&#35777;&#26694;&#26550;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#27604;&#36739;&#32467;&#26524;&#29992;&#20110;&#37327;&#21270;&#25152;&#24212;&#29992;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20174;&#30456;&#20284;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#35282;&#24230;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) based additive manufacturing (AM) modeling is an emerging field to reuse the data from historical products and mitigate the data insufficiency in modeling new products. Although some trials have been conducted recently, the inherent challenges of applying TL in AM modeling are seldom discussed, e.g., which source domain to use, how much target data is needed, and whether to apply data preprocessing techniques. This paper aims to answer those questions through a case study defined based on an open-source dataset about metal AM products. In the case study, five TL methods are integrated with decision tree regression (DTR) and artificial neural network (ANN) to construct six TL-based models, whose performances are then compared with the baseline DTR and ANN in a proposed validation framework. The comparisons are used to quantify the performance of applied TL methods and are discussed from the perspective of similarity, training data size, and data preprocessing. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33014;&#22218;&#32593;&#32476;&#32467;&#26500;&#30340;&#32570;&#38519;&#65292;&#35777;&#26126;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#38480;&#20110;&#21407;&#22987;&#35774;&#35745;&#65292;&#32780;&#26159;&#23384;&#22312;&#20110;&#35768;&#22810;&#39046;&#20808;&#30340;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;&#36825;&#31181;&#20869;&#22312;&#30340;&#35774;&#35745;&#30456;&#20284;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11178</link><description>&lt;p&gt;
&#28040;&#22833;&#30340;&#28608;&#27963;&#65306;&#28145;&#24230;&#33014;&#22218;&#32593;&#32476;&#30340;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vanishing Activations: A Symptom of Deep Capsule Networks. (arXiv:2305.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33014;&#22218;&#32593;&#32476;&#32467;&#26500;&#30340;&#32570;&#38519;&#65292;&#35777;&#26126;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#38480;&#20110;&#21407;&#22987;&#35774;&#35745;&#65292;&#32780;&#26159;&#23384;&#22312;&#20110;&#35768;&#22810;&#39046;&#20808;&#30340;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;&#36825;&#31181;&#20869;&#22312;&#30340;&#35774;&#35745;&#30456;&#20284;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;&#26159;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#25110;&#30697;&#38453;&#34920;&#31034;&#32780;&#19981;&#26159;&#26631;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#12290;&#26368;&#21021;&#24320;&#21457;&#33014;&#22218;&#32593;&#32476;&#30340;&#30446;&#30340;&#26159;&#21019;&#24314;&#19968;&#20010;&#21160;&#24577;&#35299;&#26512;&#26641;&#65292;&#20854;&#20013;&#35270;&#35273;&#27010;&#24565;&#20174;&#37096;&#20998;&#36880;&#28176;&#21457;&#23637;&#25104;&#20026;&#23436;&#25972;&#30340;&#23545;&#35937;&#12290;&#26089;&#26399;&#30340;&#33014;&#22218;&#32593;&#32476;&#23454;&#29616;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#21644;&#20445;&#25345;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21407;&#22987;&#33014;&#22218;&#32593;&#32476;&#32467;&#26500;&#30340;&#32570;&#38519;&#65292;&#29305;&#21035;&#26159;&#22312;&#26500;&#24314;&#35299;&#26512;&#26641;&#21644;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#26131;&#21463;&#28040;&#22833;&#26799;&#24230;&#30340;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#35843;&#26597;&#19968;&#31995;&#21015;&#39046;&#20808;&#30340;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#65292;&#35777;&#26126;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#38480;&#20110;&#21407;&#22987;&#35774;&#35745;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22823;&#37096;&#20998;&#30340;&#33014;&#22218;&#32593;&#32476;&#30740;&#31350;&#24050;&#32463;&#20135;&#29983;&#20102;&#26550;&#26500;&#65292;&#34429;&#28982;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#22522;&#26412;&#30456;&#20284;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#20869;&#22312;&#30340;&#35774;&#35745;&#30456;&#20284;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks, an extension to Neural Networks utilizing vector or matrix representations instead of scalars, were initially developed to create a dynamic parse tree where visual concepts evolve from parts to complete objects. Early implementations of Capsule Networks achieved and maintain state-of-the-art results on various datasets. However, recent studies have revealed shortcomings in the original Capsule Network architecture, notably its failure to construct a parse tree and its susceptibility to vanishing gradients when deployed in deeper networks. This paper extends the investigation to a range of leading Capsule Network architectures, demonstrating that these issues are not confined to the original design. We argue that the majority of Capsule Network research has produced architectures that, while modestly divergent from the original Capsule Network, still retain a fundamentally similar structure. We posit that this inherent design similarity might be impeding the scalabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11067</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#20869;&#23481;&#20016;&#23500;&#12289;&#25925;&#20107;&#36830;&#36143;&#30340;&#28459;&#30011;
&lt;/p&gt;
&lt;p&gt;
Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#20445;&#25345;&#38899;&#20048;&#23478;&#38899;&#20048;&#39118;&#26684;&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#26410;&#23436;&#25104;&#30340;&#38899;&#20048;&#20316;&#21697;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#28459;&#30011;&#25925;&#20107;&#65292;&#24182;&#20445;&#25345;&#33402;&#26415;&#23478;&#30340;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24773;&#33410;&#21644;&#23545;&#35805;&#65292;&#28982;&#21518;&#20351;&#29992;stable diffusion&#29983;&#25104;&#28459;&#30011;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;AI&#29983;&#25104;&#25925;&#20107;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#23545;stable diffusion&#36827;&#34892;fine-tuning&#65292;&#36798;&#21040;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#26469;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.10869</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Free Lunch for Privacy Preserving Distributed Graph Learning. (arXiv:2305.10869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#26469;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#26426;&#22120;&#20154;&#12289;&#36890;&#20449;&#12289;&#21307;&#23398;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#22270;&#24418;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#20123;&#23646;&#20110;&#19981;&#21516;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#20851;&#38190;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38382;&#39064;&#20351;&#24471;&#24212;&#29992;&#22270;&#24418;&#23398;&#20064;&#21464;&#24471;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#29992;&#25143;&#20391;&#30340;&#29305;&#24449;&#26469;&#39044;&#22788;&#29702;&#25968;&#25454;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#19979;&#19968;&#27493;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#31169;&#20154;&#23646;&#24615;&#36827;&#34892;&#25512;&#26029;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#22270;&#24418;&#23398;&#20064;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#20026;&#20102;&#22312;&#26381;&#21153;&#22120;&#31471;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#19988;&#39640;&#24230;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on graphs is becoming prevalent in a wide range of applications including social networks, robotics, communication, medicine, etc. These datasets belonging to entities often contain critical private information. The utilization of data for graph learning applications is hampered by the growing privacy concerns from users on data sharing. Existing privacy-preserving methods pre-process the data to extract user-side features, and only these features are used for subsequent learning. Unfortunately, these methods are vulnerable to adversarial attacks to infer private attributes. We present a novel privacy-respecting framework for distributed graph learning and graph-based machine learning. In order to perform graph learning and other downstream tasks on the server side, this framework aims to learn features as well as distances without requiring actual features while preserving the original structural properties of the raw data. The proposed framework is quite generic and highly a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10631</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;U-Net&#22312;&#20998;&#21106;&#30452;&#32928;&#30284;&#27835;&#30103;&#26399;&#38388;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#26102;&#65292;&#30001;&#20110;&#22810;&#27425;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#23548;&#33268;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#38169;&#20301;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;MRI&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#22312;&#32534;&#30721;&#20013;&#20351;&#29992;&#20102;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#12290;2&#65289;&#35774;&#35745;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20445;&#25345;U-Net&#30340;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;&#23545;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20043;&#65292;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21487;&#20197;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#20351;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#31283;&#23450;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#19981;&#31283;&#23450;&#24615;&#24182;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10625</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#27979;&#37327;&#21644;&#20943;&#23569;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10625
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#31283;&#23450;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#19981;&#31283;&#23450;&#24615;&#24182;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#20381;&#36182;&#30340;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#26500;&#24314;&#32773;&#24448;&#24448;&#24456;&#38590;&#30830;&#20445;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#20687;&#38543;&#26426;&#21021;&#22987;&#21270;&#36825;&#26679;&#30340;&#26080;&#20851;&#32454;&#33410;&#21487;&#33021;&#24847;&#22806;&#22320;&#25913;&#21464;&#35757;&#32451;&#31995;&#32479;&#30340;&#36755;&#20986;&#65292;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#22312;&#21516;&#19968;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#39044;&#27979;&#20173;&#28982;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#30456;&#24403;&#19968;&#37096;&#20998;&#26597;&#35810;&#30340;&#39044;&#27979;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#65292;&#22914;&#36328;&#35757;&#32451;&#36816;&#34892;&#25110;&#21333;&#27425;&#35757;&#32451;&#20869;&#30340;&#27599;&#20010;&#26679;&#26412;&#30340;&#8220;&#26631;&#31614;&#29109;&#8221;&#65292;&#26469;&#37327;&#21270;&#36825;&#31181;&#29616;&#35937;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10379</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31526;&#21495;&#22238;&#24402;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#26041;&#31243;&#25311;&#21512;&#21040;&#25968;&#25454;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#27905;&#26131;&#25026;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;SR&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#12290;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;SR&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#25552;&#20986;&#19979;&#19968;&#27493;&#23454;&#39564;&#12290;&#29289;&#29702;&#32422;&#26463;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25913;&#21892;&#25152;&#24314;&#35758;&#30340;&#26041;&#31243;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;SR&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#23545;&#29616;&#26377;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#65292;&#38416;&#36848;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.08070</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32852;&#37030;&#35780;&#20272;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Federated Evaluation in Federated Learning. (arXiv:2305.08070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#23545;&#29616;&#26377;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#65292;&#38416;&#36848;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#37117;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#38598;&#20013;&#31649;&#29702;&#65292;&#22240;&#27492;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#31616;&#21333;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#27169;&#22411;&#35780;&#20272;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#34987;&#31216;&#20026;&#26412;&#25991;&#20013;&#30340;&#32852;&#37030;&#35780;&#20272;&#12290;&#36825;&#26159;&#22240;&#20026;&#23458;&#25143;&#31471;&#19981;&#20250;&#20844;&#24320;&#20854;&#21407;&#22987;&#25968;&#25454;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#26368;&#32456;&#36890;&#36807;&#23637;&#26395;&#19968;&#20123;&#25361;&#25112;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional machine learning, it is trivial to conduct model evaluation since all data samples are managed centrally by a server. However, model evaluation becomes a challenging problem in federated learning (FL), which is called federated evaluation in this work. This is because clients do not expose their original data to preserve data privacy. Federated evaluation plays a vital role in client selection, incentive mechanism design, malicious attack detection, etc. In this paper, we provide the first comprehensive survey of existing federated evaluation methods. Moreover, we explore various applications of federated evaluation for enhancing FL performance and finally present future research directions by envisioning some challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.07889</link><description>&lt;p&gt;
&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32467;&#26500;&#24037;&#31243;&#30456;&#32467;&#21512;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29992;&#20110;&#21069;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#27169;&#25311;&#65289;&#21644;&#21453;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65289;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#25552;&#20986;&#20102;VINO&#65288;&#36710;&#36742;-&#26725;&#26753;&#30456;&#20114;&#20316;&#29992;&#31070;&#32463;&#36816;&#31639;&#22120;&#65289;&#65292;&#20316;&#20026;&#26725;&#26753;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;VINO&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36816;&#34892;&#21442;&#25968;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#27169;&#25311;&#65292;&#32771;&#34385;&#32467;&#26500;&#21021;&#22987;&#25439;&#20260;&#22330;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;VBI-FE&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#22312;&#22235;&#31181;&#25439;&#20260;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#20135;&#29983;&#20102;VBI-EXP&#25968;&#25454;&#38598;&#12290;&#22312;VINO&#36890;&#36807;VBI-FE&#39044;&#35757;&#32451;&#24182;&#22312;&#20581;&#24247;&#29366;&#24577;&#19979;&#36890;&#36807;VBI-EXP&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23454;&#29616;&#20102;&#20197;&#19979;&#20004;&#20010;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#21069;&#21521;&#30340;VINO&#27604;FE&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#20174;&#25439;&#20260;&#22330;&#36755;&#20837;&#39044;&#27979;&#32467;&#26500;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#21453;&#21521;&#30340;VINO&#21487;&#20197;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06743</link><description>&lt;p&gt;
&#38024;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#30340;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#38544;&#24335;&#33539;&#25968;&#39044;&#27979;&#22120;&#65288;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#20197;Tsallis&#29109;&#20316;&#20026;prox&#20989;&#25968;&#65289;&#26159;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65288;MAB&#65289;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#22797;&#26434;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#26377;&#30028;&#22870;&#21169;&#25110;&#20854;&#20182;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#26368;&#36817;&#26377;&#20851;&#26368;&#20339;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30340;&#30740;&#31350;&#24050;&#32463;&#38024;&#23545;&#23545;&#25163;&#24615;&#21644;&#38543;&#26426;&#37325;&#23614;MAB&#35774;&#32622;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#20294;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#22870;&#21169;&#20998;&#24067;&#37325;&#23614;&#30340;MAB&#38382;&#39064;&#25552;&#20986;&#20102;&#24102;&#21098;&#36753;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#22312;&#22870;&#21169;&#20998;&#24067;&#19978;&#25552;&#20986;&#28176;&#36827;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37325;&#23614;&#38543;&#26426;MAB&#38382;&#39064;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#26368;&#22909;&#30340;&#20108;&#32773;&#32467;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.04288</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#30340;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#20316;&#26500;&#24314;&#20855;&#26377;&#25552;&#39640;&#25928;&#29992;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#20449;&#24687;&#12290;&#24517;&#39035;&#37319;&#29992;&#36866;&#24403;&#30340;&#20445;&#25252;&#26426;&#21046;&#26469;&#28385;&#36275;&#20445;&#25252;&#38544;&#31169;&#21644;&#32500;&#25252;&#39640;&#27169;&#22411;&#25928;&#29992;&#30340;&#35201;&#27714;&#12290;&#30446;&#21069;&#37319;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#26412;&#36136;&#65292;&#21253;&#25324;&#8220;&#38543;&#26426;&#21270;&#26426;&#21046;&#8221;&#21644;&#8220;&#21387;&#32553;&#26426;&#21046;&#8221;&#65292;&#26159;&#36890;&#36807;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#21644;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#34913;&#37327;&#25928;&#29992;&#12290;&#25105;&#20204;&#24819;&#35201;&#30830;&#23450;&#22312;&#20160;&#20040;&#26222;&#36941;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#36884;&#24452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25928;&#29992;&#25439;&#22833;&#30340;&#19978;&#38480;&#65292;&#29992;&#20004;&#20010;&#20027;&#35201;&#39033;&#31216;&#20026;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2305.02749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#35299;&#37322;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#34892;&#21160;&#21487;&#33021;&#23545;&#26410;&#26469;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#32780;&#19981;&#39044;&#20808;&#30693;&#36947;&#29615;&#22659;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#21160;&#20316;&#30340;&#24433;&#21709;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#38142;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#34892;&#21160;&#26159;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#20302;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#35299;&#37322;&#24615;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.02299</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#22312;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21305;&#37197;&#20102;&#23494;&#38598;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#65292;&#21516;&#26102;&#20351;&#24471;&#31232;&#30095;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#24471;&#21040;&#30340;&#27169;&#22411;&#39640;&#24230;&#31232;&#30095;&#65292;&#29702;&#35770;&#19978;&#35757;&#32451;&#26356;&#20415;&#23452;&#65292;&#20294;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#21152;&#36895;&#20381;&#28982;&#20855;&#26377;&#20154;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181; DST &#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450; N:M &#31232;&#30095;&#26041;&#27861;&#65288;&#24120;&#25968;&#25159;&#20837;&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#32463;&#36807;&#23545; PyTorch CPU &#23454;&#29616;&#30340;&#31616;&#21333;&#34920;&#31034;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/calgaryml/condensed-sparsity &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00510</link><description>&lt;p&gt;
&#36890;&#21521;&#33258;&#30001;&#35745;&#31639;&#26550;&#26500;: &#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#20803;&#23431;&#23449;&#34394;&#25311;&#24314;&#31569;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#24418;&#29366;&#29983;&#25104;&#25216;&#26415;&#27491;&#22312;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24314;&#31569;&#35774;&#35745;&#20004;&#26041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#35843;&#26597;&#21644;&#27604;&#36739;&#24403;&#21069;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;3D&#24863;&#30693;&#22270;&#20687;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;187&#31687;&#25991;&#31456;(&#21344;2018-2022&#24180;&#38388;&#21457;&#34920;&#25991;&#31456;&#30340;80.7%)&#65292;&#20197;&#22238;&#39038;&#22312;&#34394;&#25311;&#29615;&#22659;&#19979;&#24314;&#31569;&#29983;&#25104;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#65292;&#38480;&#20110;&#24314;&#31569;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31569;&#30740;&#31350;&#12289;&#34394;&#25311;&#29615;&#22659;&#21644;&#30456;&#20851;&#25216;&#26415;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#25509;&#30528;&#22238;&#39038;&#20102;&#31163;&#25955;&#20307;&#32032;&#29983;&#25104;&#12289;&#30001;2D&#22270;&#20687;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#20197;&#21450;&#26465;&#20214;&#21442;&#25968;&#30340;&#26368;&#36817;&#36235;&#21183;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;3D&#29983;&#25104;&#21644;&#21442;&#25968;&#21270;&#25511;&#21046;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#21253;&#25324;&#29983;&#25104;&#22810;&#26679;&#24615;&#12289;&#26032;&#22411;&#36755;&#20986;&#21644;&#23884;&#20837;&#24335;&#26500;&#24314;&#31561;&#22235;&#20010;&#30740;&#31350;&#35758;&#31243;&#21487;&#33021;&#20250;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23545;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#30340;&#21464;&#20998;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#23545;&#21442;&#25968;&#24418;&#24335;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12141</link><description>&lt;p&gt;
&#21464;&#20998;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65306;&#20855;&#26377;&#26080;&#26465;&#20214;&#25193;&#25955;&#20808;&#39564;&#30340;&#28145;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior. (arXiv:2304.12141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23545;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#30340;&#21464;&#20998;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#23545;&#21442;&#25968;&#24418;&#24335;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#30340;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22240;&#20026;&#39640;&#24230;&#19981;&#29616;&#23454;&#30340;&#24314;&#27169;&#20551;&#35774;&#65292;&#21363;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;p(x|z)&#21487;&#20197;&#36817;&#20284;&#20026;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#20998;&#24067;&#65292;&#25152;&#20197;&#30001;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#27169;&#31946;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23545;&#26465;&#20214;&#25968;&#25454;&#20998;&#24067;p(x|z)&#36827;&#34892;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#21019;&#24314;&#31867;&#20284;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23545;p(x|z)&#20570;&#39640;&#26031;&#20551;&#35774;&#65292;&#29978;&#33267;&#19981;&#38656;&#35201;&#35757;&#32451;&#35299;&#30721;&#22120;&#32593;&#32476;&#12290;&#36890;&#36807;Bayes'&#35268;&#21017;&#65292;&#21487;&#20197;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#32452;&#21512;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#34920;&#36798;&#20016;&#23500;&#30340;p(x|z)&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#21442;&#25968;&#24418;&#24335;p(x|z)&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65292;&#22240;&#27492;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational auto-encoders (VAEs) are one of the most popular approaches to deep generative modeling. Despite their success, images generated by VAEs are known to suffer from blurriness, due to a highly unrealistic modeling assumption that the conditional data distribution $ p(\textbf{x} | \textbf{z})$ can be approximated as an isotropic Gaussian. In this work we introduce a principled approach to modeling the conditional data distribution $p(\textbf{x} | \textbf{z})$ by incorporating a diffusion model. We show that it is possible to create a VAE-like deep latent variable model without making the Gaussian assumption on $ p(\textbf{x} | \textbf{z}) $ or even training a decoder network. A trained encoder and an unconditional diffusion model can be combined via Bayes' rule for score functions to obtain an expressive model for $ p(\textbf{x} | \textbf{z}) $. Our approach avoids making strong assumptions on the parametric form of $ p(\textbf{x} | \textbf{z}) $, and thus allows to significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#31574;&#30053;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#25298;&#32477;&#26381;&#21153;(Dos)&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#20840;&#23616;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#38544;&#24418;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10783</link><description>&lt;p&gt;
&#25298;&#32477;&#26381;&#21153;&#25110;&#32454;&#31890;&#24230;&#25511;&#21046;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#28789;&#27963;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning. (arXiv:2304.10783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#31574;&#30053;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#25298;&#32477;&#26381;&#21153;(Dos)&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#20840;&#23616;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#38544;&#24418;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#65292;&#25932;&#23545;&#26041;&#20250;&#30772;&#22351;&#20840;&#23616;&#32858;&#21512;&#32467;&#26524;&#24182;&#36896;&#25104;&#25298;&#32477;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;(FMPA)&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#21151;&#33021;&#25915;&#20987;&#30446;&#26631;&#12290;&#26412;&#25991;&#32771;&#34385;&#22914;&#19979;&#23454;&#38469;&#24773;&#26223;&#65306;&#25932;&#23545;&#26041;&#27809;&#26377;&#20851;&#20110;FL&#31995;&#32479;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#35268;&#21017;&#25110;&#33391;&#24615;&#35774;&#22791;&#19978;&#30340;&#26356;&#26032;&#65289;&#12290;FMPA&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#20449;&#24687;&#26500;&#24314;&#20272;&#35745;&#22120;&#65292;&#23558;&#19979;&#19968;&#36718;&#20840;&#23616;&#27169;&#22411;&#39044;&#27979;&#20026;&#33391;&#24615;&#21442;&#32771;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#21442;&#32771;&#27169;&#22411;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31934;&#24230;&#20302;&#21644;&#25200;&#21160;&#23567;&#30340;&#27602;&#21270;&#27169;&#22411;&#12290;FMPA&#19981;&#20165;&#21487;&#20197;&#36798;&#21040;DoS&#30340;&#30446;&#26631;&#65292;&#36824;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#21551;&#21160;&#32454;&#31890;&#24230;&#21487;&#25511;&#25915;&#20987;&#65292;&#20174;&#32780;&#31934;&#30830;&#38477;&#20302;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;FMPA&#22312;&#20960;&#31181;FL&#22330;&#26223;&#19979;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#21253;&#25324;&#20108;&#20803;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#19981;&#21516;&#30340;&#25915;&#20987;&#30446;&#26631;&#21644;&#25915;&#20987;&#30693;&#35782;&#27700;&#24179;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FMPA&#21487;&#20197;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#23454;&#29616;&#25152;&#38656;&#30340;&#25915;&#20987;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#24418;&#21644;&#19981;&#21487;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is vulnerable to poisoning attacks, where adversaries corrupt the global aggregation results and cause denial-of-service (DoS). Unlike recent model poisoning attacks that optimize the amplitude of malicious perturbations along certain prescribed directions to cause DoS, we propose a Flexible Model Poisoning Attack (FMPA) that can achieve versatile attack goals. We consider a practical threat scenario where no extra knowledge about the FL system (e.g., aggregation rules or updates on benign devices) is available to adversaries. FMPA exploits the global historical information to construct an estimator that predicts the next round of the global model as a benign reference. It then fine-tunes the reference model to obtain the desired poisoned model with low accuracy and small perturbations. Besides the goal of causing DoS, FMPA can be naturally extended to launch a fine-grained controllable attack, making it possible to precisely reduce the global accuracy. Armed wi
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.01951</link><description>&lt;p&gt;
TransPimLib&#65306;&#29992;&#20110;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01951
&lt;/p&gt;
&lt;p&gt;
TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#65288;PIM&#65289;&#25215;&#35834;&#20943;&#36731;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30495;&#23454;PIM&#31995;&#32479;&#26377;&#19968;&#20010;&#20869;&#22312;&#30340;&#21155;&#21183;&#65292;&#21363;&#23427;&#20204;&#30340;&#30828;&#20214;&#27604;&#20256;&#32479;&#30340;&#22788;&#29702;&#22120;&#65288;CPU&#12289;GPU&#65289;&#26356;&#21152;&#21463;&#38480;&#65292;&#22240;&#20026;&#22312;&#20869;&#23384;&#38468;&#36817;&#25110;&#20869;&#37096;&#26500;&#24314;&#22788;&#29702;&#20803;&#20214;&#30340;&#38590;&#24230;&#21644;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#29992;&#30340;PIM&#26550;&#26500;&#25903;&#25345;&#30456;&#24403;&#26377;&#38480;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#36229;&#36234;&#20989;&#25968;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#30340;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#26041;&#26681;&#65289;&#12290;&#36825;&#20123;&#25805;&#20316;&#23545;&#20110;&#19968;&#20123;&#29616;&#20195;&#24037;&#20316;&#36127;&#36733;&#23588;&#20854;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#29992;&#30340;PIM&#31995;&#32479;&#20013;&#25552;&#20379;&#23545;&#36229;&#36234;&#65288;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#65289;&#20989;&#25968;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TransPimLib&#65292;&#36825;&#26159;&#19968;&#20010;&#24211;&#65292;&#25552;&#20379;&#22522;&#20110;CORDIC&#21644;LUT&#30340;&#19977;&#35282;&#20989;&#25968;&#12289;&#21452;&#26354;&#20989;&#25968;&#12289;&#25351;&#25968;&#12289;&#23545;&#25968;&#12289;&#24179;&#26041;&#26681;&#31561;&#38590;&#20197;&#35745;&#31639;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15477</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#30340;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20869;&#22312;&#33021;&#22815;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#30456;&#20851;&#24615;&#65292;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21453;&#26144;SPD&#27969;&#24418;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22266;&#23450;&#24230;&#37327;&#24352;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;SPD&#30697;&#38453;&#23398;&#20064;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;SPD&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#25289;&#22238;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;SPD&#27969;&#24418;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#24230;&#37327;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;SPD&#32593;&#32476;&#21487;&#20197;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
&lt;/p&gt;</description></item><item><title>SAM&#26159;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33719;&#24471;&#26356;&#24179;&#22374;&#65288;&#21363;&#26356;&#19981;&#38160;&#21033;&#65289;&#30340;&#35299;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20004;&#20010;&#32479;&#35745;&#38382;&#39064;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35777;&#26126;&#20102;SAM&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#27604;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#36866;&#29992;&#20110;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#32622;&#34920;&#26126;&#65292;SAM&#30340;&#35299;&#26356;&#19981;&#38160;&#21033;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2302.11836</link><description>&lt;p&gt;
&#20851;&#20110;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#32479;&#35745;&#24615;&#36136;&#65306;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees. (arXiv:2302.11836v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11836
&lt;/p&gt;
&lt;p&gt;
SAM&#26159;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33719;&#24471;&#26356;&#24179;&#22374;&#65288;&#21363;&#26356;&#19981;&#38160;&#21033;&#65289;&#30340;&#35299;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20004;&#20010;&#32479;&#35745;&#38382;&#39064;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35777;&#26126;&#20102;SAM&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#27604;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#36866;&#29992;&#20110;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#32622;&#34920;&#26126;&#65292;SAM&#30340;&#35299;&#26356;&#19981;&#38160;&#21033;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270; (SAM) &#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#33719;&#24471;&#26356;&#24179;&#22374;&#65288;&#21363;&#26356;&#19981;&#38160;&#21033;&#65289;&#30340;&#35299;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#30340;&#26368;&#26032;&#20248;&#21270;&#26694;&#26550;&#12290;&#30001;&#20110;SAM&#22312;&#25968;&#20540;&#19978;&#21313;&#20998;&#25104;&#21151;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#35770;&#25991;&#30740;&#31350;&#20102;&#35813;&#26694;&#26550;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#24182;&#34920;&#26126;SAM&#30340;&#35299;&#30830;&#23454;&#26159;&#24179;&#22374;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;SAM&#30340;&#32479;&#35745;&#24615;&#36136;&#26041;&#38754;&#65292;&#29702;&#35770;&#25506;&#32034;&#26377;&#38480;&#12290;&#26412;&#25991;&#30452;&#25509;&#30740;&#31350;SAM&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;SAM&#33021;&#22815;&#36827;&#34892;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#32479;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#20855;&#26377;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22238;&#24402;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;SAM&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;(GD)&#30456;&#27604;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#28041;&#21450;&#20984;&#21644;&#38750;&#20984;&#35774;&#32622;&#65292;&#24182;&#34920;&#26126;SAM&#29305;&#21035;&#36866;&#29992;&#20110;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;SAM&#30340;&#35299;&#20063;&#26356;&#19981;&#38160;&#21033;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is a recent optimization framework aiming to improve the deep neural network generalization, through obtaining flatter (i.e. less sharp) solutions. As SAM has been numerically successful, recent papers have studied the theoretical aspects of the framework and have shown SAM solutions are indeed flat. However, there has been limited theoretical exploration regarding statistical properties of SAM. In this work, we directly study the statistical performance of SAM, and present a new theoretical explanation of why SAM generalizes well. To this end, we study two statistical problems, neural networks with a hidden layer and kernel regression, and prove under certain conditions, SAM has smaller prediction error over Gradient Descent (GD). Our results concern both convex and non-convex settings, and show that SAM is particularly well-suited for non-convex problems. Additionally, we prove that in our setup, SAM solutions are less sharp as well, showing our res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#36924;&#36817;&#35823;&#24046;&#21644;&#20272;&#35745;&#35823;&#24046;/&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#32447;&#24615;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#19979;&#65292;&#36817;&#26399;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#28982;&#32780;&#23578;&#26410;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.09703</link><description>&lt;p&gt;
&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#20174;&#32447;&#24615;&#21040;&#38750;&#32447;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Function Approximation: From Linear to Nonlinear. (arXiv:2302.09703v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#36924;&#36817;&#35823;&#24046;&#21644;&#20272;&#35745;&#35823;&#24046;/&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#32447;&#24615;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#19979;&#65292;&#36817;&#26399;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#28982;&#32780;&#23578;&#26410;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#36924;&#36817;&#26159;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#26088;&#22312;&#35299;&#20915;&#39640;&#32500;&#24230;&#22823;&#29366;&#24577;&#31354;&#38388;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#36924;&#36817;&#35823;&#24046;&#21644;&#20272;&#35745;&#35823;&#24046;/&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#36924;&#36817;&#35823;&#24046;&#26377;&#20851;&#30340;&#21508;&#31181;&#24615;&#36136;&#65292;&#24182;&#32473;&#20986;&#20102;&#36716;&#31227;&#27010;&#29575;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20855;&#20307;&#26465;&#20214;&#65292;&#20351;&#24471;&#36825;&#20123;&#24615;&#36136;&#25104;&#31435;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#27604;&#30417;&#30563;&#23398;&#20064;&#26356;&#20026;&#22797;&#26434;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20998;&#24067;&#19981;&#21305;&#37197;&#29616;&#35937;&#12290;&#22312;&#32447;&#24615;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#19979;&#65292;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#31639;&#27861;&#37117;&#21487;&#20197;&#23454;&#29616;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#19982;&#29305;&#24449;&#25968;&#37327;&#12289;&#21095;&#38598;&#38271;&#24230;&#21644;&#20934;&#30830;&#24615;&#26377;&#20851;&#65292;&#23613;&#31649;&#23578;&#26410;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Function approximation has been an indispensable component in modern reinforcement learning algorithms designed to tackle problems with large state spaces in high dimensions. This paper reviews recent results on error analysis for these reinforcement learning algorithms in linear or nonlinear approximation settings, emphasizing approximation error and estimation error/sample complexity. We discuss various properties related to approximation error and present concrete conditions on transition probability and reward function under which these properties hold true. Sample complexity analysis in reinforcement learning is more complicated than in supervised learning, primarily due to the distribution mismatch phenomenon. With assumptions on the linear structure of the problem, numerous algorithms in the literature achieve polynomial sample complexity with respect to the number of features, episode length, and accuracy, although the minimax rate has not been achieved yet. These results rely 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#22914;&#26524;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#21017;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07384</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#19979;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Geometry of Neural Nets' Parameter Spaces Under Reparametrization. (arXiv:2302.07384v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#22914;&#26524;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#21017;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#21442;&#25968;&#21270;&#26159;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#20294;&#20063;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#22312;Hessian&#24179;&#22374;&#24230;&#27979;&#37327;&#12289;&#20248;&#21270;&#36712;&#36857;&#21644;&#27010;&#29575;&#23494;&#24230;&#27169;&#24335;&#31561;&#26041;&#38754;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#19979;&#28216;&#20998;&#26512;&#21464;&#24471;&#26356;&#20026;&#22797;&#26434;&#65306;&#20363;&#22914;&#65292;&#30001;&#20110;&#20219;&#24847;&#30340;&#37325;&#21442;&#25968;&#21270;&#37117;&#21487;&#20197;&#25913;&#21464;&#20108;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#26080;&#27861;&#26126;&#30830;&#22320;&#23558;&#24179;&#22374;&#24230;&#19982;&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#40654;&#26364;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#30340;&#19981;&#21464;&#24615;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22914;&#26524;&#25105;&#20204;&#26174;&#24335;&#22320;&#34920;&#31034;&#24230;&#37327;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#30456;&#20851;&#21464;&#25442;&#35268;&#21017;&#65292;&#37027;&#20040;&#19981;&#21464;&#24615;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#19968;&#28857;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23613;&#31649;&#24230;&#37327;&#22987;&#32456;&#23384;&#22312;&#65292;&#20294;&#36890;&#24120;&#34987;&#38544;&#24335;&#22320;&#20551;&#23450;&#20026;&#21333;&#20301;&#30697;&#38453;&#65292;&#24182;&#22240;&#27492;&#20174;&#31526;&#21495;&#20013;&#30465;&#30053;&#65292;&#28982;&#21518;&#22312;&#37325;&#21442;&#25968;&#21270;&#19979;&#20002;&#22833;&#20102;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#34913;&#37327;&#24179;&#22374;&#24230;&#25152;&#24102;&#26469;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#31070;&#32463;&#23481;&#37327;&#32858;&#31867;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25968;&#25454;&#28857;&#20998;&#37197;&#21040;&#31751;&#20013;&#24515;&#30340;&#27010;&#29575;&#65292;&#32467;&#21512;&#19968;&#31181;&#31867;&#20284;&#20110;K&#22343;&#20540;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#22312;&#23481;&#37327;&#32422;&#26463;&#19979;&#23545;&#32858;&#31867;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#12290;&#36890;&#36807;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.05134</link><description>&lt;p&gt;
&#31070;&#32463;&#23481;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Neural Capacitated Clustering. (arXiv:2302.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#31070;&#32463;&#23481;&#37327;&#32858;&#31867;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25968;&#25454;&#28857;&#20998;&#37197;&#21040;&#31751;&#20013;&#24515;&#30340;&#27010;&#29575;&#65292;&#32467;&#21512;&#19968;&#31181;&#31867;&#20284;&#20110;K&#22343;&#20540;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#22312;&#23481;&#37327;&#32422;&#26463;&#19979;&#23545;&#32858;&#31867;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#12290;&#36890;&#36807;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28145;&#24230;&#32858;&#31867;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#29992;&#20110;&#32422;&#26463;&#32858;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#36890;&#24120;&#21487;&#20197;&#20351;&#29992;&#25104;&#23545;&#32422;&#26463;&#26469;&#25351;&#23548;&#25968;&#25454;&#30340;&#20998;&#32452;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#38382;&#39064;&#20855;&#26377;&#32858;&#31867;&#32423;&#21035;&#30340;&#32422;&#26463;&#65292;&#20363;&#22914;&#23481;&#37327;&#32422;&#26463;&#32858;&#31867;&#38382;&#39064;&#65288;CCP&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#28857;&#20855;&#26377;&#26435;&#37325;&#65292;&#27599;&#20010;&#31751;&#20013;&#25152;&#26377;&#28857;&#30340;&#26435;&#37325;&#24635;&#21644;&#30001;&#35268;&#23450;&#30340;&#23481;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CCP&#26041;&#27861;&#8220;&#31070;&#32463;&#23481;&#37327;&#32858;&#31867;&#8221;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25968;&#25454;&#28857;&#20998;&#37197;&#21040;&#31751;&#20013;&#24515;&#30340;&#27010;&#29575;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#20182;&#38382;&#39064;&#23454;&#20363;&#30340;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#24471;&#20998;&#22312;&#23481;&#37327;&#32422;&#26463;&#19979;&#36845;&#20195;&#22320;&#31867;&#20284;&#20110;K&#22343;&#20540;&#30340;&#36807;&#31243;&#26469;&#25913;&#36827;&#20998;&#37197;&#12290;&#22312;&#20154;&#36896;&#25968;&#25454;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on deep clustering has found new promising methods also for constrained clustering problems. Their typically pairwise constraints often can be used to guide the partitioning of the data. Many problems however, feature cluster-level constraints, e.g. the Capacitated Clustering Problem (CCP), where each point has a weight and the total weight sum of all points in each cluster is bounded by a prescribed capacity. In this paper we propose a new method for the CCP, Neural Capacited Clustering, that learns a neural network to predict the assignment probabilities of points to cluster centers from a data set of optimal or near optimal past solutions of other problem instances. During inference, the resulting scores are then used in an iterative k-means like procedure to refine the assignment under capacity constraints. In our experiments on artificial data and two real world datasets our approach outperforms several state-of-the-art mathematical and heuristic solvers from the liter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#65292;&#23454;&#29616;&#26368;&#20248; oracle &#22797;&#26434;&#24615;&#25152;&#24517;&#38656;&#35201;&#30340;&#20869;&#23384;&#20026;&#20108;&#27425;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702; 1-Lipschitz &#20984;&#20989;&#25968;&#26102;&#65292;&#20351;&#29992; $d^{2-\delta}$ &#20869;&#23384;&#30340;&#20219;&#20309;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892; $\tilde\Omega(d^{1+\delta/3})$ &#27425;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#22312;&#21487;&#34892;&#24615;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#33267;&#22810; $d^{2-\delta}$ &#23384;&#20648;&#22120;&#23481;&#37327;&#30340;&#20998;&#31163; oracle &#38656;&#35201;&#36827;&#34892; $\tilde\Omega(d^{1+\delta})$ &#27425;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2302.04963</link><description>&lt;p&gt;
&#20108;&#27425;&#20869;&#23384;&#26159;&#23454;&#29616;&#20984;&#20248;&#21270;&#26368;&#20248;&#26597;&#35810;&#22797;&#26434;&#24230;&#25152;&#24517;&#38656;&#30340;&#65306;&#36136;&#24515;&#26159;&#24085;&#32047;&#25176;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quadratic Memory is Necessary for Optimal Query Complexity in Convex Optimization: Center-of-Mass is Pareto-Optimal. (arXiv:2302.04963v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#65292;&#23454;&#29616;&#26368;&#20248; oracle &#22797;&#26434;&#24615;&#25152;&#24517;&#38656;&#35201;&#30340;&#20869;&#23384;&#20026;&#20108;&#27425;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702; 1-Lipschitz &#20984;&#20989;&#25968;&#26102;&#65292;&#20351;&#29992; $d^{2-\delta}$ &#20869;&#23384;&#30340;&#20219;&#20309;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892; $\tilde\Omega(d^{1+\delta/3})$ &#27425;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#22312;&#21487;&#34892;&#24615;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#33267;&#22810; $d^{2-\delta}$ &#23384;&#20648;&#22120;&#23481;&#37327;&#30340;&#20998;&#31163; oracle &#38656;&#35201;&#36827;&#34892; $\tilde\Omega(d^{1+\delta})$ &#27425;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32473;&#20986;&#20102;&#20984;&#20248;&#21270;&#21450;&#30456;&#20851;&#21487;&#34892;&#24615;&#38382;&#39064;&#30340;&#26597;&#35810;&#22797;&#26434;&#24615;&#19979;&#30028;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23454;&#29616;&#20984;&#20248;&#21270;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#30340;&#26368;&#20248; oracle &#22797;&#26434;&#24615;&#25152;&#24517;&#38656;&#30340;&#20869;&#23384;&#26159;&#20108;&#27425;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#22312;&#32500;&#24230; $d$ &#20013;&#20351;&#29992; $\tilde O(d^2)$ &#20869;&#23384;&#21644; $\tilde O(d)$ &#26597;&#35810;&#30340;&#36136;&#24515;&#20999;&#24179;&#38754;&#31639;&#27861;&#23545;&#20984;&#20248;&#21270;&#21644;&#21487;&#34892;&#24615;&#38382;&#39064;&#26469;&#35828;&#37117;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#65292;&#31934;&#24230;&#20026; $1/d^4$&#65292;&#19978;&#38480;&#20026;&#23545;&#25968;&#22240;&#23376;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20026;&#20102;&#22312;&#21333;&#20301;&#29699;&#19978;&#23558; $1$-Lipschitz &#20984;&#20989;&#25968;&#26368;&#23567;&#21270;&#21040; $1/d^4$ &#30340;&#31934;&#24230;&#65292;&#20219;&#20309;&#20351;&#29992;&#33267;&#22810; $d^{2-\delta}$ &#20010;&#20869;&#23384;&#20301;&#30340;&#30830;&#23450;&#24615;&#19968;&#38454;&#31639;&#27861;&#37117;&#24517;&#39035;&#36827;&#34892; $\tilde\Omega(d^{1+\delta/3})$ &#27425;&#26597;&#35810;&#65292;&#20854;&#20013; $\delta\in[0,1]$&#12290;&#23545;&#20110;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#22312;&#20854;&#21482;&#26377;&#35775;&#38382;&#20998;&#31163; oracle &#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#26435;&#34913;&#65306;&#23545;&#20110;&#33267;&#22810; $d^{2-\delta}$ &#30340;&#23384;&#20648;&#22120;&#23481;&#37327;&#65292;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#37327;&#20026; $\tilde\Omega(d^{1+\delta})$&#12290;&#36825;&#35299;&#20915;&#20102; COLT 2019 &#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give query complexity lower bounds for convex optimization and the related feasibility problem. We show that quadratic memory is necessary to achieve the optimal oracle complexity for first-order convex optimization. In particular, this shows that center-of-mass cutting-planes algorithms in dimension $d$ which use $\tilde O(d^2)$ memory and $\tilde O(d)$ queries are Pareto-optimal for both convex optimization and the feasibility problem, up to logarithmic factors. Precisely, we prove that to minimize $1$-Lipschitz convex functions over the unit ball to $1/d^4$ accuracy, any deterministic first-order algorithms using at most $d^{2-\delta}$ bits of memory must make $\tilde\Omega(d^{1+\delta/3})$ queries, for any $\delta\in[0,1]$. For the feasibility problem, in which an algorithm only has access to a separation oracle, we show a stronger trade-off: for at most $d^{2-\delta}$ memory, the number of queries required is $\tilde\Omega(d^{1+\delta})$. This resolves a COLT 2019 open problem 
&lt;/p&gt;</description></item><item><title>&#20044;&#20811;&#20848;&#21361;&#26426;&#24341;&#36215;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#35758;&#39064;&#24577;&#24230;&#30340;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#12290;&#30740;&#31350;&#32773;&#36816;&#29992;&#22810;&#35821;&#35328;&#20998;&#26512;&#25216;&#26415;&#23545;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30456;&#20851;&#25253;&#36947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#23545;&#31227;&#27665;&#35758;&#39064;&#35752;&#35770;&#30340;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.02813</link><description>&lt;p&gt;
&#31227;&#27665;&#35758;&#39064;&#30340;&#20877;&#23450;&#20041;&#65311;&#20044;&#20811;&#20848;&#21361;&#26426;&#26399;&#38388;&#27431;&#27954;&#24577;&#24230;&#21464;&#21270;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis. (arXiv:2302.02813v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02813
&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#21361;&#26426;&#24341;&#36215;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#35758;&#39064;&#24577;&#24230;&#30340;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#12290;&#30740;&#31350;&#32773;&#36816;&#29992;&#22810;&#35821;&#35328;&#20998;&#26512;&#25216;&#26415;&#23545;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30456;&#20851;&#25253;&#36947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#23545;&#31227;&#27665;&#35758;&#39064;&#35752;&#35770;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#25112;&#20105;&#20284;&#20046;&#31215;&#26497;&#25913;&#21464;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#36825;&#19968;&#20851;&#38190;&#31038;&#20250;&#35758;&#39064;&#30340;&#24577;&#24230;&#8212;&#8212;&#33267;&#23569;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#26469;&#35828;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35813;&#35758;&#39064;&#22312;&#32593;&#32476;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21453;&#26144;&#65292;&#20197;&#27492;&#23558;&#32593;&#19978;&#30340;&#35758;&#39064;&#34920;&#36798;&#19982;&#31038;&#20250;&#23545;&#20854;&#30340;&#24863;&#30693;&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32467;&#21512;&#24182;&#25913;&#32534;&#20102;&#39046;&#20808;&#30340;&#33258;&#21160;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#65292;&#37319;&#29992;&#26032;&#22411;&#30340;&#22810;&#35821;&#35328;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#20174;2021&#24180;9&#26376;&#24320;&#22987;&#30340;&#19968;&#24180;&#20869;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;565&#23478;&#27431;&#27954;&#26032;&#38395;&#26426;&#26500;&#21457;&#24067;&#30340;550&#19975;&#26465;&#25512;&#29305;&#24086;&#23376;&#21644;&#22238;&#22797;&#65292;&#36827;&#34892;&#20102;&#20851;&#20110;&#31227;&#27665;&#30456;&#20851;&#30340;&#23186;&#20307;&#25253;&#36947;&#21644;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#31181;&#35752;&#35770;&#37325;&#22609;&#65292;&#21487;&#20197;&#36890;&#36807;&#26415;&#35821;&#30340;&#21464;&#21270;&#26469;&#35828;&#26126;&#65292;&#20363;&#22914;&#65292;&#20174;&#8220;&#31227;&#27665;&#8221;&#21040;&#8220;&#38590;&#27665;&#8221;&#65292;&#29978;&#33267;&#24120;&#24120;&#24378;&#35843;&#8220;&#30495;&#27491;&#30340;&#38590;&#27665;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The war in Ukraine seems to have positively changed the attitude toward the critical societal topic of migration in Europe -- at least towards refugees from Ukraine. We investigate whether this impression is substantiated by how the topic is reflected in online news and social media, thus linking the representation of the issue on the Web to its perception in society. For this purpose, we combine and adapt leading-edge automatic text processing for a novel multilingual stance detection approach. Starting from 5.5M Twitter posts published by 565 European news outlets in one year, beginning September 2021, plus replies, we perform a multilingual analysis of migration-related media coverage and associated social media interaction for Europe and selected European countries.  The results of our analysis show that there is actually a reframing of the discussion illustrated by the terminology change, e.g., from "migrant" to "refugee", often even accentuated with phrases such as "real refugees
&lt;/p&gt;</description></item><item><title>PubGraph&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#20840;&#38754;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#21644;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#31185;&#23398;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.02231</link><description>&lt;p&gt;
PubGraph: &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
PubGraph: A Large-Scale Scientific Knowledge Graph. (arXiv:2302.02231v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02231
&lt;/p&gt;
&lt;p&gt;
PubGraph&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#20840;&#38754;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#21644;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#31185;&#23398;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#30740;&#20986;&#29256;&#29289;&#26159;&#20998;&#20139;&#26032;&#21457;&#29616;&#12289;&#26032;&#26041;&#27861;&#12289;&#26032;&#25216;&#26415;&#21644;&#27934;&#35265;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36164;&#28304;&#26469;&#25429;&#25417;&#20986;&#29256;&#29289;&#12289;&#20316;&#32773;&#21644;&#26399;&#21002;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#65292;&#32473;&#23545;&#31185;&#23398;&#26377;&#26356;&#28145;&#20837;&#29702;&#35299;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PubGraph&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#31185;&#23398;&#36827;&#23637;&#30340;&#36164;&#28304;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20855;&#26377;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#12289;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#21644;15&#20159;&#20010;&#38480;&#23450;&#35789;&#36793;&#32536;&#12290; PubGraph&#26159;&#20840;&#38754;&#30340;&#65292;&#24182;&#20351;&#29992;Wikidata&#26412;&#20307;&#35770;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65288;&#21253;&#25324;Wikidata&#12289;OpenAlex&#21644;Semantic Scholar&#65289;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#36825;&#20123;&#26469;&#28304;&#30340;&#20803;&#25968;&#25454;&#22806;&#65292;PubGraph&#36824;&#21253;&#25324;&#26469;&#33258;&#36741;&#21161;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25903;&#25345;&#20851;&#20110;&#31185;&#23398;&#32593;&#32476;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20960;&#20010;&#22823;&#35268;&#27169;&#8203;&#8203;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research publications are the primary vehicle for sharing scientific progress in the form of new discoveries, methods, techniques, and insights. Unfortunately, the lack of a large-scale, comprehensive, and easy-to-use resource capturing the myriad relationships between publications, their authors, and venues presents a barrier to applications for gaining a deeper understanding of science. In this paper, we present PubGraph, a new resource for studying scientific progress that takes the form of a large-scale knowledge graph (KG) with more than 385M entities, 13B main edges, and 1.5B qualifier edges. PubGraph is comprehensive and unifies data from various sources, including Wikidata, OpenAlex, and Semantic Scholar, using the Wikidata ontology. Beyond the metadata available from these sources, PubGraph includes outputs from auxiliary community detection algorithms and large language models. To further support studies on reasoning over scientific networks, we create several large-scale ben
&lt;/p&gt;</description></item><item><title>Sancdifi&#26159;&#19968;&#31181;&#26377;&#25928;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;salience map&#30340;masks&#35843;&#33410;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21435;&#38500;&#34987;&#21518;&#38376;&#25915;&#20987;&#27745;&#26579;&#30340;&#25968;&#25454;&#20013;&#30340;&#35302;&#21457;&#22120;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#20063;&#33021;&#24674;&#22797;&#20986;&#31361;&#20986;&#29305;&#24449;&#65292;&#32780;&#19988;&#26080;&#38656;&#20351;&#29992;&#29305;&#27931;&#20234;&#32593;&#32476;&#27169;&#22411;&#21442;&#25968;&#65292;&#20316;&#20026;&#19968;&#31181;&#40657;&#30418;&#38450;&#24481;&#26426;&#21046;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.13862</link><description>&lt;p&gt;
&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26174;&#33879;&#26465;&#20214;&#25193;&#25955;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Salient Conditional Diffusion for Defending Against Backdoor Attacks. (arXiv:2301.13862v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13862
&lt;/p&gt;
&lt;p&gt;
Sancdifi&#26159;&#19968;&#31181;&#26377;&#25928;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;salience map&#30340;masks&#35843;&#33410;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21435;&#38500;&#34987;&#21518;&#38376;&#25915;&#20987;&#27745;&#26579;&#30340;&#25968;&#25454;&#20013;&#30340;&#35302;&#21457;&#22120;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#20063;&#33021;&#24674;&#22797;&#20986;&#31361;&#20986;&#29305;&#24449;&#65292;&#32780;&#19988;&#26080;&#38656;&#20351;&#29992;&#29305;&#27931;&#20234;&#32593;&#32476;&#27169;&#22411;&#21442;&#25968;&#65292;&#20316;&#20026;&#19968;&#31181;&#40657;&#30418;&#38450;&#24481;&#26426;&#21046;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#8212;&#8212;&#26174;&#33879;&#26465;&#20214;&#25193;&#25955;&#65288;Sancdifi&#65289;&#65292;&#26159;&#30446;&#21069;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26426;&#21046;&#12290;Sancdifi&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#23545;&#19968;&#24352;&#21152;&#20837;&#22122;&#22768;&#30340;&#22270;&#20687;&#36827;&#34892;&#38477;&#36136;&#22788;&#29702;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#36870;&#25193;&#25955;&#37325;&#26500;&#35813;&#22270;&#20687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#29983;&#25104;&#25513;&#30721;&#26469;&#35843;&#33410;&#25193;&#25955;&#65292;&#20801;&#35768;DDPM&#23545;&#26368;&#31361;&#20986;&#30340;&#20687;&#32032;&#36827;&#34892;&#26356;&#24378;&#30340;&#25193;&#25955;&#12290;&#22240;&#27492;&#65292;Sancdifi&#22312;&#21435;&#38500;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#27745;&#26579;&#30340;&#25968;&#25454;&#20013;&#30340;&#35302;&#21457;&#22120;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;Sancdifi&#21487;&#20197;&#21487;&#38752;&#22320;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#24212;&#29992;&#65292;&#24182;&#24674;&#22797;&#31361;&#20986;&#29305;&#24449;&#12290;&#36825;&#31181;&#24615;&#33021;&#26159;&#22312;&#26080;&#38656;&#20351;&#29992;&#29305;&#27931;&#20234;&#32593;&#32476;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;Sancdifi&#20316;&#20026;&#19968;&#31181;&#40657;&#30418;&#38450;&#24481;&#26426;&#21046;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising diffusion probabilistic model (DDPM) to degrade an image with noise and then recover said image using the learned reverse diffusion. Critically, we compute saliency map-based masks to condition our diffusion, allowing for stronger diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is highly effective at diffusing out triggers in data poisoned by backdoor attacks. At the same time, it reliably recovers salient features when applied to clean data. This performance is achieved without requiring access to the model parameters of the Trojan network, meaning Sancdifi operates as a black-box defense.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#22312;&#27491;&#30830;&#35828;&#26126;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#20998;&#26512;&#34920;&#26126;&#22240;&#26524;&#25512;&#26029;&#30340;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.12930</link><description>&lt;p&gt;
&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#26368;&#22823;&#20284;&#28982;&#19982;&#29420;&#31435;&#24615;&#26816;&#39564;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing. (arXiv:2301.12930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411;&#65292;&#35813;&#35770;&#25991;&#22312;&#27491;&#30830;&#35828;&#26126;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#22312;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#20998;&#26512;&#34920;&#26126;&#22240;&#26524;&#25512;&#26029;&#30340;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#25512;&#26029;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#27491;&#30830;&#22240;&#26524;&#26041;&#21521;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24322;&#26041;&#24046;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#20989;&#25968;&#27169;&#22411; (LSNM) &#32467;&#21512;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#65292;&#22312;&#27491;&#30830;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#29992;&#25143;&#38169;&#35823;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#30340;&#24418;&#24335;&#26102;&#65292;&#31934;&#24230;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#22833;&#36133;&#20027;&#35201;&#21457;&#29983;&#22312;&#21453;&#22240;&#26524;&#26041;&#21521;&#30340;&#26465;&#20214;&#26041;&#24046;&#23567;&#20110;&#22240;&#26524;&#26041;&#21521;&#30340;&#26465;&#20214;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#21457;&#29616;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#36873;&#25321;&#21487;&#20197;&#22312;&#32570;&#20047;&#22122;&#22768;&#20998;&#24067;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#31283;&#23450;&#32780;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental problem of causal discovery is cause-effect inference, learning the correct causal direction between two random variables. Significant progress has been made through modelling the effect as a function of its cause and a noise term, which allows us to leverage assumptions about the generating function class. The recently introduced heteroscedastic location-scale noise functional models (LSNMs) combine expressive power with identifiability guarantees. LSNM model selection based on maximizing likelihood achieves state-of-the-art accuracy, when the noise distributions are correctly specified. However, through an extensive empirical evaluation, we demonstrate that the accuracy deteriorates sharply when the form of the noise distribution is misspecified by the user. Our analysis shows that the failure occurs mainly when the conditional variance in the anti-causal direction is smaller than that in the causal direction. As an alternative, we find that causal model selection throu
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>TinyML&#37096;&#32626;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#21487;&#20197;&#35299;&#38145;&#26080;&#25968;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#39033;&#26032;&#20852;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21487;&#25345;&#32493;&#21457;&#23637;&#25361;&#25112;&#65292;&#20294;&#38656;&#35201;&#35780;&#20272;&#21644;&#32531;&#35299;&#20854;&#29615;&#22659;&#24433;&#21709;&#20197;&#30830;&#20445;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11899</link><description>&lt;p&gt;
TinyML&#30340;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#65306;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#23545;&#24494;&#25511;&#21046;&#22120;&#30340;&#29615;&#22659;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers. (arXiv:2301.11899v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11899
&lt;/p&gt;
&lt;p&gt;
TinyML&#37096;&#32626;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#21487;&#20197;&#35299;&#38145;&#26080;&#25968;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#39033;&#26032;&#20852;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21487;&#25345;&#32493;&#21457;&#23637;&#25361;&#25112;&#65292;&#20294;&#38656;&#35201;&#35780;&#20272;&#21644;&#32531;&#35299;&#20854;&#29615;&#22659;&#24433;&#21709;&#20197;&#30830;&#20445;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#22686;&#38271;&#30340;&#30899;&#25490;&#25918;&#21644;&#20840;&#29699;&#22403;&#22334;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#29615;&#22659;&#26410;&#26469;&#30340;&#21487;&#25345;&#32493;&#24615;&#20851;&#27880;&#12290;&#24555;&#36895;&#22686;&#38271;&#30340;&#29289;&#32852;&#32593;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;Tiny Machine Learning&#65288;TinyML&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26377;&#26426;&#20250;&#36890;&#36807;&#21487;&#25345;&#32493;&#35745;&#31639;&#23454;&#36341;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#29615;&#22659;&#25361;&#25112;&#12290;TinyML&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#37096;&#32626;&#21040;&#20302;&#25104;&#26412;&#12289;&#20302;&#21151;&#32791;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#23454;&#29616;&#20102;&#35774;&#22791;&#19978;&#30340;&#20256;&#24863;&#22120;&#20998;&#26512;&#65292;&#20174;&#32780;&#37322;&#25918;&#20986;&#26080;&#25968;&#30340;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;ML&#24212;&#29992;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;TinyML&#24212;&#29992;&#24212;&#23545;&#20851;&#38190;&#21487;&#25345;&#32493;&#24615;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#29615;&#22659;&#36275;&#36857;&#12290;&#36890;&#36807;&#23436;&#20840;&#30340;&#29983;&#21629;&#21608;&#26399;&#20998;&#26512;&#65288;LCA&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;TinyML&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#20943;&#23569;&#20854;&#20182;&#34892;&#19994;&#25490;&#25918;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#25269;&#28040;&#30899;&#25490;&#25918;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#21487;&#25345;&#32493;&#22320;&#25193;&#22823;&#35268;&#27169;&#65292;TinyML&#30340;&#22686;&#38271;&#21487;&#33021;&#20250;&#32473;&#29615;&#22659;&#36896;&#25104;&#37325;&#22823;&#36127;&#25285;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;TinyML&#31038;&#21306;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#24517;&#39035;&#31215;&#26497;&#35780;&#20272;&#21644;&#32531;&#35299;&#36825;&#39033;&#25216;&#26415;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#20197;&#30830;&#20445;&#20854;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environment's future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23569;&#26679;&#26412;&#24320;&#25918;&#38598;&#35782;&#21035;&#38382;&#39064;&#30340;&#24320;&#25918;&#38598;&#20284;&#28982;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21033;&#29992;&#26410;&#26631;&#35760;&#26597;&#35810;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08390</link><description>&lt;p&gt;
&#38024;&#23545;&#23569;&#26679;&#26412;&#24320;&#25918;&#38598;&#35782;&#21035;&#38382;&#39064;&#30340;&#24320;&#25918;&#38598;&#20284;&#28982;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Open-Set Likelihood Maximization for Few-Shot Learning. (arXiv:2301.08390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23569;&#26679;&#26412;&#24320;&#25918;&#38598;&#35782;&#21035;&#38382;&#39064;&#30340;&#24320;&#25918;&#38598;&#20284;&#28982;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21033;&#29992;&#26410;&#26631;&#35760;&#26597;&#35810;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#24320;&#25918;&#38598;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#22312;&#21482;&#26377;&#24456;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23545;&#19968;&#32452;&#31867;&#21035;&#20013;&#30340;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21516;&#26102;&#26816;&#27979;&#19981;&#23646;&#20110;&#20219;&#20309;&#24050;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#27969;&#34892;&#30340;&#20256;&#23548;&#35774;&#32622;&#65292;&#21033;&#29992;&#25512;&#29702;&#26102;&#26410;&#26631;&#35760;&#30340;&#26597;&#35810;&#23454;&#20363;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#20256;&#23548;&#26041;&#27861;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#21407;&#21017;&#65292;&#20854;&#20013;&#38500;&#20102;&#36890;&#24120;&#30340;&#21442;&#25968;&#27169;&#22411;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19979;&#35843;&#28508;&#22312;&#24322;&#24120;&#20540;&#24433;&#21709;&#30340;&#28508;&#22312;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#20174;&#25903;&#25345;&#38598;&#23884;&#20837;&#30417;&#30563;&#32422;&#26463;&#21644;&#38468;&#21152;&#24809;&#32602;&#65292;&#20197;&#38450;&#27490;&#23545;&#26597;&#35810;&#38598;&#30340;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#22359;&#22352;&#26631;&#19979;&#38477;&#65292;&#36718;&#27969;&#20849;&#21516;&#20248;&#21270;&#28508;&#22312;&#24471;&#20998;&#21644;&#21442;&#25968;&#27169;&#22411;&#65292;&#20174;&#32780;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Open-Set Likelihood Optimization&#65288;OSLO&#65289;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24320;&#25918;&#38598;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation \textit{Open-Set Likelihood Op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00899</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#32500;&#20256;&#24863;&#22120;&#21453;&#39304;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback. (arXiv:2301.00899v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#20316;&#29289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#21508;&#31181;&#27979;&#37327;&#20540;&#33258;&#36866;&#24212;&#22320;&#26045;&#21152;&#27700;&#20998;&#65292;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#28748;&#28297;&#35843;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21046;&#23450;&#33258;&#24049;&#30340;&#20248;&#21270;&#38382;&#39064;&#24182;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#65292;&#20197;&#21152;&#24555;&#25216;&#26415;&#36827;&#27493;&#12290;&#20854;&#26377;&#25928;&#24615;&#24050;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has considerable potential to improve irrigation scheduling in many cropping systems by applying adaptive amounts of water based on various measurements over time. The goal is to discover an intelligent decision rule that processes information available to growers and prescribes sensible irrigation amounts for the time steps considered. Due to the technical novelty, however, the research on the technique remains sparse and impractical. To accelerate the progress, the paper proposes a principled framework and actionable procedure that allow researchers to formulate their own optimisation problems and implement solution algorithms based on deep reinforcement learning. The effectiveness of the framework was demonstrated using a case study of irrigated wheat grown in a productive region of Australia where profits were maximised. Specifically, the decision rule takes nine state variable inputs: crop phenological stage, leaf area index, extractable soil water for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2212.12611</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#35782;&#21035;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Your diffusion model secretly knows the dimension of the data manifold. (arXiv:2212.12611v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#30340;&#32500;&#24230;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;&#25193;&#25955;&#27169;&#22411;&#20272;&#31639;&#25968;&#25454;&#27969;&#24418;&#32500;&#24230;&#30340;&#26032;&#26694;&#26550;&#12290;&#25193;&#25955;&#27169;&#22411;&#36880;&#28176;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#26799;&#24230;&#65292;&#21363;&#22122;&#22768;&#27745;&#26579;&#29256;&#26412;&#30340;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65292;&#19981;&#21516;&#32423;&#21035;&#30340;&#27745;&#26579;&#31243;&#24230;&#23545;&#24212;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#32858;&#28966;&#20110;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#27969;&#24418;&#65292;&#37027;&#20040;&#38543;&#30528;&#22122;&#22768;&#27745;&#26579;&#31243;&#24230;&#30340;&#38477;&#20302;&#65292;&#26799;&#24230;&#20250;&#25351;&#21521;&#27969;&#24418;&#65292;&#22240;&#20026;&#36825;&#20010;&#26041;&#21521;&#26159;&#26368;&#22823;&#20284;&#28982;&#22686;&#21152;&#30340;&#26041;&#21521;&#12290;&#22240;&#27492;&#65292;&#22312;&#27745;&#26579;&#31243;&#24230;&#36739;&#20302;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#27969;&#24418;&#27491;&#24120;&#21521;&#37327;&#30340;&#36924;&#36817;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20272;&#35745;&#20999;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#20063;&#23601;&#26159;&#25968;&#25454;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#27969;&#24418;&#32500;&#24230;&#30340;&#31532;&#19968;&#20010;&#20272;&#31639;&#22120;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#24050;&#32463;&#25104;&#29087;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel framework for estimating the dimension of the data manifold using a trained diffusion model. A diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that, if the data concentrates around a manifold embedded in the high-dimensional ambient space, then as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, for small levels of corruption, the diffusion model provides us with access to an approximation of the normal bundle of the data manifold. This allows us to estimate the dimension of the tangent space, thus, the intrinsic dimension of the data manifold. To the best of our knowledge, our method is the first estimator of the data manifold dimension based on diffusion models and it outperforms well established statis
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2212.09667</link><description>&lt;p&gt;
&#27880;&#37325;&#35270;&#35273;&#12289;&#23646;&#24615;&#21644;&#29702;&#24615;&#65306;&#36808;&#21521;&#29289;&#29702;&#23433;&#20840;&#21644;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09667
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#24066;&#22330;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#29992;&#25143;&#30340;&#36523;&#20307;&#23433;&#20840;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19981;&#21463;&#38480;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#25512;&#33616;&#21361;&#38505;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#38544;&#34109;&#30340;&#19981;&#23433;&#20840;&#25991;&#26412;&#26159;&#19968;&#20010;&#29305;&#21035;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#20986;&#29616;&#22312;&#26085;&#24120;&#22330;&#26223;&#20013;&#65292;&#24182;&#19988;&#24456;&#38590;&#34987;&#26816;&#27979;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FARM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22312;&#23433;&#20840;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FARM&#27880;&#37325;&#20110;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#30830;&#35748;&#22312;&#29305;&#23450;&#24773;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21487;&#20449;&#28304;&#36827;&#34892;&#24402;&#22240;&#20197;&#33719;&#21462;&#27492;&#20449;&#24687;&#12290;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#20998;&#31867;&#21407;&#22987;&#25991;&#26412;&#30340;&#23433;&#20840;&#24615;&#24182;&#29983;&#25104;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#29702;&#65292;&#25581;&#31034;&#31995;&#32479;&#23545;&#29305;&#23450;&#29992;&#25143;&#32676;&#20307;&#30340;&#39118;&#38505;&#65292;&#24182;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#31649;&#29702;&#20854;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24110;&#21161;&#25919;&#31574;&#21046;&#23450;&#32773;&#20026;&#28040;&#36153;&#32773;&#23433;&#20840;&#25552;&#20379;&#20855;&#20307;&#30340;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FARM&#22312;&#35782;&#21035;&#19981;&#23433;&#20840;&#25991;&#26412;&#21644;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>ERNIE-Code&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20004;&#31181;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26234;&#33021;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;</title><link>http://arxiv.org/abs/2212.06742</link><description>&lt;p&gt;
ERNIE-Code: &#36229;&#36234;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#36328;&#35821;&#35328;&#32534;&#31243;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06742
&lt;/p&gt;
&lt;p&gt;
ERNIE-Code&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20004;&#31181;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26234;&#33021;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#20351;&#29992;&#21516;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21487;&#33021;&#20351;&#29992;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#36825;&#20250;&#23548;&#33268;&#27807;&#36890;&#21644;&#24037;&#20316;&#25928;&#29575;&#30340;&#24040;&#22823;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26426;&#31243;&#24207;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26159;&#26377;&#25928;&#30340;&#65292;&#28982;&#32780;&#23427;&#20204;&#24635;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24314;&#31435;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#26725;&#26753;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;ERNIE-Code&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#26222;&#36941;&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65306;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#20174;&#21333;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#25110;&#32534;&#31243;&#35821;&#35328;&#20013;&#23398;&#20064;&#27169;&#24335;&#65307;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20381;&#38752;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ERNIE-Code&#22312;&#20195;&#30721;&#26234;&#33021;&#30340;&#24191;&#27867;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#20195;&#30721;&#21040;&#25991;&#26412;&#65292;&#25991;&#26412;&#21040;&#20195;&#30721;&#65292;&#20195;&#30721;&#21040;&#20195;&#30721;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;</title><link>http://arxiv.org/abs/2212.03281</link><description>&lt;p&gt;
Copula&#32852;&#21512;&#39044;&#27979;&#29992;&#20110;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861; CopulaCPTS&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20854;&#32622;&#20449;&#21306;&#38388;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26159;&#26500;&#24314;&#24378;&#22823;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25311;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26080;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31639;&#27861;&#65292;&#22240;&#20854;&#26131;&#20110;&#23454;&#29616;&#12289;&#32479;&#35745;&#35206;&#30422;&#20445;&#35777;&#21644;&#23545;&#24213;&#23618;&#39044;&#27979;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#32622;&#20449;&#39044;&#27979;&#31639;&#27861;&#20165;&#38480;&#20110;&#21333;&#27493;&#39044;&#27979;&#65292;&#26410;&#32771;&#34385;&#26102;&#24207;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181; Copula &#32852;&#21512;&#39044;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#20803;&#12289;&#22810;&#27493;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979; CopulaCPTS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; CopulaCPTS &#20855;&#26377;&#26377;&#38480;&#30340;&#26679;&#26412;*&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; CopulaCPTS &#30340;&#22810;&#27493;&#39044;&#27979;&#21487;&#20135;&#29983;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#31934;&#20934;&#21644;&#26356;&#38160;&#21033;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.03000</link><description>&lt;p&gt;
SODA&#65306;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SODA&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#20854;&#20013;&#21547;&#26377;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#26816;&#39564;&#20102;SODA&#22312;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#65288;&#22914;&#20351;&#29992;&#38463;&#29255;&#31867;&#33647;&#29289;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: We aim to develop an open-source natural language processing (NLP) package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models to extract social determinants of health (SDoH) for cancer patients, examine the generalizability of SODA to a new disease domain (i.e., opioid use), and evaluate the extraction rate of SDoH using cancer populations.  Methods: We identified SDoH categories and attributes and developed an SDoH corpus using clinical notes from a general cancer cohort. We compared four transformer-based NLP models to extract SDoH, examined the generalizability of NLP models to a cohort of patients prescribed with opioids, and explored customization strategies to improve performance. We applied the best NLP model to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804), and colorectal cancer (n=6,240) cohorts.  Results and Conclusion: We developed a corpus of 629 cancer patients notes with annotations of 13,193 SDoH concepts/attribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#26356;&#25913;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;Transformer&#65289;&#20013;&#37319;&#26679;&#65292;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;</title><link>http://arxiv.org/abs/2211.17192</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;Transformer&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#26356;&#25913;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;Transformer&#65289;&#20013;&#37319;&#26679;&#65292;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Transformer&#31561;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#29702;&#26159;&#32531;&#24930;&#30340;&#65292;&#22240;&#20026;&#35299;&#30721;K&#20010;&#26631;&#35760;&#38656;&#35201;&#36816;&#34892;K&#27425;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25237;&#26426;&#35299;&#30721;&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#22810;&#20010;&#26631;&#35760;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;&#25105;&#20204;&#22312;T5-XXL&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#24182;&#26174;&#31034;&#30456;&#23545;&#20110;&#26631;&#20934;T5X&#23454;&#29616;&#65292;&#20854;&#21152;&#36895;&#20102;2X-3X&#65292;&#36755;&#20986;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference from large autoregressive models like Transformers is slow decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#30456;&#27604;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#19981;&#26356;&#39640;&#65292;&#20004;&#32773;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#36890;&#36807;&#20851;&#20110;&#26641;&#23485;&#30340;&#36793;&#30028;&#30028;&#23450;&#24471;&#21040;&#36739;&#22909;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2211.13447</link><description>&lt;p&gt;
&#35770;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of Counterfactual Reasoning. (arXiv:2211.13447v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#30456;&#27604;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#19981;&#26356;&#39640;&#65292;&#20004;&#32773;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#36890;&#36807;&#20851;&#20110;&#26641;&#23485;&#30340;&#36793;&#30028;&#30028;&#23450;&#24471;&#21040;&#36739;&#22909;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#19982;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#19978;&#36827;&#34892;&#20851;&#32852;&#21644;&#24178;&#39044;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#35745;&#31639;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#24182;&#19981;&#27604;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;SCMs&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#26356;&#38590;&#12290;&#31532;&#19968;&#20010;&#26694;&#26550;&#22522;&#20110;&#26641;&#23485;&#30340;&#27010;&#24565;&#65292;&#24182;&#21253;&#25324;&#32463;&#20856;&#30340;&#21464;&#37327;&#28040;&#38500;&#21644;&#32852;&#32467;&#26641;&#31639;&#27861;&#12290;&#31532;&#20108;&#20010;&#26694;&#26550;&#21017;&#22522;&#20110;&#26356;&#36817;&#26399;&#19988;&#26356;&#31934;&#32454;&#30340;&#22240;&#26524;&#26641;&#23485;&#27010;&#24565;&#65292;&#38024;&#23545;&#20855;&#26377;&#21151;&#33021;&#20381;&#36182;&#24615;&#30340;&#27169;&#22411;&#65292;&#22914;SCMs&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#24314;&#35774;&#24615;&#30340;&#65292;&#22522;&#20110;&#30028;&#23450;&#21452;&#32593;&#32476;&#65288;&#29992;&#20110;&#26631;&#20934;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#21253;&#25324;&#29616;&#23454;&#21644;&#24819;&#35937;&#20004;&#31181;&#24773;&#20917;&#65289;&#30340;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#65292;&#21040;&#22522;&#30784;SCM&#32467;&#26500;&#30340;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#21518;&#32773;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#19981;&#20250;&#36229;&#36807;&#21069;&#32773;&#30340;&#20004;&#20493;&#21152;&#19968;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the computational complexity of counterfactual reasoning in relation to the complexity of associational and interventional reasoning on structural causal models (SCMs). We show that counterfactual reasoning is no harder than associational or interventional reasoning on fully specified SCMs in the context of two computational frameworks. The first framework is based on the notion of treewidth and includes the classical variable elimination and jointree algorithms. The second framework is based on the more recent and refined notion of causal treewidth which is directed towards models with functional dependencies such as SCMs. Our results are constructive and based on bounding the (causal) treewidth of twin networks -- used in standard counterfactual reasoning that contemplates two worlds, real and imaginary -- to the (causal) treewidth of the underlying SCM structure. In particular, we show that the latter (causal) treewidth is no more than twice the former plus one. Hence, if a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#38543;&#26426;&#36807;&#31243;&#25193;&#25955;&#26041;&#27861;&#23558;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#23454;&#29616;&#20102;&#36866;&#29992;&#20110;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#21644;&#25554;&#34917;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.02590</link><description>&lt;p&gt;
&#29992;&#38543;&#26426;&#36807;&#31243;&#25193;&#25955;&#26041;&#27861;&#23558;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20026;&#36830;&#32493;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion. (arXiv:2211.02590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#38543;&#26426;&#36807;&#31243;&#25193;&#25955;&#26041;&#27861;&#23558;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#23454;&#29616;&#20102;&#36866;&#29992;&#20110;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#21644;&#25554;&#34917;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#31561;&#26102;&#38388;&#25968;&#25454;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#28508;&#22312;&#20989;&#25968;&#30340;&#31163;&#25955;&#21270;&#27979;&#37327;&#20540;&#12290;&#20026;&#20102;&#20026;&#36825;&#26679;&#30340;&#25968;&#25454;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24517;&#39035;&#23545;&#25511;&#21046;&#23427;&#30340;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23450;&#20041;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#25968;&#25454;&#12290;&#21069;&#21521;&#36807;&#31243;&#36880;&#28176;&#21521;&#20989;&#25968;&#28155;&#21152;&#22122;&#22768;&#65292;&#20445;&#25345;&#20854;&#36830;&#32493;&#24615;&#65292;&#32780;&#23398;&#20064;&#21040;&#30340;&#21453;&#21521;&#36807;&#31243;&#21017;&#31227;&#38500;&#22122;&#22768;&#24182;&#36820;&#22238;&#20989;&#25968;&#20316;&#20026;&#26032;&#30340;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#22122;&#22768;&#28304;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#21435;&#22122;&#21644;&#20998;&#20540;&#21305;&#37197;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#29992;&#20110;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#21644;&#25554;&#34917;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#35299;&#37322;&#20026;&#31070;&#32463;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal data such as time series can be viewed as discretized measurements of the underlying function. To build a generative model for such data we have to model the stochastic process that governs it. We propose a solution by defining the denoising diffusion model in the function space which also allows us to naturally handle irregularly-sampled observations. The forward process gradually adds noise to functions, preserving their continuity, while the learned reverse process removes the noise and returns functions as new samples. To this end, we define suitable noise sources and introduce novel denoising and score-matching models. We show how our method can be used for multivariate probabilistic forecasting and imputation, and how our model can be interpreted as a neural process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2211.00980</link><description>&lt;p&gt;
&#22312;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#27719;&#24635;&#12289;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#21644;&#25512;&#33616;&#31561;&#12290;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#35299;&#65292;&#20351;&#24471;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#65292;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#23376;&#27169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;&#25928;&#29992;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#32676;&#20307;&#30001;&#20960;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#32452;&#32452;&#25104;&#26102;&#65292;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#25928;&#29992;&#26159;&#21542;&#20844;&#24179;&#22320;&#20998;&#37197;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#20013;&#12290;&#34429;&#28982;&#25928;&#29992;&#21644;&#20844;&#24179;&#30446;&#26631;&#37117;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20114;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#19968;&#36215;&#20248;&#21270;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65288;BSM&#65289;&#65292;&#20197;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2211.00251</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differentiable Model Selection for Ensemble Learning. (arXiv:2211.00251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#21019;&#36896;&#20934;&#30830;&#21644;&#31283;&#20581;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#35774;&#35745;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#20219;&#20309;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#30340;&#26368;&#20339;&#20998;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32452;&#21512;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27979;&#35797;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#19988;&#22312;&#21508;&#31181;&#35774;&#32622;&#21644;&#23398;&#20064;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a strategy aimed at creating accurate and robust models. A key challenge in designing these algorithms is identifying the optimal model for classifying any particular input sample. This paper addresses this challenge and proposes a novel framework for differentiable model selection integrating machine learning and combinatorial optimization. The framework is tailored for ensemble learning, a strategy that combines the outputs of individually pre-trained models, and learns to select appropriate ensemble members for a particular input sample by transforming the ensemble learning task into a differentiable selection program trained end-to-end within the ensemble learning model. Tested on various tasks, the proposed framework demonstrates its versatility and effectiveness, outperforming conventional and advanced consensus rules across a variety of settings and learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20102;&#23545;HTML&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20351;&#20854;&#22312;&#35821;&#20041;&#20998;&#31867;&#12289;&#25551;&#36848;&#29983;&#25104;&#21644;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;HTML&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.03945</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;HTML
&lt;/p&gt;
&lt;p&gt;
Understanding HTML with Large Language Models. (arXiv:2210.03945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20102;&#23545;HTML&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20351;&#20854;&#22312;&#35821;&#20041;&#20998;&#31867;&#12289;&#25551;&#36848;&#29983;&#25104;&#21644;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;HTML&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;HTML&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#8212;&#8212;&#21363;&#35299;&#26512;&#32593;&#39029;&#30340;&#21407;&#22987;HTML&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#32593;&#32476;&#20219;&#21153;&#12289;&#29228;&#21462;&#21644;&#27983;&#35272;&#22120;&#36741;&#21161;&#26816;&#32034;&#31561;&#26041;&#38754;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#23436;&#20840;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65288;&#24494;&#35843;LLMs&#65289;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#65306;&#65288;i&#65289;HTML&#20803;&#32032;&#30340;&#35821;&#20041;&#20998;&#31867;&#65292;&#65288;ii&#65289;HTML&#36755;&#20837;&#30340;&#25551;&#36848;&#29983;&#25104;&#65292;&#20197;&#21450;&#65288;iii&#65289;HTML&#39029;&#38754;&#30340;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;HTML&#29702;&#35299;&#24320;&#21457;&#20102;&#19987;&#29992;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#20110;&#26631;&#20934;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;LLMs&#38750;&#24120;&#36866;&#29992;&#20110;HTML&#29702;&#35299;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#24494;&#35843;&#21518;&#30340;LLMs&#22312;&#35821;&#20041;&#20998;&#31867;&#26041;&#38754;&#27604;&#20165;&#22522;&#20110;&#20219;&#21153;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#39640;12%&#12290;&#27492;&#22806;&#65292;&#24403;&#23427;&#20204;&#34987;&#24494;&#35843;&#20110;MiniW&#30340;&#25968;&#25454;&#26102;&#65292;LLMs&#30340;&#25551;&#36848;&#29983;&#25104;&#22312;&#20154;&#31867;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#19982;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#65292;&#32780;&#19988;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#22320;&#33258;&#20027;&#22320;&#27983;&#35272;HTML&#39029;&#38754;&#65292;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniW
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.00637</link><description>&lt;p&gt;
&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20854;&#20013;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#34920;&#31034;&#12290;&#26412;&#35770;&#25991;&#27491;&#24335;&#21270;&#20102;&#23547;&#25214;&#26368;&#20339;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#25968;&#23398;&#38382;&#39064;&#24182;&#34920;&#24449;&#20854;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;BAE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;BAE&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#19968;&#20010;&#27969;&#22411;&#19978;&#65292;&#20854;&#32500;&#25968;&#20026;&#29983;&#25104;&#38382;&#39064;&#30340;&#26368;&#20339;&#21487;&#21387;&#32553;&#32500;&#24230;&#12290;&#25105;&#20204;&#24378;&#35843;BAE&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#20960;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#26041;&#21521;&#20043;&#38388;&#30340;&#24778;&#20154;&#32852;&#31995;&#65292;&#22914;&#26377;&#26465;&#20214;&#30340;GAN&#65292;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#31283;&#23450;&#25193;&#25955;&#65292;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BAE&#22914;&#20309;&#25214;&#21040;&#26368;&#20248;&#30340;&#20302;&#32500;&#28508;&#22312;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#25552;&#39640;&#37492;&#21035;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21387;&#32553;&#8220;&#24694;&#24615;&#8221;&#25968;&#25454;&#32500;&#24230;&#65292;BAE&#23548;&#33268;&#26799;&#24230;&#26356;&#21152;&#24179;&#28369;&#21644;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;NIE&#65289;&#21644;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;ANIE&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#31215;&#20998;&#31639;&#23376;&#36827;&#34892;&#27169;&#22411;&#24314;&#31435;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15190</link><description>&lt;p&gt;
&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Integral Equations. (arXiv:2209.15190v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;NIE&#65289;&#21644;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;ANIE&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#31215;&#20998;&#31639;&#23376;&#36827;&#34892;&#27169;&#22411;&#24314;&#31435;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#20998;&#26041;&#31243; (IEs) &#26159;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26102;&#31354;&#31995;&#32479;&#30340;&#26041;&#31243;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#29702;&#35770;&#21644;&#24212;&#29992;&#31185;&#23398;&#20013;&#25214;&#21040;&#20102;&#37325;&#35201;&#24212;&#29992;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#24037;&#31243;&#23398;&#12290;&#34429;&#28982;&#23384;&#22312;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;IEs&#65292;&#20294;&#19981;&#23384;&#22312;&#21487;&#20197;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;IE&#21644;&#20854;&#30456;&#20851;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243; (NIE)&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;IE&#27714;&#35299;&#22120;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#30340;&#31215;&#20998;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243; (ANIE)&#65292;&#20854;&#20013;&#31215;&#20998;&#34987;&#33258;&#27880;&#24847;&#21147;&#26367;&#25442;&#65292;&#36825;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12289;&#23481;&#37327;&#65292;&#24182;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;(A)NIE&#22312;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral equations (IEs) are equations that model spatiotemporal systems with non-local interactions. They have found important applications throughout theoretical and applied sciences, including in physics, chemistry, biology, and engineering. While efficient algorithms exist for solving given IEs, no method exists that can learn an IE and its associated dynamics from data alone. In this paper, we introduce Neural Integral Equations (NIE), a method that learns an unknown integral operator from data through an IE solver. We also introduce Attentional Neural Integral Equations (ANIE), where the integral is replaced by self-attention, which improves scalability, capacity, and results in an interpretable model. We demonstrate that (A)NIE outperforms other methods in both speed and accuracy on several benchmark tasks in ODE, PDE, and IE systems of synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25512;&#23548;&#20102;&#26368;&#20248;&#21464;&#25442;&#24182;&#35777;&#26126;&#20102;&#24403;&#20854;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#20174;&#32780;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.05042</link><description>&lt;p&gt;
&#20851;&#20110;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#30340;&#20248;&#21270;&#26223;&#35266;: &#22522;&#20110;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25512;&#23548;&#20102;&#26368;&#20248;&#21464;&#25442;&#24182;&#35777;&#26126;&#20102;&#24403;&#20854;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#20174;&#32780;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25910;&#25947;&#21462;&#20915;&#20110;&#22522;&#30784;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#20248;&#21270;&#26223;&#35266;&#12290;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#36825;&#20123;&#31639;&#27861;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#22823;&#22810;&#21482;&#32771;&#34385;&#38745;&#24577;&#20840;&#29366;&#24577;&#25110;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#65288;&#25511;&#21046;&#22120;&#65289;&#30340;&#20248;&#21270;&#26223;&#35266;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#31574;&#30053;&#65288;&#31616;&#31216; dLQR&#65289;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#20854;&#20248;&#21270;&#26223;&#35266;&#30456;&#23545;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102; dLQR &#25104;&#26412;&#22914;&#20309;&#38543;&#21160;&#24577;&#25511;&#21046;&#22120;&#30340;&#22352;&#26631;&#21464;&#25442;&#32780;&#21464;&#21270;&#65292;&#28982;&#21518;&#25512;&#23548;&#20102;&#32473;&#23450;&#21487;&#35266;&#25511;&#31283;&#23450;&#25511;&#21046;&#22120;&#30340;&#26368;&#20248;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#26680;&#24515;&#32467;&#26524;&#26159; dLQR &#22312;&#21487;&#35266;&#27979;&#26102;&#38745;&#27490;&#28857;&#30340;&#21807;&#19968;&#24615;&#65292;&#36825;&#20026;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#21160;&#24577;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25253;&#20215;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#21450;&#19981;&#30830;&#23450;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#19982;&#26174;&#33879;&#30340;&#32463;&#27982;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2209.02009</link><description>&lt;p&gt;
&#39118;&#33021;&#20132;&#26131;&#30340;&#22312;&#32447;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Online Decision Making for Trading Wind Energy. (arXiv:2209.02009v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25253;&#20215;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#21450;&#19981;&#30830;&#23450;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#19982;&#26174;&#33879;&#30340;&#32463;&#27982;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#26694;&#26550;&#20869;&#65292;&#29992;&#20110;&#22312;&#30005;&#21147;&#24066;&#22330;&#20013;&#20132;&#26131;&#39118;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#36880;&#20998;&#37327;&#33258;&#36866;&#24212;&#21464;&#20307;&#19982;&#26368;&#26032;&#36827;&#23637;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#26032;&#38395;&#20379;&#24212;&#21830;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#36825;&#23548;&#33268;&#19968;&#31181;&#22312;&#32447;&#25253;&#20215;&#26041;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#65292;&#21516;&#26102;&#36866;&#24212;&#33021;&#28304;&#20135;&#29983;&#21644;&#30005;&#21147;&#24066;&#22330;&#30340;&#19981;&#31283;&#23450;&#29305;&#24449;&#65292;&#32780;&#19988;&#35745;&#31639;&#36127;&#25285;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#22522;&#20110;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#36739;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#26174;&#33879;&#30340;&#32463;&#27982;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and develop a new algorithm for trading wind energy in electricity markets, within an online learning and optimization framework. In particular, we combine a component-wise adaptive variant of the gradient descent algorithm with recent advances in the feature-driven newsvendor model. This results in an online offering approach capable of leveraging data-rich environments, while adapting to the nonstationary characteristics of energy generation and electricity markets, also with a minimal computational burden. The performance of our approach is analyzed based on several numerical experiments, showing both better adaptability to nonstationary uncertain parameters and significant economic gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#65292;ID&#24615;&#33021;&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#21453;&#30456;&#20851;&#20851;&#31995;&#65292;&#25552;&#31034;&#38656;&#35201;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21333;&#32431;&#20851;&#27880;ID&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00613</link><description>&lt;p&gt;
ID&#21644;OOD&#24615;&#33021;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#26102;&#26159;&#21453;&#30456;&#20851;&#30340;
&lt;/p&gt;
&lt;p&gt;
ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets. (arXiv:2209.00613v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#65292;ID&#24615;&#33021;&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#21453;&#30456;&#20851;&#20851;&#31995;&#65292;&#25552;&#31034;&#38656;&#35201;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21333;&#32431;&#20851;&#27880;ID&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#27604;&#36739;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;NLP&#27169;&#22411;&#30340;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#23427;&#20204;&#25253;&#36947;&#20102;&#39057;&#32321;&#30340;&#27491;&#30456;&#20851;&#24615;&#65292;&#24182;&#26377;&#19968;&#20123;&#24778;&#20154;&#30340;&#30740;&#31350;&#29978;&#33267;&#27809;&#26377;&#35266;&#23519;&#21040;&#21453;&#30456;&#20851;&#65292;&#34920;&#26126;&#24517;&#39035;&#36827;&#34892;&#26435;&#34913;&#12290;&#30830;&#23450;&#21453;&#21521;&#27169;&#24335;&#30340;&#21487;&#33021;&#24615;&#24456;&#37325;&#35201;&#65292;&#20197;&#30830;&#23450;ID&#24615;&#33021;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;OOD&#27867;&#21270;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#26174;&#31034;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;ID&#24615;&#33021;&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#30340;&#21453;&#30456;&#20851;&#20851;&#31995;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30830;&#23454;&#23384;&#22312; - &#19981;&#20165;&#22312;&#29702;&#35770;&#26368;&#22351;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#21363;&#20351;&#22312;&#26368;&#23567;&#32447;&#24615;&#35774;&#32622;&#20013;&#20063;&#21487;&#20197;&#20986;&#29616;&#36825;&#20123;&#24773;&#20917;&#20197;&#21450;&#20026;&#20160;&#20040;&#20197;&#21069;&#30340;&#30740;&#31350;&#21487;&#33021;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#30340;&#20559;&#35265;&#32780;&#24573;&#30053;&#20102;&#36825;&#20123;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#23548;&#33268;&#30340;&#24314;&#35758;&#19982;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#22823;&#37096;&#20998;&#30456;&#21453;&#12290; - &#39640;OOD&#24615;&#33021;&#26377;&#26102;&#38656;&#35201;&#29306;&#29298;ID&#24615;&#33021;&#12290;- &#21333;&#32431;&#20851;&#27880;ID&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several studies have compared the in-distribution (ID) and out-of-distribution (OOD) performance of models in computer vision and NLP. They report a frequent positive correlation and some surprisingly never even observe an inverse correlation indicative of a necessary trade-off. The possibility of inverse patterns is important to determine whether ID performance can serve as a proxy for OOD generalization capabilities.  This paper shows with multiple datasets that inverse correlations between ID and OOD performance do happen in real-world data - not only in theoretical worst-case settings. We also explain theoretically how these cases can arise even in a minimal linear setting, and why past studies could miss such cases due to a biased selection of models.  Our observations lead to recommendations that contradict those found in much of the current literature. - High OOD performance sometimes requires trading off ID performance. - Focusing on ID performance alone may not lead to optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedD3&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#20165;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#26469;&#36866;&#24212;&#20351;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2208.11311</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;&#30340;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedD3&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#20165;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#26469;&#36866;&#24212;&#20351;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25152;&#26377;&#30340;&#32852;&#32593;&#23458;&#25143;&#31471;&#21327;&#20316;&#22320;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#21363;&#20351;&#22312;&#36845;&#20195;&#36890;&#20449;&#20013;&#20849;&#20139;&#24050;&#35757;&#32451;&#30340;&#37096;&#20998;&#27169;&#22411;&#65292;&#20063;&#24448;&#24448;&#20250;&#23548;&#33268;&#24213;&#23618;&#32593;&#32476;&#20013;&#30340;&#20005;&#37325;&#36890;&#20449;&#29942;&#39048;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedD3&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#24182;&#38598;&#25104;&#20102;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#12290;FedD3&#19981;&#21516;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#20849;&#20139;&#27169;&#22411;&#26356;&#26032;&#65292;&#23427;&#20801;&#35768;&#36830;&#25509;&#30340;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#25552;&#28860;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20174;&#32593;&#32476;&#20013;&#32858;&#21512;&#37027;&#20123;&#20998;&#25955;&#30340;&#25552;&#28860;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#26080;&#27861;&#35782;&#21035;&#30340;&#22270;&#20687;&#65289;&#24182;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;FedD3&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#23427;&#33021;&#22815;&#22312;&#20351;&#29992;&#22330;&#26223;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage sc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#23558;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#31616;&#21270;&#20026;&#32500;&#24230;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#31934;&#30830;&#22797;&#29616;&#20854;&#36793;&#32536;&#20998;&#24067;&#65292;&#39318;&#27425;&#25193;&#23637;&#20102;&#23454;&#29616;&#29702;&#35770;&#24037;&#20855;&#21040;&#36825;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2208.05968</link><description>&lt;p&gt;
&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20195;&#25968;&#32422;&#31616;
&lt;/p&gt;
&lt;p&gt;
Algebraic Reduction of Hidden Markov Models. (arXiv:2208.05968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#29992;&#20110;&#23558;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#31616;&#21270;&#20026;&#32500;&#24230;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#31934;&#30830;&#22797;&#29616;&#20854;&#36793;&#32536;&#20998;&#24067;&#65292;&#39318;&#27425;&#25193;&#23637;&#20102;&#23454;&#29616;&#29702;&#35770;&#24037;&#20855;&#21040;&#36825;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31995;&#32479;&#29702;&#35770;&#26041;&#27861;&#65292;&#21033;&#29992;&#36866;&#24403;&#30340;&#27010;&#29575;&#31354;&#38388;&#30340;&#20195;&#25968;&#34920;&#31034;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#31616;&#21270;&#20026;&#19968;&#20010;&#26356;&#23567;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#20197;&#23436;&#20840;&#20877;&#29616;&#30456;&#21516;&#36793;&#32536;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#31639;&#23376;&#36820;&#22238;&#31895;&#31890;&#24230;&#31561;&#25928;&#30340;HMM: &#31532;&#19968;&#31181;&#36820;&#22238;&#31934;&#30830;&#22797;&#21046;&#32473;&#23450;&#36755;&#20986;&#36807;&#31243;&#30340;&#21333;&#26102;&#38388;&#20998;&#24067;&#30340;&#27169;&#22411;&#65292;&#32780;&#31532;&#20108;&#31181;&#21017;&#20445;&#30041;&#23436;&#25972;(&#22810;&#26102;&#38388;)&#20998;&#24067;&#12290;&#35813;&#31616;&#21270;&#26041;&#27861;&#19981;&#20165;&#21033;&#29992;&#20102;&#35266;&#27979;&#36755;&#20986;&#30340;&#32467;&#26500;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#20854;&#21021;&#22987;&#26465;&#20214;&#65292;&#21482;&#35201;&#21518;&#32773;&#24050;&#30693;&#25110;&#23646;&#20110;&#32473;&#23450;&#30340;&#23376;&#31867;&#12290;&#23545;&#20110;&#21487;&#35266;&#27979;&#31867;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of reducing a Hidden Markov Model (HMM) to one of smaller dimension that exactly reproduces the same marginals is tackled by using a system-theoretic approach. Realization theory tools are extended to HMMs by leveraging suitable algebraic representations of probability spaces. We propose two algorithms that return coarse-grained equivalent HMMs obtained by stochastic projection operators: the first returns models that exactly reproduce the single-time distribution of a given output process, while in the second the full (multi-time) distribution is preserved. The reduction method exploits not only the structure of the observed output, but also its initial condition, whenever the latter is known or belongs to a given subclass. Optimal algorithms are derived for a class of HMM, namely observable ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#37197;&#32622;&#30340;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#26041;&#26696;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#28040;&#38500;&#37319;&#26679;&#36335;&#24452;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#26131;&#20110;&#24182;&#34892;&#21270;&#37319;&#26679;&#36807;&#31243;&#65292;&#36890;&#36807;&#26465;&#20214;&#35774;&#32622;&#23558;&#37197;&#32622;&#37319;&#26679;&#24341;&#23548;&#21040;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2207.14530</link><description>&lt;p&gt;
&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#31232;&#26377;&#20107;&#20214;&#37319;&#26679;&#30340;&#26465;&#20214;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditioning Normalizing Flows for Rare Event Sampling. (arXiv:2207.14530v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#37197;&#32622;&#30340;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#26041;&#26696;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#28040;&#38500;&#37319;&#26679;&#36335;&#24452;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#26131;&#20110;&#24182;&#34892;&#21270;&#37319;&#26679;&#36807;&#31243;&#65292;&#36890;&#36807;&#26465;&#20214;&#35774;&#32622;&#23558;&#37197;&#32622;&#37319;&#26679;&#24341;&#23548;&#21040;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22797;&#26434;&#20998;&#23376;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#36890;&#24120;&#28041;&#21450;&#30740;&#31350;&#38271;&#23551;&#21629;&#31283;&#23450;&#24577;&#20043;&#38388;&#30340;&#19981;&#32463;&#24120;&#36716;&#25442;&#12290;&#36890;&#24120;&#37319;&#29992;&#30340;&#31232;&#26377;&#20107;&#20214;&#37319;&#26679;&#26041;&#27861;&#26159;&#20351;&#29992;&#36712;&#36857;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#29983;&#25104;&#36716;&#25442;&#36335;&#24452;&#30340;&#38598;&#21512;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#38543;&#21518;&#37319;&#26679;&#36335;&#24452;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#20197;&#21450;&#22312;&#24182;&#34892;&#21270;&#37319;&#26679;&#36807;&#31243;&#20013;&#23384;&#22312;&#22266;&#26377;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#37197;&#32622;&#30340;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#26041;&#26696;&#12290;&#36825;&#20123;&#37197;&#32622;&#26159;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#33719;&#24471;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31867;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20998;&#24067;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#19981;&#20165;&#28040;&#38500;&#20102;&#25152;&#35775;&#38382;&#36335;&#24452;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19988;&#37319;&#26679;&#36807;&#31243;&#26131;&#20110;&#24182;&#34892;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#21487;&#20197;&#23558;&#37197;&#32622;&#30340;&#37319;&#26679;&#24341;&#23548;&#21040;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;...
&lt;/p&gt;
&lt;p&gt;
Understanding the dynamics of complex molecular processes is often linked to the study of infrequent transitions between long-lived stable states. The standard approach to the sampling of such rare events is to generate an ensemble of transition paths using a random walk in trajectory space. This, however, comes with the drawback of strong correlations between subsequently sampled paths and with an intrinsic difficulty in parallelizing the sampling process. We propose a transition path sampling scheme based on neural-network generated configurations. These are obtained employing normalizing flows, a neural network class able to generate statistically independent samples from a given distribution. With this approach, not only are correlations between visited paths removed, but the sampling process becomes easily parallelizable. Moreover, by conditioning the normalizing flow, the sampling of configurations can be steered towards regions of interest. We show that this approach enables the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.03075</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#21307;&#23398;&#39046;&#22495;&#26159;&#37319;&#29992;FL&#30340;&#26368;&#36866;&#21512;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#24517;&#39035;&#23562;&#37325;&#24739;&#32773;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25552;&#20379;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#24212;&#29992;FL&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21363;&#38271;&#26399;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#30382;&#32932;&#30284;&#22270;&#20687;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#25552;&#20986;&#32463;&#39564;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#12290;&#28508;&#22312;&#30340;FL&#29992;&#25143;&#65292;&#22914;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#22522;&#20934;&#20316;&#20026;&#37319;&#29992;FL&#30340;&#25351;&#21335;&#65292;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#35797;&#38169;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#20197;&#20445;&#30041;&#29616;&#23454;&#19990;&#30028;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#38024;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;FL&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#20004;&#31181;&#20856;&#22411;FL&#31639;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;&#22522;&#20110;&#19977;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#25552;&#20379;&#20102;&#22312;&#23433;&#20840;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#65292;&#24212;&#29992;FL&#20174;&#32780;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.05880</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Confident Sinkhorn Allocation for Pseudo-Labeling. (arXiv:2206.05880v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#23545;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#25968;&#25454;&#22686;&#24378;&#30340;&#20869;&#22312;&#31354;&#38388;&#21644;&#35821;&#20041;&#32467;&#26500;&#25104;&#21151;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#27809;&#26377;&#36866;&#24403;&#30340;&#32467;&#26500;&#25110;&#19981;&#21464;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#20266;&#26631;&#31614;&#65288;PL&#65289;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;PL &#23545;&#38408;&#20540;&#25935;&#24863;&#65292;&#22914;&#26524;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#38169;&#35823;&#20998;&#37197;&#65292;&#21017;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#20266;&#26631;&#31614;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#27888;&#26862;&#22561;&#20998;&#37197;&#30340;&#26041;&#27861;(CSA)&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#20165;&#23545;&#39640;&#32622;&#20449;&#24230;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#12290;CSA &#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#36825;&#19968;&#23454;&#38469;&#37325;&#35201;&#39046;&#22495;&#20013;&#32988;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#26131;&#23454;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning is a critical tool in reducing machine learning's dependence on labeled data. It has been successfully applied to structured data, such as images and natural language, by exploiting the inherent spatial and semantic structure therein with pretrained models or data augmentation. These methods are not applicable, however, when the data does not have the appropriate structure, or invariances. Due to their simplicity, pseudo-labeling (PL) methods can be widely used without any domain assumptions. However, PL is sensitive to a threshold and can perform poorly if wrong assignments are made due to overconfidence. This paper studies theoretically the role of uncertainty to pseudo-labeling and proposes Confident Sinkhorn Allocation (CSA), which identifies the best pseudo-label allocation via optimal transport to only samples with high confidence scores. CSA outperforms the current state-of-the-art in this practically important area of semi-supervised learning. Additiona
&lt;/p&gt;</description></item><item><title>DELTA &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#26469;&#20943;&#23569;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#25152;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20197;&#32531;&#35299;&#29616;&#26377;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23427;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DELTA &#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#24182;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.13925</link><description>&lt;p&gt;
DELTA: &#22810;&#26679;&#21270;&#23458;&#25143;&#25277;&#26679;&#29992;&#20110;&#24555;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DELTA: Diverse Client Sampling for Fasting Federated Learning. (arXiv:2205.13925v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13925
&lt;/p&gt;
&lt;p&gt;
DELTA &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#26469;&#20943;&#23569;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#25152;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20197;&#32531;&#35299;&#29616;&#26377;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23427;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DELTA &#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#24182;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#39640;&#25928;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;&#23458;&#25143;&#31471;&#25277;&#26679;&#26041;&#26696;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#20986;&#19981;&#20855;&#20195;&#34920;&#24615;&#23376;&#38598;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#30340;&#26174;&#33879;&#26041;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#30340;&#20943;&#24930;&#12290;&#29616;&#26377;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#33021;&#20250;&#26377;&#20559;&#24046;&#65292;&#25110;&#32773;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DELTA&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#12290;DELTA &#21051;&#30011;&#20102;&#23458;&#25143;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#23616;&#37096;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#25152;&#38656;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20195;&#34920;&#24615;&#23458;&#25143;&#31471;&#12290;&#27492;&#22806;&#65292;DELTA &#26159;&#19968;&#31181;&#32463;&#36807;&#35777;&#26126;&#30340;&#26368;&#20248;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#65292;&#22312;&#20943;&#23569;&#30001;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24341;&#36215;&#30340;&#26041;&#24046;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26080;&#20559;&#25277;&#26679;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#35299;&#20915;&#20840;&#23458;&#25143;&#31471;&#26799;&#24230;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#29256;&#26412;&#30340; DELTA&#65292;&#23427;&#21462;&#20915;&#20110;&#21487;&#29992;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial client participation has been widely adopted in Federated Learning (FL) to reduce the communication burden efficiently. However, an inadequate client sampling scheme can lead to the selection of unrepresentative subsets, resulting in significant variance in model updates and slowed convergence. Existing sampling methods are either biased or can be further optimized for faster convergence.In this paper, we present DELTA, an unbiased sampling scheme designed to alleviate these issues. DELTA characterizes the effects of client diversity and local variance, and samples representative clients with valuable information for global model updates. In addition, DELTA is a proven optimal unbiased sampling scheme that minimizes variance caused by partial client participation and outperforms other unbiased sampling schemes in terms of convergence. Furthermore, to address full-client gradient dependence,we provide a practical version of DELTA depending on the available clients' information, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PECCO&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#38388;&#30340;&#23545;&#31216;&#24615;&#21644;&#33021;&#37327;&#35780;&#20998;&#35268;&#21017;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2205.01927</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#30340;&#27010;&#29575;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Symmetry for Multi-Agent Dynamics. (arXiv:2205.01927v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PECCO&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#38388;&#30340;&#23545;&#31216;&#24615;&#21644;&#33021;&#37327;&#35780;&#20998;&#35268;&#21017;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#39550;&#39542;&#31561;&#39046;&#22495;&#12290;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#32780;&#20135;&#29983;&#27010;&#29575;&#39044;&#27979;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#19982;&#35780;&#20272;&#39118;&#38505;&#26041;&#38754;&#23545;&#19979;&#28216;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#65292;&#22914;&#36816;&#21160;&#35268;&#21010;&#19982;&#36991;&#30896;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#36890;&#24120;&#21253;&#21547;&#20869;&#37096;&#23545;&#31216;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#65292;&#29305;&#21035;&#26159;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#36824;&#21487;&#20197;&#25913;&#21892;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#33021;&#37327;&#35780;&#20998;&#20316;&#20026;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#26469;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21363;&#27010;&#29575;&#31561;&#21464;&#36830;&#32493;&#21367;&#31215;&#65288;PECCO&#65289;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;PECCO&#23558;&#31561;&#21464;&#30340;&#36830;&#32493;&#21367;&#31215;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#30340;&#32852;&#21512;&#36895;&#24230;&#20998;&#24067;&#24314;&#27169;&#19978;&#12290;&#23427;&#20351;&#29992;&#21160;&#24577;&#31215;&#20998;&#23558;&#19981;&#30830;&#23450;&#24615;&#20174;&#36895;&#24230;&#20256;&#25773;&#21040;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#22330;&#26223;&#30340;&#32467;&#26524;&#20013;&#39564;&#35777;&#20102;PECCO&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-agent dynamics is a core AI problem with broad applications in robotics and autonomous driving. While most existing works focus on deterministic prediction, producing probabilistic forecasts to quantify uncertainty and assess risks is critical for downstream decision-making tasks such as motion planning and collision avoidance. Multi-agent dynamics often contains internal symmetry. By leveraging symmetry, specifically rotation equivariance, we can improve not only the prediction accuracy but also uncertainty calibration. We introduce Energy Score, a proper scoring rule, to evaluate probabilistic predictions. We propose a novel deep dynamics model, Probabilistic Equivariant Continuous COnvolution (PECCO) for probabilistic prediction of multi-agent trajectories. PECCO extends equivariant continuous convolution to model the joint velocity distribution of multiple agents. It uses dynamics integration to propagate the uncertainty from velocity to position. On both synthetic a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.02128</link><description>&lt;p&gt;
&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Bayesian Optimization with $\phi$-divergences. (arXiv:2203.02128v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#30740;&#31350;&#22240;&#20854;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35768;&#22810;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#23427;&#38754;&#20020;&#30528;&#22810;&#26041;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20165;&#26377;&#23569;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36825;&#20010;&#26041;&#21521;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$\phi$-&#31163;&#25955;&#24230;&#30340;&#20998;&#24067;&#40065;&#26834;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\phi$-divergences, which subsumes many popular choices, such as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Lei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Sparsemax&#30340;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;Transformer&#19981;&#19968;&#23450;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#26377;&#28145;&#21051;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2202.03932</link><description>&lt;p&gt;
Transformer&#26356;&#21152;&#31283;&#20581;&#21527;&#65311;&#38754;&#21521;Transformer&#30340;&#30830;&#20999;&#31283;&#20581;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Are Transformers More Robust? Towards Exact Robustness Verification for Transformers. (arXiv:2202.03932v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Sparsemax&#30340;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;Transformer&#19981;&#19968;&#23450;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#26377;&#28145;&#21051;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26032;&#20852;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65292;Transformer&#34987;&#24212;&#29992;&#20110;&#20247;&#22810;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;&#20302;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;Sparsemax&#30340;Transformers&#65292;&#23558;&#25214;&#21040;&#23427;&#20204;&#30340;&#26368;&#22823;&#31283;&#20581;&#24615;&#38477;&#20302;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#32534;&#31243;&#65288;MIQCP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20004;&#20010;&#21487;&#23884;&#20837;MIQCP&#32534;&#30721;&#24182;&#22823;&#24133;&#21152;&#36895;&#20854;&#27714;&#35299;&#30340;&#39044;&#22788;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Land Departure Warning&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#22522;&#20110;Sparsemax&#30340;Transformers&#19982;&#26356;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;Transformer&#24182;&#19981;&#19968;&#23450;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#30340;&#28145;&#21051;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#25239;&#30456;&#20851;&#22122;&#22768;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#26159;&#22240;&#20026; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;</title><link>http://arxiv.org/abs/2202.02831</link><description>&lt;p&gt;
&#25239;&#30456;&#20851;&#22122;&#22768;&#27880;&#20837;&#29992;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Anticorrelated Noise Injection for Improved Generalization. (arXiv:2202.02831v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#25239;&#30456;&#20851;&#22122;&#22768;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#26159;&#22240;&#20026; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20154;&#24037;&#22122;&#22768;&#27880;&#20837;&#26799;&#24230;&#19979;&#38477;&#24120;&#24120;&#34987;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#25200;&#21160;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20351;&#29992;&#30340;&#26159;&#19981;&#30456;&#20851;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#30456;&#20851;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#30446;&#26631;&#20989;&#25968;&#65292;&#21457;&#29616;&#24102;&#26377;&#25239;&#30456;&#20851;&#25200;&#21160;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;"Anti-PGD"&#65289;&#27604;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#24120;&#35268;&#30340;&#65288;&#19981;&#30456;&#20851;&#30340;&#65289;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#26377;&#30528;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102; Anti-PGD &#33021;&#22815;&#31227;&#21160;&#21040;&#26356;&#23485;&#30340;&#26368;&#23567;&#20540;&#28857;&#65292;&#32780; GD &#21644; PGD &#20250;&#20572;&#28382;&#22312;&#27425;&#20248;&#21306;&#22495;&#29978;&#33267;&#21457;&#25955;&#12290;&#36825;&#19968;&#26032;&#39062;&#30340;&#25239;&#30456;&#20851;&#22122;&#22768;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#32852;&#31995;&#20026;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injecting artificial noise into gradient descent (GD) is commonly employed to improve the performance of machine learning models. Usually, uncorrelated noise is used in such perturbed gradient descent (PGD) methods. It is, however, not known if this is optimal or whether other types of noise could provide better generalization performance. In this paper, we zoom in on the problem of correlating the perturbations of consecutive PGD steps. We consider a variety of objective functions for which we find that GD with anticorrelated perturbations ("Anti-PGD") generalizes significantly better than GD and standard (uncorrelated) PGD. To support these experimental findings, we also derive a theoretical analysis that demonstrates that Anti-PGD moves to wider minima, while GD and PGD remain stuck in suboptimal regions or even diverge. This new connection between anticorrelated noise and generalization opens the field to novel ways to exploit noise for training machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;NVM&#20132;&#21449;&#22411;&#23384;&#20648;&#22120;&#20869;&#22312;&#40065;&#26834;&#24615;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#25506;&#32034;&#22914;&#20309;&#35774;&#35745;&#40065;&#26834;&#30340;DNN&#12290;&#23545;&#32593;&#32476;&#26410;&#21463;&#24178;&#25200;&#36755;&#20837;&#25968;&#25454;&#19979;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#30340;&#32593;&#32476;&#20855;&#26377;&#26356;&#20302;&#30340;S&#20540;&#12290;</title><link>http://arxiv.org/abs/2109.09060</link><description>&lt;p&gt;
&#20851;&#20110;NVM&#20132;&#21449;&#22411;&#23384;&#20648;&#22120;&#19978;&#23545;&#25239;&#35757;&#32451;&#32593;&#32476;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Noise Stability and Robustness of Adversarially Trained Networks on NVM Crossbars. (arXiv:2109.09060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;NVM&#20132;&#21449;&#22411;&#23384;&#20648;&#22120;&#20869;&#22312;&#40065;&#26834;&#24615;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#25506;&#32034;&#22914;&#20309;&#35774;&#35745;&#40065;&#26834;&#30340;DNN&#12290;&#23545;&#32593;&#32476;&#26410;&#21463;&#24178;&#25200;&#36755;&#20837;&#25968;&#25454;&#19979;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#30340;&#32593;&#32476;&#20855;&#26377;&#26356;&#20302;&#30340;S&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#32593;&#32476;&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;NVM&#65289;&#20132;&#21449;&#22411;&#23384;&#20648;&#22120;&#30340;&#21152;&#36895;&#22120;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38500;&#20102;&#26174;&#33879;&#25552;&#39640;&#33021;&#25928;&#21644;&#24615;&#33021;&#22806;&#65292;&#36825;&#31181;&#36817;&#20284;&#30828;&#20214;&#36824;&#20855;&#26377;&#20869;&#22312;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38024;&#23545;&#26410;&#21463;&#24178;&#25200;&#36755;&#20837;&#25968;&#25454;&#35757;&#32451;&#30340;&#22522;&#26412;DNN&#23545;&#20854;&#20869;&#22312;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#35757;&#32451;DNN&#24050;&#25104;&#20026;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#25216;&#26415;&#65292;&#20165;&#20381;&#38752;&#30828;&#20214;&#30340;&#20869;&#22312;&#40065;&#26834;&#24615;&#21487;&#33021;&#19981;&#36275;&#20197;&#24212;&#23545;&#27492;&#31867;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;NVM&#20132;&#20114;&#24335;&#23384;&#20648;&#22120;&#27169;&#25311;&#30828;&#20214;&#30340;&#20869;&#22312;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#40065;&#26834;DNN&#30340;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#32593;&#32476;&#22312;&#26410;&#21463;&#24178;&#25200;&#36755;&#20837;&#19979;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#23545;&#25239;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#20869;&#37096;&#28608;&#27963;&#25317;&#26377;&#26356;&#20302;&#30340;S
&lt;/p&gt;
&lt;p&gt;
Applications based on Deep Neural Networks (DNNs) have grown exponentially in the past decade. To match their increasing computational needs, several Non-Volatile Memory (NVM) crossbar based accelerators have been proposed. Recently, researchers have shown that apart from improved energy efficiency and performance, such approximate hardware also possess intrinsic robustness for defense against adversarial attacks. Prior works quantified this intrinsic robustness for vanilla DNNs trained on unperturbed inputs. However, adversarial training of DNNs is the benchmark technique for robustness, and sole reliance on intrinsic robustness of the hardware may not be sufficient. In this work, we explore the design of robust DNNs through the amalgamation of adversarial training and intrinsic robustness of NVM crossbar-based analog hardware. First, we study the noise stability of such networks on unperturbed inputs and observe that internal activations of adversarially trained networks have lower S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26679;&#20855;&#26377;&#26174;&#33879;&#25361;&#25112;&#24615;&#30340;&#21322;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#21363;&#20351;&#22823;&#22810;&#25968;&#24037;&#20154;&#30340;&#34892;&#20026;&#26159;&#23545;&#25239;&#30340;&#65292;&#24182;&#19988;&#20854;&#20313;&#30340;&#20154;&#20250;&#20687;Massart&#22122;&#22768;&#19968;&#26679;&#24037;&#20316;&#65292;&#20294;&#20247;&#21253;PAC&#23398;&#20064;&#38408;&#20540;&#20989;&#25968;&#30340;&#20551;&#35774;&#31867;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#26631;&#27880;&#25104;&#26412;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#26597;&#35810;&#22823;&#22823;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2106.07080</link><description>&lt;p&gt;
&#20247;&#21253;PAC&#23398;&#20064;&#20013;&#30340;&#21322;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-verified PAC Learning from the Crowd. (arXiv:2106.07080v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26679;&#20855;&#26377;&#26174;&#33879;&#25361;&#25112;&#24615;&#30340;&#21322;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#21363;&#20351;&#22823;&#22810;&#25968;&#24037;&#20154;&#30340;&#34892;&#20026;&#26159;&#23545;&#25239;&#30340;&#65292;&#24182;&#19988;&#20854;&#20313;&#30340;&#20154;&#20250;&#20687;Massart&#22122;&#22768;&#19968;&#26679;&#24037;&#20316;&#65292;&#20294;&#20247;&#21253;PAC&#23398;&#20064;&#38408;&#20540;&#20989;&#25968;&#30340;&#20551;&#35774;&#31867;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#26631;&#27880;&#25104;&#26412;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#26597;&#35810;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38408;&#20540;&#20989;&#25968;&#30340;&#20247;&#21253;PAC&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#19968;&#33324;&#20551;&#35774;&#19979;&#65292;&#21482;&#26377;&#22312;&#26377;&#26174;&#33879;&#27604;&#20363;&#30340;&#24037;&#20154;&#26159;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#65292;&#25165;&#33021;&#30830;&#31435;&#20855;&#26377;&#26597;&#35810;&#25928;&#29575;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#8212;&#8212;&#22823;&#22810;&#25968;&#20154;&#21487;&#33021;&#20250;&#34892;&#20107;&#23545;&#25239;&#65292;&#24182;&#19988;&#21097;&#19979;&#30340;&#20154;&#20250;&#20687;Massart&#22122;&#22768;&#19968;&#26679;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26356;&#26222;&#36941;&#30340;&#25171;&#30772;&#23436;&#32654;&#24615;&#20551;&#35774;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Charikar&#31561;&#20154;&#25552;&#20986;&#30340;&#21322;&#39564;&#35777;&#27169;&#22411;&#19979;&#65292;&#21482;&#35201;&#26377;(&#26377;&#38480;&#30340;)&#35775;&#38382;&#21463;&#20449;&#20219;&#30340;oracle&#23601;&#33021;&#22815;&#23545;&#22522;&#26412;&#30340;&#20551;&#35774;&#31867;&#36827;&#34892;PAC&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#21487;&#31649;&#29702;&#30340;&#26631;&#31614;&#26597;&#35810;&#37327;&#19979;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#26356;&#23481;&#26131;&#33719;&#24471;&#30340;&#27604;&#36739;&#26597;&#35810;&#26469;&#26497;&#22823;&#22320;&#20943;&#23569;&#26631;&#35760;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;PAC&#20445;&#35777;&#19982;&#21322;&#39564;&#35777;&#25110;&#21015;&#34920;&#21487;&#35299;&#20915;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#21457;&#23637;&#19981;&#21516;&#65292;&#36825;&#20123;&#21457;&#23637;&#20851;&#38190;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of crowdsourced PAC learning of threshold functions. This is a challenging problem and only recently have query-efficient algorithms been established under the assumption that a noticeable fraction of the workers are perfect. In this work, we investigate a more challenging case where the majority may behave adversarially and the rest behave as the Massart noise - a significant generalization of the perfectness assumption. We show that under the {semi-verified model} of Charikar et al. (2017), where we have (limited) access to a trusted oracle who always returns correct annotations, it is possible to PAC learn the underlying hypothesis class with a manageable amount of label queries. Moreover, we show that the labeling cost can be drastically mitigated via the more easily obtained comparison queries. Orthogonal to recent developments in semi-verified or list-decodable learning that crucially rely on data distributional assumptions, our PAC guarantee holds by explori
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#30001;&#20845;&#20010;&#21307;&#30103;&#20013;&#24515;&#25552;&#20379;&#30340;&#21253;&#21547;&#36229;&#36807;300&#21517;&#24739;&#32773;&#25968;&#25454;&#30340;&#22810;&#20013;&#24515;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20687;&#32032;&#32423;&#20998;&#21106;&#21644;&#35814;&#32454;&#30340;&#24687;&#32905;&#26631;&#27880;&#65292;&#21487;&#29992;&#20110;&#20005;&#26684;&#27979;&#35797;&#33258;&#21160;&#21270;&#30340;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.04463</link><description>&lt;p&gt;
&#19968;&#20221;&#29992;&#20110;&#24191;&#27867;&#35780;&#20272;&#30340;&#22810;&#20013;&#24515;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A multi-centre polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#30001;&#20845;&#20010;&#21307;&#30103;&#20013;&#24515;&#25552;&#20379;&#30340;&#21253;&#21547;&#36229;&#36807;300&#21517;&#24739;&#32773;&#25968;&#25454;&#30340;&#22810;&#20013;&#24515;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20687;&#32032;&#32423;&#20998;&#21106;&#21644;&#35814;&#32454;&#30340;&#24687;&#32905;&#26631;&#27880;&#65292;&#21487;&#29992;&#20110;&#20005;&#26684;&#27979;&#35797;&#33258;&#21160;&#21270;&#30340;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#32928;&#24687;&#32905;&#26159;&#32467;&#32928;&#38236;&#26816;&#26597;&#35782;&#21035;&#30340;&#24050;&#30693;&#30284;&#30151;&#21069;&#20307;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#24687;&#32905;&#26159;&#33391;&#24615;&#30340;&#65292;&#20294;&#24687;&#32905;&#30340;&#25968;&#37327;&#12289;&#22823;&#23567;&#21644;&#34920;&#38754;&#32467;&#26500;&#19982;&#32467;&#32928;&#30284;&#30340;&#39118;&#38505;&#30456;&#20851;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#33258;&#21160;&#21270;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#23578;&#26410;&#22312;&#22823;&#22411;&#22810;&#20013;&#24515;&#19987;&#38376;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#32570;&#20047;&#32508;&#21512;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20154;&#32676;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20845;&#20010;&#29420;&#29305;&#30340;&#20013;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;300&#22810;&#21517;&#24739;&#32773;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#21333;&#24103;&#21644;&#24207;&#21015;&#25968;&#25454;&#65292;&#20855;&#26377;3762&#20010;&#27880;&#37322;&#30340;&#24687;&#32905;&#26631;&#31614;&#65292;&#31934;&#30830;&#21246;&#30011;&#20102;&#20845;&#20301;&#36164;&#28145;&#32963;&#32928;&#30149;&#23398;&#23478;&#39564;&#35777;&#30340;&#24687;&#32905;&#36793;&#30028;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#30001;&#35745;&#31639;&#31185;&#23398;&#23478;&#22242;&#38431;&#31574;&#21010;&#30340;&#26368;&#20840;&#38754;&#30340;&#26816;&#27979;&#21644;&#20687;&#32032;&#32423;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;PolypGen&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyps in the colon are widely known cancer precursors identified by colonoscopy. Whilst most polyps are benign, the polyp's number, size and surface structure are linked to the risk of colon cancer. Several methods have been developed to automate polyp detection and segmentation. However, the main issue is that they are not tested rigorously on a large multicentre purpose-built dataset, one reason being the lack of a comprehensive public dataset. As a result, the developed methods may not generalise to different population datasets. To this extent, we have curated a dataset from six unique centres incorporating more than 300 patients. The dataset includes both single frame and sequence data with 3762 annotated polyp labels with precise delineation of polyp boundaries verified by six senior gastroenterologists. To our knowledge, this is the most comprehensive detection and pixel-level segmentation dataset (referred to as \textit{PolypGen}) curated by a team of computational scientists 
&lt;/p&gt;</description></item><item><title>Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2106.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26799;&#24230;&#31283;&#23450;&#30340;&#31070;&#32463;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04088
&lt;/p&gt;
&lt;p&gt;
Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Householder&#26435;&#37325;&#21644;&#32477;&#23545;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Householder-absolute&#31070;&#32463;&#23618;&#25110;&#31616;&#31216;Han&#23618;&#12290;&#19982;&#20855;&#26377;$d$&#20010;&#31070;&#32463;&#20803;&#21644;$d$&#20010;&#36755;&#20986;&#30340;&#20840;&#36830;&#25509;&#23618;&#30456;&#27604;&#65292;Han&#23618;&#23558;&#21442;&#25968;&#25968;&#37327;&#21644;&#30456;&#24212;&#30340;&#22797;&#26434;&#24230;&#20174;$O&#65288;d ^ 2&#65289;$&#38477;&#20302;&#21040;$O&#65288;d&#65289;$&#12290;Han&#23618;&#32467;&#26500;&#20445;&#35777;&#20102;&#20004;&#20010;&#29702;&#24819;&#23646;&#24615;&#65306;&#65288;1&#65289;&#26799;&#24230;&#31283;&#23450;&#24615;&#65288;&#19981;&#20250;&#20986;&#29616;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;1-Lipschitz&#36830;&#32493;&#24615;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#31574;&#30053;&#22320;&#20351;&#29992;Han&#23618;&#26367;&#25442;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#20174;&#32780;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;Han&#23618;&#32467;&#26500;&#22312;&#19968;&#20123;&#23567;&#22411;&#21270;&#30340;&#27169;&#22411;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#35752;&#35770;&#20854;&#24403;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
&lt;/p&gt;</description></item></channel></rss>